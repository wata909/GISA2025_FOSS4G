[
  {
    "year": 2022,
    "conference_title": "FOSS4G 2022 general tracks",
    "start_date": "2022-08-22",
    "end_date": "2022-08-28",
    "total_events": 328,
    "total_presentations": 328,
    "academic_presentations": 0,
    "non_academic_presentations": 328,
    "types": {
      "General session talk": 214,
      "Lightning talk": 100,
      "Keynote": 5,
      "Side event": 4,
      "Full session": 3,
      "Full day": 2
    },
    "tracks": {
      "Use cases & applications": 138,
      "State of software": 117,
      "Open Data": 23,
      "Transition to FOSS4G": 14,
      "A European approach to geospatial open source": 12,
      "Education": 9,
      "AI4EO Challenges & Opportunities": 8,
      "Unknown": 7
    },
    "presentations": [
      {
        "title": "Mapathon",
        "type": "Side event",
        "track": "Unknown",
        "abstract": "A Mapathon is a coordinated mapping event, specially connected to the OpenStreetMap projecy. It is often held inside (armchair mapping) but can also be an outside or combined activity. People will join all together in a place, or online, where they can improve OpenStreetMap data, usually working together on a emergency project provided by HOT Tasking manager                                                                                       \nA Mapathon is a great opportunity for volunteers to digitally connect and create urgently needed data for communities around the world so that local and international decision-makers can use these maps and data to better respond to crises affecting these areas. Up-to-date maps are important for the success of many humanitarian initiatives around the world in responding to disasters as soon as possible, it help to save lifes.\nThis Mapathon will be held in Firenze the 23 August 2022 and it is co-organized by HOT, OSGeo, OSM Foundation, Youth Mappers.",
        "description": ""
      },
      {
        "title": "HOTOSM Mapathon - Humanitarian Mapping with OpenStreetmap",
        "type": "Side event",
        "track": "Open Data",
        "abstract": "Join the Humanitarian OpenStreetMap team staff and volunteers to support humanitarian mapping activities around the world. The session will be an organized mapping activity, focusing on an active project which requires mapping support from the worldwide OpenStreetMap community. The mapathon is open to all, including people who don’t have previous OpenStreetMap experience. It is suggested that you join with a laptop, however it is also possible to contribute with other mobile tools.",
        "description": ""
      },
      {
        "title": "Youth Mappers documentary's premiere",
        "type": "Side event",
        "track": "Open Data",
        "abstract": "[YouthMappers](https://www.youthmappers.org/) is a global movement of university students addressing development challenges and community resilience using public geospatial tools and data. Meet the student leaders effecting change in their communities across the globe and follow the work of a student-led project in Sierra Leone supporting rural electrification for expanded electricity access. Hear the voices of [YouthMappers](https://www.youthmappers.org/) members who have not only built their mapping skills through [YouthMappers](https://www.youthmappers.org/) but have grown as global citizens. This short documentary tells their stories and will be screened for the first time in Florence on **<a target=\"_blank\" href=\"https://umap.openstreetmap.fr/it/map/foss4g-2022-sites_776000#18/43.80059/11.24543\" data-iframe=\"true\">22 August at 3:30pm in Auditorium A of the University complex in Viale Morgagni.</a>**\n\n**Outline programme**\n\n* 3:30 pm *setup / preparations to begin*\n* 3:45 pm *About YouthMappers* (Solis, Zeballos, Stokes)\n* 4:00 pm *Film Showing*\n* 4:30 pm *Panelists of YouthMappers*\n* 5:00 pm *Launch of the Book on SDGs*\n* 5:15 pm *adjourn*\n* 5:30 pm *all take down is finished / group is departed*",
        "description": ""
      },
      {
        "title": "Opening Session",
        "type": "Full session",
        "track": "Unknown",
        "abstract": "Opening session with institutional greetings.",
        "description": ""
      },
      {
        "title": "YouthMappers: a global youth movement on open mapping for humanitarian and development action",
        "type": "Keynote",
        "track": "Education",
        "abstract": "Increasingly ubiquitous open spatial technologies offer the opportunity for new actors to participate in creating knowledge about the places where they live and work, and where they navigate the shocks and stresses of an uncertain world. This presentation offers insights about university student engagement in open mapping through the experiences of YouthMappers around the world. This inclusive international network of youth-led, faculty-mentored, and technology-enabled chapters on more than 300 campuses in 65+ countries work together to organize, collaborate, and implement mapping action that respond to needs around the globe. Specific examples will illuminate how students are creating and using spatial information that is collected using open software tools and made publicly available through open platforms, and in turn inform us about both the power and the limits of open spatial technologies in the hands of students working to build a more resilient, equitable and sustainable future.",
        "description": ""
      },
      {
        "title": "Map Kibera Mapping Counties on OpenStreetMap",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "The talk will focus on how Map Kibera has empowered different Counties in Kenya to Map their projects using Open Street Map, Kobo collect and ODK. During the exercise Map Kibera collaborated with World Bank to produce large maps that are used during the participatory budgeting meetings for members of the community to decide the projects they want implemented. Map Kibera's approach is to empower the youth and some selected County officials with new mapping skills and methodologies. Members of the community are then able to make informed decisions by seeing what they already have and what might be missing. Map Kibera also helped the Four selected counties to develop a website that shows the status and the budget allocation for each and every project mapped. This helps promote transparency and accountability within the counties. Before the mapping happened, people would be asked to propose what project they wanted but it became hard without knowing what was already in existence. Now they can tell, We have a hospital we want a school for example.",
        "description": ""
      },
      {
        "title": "Crowdsourcing the Future of Government Data: OpenStreetMap in the Public Domain",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Is your government utilizing OpenStreetMap data in their workflows? How can crowdsourcing be leveraged to improve the completeness, freshness, and breadth of government geospatial data? As the world’s largest crowdsourced geospatial database, OpenStreetMap is poised to serve this need.  Two years ago, OpenStreetMap US formed a Government Working Group to seek out mutually beneficial relationships between the public and open data communities. As part of this effort, OSM community members and representatives from federal agencies have been investigating solutions for feature collection. This collaboration has led to the development of Public Domain Map, which connects OpenStreetMap and government datasets. Through the Public Domain Map workflow, OpenStreetMap and government open geospatial data becomes more complete, current and readily usable by government agencies and the millions of users relying on both datasets. In this session, I will share the journey of Public Domain Map, how the project is bringing together US federal agencies and open source contributors to meet this goal, and how you can be part of it.",
        "description": ""
      },
      {
        "title": "Data Management in Earth Observation - The Potential of Open Digital Twins of the Earth",
        "type": "Keynote",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Several ambitious initiatives, such as Destination Earth of the European Union,  are harnessing large amounts of Earth Observation and other data to develop so-called digital twins of the Earth. The combination of large and diverse data assets, cloud- & HPC-computing as well as sophisticates models and AI algorithms now permit to generate predictive EO scenarios. One key aspect of Digital Twins is the possibility to employ openness as one of the key principles for all its functions. Data policy, data access, software development, processing and information management are important considerations for engaging and integrating users of all kinds. Due to their high potential for science and society the European Space Agency, with its unique geospatial data holdings, is fully engaged in the development of Digital Twins of the Earth.",
        "description": ""
      },
      {
        "title": "\"Earth in Colour\" with EarthDaily Analytics",
        "type": "General session talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "EarthDaily Analytics is building a powerful new constellation that will collect scientific-grade, 5 meter resolution imagery of the planet in a unique combination of 22 spectral bands using 3 different camera types, covering a broad spectral range from visible to thermal wavelengths.  The mission will be launched in 2023 and will allow us to see the Earth’s global land mass each day in a wholly new way with more spectral bands, higher revisit, and at a higher resolution than ever before.  It will allow us to monitor, detect changes, alert, and predict what is happening anywhere on the planet to help with some of world’s most pressing challenges in agriculture, Environmental, Social and Governance (ESG), and disaster prevention and recovery. \n\nThis mission has been made possible by a near-perfect convergence of three major technology breakthroughs in the last 10 years: 1) lower cost satellite launch and manufacturing, 2) advancements in computer vision and machine learning to support automation of petabyte scale processing, and 3) cloud compute power and storage necessary to drive the processing and calibration of trillions of pixels each day. Together these three emerging technologies are key to driving next generation geospatial insights, but to bring them together requires a software solution capable of handing the complexity of raw satellite with automation driven by machine learning, and cloud-based Big Geo Data pipelines for cost-effective scale and latency.  \n\nAt EarthDaily Analytics, our software solution has been made possible by leveraging many open source software packages to form the backbone for our satellite processing, calibration and quality services called the EarthPipeline.  Together with open source packages and custom machine learning and computer vision approaches, we are working on delivering true scientific satellite image products that can be applied directly to algorithms without the need for very costly (and dreaded) end user data normalization and correction procedures.  This talk will focus on how EarthDaily Analytics uses open source packages and machine learning to create normalized scientific quality data, and will also provide some example applications of how the data can be used.",
        "description": ""
      },
      {
        "title": "HIECTOR: Hierarchical object detector for cost-efficient detection at scale",
        "type": "General session talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Object detection, classification and semantic segmentation are ubiquitous and fundamental tasks in extracting, interpreting and understanding the information acquired by satellite imagery. Applications for locating and classifying man-made objects, such as buildings, roads, aeroplanes, and cars typically require Very High Resolution (VHR) imagery, with spatial resolution ranging approximately from 0.3 m to 5 m. However, such VHR imagery is generally proprietary and commercially available at a high cost. This prevents its uptake from the wider community, in particular when analysis at large scale is desired. HIECTOR (HIErarchical deteCTOR) tackles the problem of efficiently scaling object detection in satellite imagery to large areas by leveraging the sparsity of such objects over the considered area-of-interest (AOI). This talk presents a hierarchical method for detection of man-made objects, using multiple satellite image sources with different Ground Sample Distance (GSD). The detection is carried out in a hierarchical fashion, starting at the lowest resolution and proceeding to the highest. Detections at each stage of the pyramid are used to request imagery and apply the detection at the next higher resolution, therefore reducing the amount of data required and processed. We evaluate HIECTOR for the task of building detection for a middle-eastern country, estimating oriented bounding boxes around each object of interest.\n\nFor the detection of buildings, HIECTOR is demonstrated using the following data sources: Sentinel-2 imagery with 10 m GSD, Airbus SPOT imagery pan-sharpened to 1.5 m pixel size and Airbus Pleiades imagery pan-sharpened to 0.5 m pixel size. Sentinel-2 imagery is openly available, making their use very cost efficient. The Single-Stage Rotation-Decoupled Detector (SSRDD) algorithm is used. Given that single buildings are not discernible at 10 m GSD, a bounding box does not describe a single building but rather a cluster of buildings. The estimated bounding boxes at 10 m are joined and the resulting polygon area is used to further request SPOT imagery at the pan-sharpened pixels size of 1.5 m. In the case of SPOT imagery, given the higher spatial resolution, one bounding box is estimated for each building. As a final step, predictions are improved in areas with low confidence by requesting Airbus Pleiades imagery at the pan-sharpened 0.5 m pixel size. Ablation studies show that HIECTOR achieves a mean Average Precision (mAP) score of 0.383 and 20-fold reduction in costs compared to using only VHR at the highest resolution, which achieves a mAP of 0.452.\n\nCode will be released under MIT license. We will also release the trained models on Sentinel, SPOT and Pleiades imagery. In addition, manually labelled building footprints over Dakar will be open-sourced to allow users evaluate the generalisation of the models over different geographical areas. The Sentinel Hub service is used by HIECTOR to request the commercial imagery sources on the specified polygons determined at each level of the pyramid, allowing to request, access and process specific sub-parts of the AOI.",
        "description": ""
      },
      {
        "title": "No-code geoAI",
        "type": "General session talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Artificial Intelligence (AI) has made an impact in almost every field and has become an incredibly powerful technology. However, while some players in big tech and academia have access to a highly skilful workforce that can create sophisticated AI solutions, many companies, governments, public organisations, and other societal stakeholders are lacking sufficient AI expertise and know-how. \n\nMany AI and Machine Learning (ML) frameworks aim to simplify and democratise AI development, even if typically focusing on those with software engineering skills. Some of these solutions, those that provide no-code tools, get the closest to the ideal of enabling \"any person without prior training”. Collectively, these could represent a major breakthrough, as it has been proven time and time again that many businesses and organisations still struggle to implement AI to its full potential and scale.\n\nVisual, often drag-and-drop, no-code AI tools can make AI less intimidating and more comprehensible to non-technical profiles and those who lack the time and resources to build such systems from the ground up. No-code AI frameworks are expected to require minimum technical knowledge to develop practical AI solutions at scale. This is an emerging field, like was previously the case with no-code web development, starting with Dreamweaver and MS Frontpage, the first WYSIWYG (what you see is what you get) solutions, both launched in 1997. \n\nThe European Commission is supporting the establishment of an AI-on-demand platform that will provide easy and simple access to AI tools that are made in Europe and are ‘trustworthy’. The platform will gather all the AI resources (algorithms and tools), and make them available to the potential users, businesses, and public administration, with the necessary services to facilitate their integration.\n\nWithin the context of the EU’s commitment to trustworthy AI, we are exploring the landscape of AI solutions for non-experts, including no-code, low-code, AutoML and similar approaches, and evaluating them in an experimental setting via prototyping. Our findings are expected to inform the development of relevant European digital initiatives. For instance, we will consider how these emerging AI solutions for non-experts could be integrated and used in the context of digital infrastructures such as EuroGEOSS or the European Data Spaces. \n\nFurther democratization of AI will happen when domain experts without prior AI expertise are enabled to tap into high-quality data to solve complex problems on their own, with technical details being abstracted away, such as algorithm selection, model training, software frameworks, hardware dependencies, and platform aspects. We expect that open-source AI technologies for non-experts represent a step towards such a future.\n\nDuring the conference, we will present the initial results of our geoAI activity, including a high-level architecture and a stack expected to be composed of various open-source software tools and open standards for digital infrastructures, data engineering and AI development. Our intention is to gather feedback from the audience and establish possible future collaborations in this space.",
        "description": ""
      },
      {
        "title": "Cloud Optimized Point Cloud: Compressed, Geospatial, Lossless and Compatible Data Organization for Analysis Ready Point Cloud Data",
        "type": "General session talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Point cloud data are an important component of geospatial data workflows, but software and formats to manage it often have compromises that work against efficient storage and processing of data. While commonly seen characterizing topographic information in LiDAR applications, point cloud data are an important driver of change detection applications in SAR workflows and provide important raw data to bring the physical world to the augmented one through handset capture on devices like the iPhone 12+. COPC.io is an open specification by Hobu, Inc. for organizing point cloud data in LAZ that allows it to be streamable over HTTP, selectable for resolution or spatial window, and adaptable to existing point cloud workflows in a backward compatible way. We will discuss the design choices and evolution of COPC, demonstrate its use in PDAL and QGIS scenarios, and show how COPC can be used in the cloud for management of massive point cloud collections.",
        "description": ""
      },
      {
        "title": "Open Source Point Cloud Semantic Segmentation Using AI/ML",
        "type": "General session talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Assigning semantic labels to points within a point cloud aids in both visual interpretation of the data and as a preprocessing step to other forms of analysis like building footprint extraction, hydrological modeling, and biomass estimation. Our talk will focus primarily on earth observation data and airborne lidar data sources in particular, where labels are commonly aligned with those classes specified in the ASPRS LAS specification (e.g., ground, vegetation, and building), but we are also beginning to explore the extension of these same methods to data generated by commodity, consumer-grade devices like iPhones. For many years, hand-tuned models have been developed for this segmentation task, building on reasonable assumptions about the data. For example, ground points should include those lowest elevation returns within a local window or building segments should typically be planar. Within the past decade, we have seen a surge in AI/ML powered models that are able in many cases of outperforming the prior methods, being able to learn novel features and adapt to the intrinsic variability of data. We will provide an overview of the open source ecosystem powering this trend, from benchmark datasets like US3D and DALES to machine learning frameworks (i.e., PyTorch and Tensorflow) and key libraries such as PDAL, Open3D, and PyG.",
        "description": ""
      },
      {
        "title": "KartAi – An open living lab for Ai in Norway",
        "type": "General session talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "High resolution aerial photos combined with accurate map data represents a perfect data set for training artificial intelligence models. The ‘KartAi’ project is an innovation project in public sector aimed at developing Ai-methods that detects buildings not in the cadastre or the building map dataset. Thereafter involving the property owner/citizen in a digital dialog and validate or crowdsource more detailed data. The foundation for this is high quality datasets for training and validating the different Ai-models. High resolution aerial photos are collected in large parts of Norway on a regular basis – often yearly – in a collaboration between federal and municipal. Thereby there exists a vast amount of extremely detailed image data combined with building map data and cadastre data. However, training the Ai-models have uncovered that minor errors and ‘skewed’ photos and/or vector data affects the results of the segmentation of roof tops/buildings. Therefore the KartAi projects has made fine tuned and accurate training data sets in several geographical areas optimized for training on detecting and segmenting buildings. \nIn several large scale experiments, a multitude of existing models, newer models and own models have been training and validated. Additionally we have included LIDAR-height data to enhance the precision of segmenting between the likes of roofs and terraces. Training the models on the existing data yields good results. However, when finetuning with the high accurate data – the models show impressing results. \nSpatial Ai projects like KartAi are at the mercy of volumes of good training data. Our experience show that even more accurate data sets improve the models even further. Therefore, the project has made efforts that have resulted in the release of the training data sets publicly – as well as all of the results data for the different models and approaches that have been developed. This is an effort into developing a more open living lab for Spatial Ai in Norway. Our hope is that sharing the knowledge and data created can ensure that other Ai-models have easier access to high resolution and high accuracy data – to train models in the open living lab – and apply the models internationally where data is scarcer.",
        "description": ""
      },
      {
        "title": "State of GeoNetwork",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The GeoNetwork-opensource project is a catalog application facilitating the discovery of resources within any local, regional, national or global \"Spatial Data Infrastructure\" (SDI). GeoNetwork is an established technology - recognized as an OSGeo Project and a member of the foss4g community for over a decade. \n\nThe GeoNetwork team would love to share what we have been up to in 2022! \n\nThe GeoNetwork team is excited to talk about the different projects that have contributed with the new features added to the software during the last twelve months. Our rich ecosystem of schema plugins continues to improve; with national teams pouring fixes, improvements and new features into the core application.\n\nWe will also talk a bit about the health and happiness of the GeoNetwork opensource team. Progress of our main branches (3.12.x and 4.0.x), and release schedule.\n\nAttend this presentation for the latest from the GeoNetwork community and this vibrant technology platform.",
        "description": ""
      },
      {
        "title": "State of GeoNode",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "GeoNode is a Web Spatial Content Management System based entirely on Open Source tools whose purpose is to promote the sharing of data and their management in a simple environment where even non-expert users of GIS technologies can view, edit, manage, and share spatial data, maps, prints and documents attached.\n\nThis presentation provides a summary of new features added to GeoNode in the year  up to the latest releases of GeoNode together with a glimpse of what we have planned for next year and beyond, straight from the core developers.\n\nThe purpose of this presentation is to introduce the attendees to those which are the GeoNode current capabilities and to some practical use cases of particular interest in order to also highlight the possibility of customization and integration. Finally,  we will provide a summary of new features added to GeoNode in the last  release up to the latest releases of GeoNode together with a glimpse of what we have planned for next year and beyond, straight from the core developers.",
        "description": ""
      },
      {
        "title": "State of deegree: What's new in 2022",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "This presentation provides a summary of new features added to deegree in the latest releases of deegree webservices and deegree OGC API together with a glimpse of what we have planned for next year and beyond.\nInitiated in 2002 the OSGeo project deegree has developed over the last 20 years to an important building block for Spatial Data Infrastructures (SDI). As the implementation of the INSPIRE directive is fully underway it requires stable and mature software solutions based on OGC standards such as GML, WFS and WMS. One of the goals of the deegree project is to provide implementation of those standards.\nIn this talk we will focus on the recent improvements available in deegree webservices and our updated roadmap for full Java 11 support comming with version 3.5. We will show how the support for OGC API - Features Core, part 1 and 2 has been implemented and can be used with existing configurations.  \n\nFinally we will show the directions for the project and what future developments are currently planned.",
        "description": ""
      },
      {
        "title": "OpenLayers Feature Frenzy",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "A decade ago, OpenLayers was the number one choice for a web mapping library. With the rewrite in 2012 and the rising competition of Leaflet and Mapbox GL JS, there was a phase when the project lost popularity. Today, it has found its niche as a full-featured, flexible, and high-performance geospatial JavaScript library that users can count on for the long haul, especially when their mapping needs get more complex.\n\nThis talk will provide you with a tour of the latest features in the library, including daring live demonstrations. We will present our recent and ongoing work on adding new features and making the library more fun to work with.\n\nWhether you're a developer or decision maker, come to this talk to learn about the current status of OpenLayers.  We’ll provide you with a glimpse into the future of the library and leave you motivated to get mapping with OpenLayers.",
        "description": ""
      },
      {
        "title": "MapLibre GL 2.x - JavaScript maps with React, Vue.js, Svelte or Angular",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "What's new in MapLibre version 2? We'll explore all the latest features of the library, including its new 3D capabilities. We'll also show you get started with MapLibre in whichever frameworks you choose to work in. This talk will be useful for beginners through to experienced web mappers interested in MapLibre.\nAlong with the 3D capabilities, there are new styling functions and also the move from JavaScript to Typescript to discuss. We'll show you some snippets of these new features and demonstrate the power of MapLibre.\nFor those getting started with web mapping or learning new frameworks, we will show you how to use MapLibre in your own application. We will introduce practical code examples in React, Vue.js, Angular or Svelte, creating a simple map component.\nBy the end of this talk, you should have all you need to get started building apps with MapLibre and the main JavaScript frameworks.",
        "description": ""
      },
      {
        "title": "MapStore, a year in review",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "**MapStore** is an open source product developed for creating, saving and sharing in a simple and intuitive way maps, dashboards, charts and geostories directly online in your browser. MapStore is cross-browser and mobile ready, it allows users to: \n\n- *Search* and load geospatial content served using widely used protocols (WMS, WFS, WMTS, TMS, CSW) and formats (GML, Shapefile, GeoJSON, KML/KMZ etc..)\n- *Manage maps* (create, modify, share, delete, search), charts, dashboard and stories directly online\n- *Manage users*, groups and their permissions over the various resources MapStore can manage\n- *Edit data online* via WFS-T with advanced filtering capabilities\n- *Deeply customize the look&feel* to follow strict corporate guidelines\n- *Manage different application contexts* through an advanced wizard to have customized WebGIS MapStore viewers for different use cases (custom plugins set, map and theme)\n\n\nYou can use **MapStore** as a product to deploy simple geoportals by using the standard functionalities it provides but you can also use MapStore as a framework to develop sophisticated WebGIS portals by reusing and extending its core building blocks.\n\n\n**MapStore** is built on top of React and Redux and its core does not explicitly depend on any mapping engine but it can support both OpenLayers, Leaflet and Cesium; additional mapping engines could be also supported (MapBox GL is in the working) to avoid any tight dependency on a single engine.\n\nThe presentation will give the audience an extensive overview of the MapStore  functionalities for the creation of mapping portals, covering both previous work as well work for the future releases.  Eventually, a range of MapStore case studies will be presented to demonstrate what our clients (like City of Genova, City of Florence, Halliburton, Austrocontrol and more) and partners are achieving with it.",
        "description": ""
      },
      {
        "title": "State of GeoWebCache",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "GeoWebCache is a popular open source tile cache server written in Java. GeoWebCache that can be used stand alone, working off a remote WMS or local tile layers, such as MBTiles. However, it's also integrated inside GeoServer, allowing simple and quick configuration, as well as transparent caching of WMS requests that happen to match a cached tile.  This presentation will provide information on the latest development for the project, including:\n\n* Performance and scalability improvements\n* OGC TileMatrixSet built-in definitions\n* Storage of tiles in more blob stores (Swift)\n* MBTiles support\n* Integration of tile caching in OGC API - Tiles (with GeoServer integration)\n* Serving vector tiles and integration with the Mapbox ecosystem (e.g., style editing with Maputnik)\n* Continued codebase QA efforts (code clean up, dependency upgrades and the like)\n\nAttend this talk for a cheerful update on what is happening with this project, whether you are an expert user, a developer, or simply curious what it can do for you.",
        "description": ""
      },
      {
        "title": "GeoNetwork and a11y: Introducing accessibility in OSGeo applications",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "When websites and web tools are properly designed and coded, people with disabilities can use them. However, currently many sites and tools are developed with accessibility barriers that make them difficult or impossible for some people to use [W3C - Introduction to Web Accessibility].\n\nThe European accessibility requirements present a new and welcome challenge for OSGeo applications. Accessibility (a11y) goes far beyond ease-of-use, with strict guidance on making web applications easily accessible to screen readers and assistive technologies.\n\nThis talk provides an overview of implementing the accessibility guidelines WCAG 2.1, WAI-ARIA, and EN 301 549 (Harmonised European Standard). GeoNetwork is used as a case study here, with “hands-on” illustrations of successful guideline implementations and their technical and organisational challenges.\n\nMaking OSGeo Software accessible is a rewarding task that requires broad community engagement and support. Attend this talk to learn more about meeting the a11y requirements and the impact they will have on your organisation and your software solution.",
        "description": ""
      },
      {
        "title": "GeoHealthCheck - QoS Monitor for Geospatial Web Services",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Keeping (OGC) Geospatial Web Services up-and-running is best accommodated by continuous monitoring: not only downtime needs to be guarded, \nbut also whether the services are functioning correctly and do not suffer from performance and/or other Quality of Service (QoS) issues.\nGeoHealthCheck (GHC) is an Open Source Python application for monitoring uptime and availability of OGC Web Services.\nIn this talk we will explain GHC basics, how it works, how you can use and even extend GHC (plugins).\n\nThere is an abundance of standard (HTTP) monitoring tools that may guard for general status and uptime of web services. \nBut OGC web services often have their own error, \"Exception\", reporting not caught by generic HTTP uptime\ncheckers. For example, an OGC Web Mapping Service (WMS) may provide an Exception as a valid XML response or\nin a error message written \"in-image\", or an error may render a blank image. \nA generic uptime checker may assume the service is functioning as from those requests and an HTTP status \"200\" is returned.\n\nOther OGC services may have specific QoS issues that are not directly obvious. A successful and valid \"OWS GetCapabilities\" response may not \nguarantee that individual services are functioning correctly. For example an OGC Web Feature Service (WFS) based on a dynamic database may \nreturn zero Features on a GetFeature response caused by issues in an underlying database. Even standard HTTP checkers supporting \"keywords\" \nmay not detect all failure cases in OGC web services. Many OGC services will have multiple \"layers\" or feature types, how to check them all?\n\nWhat is needed is a form of semantic checking and reporting specific to OGC services!\n\nGeoHealthCheck (GHC) is an Open Source (MIT) web-based framework through which OGC-based web services can be monitored. GHC is written in \nPython (with Flask) under the umbrella of the GeoPython GitHub Organization. It is currently an OSGeo Community Project.\n\nGHC consists of two parts: (1) a web-UI app (using Flask) through which OGC service endpoint\nURLs and their checks can be managed, plus for visualising monitoring-results and (2) a monitoring engine that executes scheduled \n\"health-checks\" on the OGC service endpoints. Both parts share a common database (via SQLAlchemy, usually SQLite or PostgreSQL). \nThe database also stores all historic results, allowing for various forms of reporting.\n\nGHC is extensible: at this moment of writing a plugin-system is developed for \"Probes\" in order to support an expanding number of \ncases for OGC specific requests and -checks. Work is in progress to provide a GHC API for various integrations.\n\nLinks:\n- Website: http://geohealthcheck.org\n- Sources: https://github.com/geopython/GeoHealthCheck\n- Demo: http://geohealthcheck.osgeo.org",
        "description": ""
      },
      {
        "title": "OpenLog - Open Source drillhole data visualization in QGIS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Mining industry professionals are in a constant need of simple and efficient software solutions for drillhole visualization, management, and edition. Although several Open Source solutions are offered to partially fulfill the need, none has propagated into common professional use.\nIn partnership with a team of mining industry leaders including Orano, Evolution Mining, Sandfire Resources, Kenex, the University of Western Australia, NordGold, GoldSpot and the CEA, Oslandia has formed a consortium in 2021 to answer the demand.\nOslandia has since then been developing a high performance drillhole data visualization QGIS plugin that combines 3D, cross-section, map, and log views into a fully synchronous system. Moreover, users are able to connect OpenLog directly to their existing drillhole databases such as Acquire, Datashed or Geotic.\nThis talk will succinctly present the functionalities of OpenLog as well as its primary use cases, contribution to QGIS 3D data visualization technology, development roadmap, and future prospects.",
        "description": ""
      },
      {
        "title": "Streamlining QGIS workflows with PostgreSQL editable views and triggers",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "When using QGIS as a user interface for a PostgreSQL- & PostGIS-based registry database, user experience plays a high role. The data schemas of a registry database can hold a complex set of relations and database objects which can be hard to set up within QGIS. This was the case with a waste soil transportation registry that we developed for the City of Tampere, Finland. The main goal of the registry is to optimize soil transportation from construction sites by communicating and making it visible what soil categories are available and needed where and when. The registry enables significant savings in transportation costs as well as substantial reductions in climate emissions.\n\nWhen the relational data model gets complex, you can deploy different strategies for setting up the workflows within QGIS. One possible solution is to use editable views and triggers for enabling user-friendly workflows in QGIS. This was the case in our soil registry project. The data model, as it was, would have forced the user to create multiple new features to the database tables when he/she just wanted to add a single soil transfer from one construction site to another. This was seen as too tedious when repeated constantly. The solution was to make a database view that the user could edit in QGIS, in addition to deploying database triggers for creating the right database features, with the proper data into the correct database tables.\n\nThis presentation seeks to show how the strategy was deployed and what challenges we met during the development.",
        "description": ""
      },
      {
        "title": "Generation of 3D geometries with Artificial Intelligence for the prediction of volumetry of the Urban Code of the City of Buenos Aires",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "To make the Platform possible and meet the objectives set, it was necessary to work with different areas of the government involved in urban planning and systems, promoting the opening of data to generate a sustainable process over time for the automatic and dynamic system of interpretation of the Urban Code that governs the constructive and urban behavior of the city of Buenos Aires, which is the basis of this technological solution.\nThe platform is based on the “Digital Twin” concept, that is, a virtual and digital replica of a city's urban plan. The objective is to test any initiative on this virtual model before its real implementation, in order to reduce costs and risks.\nThe tool was developed entirely with open source resources and technologies so that other organizations can study, modify and improve its design through the availability of its source code.\nRegarding its architecture, the platform works from the information extracted from the datasets of the areas referred to the urban fabric and parcels. Then, from a set of processing algorithms that works with systematized rules generated from the text of the urban code regulations, this information is processed, allowing the generation of 3D graphics of each of the city parcels. The volumetry of each parcel allows to know the buildability, the maximum allowed height and the allowed construction alternatives for each parcel, which integrate the platform's viewer.",
        "description": ""
      },
      {
        "title": "State of GeoPandas and friends",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "GeoPandas is one of the core packages in the Python ecosystem to work with geospatial vector data. By combining the power of several open source geo tools (GEOS/Shapely, GDAL/fiona, PROJ/pyproj) and extending the pandas data analysis library to work with geographic objects, it is designed to make working with geospatial data in Python easier. GeoPandas enables you to easily do operations in Python that would otherwise require desktop applications like QGIS or a spatial database such as PostGIS.\n\nThis talk will give an overview of recent developments in the GeoPandas community, both in the project itself as in the broader ecosystem of packages on which GeoPandas depends or that extend GeoPandas. We will highlight some changes and new features in recent GeoPandas versions, such as the new interactive explore() visualisation method, improvements in joining based on proximity, better IO options for PostGIS and Apache Parquet and Feather files, and others. But some of the important improvements coming to GeoPandas are happening in other packages. The Shapely 2.0 release is nearing completion, and will provide fast vectorized versions of all its geospatial functionalities. This will help to substantially improve the performance of GeoPandas. In the area of reading and writing traditional GIS files using GDAL, the pyogrio package is being developed to provide a speed-up on that front. Another new project is dask-geopandas, which is merging the geospatial capabilities of GeoPandas with the scalability of Dask. This way, we can achieve parallel and distributed geospatial operations.",
        "description": ""
      },
      {
        "title": "State of MovingPandas: analyze all those tracks (not just GPS)",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "This talk presents the current state of [MovingPandas](http://movingpandas.org) and related movement data analysis tools. MovingPandas has been growing steadily since its first publication in 2018 (with more than 24 contributors to date). Building on GeoPandas and GeoViews, MovingPandas provides movement data analysis tools that support efficient exploratory data analysis through interactive (visual) analysis. Early functionality and demos were focused on dealing with GPS tracking data (including vehicle and animal tracks). This talk presents recent developments towards supporting other track data, including examples from sports tracking (movement in real space, extracted from video footage) and eye or mouse tracking (movement in virtual space). Among many other details, this includes support for local coordinate systems, integration of context beyond geographic base maps, as well as trajectory generalization, segmentation, and distance measures. Finally, we revisit the origins of MovingPandas: the QGIS plugin Trajectools; and review the steps necessary to bring MovingPandas' trajectory analysis tools to QGIS.",
        "description": ""
      },
      {
        "title": "An introduction to deck.gl for data visualization",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "deck.gl is one of the most advanced open-source libraries for data visualization. In this session we will discuss how its WebGL-powered engine can be used to perform visual exploratory data analysis of large datasets. This library is quickly becoming one of the most used in the FOSS4G world due to its open governance model and compatibility with other mapping libraries like MapLibre GL JS. \n\nWe will learn how a deck.gl visualization is structured and what the main concepts are: views, layers and accessors. We will discuss its reactive architecture and how it can be used to build simple scripting prototypes and complex applications with modern JavaScript frameworks like React, Angular or Vue.js. \n\nWe will present different examples ranging from simple layer visualizations to thematic and choropleth maps to advanced interactive 3D visualizations including animations.\n\nFinally we will focus on specific use cases for large data visualization, from datasets with hundreds of thousands of features with data formats like GeoJSON to datasets with billions of features using advanced tiling schemes.",
        "description": ""
      },
      {
        "title": "LERC, an innovative compression algorithm for raster data",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Although in use for about 10 years in various Esri products and services, the LERC (Limited Error Raster Compression) raster compression algorithm has only just recently made its way into the free and open-source GIS scene by its inclusion in GDAL (3.3).\n\nLERC can perform both lossless and lossy raster data compression. To achieve its impressive compression ratios and speed LERC employs two major basic tricks:\n- The raster is processed and compressed in small two-dimensional blocks, taking advantage of spatial autocorrelation (neighboring values usually being more alike than others).\n- The raster values are quantized (absolute values are replaced by differences between neighbors) and bitstuffed to minimize the number of bits required to store them, this is especially useful for high bit depth data.\n\nFor lossy compression LERC will follow a user-configurable maximum error threshold (the \"limited error\" in its name). Want to compress your DEM and allow up to one centimeter of error? No problemo!\n\nLERC is patented by Esri but thanks to the choice of the permissive Apache License it is freely usable by anyone.\n\nThe talk will try explain the algorithm on a basic level, understandable by non-experts, and show its performance with some examples.",
        "description": ""
      },
      {
        "title": "State of Oskari (for end-users)",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Oskari (www.oskari.org) is used around the world to provide map applications with integrations to spatial and statistical data and service APIs. Oskari can be utilized as a Web GIS with a regular browser or via embedded maps controllable with a simple API. The embedded maps are created in a easy-to-use WYSIWYG-tool enabling users to add a map component to their websites/services without any programming skills. The additional API can be used to integrate to existing APIs and services for richer functionality.\n\nThis presentation will cover the basics of Oskari and new features introduced during 2021-2022. The focus will be on functionalities for end-users and administrators, such as: new styling tools, map layer analytics and diagnostics tool, metadata supported automation to statistical data visualisation, enhanced support for theming and mobile use. There will be a separate presentation about technical developments in Oskari focusing on developer experience. You can try the features of vanilla Oskari in our demo environment (demo.oskari.org), it has the newest stable version without any customisation.",
        "description": ""
      },
      {
        "title": "State of Oskari (for developers)",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "Oskari is used world wide to provide web based map applications that are built on top of existing spatial data infrastructures. Oskari offers building blocks for creating and customizing your own geoportals and allows embedding maps to other sites that can be controlled with a simple API. In addition to showing data from spatial services, Oskari offers hooks for things like using your own search backend and fetching/presenting statistical data.\n\nThis presentation will go through the improvements to existing functionalities and new features introduced in Oskari during the last year. The focus will be on functionalities from developer perspective like:\n\n- Improvements for working with vector features\n- API-improvements for embedded maps.\n- Rewrite of service capabilities parsing and handling\n- Planned developments for better theming support and mobile-device friendliness for the geoportal\n\nYou can try some of the functionalities Oskari offers out-of-the-box on our sample application: https://demo.oskari.org.\n\nLink: https://oskari.org",
        "description": ""
      },
      {
        "title": "The Open Source GIS Stack (OSGS)",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "In the last few years, open source GIS has been developing relatively rapidly with an increase in the number of open-source GIS software available for performing various specialty functions. With this increase came the problem of managing the dependencies of different software when installing them on the same machine or getting them to work together to accomplish a task.  How was the setting up process the last time you needed to make a map, share it and write about how you made the map? This is where the docker-based Open Source GIS Stack (OSGS) comes in. \n\nOSGS is a rich, integrated, and opinionated GIS Stack with a focus on configurability and ease of use built from open source components. The primary objective of the OSGS stack is to provide simple and effective end-to-end solutions based on open source geospatial technologies. Some of the key services offered by the OSGS platform are Nginx and Hugo for web publishing using static web pages, File Browser for file management, QGIS-Server for publishing web maps, PostgreSQL and PostGIS for database management, and Metabase for visualizing your data. We’ll take a look at how easy and painless making, sharing and writing about maps can be. \n\nThe Open Source GIS Stack by Kartoza is maintained in the Kartoza OSGS repository https://github.com/kartoza/osgs.",
        "description": ""
      },
      {
        "title": "State of TerriaJS",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "TerriaJS is an open-source framework for web-based geospatial catalogue explorers. \n\nIt uses Cesium and Leaflet to visualise 2D and 3D geospatial data, and it supports over 50 different Web APIs, file formats and open data portals.\n\nIt is almost entirely JavaScript in the browser, meaning it can even be deployed as a static website, making it simple and cheap to host.\n\nTerriaJS is used across the globe to create next-generation Digital Twin Platforms for open geospatial data discovery, visualisation and sharing - it is used to drive\n\n- [National Map](https://nationalmap.gov.au/) (Australian Gov)\n- [Digital Earth Australia Map](https://maps.dea.ga.gov.au/)\n- [Digital Earth Africa Map](https://maps.digitalearth.africa/)\n- [Pacific Map](https://map.pacificdata.org/)\n- [NSW Spatial Digital Twin](https://nsw.digitaltwin.terria.io/) (Australian State Gov)\n- and many others\n\nIn this talk, I will give:\n\n- Background information about TerriaJS and how it is used by the community\n- Current state of the project for users, developers and wider community\n- New features\n- Future plans!\n\n[https://terria.io/](https://terria.io/)\n\n[https://github.com/TerriaJS/terriajs](https://github.com/TerriaJS/terriajs)",
        "description": ""
      },
      {
        "title": "Realtime multi-user mapping with MUDraw",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "Mapping is a private operations. It is done with several different local tools and usually by one person at a time. Yet we are used to have realtime multiuser editing of spreadsheets, documents and presentations. MUDraw tries to define a protocol to enable multiuser editing of features on a map and make it available as a library for both Leaflet as well as Maplibre (and enabling cross-library data editing) in order to make map editing a group activity. It relies on the client(s) and a server part written in Python/FastAPI that can be used independently from the infrastructure in which the communication is used and can set up a persistence layer taht is connected directly to github or other storage facilities.The idea of this tool is to be able to integrate it into UMap in order to make it  a more fun to use tool, but also in a longer perspective, part of the Public History Toolkit OpenHistoryMap is developing.",
        "description": ""
      },
      {
        "title": "From a national map publication platform to infinity. And beyond, of course.",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "For over a decade, there has been an open source map publication platform in the Netherlands, known as Tailormap (formerly Flamingo). That project is maintained largely by one company, B3Partners. Currently, Tailormap is being overhauled. Nah, not overhauled, I’d say completely rebuild. And this rebuild comes with a new approach on how to distribute this software project, and how to make it accessible to other developers to contribute, to organizations to roll out independently, and to other companies to use in their customer solutions.\n\nWhat we aim for in the long run is an online geospatial platform that is easy to use for all. For now, we publish an easy to install online GIS and mapviewing application, with features like mobile editing capabilities so that it can be used for maintenance purposes as well as for data dissemination. And here, at the FOSS4G in Firenze, we will celebrate this with our international launch presentation (and party in a not yet disclosed bar).\n\nWe’d like to invite you to join us in this journey. We’re extremely happy to have been able to start this development without outside funding, and we’re looking for partners to grow Tailormap together. We’re currently looking into the OSGeo Community project program, to see if we can join. We’ll be at the B2B meeting as well, but this presentation is where we’ll show the goodies.",
        "description": ""
      },
      {
        "title": "STAC Best Practices and Tools",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The SpatioTemporal Asset Catalog (STAC) specification is a common language for describing geospatial information that is flexible enough to extend across domains and use cases. In this talk, we walk through best practices for building STAC catalogs and using STAC extensions, using real world examples. These best practices are informed by documentation, conversations with STAC contributors, and discussions within the wider community. We survey the ecosystem of open-source STAC software, which includes libraries and tools written in Python, Node.js, and more. We show examples of reading, modifying, and writing STAC catalogs with a selection of software, including PySTAC and stactools, and we show which metadata to include in your STAC objects to ensure interoperability with powerful tools like xarray and pandas. Whether you are new to the STAC ecosystem or an experienced contributor, this talk will provide you with the context and tools you need to build your best STAC!",
        "description": ""
      },
      {
        "title": "Cloud-Native Geospatial with JavaScript",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The amount of Earth Observation data we have available nowadays is exceeding the capabilities for data processing. Therefore, a lot of data is now made available in the cloud. To make digesting the data easier and more-lightweight, it is getting more and more popular to store the data in so-called “cloud-native” file formats while data processing is also moving towards the data, i.e., into the cloud. This way you only need to retrieve the actual subset of the data you are actually interested in instead of the full data set, which can be in the magnitude of gigabytes or even larger. This technology of cloud-native file formats is usually best used with Browsers, which is the users’ main interface to the internet and the cloud. There the main language is JavaScript. Therefore, this talk will give a high-level introduction about the relevant cloud-native file formats and show whether and how you can make use of these files in client-side JavaScript:\n\n- COG: Cloud-Optimized GeoTiff ( https://www.cogeo.org )\n- COPC: Cloud-Optimized Point Clouds ( https://copc.io )\n- Flatgeobuff ( https://flatgeobuf.org )\n- GeoParquet ( https://github.com/opengeospatial/geoparquet )\n- STAC: SpatioTemporal Asset Catalog ( https://stacspec.org )\n- Zarr ( https://zarr.readthedocs.io )\n\nThis talk will dig into the available open-source libraries and, if JavaScript implementations are available, show their functionality based on examples. If multiple options are available, a high-level comparison will show the main differences in functionality. For COGs for example, we’ll compare the capabilities of the popular mapping libraries Leaflet, OpenLayers and MapLibre GL.",
        "description": ""
      },
      {
        "title": "The State of Cloud-Native Geospatial",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The vision of “Cloud-Native Geospatial” is a new paradigm of performing efficient computing and data access in the cloud in an interoperable way in order to achieve scalable and repeatable analysis of geospatial data. The last few years have seen major developments in open standards and open software that are helping make this vision possible, supporting full end to end interoperable workflows on remote sensing data, from data discovery to publishing of interoperable derived products.\n\nThis talk will present the current state of the Spatio Temporal Asset Catalog (STAC) specifications (stac-spec and stac-api-spec), updates in the published STAC extensions, and the latest community developments around Analysis Ready Data (ARD). We will cover the landscape of current recommended cloud-optimized file formats, for raster, vector, and point-cloud data formats (COG, Zarr, GeoParquet, COPC). Finally, we will provide recommendations for open-source client software to use to take advantage of the emerging geospatial clouds.",
        "description": ""
      },
      {
        "title": "Serving earth observation data with GeoServer: COG, STAC, OpenSearch and more...",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Never before have we had such a rich collection of satellite imagery available to both companies and the general public. Between missions such as Landsat 8 and Sentinels and the explosion of cubesats, as well as the free availability of worldwide data from the European Copernicus program and from Drones, a veritable flood of data is made available for everyday usage.\n\nManaging, locating and displaying such a large volume of satellite images can be challenging. Join this presentation to learn how GeoServer can help with with that job, with real world examples, including:\n\n* Indexing and locating images using The OpenSearch for EO and STAC protocols.\n* Managing large volumes of satellite images, in an efficient and cost effective way, using Cloud Optimized GeoTIFFs.\n* Visualize mosaics of images, creating composite with the right set of views (filtering), in the desired stacking order (color on top, most recent on top, less cloudy on top, your choice).\n* Perform both small and large extractions of imagery using the WCS and WPS protocols.\n* Generate and view time based animations of the above mosaics, in a period of interest.\n* Perform band algebra operations using Jiffle.\n\nAttend this talk to get a good update on the latest GeoServer capabilities in the Earth Observation field.",
        "description": ""
      },
      {
        "title": "Serving oblique aerial imagery using STAC and Cloud Optimized Geotiffs",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "In this talk we are going to present how the Danish Agency for Data Supply and Efficiency (SDFE) transitioned from a purely proprietary system to an open source system based on SpatioTemporal Asset Catalog (STAC) API and Cloud Optimized GeoTiffs (COGs) for servingservicing its open data collection of 5 million oblique aerial images. The new system is built partly using existing open source components and partly on newly built open source components. It uses significantly less resources and lets third party users access the data in a standardized way.\n\nAn important part of the process has been to develop and propose a community STAC extension for perspective imagery. This extends the STAC base metadata with parameters which are needed to do photogrammetric calculations and measurements using the images. The potential of this extension is that it enables the community to build generic perspective imagery clients in which the user can do advanced photogrammetric measurements.\n\nTo  ensure support for existing clients and to lower the barrier to entry the system also supports clients without COG reading abilities. Using open source components we have built \"CogTiler\" a high performance tile server which serves jpeg tiles directly from the COGs. Most of the time this is accomplished without decompressing the jpeg data.\n\nSDFE required that all code written for this project be open source and easily available to anyone. Therefore, all the code is available on GitHub.",
        "description": ""
      },
      {
        "title": "News from actinia - let's STAC!",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "„Hello again, my name is actinia. Still new to OSGeo and a Community Project since 2019, you might have heard about me already. In short I am a REST API on top of GRASS GIS to allow location, mapset and geodata management and visualization as well as execution of the many GRASS GIS modules and addons. Processing with other tools like GDAL and snappy is supported as well. I can be installed in a cloud environment, helping to prepare, analyse and provide a large amount of geoinformation. Besides these facts about me there is also a lot to tell about what happened last year! Besides vector upload, citable DOI, QGIS and python client implementations and more, I can be a Spatio Temporal Asset Catalog myself with the actinia-stac-plugin, am able to use data registered in a STAC for processing and after processing register the resulting data. With the ongoing development of the openeo-grassgis-driver, you can use this new functionality either in my native language or via openEO API. To learn about the details, come on over!“",
        "description": ""
      },
      {
        "title": "Serverless Geospatial",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "More and more geospatial operations are happening in the cloud and often these processes are serverless.  Whether your focus is on transportation logistics, agriculture, climate change analytics, or real estate; it is now possible to do geospatial computing in the cloud for both small and large projects.  \n\nWith serverless cloud compute options for data storage, compute, and desktop; we can now host our entire infrastructure completely serverless. Let's review the current state of geospatial serverless cloud infrastructure by exploring a stack consisting of OGC publishing, QGIS Desktop for cartography and client-side processing, TiTiler tile server, STAC data catalog, and a combination of data storage options.  \n\nFor OGC publishing, we'll review MapServer and Koop using Lambda plus API Gateway.  Similarly TiTiler can also be built with Lambda and API Gateway.   QGIS Desktop can now be run in the cloud through AWS Workspaces or AppStream.  For our data storage, we can use a combination of S3, Aurora PostGIS, and Redshift.",
        "description": ""
      },
      {
        "title": "Introduction to STAC API plugin in QGIS",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "STAC or SpatialTemporal Asset Catalog is now a popular option for providers wishing to create accessible catalogs of spatiotemporal asset data for end users. STAC aims to create a standardized and performant way for providers to expose their spatiotemporal asset data, and for users to ingest that data.\nA 'spatiotemporal asset' is any file that represents information about the earth captured in a certain space and time.\nSince the development of STAC started in 2007, the STAC ecosystem was not able to use the STAC data in desktop softwares. Recently through collaboration between Kartoza and Microsoft, a QGIS (a desktop GIS application) plugin called “STAC API Browser” was developed to bridge the gap between QGIS users and STAC data.\nNow using “STAC API Browser” users can access, download, analyze and use a vast amount of imagery data offered by various STAC specification providers, such as Microsoft Planetary Computer.\nThe aim of this talk is to introduce the “STAC API Browser” plugin, give a guide on how to use the plugin inside QGIS, showcase cool things that the plugin supports and how users/developers can collaborate on the plugin project. On top of all, we will also look at how to use the QGIS temporal controller feature with the added STAC data from the plugin.",
        "description": ""
      },
      {
        "title": "Maps in Motion: Introduction to the STAC Video Extension",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "This talk will introduce a new Spatiotemporal Asset Catalog (STAC) extension for geospatial video assets. The extension is designed to standardize the metadata for all types of overhead geospatial video assets, including those collected by satellite, UAV, or airborne sensors, while accommodating situations in which the sensor moves throughout the video. The talk will include a brief overview of the STAC ecosystem (elements and extensions), and explain the Video extension’s schema. In addition, there will be a complete end-to-end demonstration including data preprocessing and STAC item creation (entirely using FOSS tools), and a FOSS method for displaying STAC Video extension-enabled items on an interactive map. The audience need not prepare in any way for this introductory presentation, although some background in STAC and geospatial video might be beneficial. Otherwise, this talk has a broad appeal for data professionals through to frontend developers who are keen to add some motion to their maps!",
        "description": ""
      },
      {
        "title": "SkinnyWMS Meteorological Web Map Service",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The European Centre for Medium-Range Weather Forecasts (ECMWF) is an intergovernmental organisation that produces global numerical weather predictions and other data for its Member and Cooperating States and the broader community. It hosts one of the largest meteorological data archives in the world. ECMWF supports the open data community by providing data through its Public Datasets program and Open Charts. Additionally, the Centre has a long history of and extensive expertise in developing and providing software to process and visualize meteorological data.\n\nIn concert with these efforts, we developed SkinnyWMS – a lightweight Web Map Service for meteorological data. SkinnyWMS is currently used in the Copernicus Climate Data Store (CDS) and Germany’s Meteorological Service (DWD) Geoportal. It provides out-of-the-box interactive visualisation for a large set of meteorological parameters. It offers built-in support for data stored in GRIB (WMO standard) and NetCDF (OGC standard) formats, which are commonly used in meteorology, climatology, and oceanography.\n\nSkinnyWMS is written in Python and is based on ECMWF’s existing free and open source software ecCodes and Magics. It is free and open source software available under the Apache License 2.0. SkinnyWMS’ code is hosted on GitHub and it's available as an Anaconda package, from PyPi and as a Docker image from Docker Hub.",
        "description": ""
      },
      {
        "title": "Human-in-the-loop Machine Learning with Realtime Model Predictions using GroundWork and Raster Vision",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Acquiring and labeling geospatial data for training machine learning models is a time-consuming and expensive process. It is made even more difficult by the lack of specialized open-source tools for dealing with the idiosyncrasies of geospatial data. At Azavea, we have encountered both of these problems before. In this talk, we will present a solution that incorporates our geospatial annotation platform, GroundWork (https://groundwork.azavea.com), with our open-source deep learning framework, Raster Vision (https://rastervision.io), to provide a human-in-the-loop active learning workflow. This workflow allows labelers to immediately see the effect of their created labels on the model’s performance, thus speeding up the labeling-training-labeling cycle and making the connection between the AI and human GIS data labelers easy and seamless. \n\nThis talk will extend the hands-on experience introduced in last year’s “Human-in-the-loop Machine Learning with GroundWork and STAC'' FOSS4G workshop. We will present an enhanced active-learning workflow that allows labelers to train a model and see predictions on-the-fly as they create labels in GroundWork. The model-training and predictions will be handled by Raster Vision. This workflow will give the labelers a clear view of the model’s current strength and weaknesses at all times, and thus allow them to direct their labeling efforts more efficiently. Newly created labels will propagate back to the AI model in real time, and an asynchronous job will continue to refine the model and predictions. This loop is backed by the open-source Raster Foundry (https://rasterfoundry.azavea.com) and Franklin (https://azavea.github.io/franklin) APIs, and is compliant with the STAC (https://stacspec.org) and OGC Features (https://www.ogc.org/standards/ogcapi-features) open standards.",
        "description": ""
      },
      {
        "title": "Composing Software: Spatial for the JAMstack generation",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "In the words of Eric Elliot; All software development is composition: The act of breaking a complex problem down to smaller parts, and then composing those smaller solutions together to form your application.\n\nInspired by Eric’s work we have begun to modularize all the things and combine your mapping libraries (Openlayers, Maplibre, et al.) with other open source libraries (Tabulator, ChartJS) for data visualizations.\n\nHaving finally broken the shackles imposed on us by the restrictions of the Internet Explorer age we can finally facilitate Javascript (ES6+) to its functional best and deliver the applications we dream of.\n\nESBuild has revolutionized the way we compile script, and dynamic imports which have long been touted as the future are finally available to us with a little help from Skypack.\n\nIn addition to tried and tested server side rendering we now have powerful vanilla Web APIs to build application views on the fly and with little reflow, and repaint.\n\nThis is a talk for the opinionated JS enthusiast.",
        "description": ""
      },
      {
        "title": "Cleaning and processing global resource data for game development",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "This talk will describe some of the tools and tricks used by the team at Sparkgeo to gather, clean, and represent global resource data (minerals, wood, and water) for use in video game development.  One of the resources collected by the team used the STAC package for the Joint Research Centre - Global Surface Water data product to deliver water occurrence as part of the end product. \n\nThis talk will describe how to use a STAC package like JRC to access and transform cloud datasets before moving on to additional datasets such as limestone, gold, and forest cover.  These other datasets required a different geospatial approach to ensure that the resources were appropriately represented in the video game.  Nevertheless, each dataset required gathering from the internet and performing an Extract-Transform-Load (ETL) task. \n\nThe use of open-source geoprocessing tools, data science methods, and data delivery formats helped ensure real-world data is used in video games.  Community standards",
        "description": ""
      },
      {
        "title": "Exploring Data Interoperability with STAC and the Microsoft Planetary Computer",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "As a part of its AI4Earth initiative, Microsoft has created a Planetary Computer (PC) for hosting and processing open geospatial data. In addition to publishing a wide range of datasets, including Sentinel-2, MODIS, and more, the PC provides a powerful API and compute system based on open-source geospatial tools and using STAC metadata for data query, discovery, and access. In this talk, we present the latest in open geospatial data access, discovery, processing, and visualization using a variety of datasets from the Planetary Computer. We demonstrate use of the odc-stac package, which leverages the power of the OpenDataCube computing platform without the need for a database backend, and how odc-stac can load, mosaic, and transform geospatial assets into xarray datasets. We dive into other data interoperability tasks, including scaling processing with Dask and leveraging a variety of cloud-native formats. Along the way, we provide recommendations for data providers and curators on how to ensure their data can be used in a rich, interoperable way by the latest in geospatial processing tools.",
        "description": ""
      },
      {
        "title": "Connecting tribes: how we connected the GRASS GIS database natively to GeoServer",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "All of us involved in the creation and publication of large amounts of geodata are familiar with the complexities of data management. In the case of geodata created with GRASS GIS, we asked ourselves how they could be made accessible to GeoServer without duplication. To overcome the previous limitation of GRASS GIS having its own data format, we connected the tribes and let Java and C/Python communicate with each other. So the challenge was to be able to efficiently read the GRASS GIS database directly with GeoServer. And why is that? Because this directly links the analytical capabilities of GRASS GIS with the exceptional geo service & publishing capabilities of GeoServer.\n\nOur approach is to use the existing GDAL-GRASS bridge, and add this bridge as a new extension to GeoServer. To this we add two new GRASS GIS addons (r.geoserver.style + r.geoserver.publish) to easily publish the data from a GRASS GIS session as an OGC service. The new GeoServer GRASS raster datastore allows to use GRASS raster data directly in a GeoServer instance. In this way it is now very easy to publish GRASS data as a web service via GeoServer without having to export the data from GRASS GIS to GeoTIFF or COG files. This works for both classic raster data and also for timeseries which can e.g. be inspected as a WMS Time.",
        "description": ""
      },
      {
        "title": "The challenges and benefits of implementing OS geo within a large contracting company",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Within large construction companies GIS is widely used to store, analyse and visualise spatial data. GIS is just one of the components of an information infrastructure and has to be an integral part that can provide data and information to other departments and subcontractors as well. Van Oord as a marine contractor deals with spatial data on a daily basis. Existing disciplines like the design team and Survey team already have their way of dealing with spatial data to fulfil their task. What is it that the organisation needs additionally to work with GIS?\nVan Oord has chosen to use FOSS4g software as a GIS backbone. This choice has its benefits but also poses challenges. Most competitors in the business use proprietary software. Clients do not necessarily follow standards and the IT department has to support our requirements. \nWe are very proud of our GIS backbone and definitely see the benefits of using FOSS4G. Come and listen to the solution we have chosen; the changes we make in the business and challenges we face.",
        "description": ""
      },
      {
        "title": "Professional field data management with QField",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "PostGIS represents the de-facto standard for managing your spatial data, while QField is the state-of-the-art application for their management in the field.\nSince QField 2.0 release in spring 22, the seamless data synchronisation experience is complete. QFieldCloud closes the loop between your company's fieldworkers and the GIS analysts.\n\nIn this talk, we'll share *features*, *best practices* and *pro-tips* for managing your projects, remote teams, and permissions in a professional setting.\n\n## About QField\nQField is an open-source app developed for efficient fieldwork in real-time in urban areas, with a 5G connection or with offline data. The mobile GIS app combines a minimal design with sophisticated technology to conveniently bring data from the field to the office. Seamless QGIS integration, GPS-centred, offline functionality, synchronisation capabilities, desktop configurable: “QField” is designed for fieldwork – simple but uncompromising. Link: https://qfield.org\n\n## About QFieldCloud\nQFieldCloud is a spatial cloud service integrated into “QField” that allows remote provisioning and synchronisation of geodata and projects. Although “QFieldCloud” is still in an advanced beta stage, it is already being used by many groups to improve their workflows significantly. Link: https://qfield.cloud",
        "description": ""
      },
      {
        "title": "SMASH, state of the art of the digital field mapping project.",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "SMASH, the smart mobile app for surveyor’s happiness, is a slick app dedicated to digital field mapping. \nThe open source flutter app for Android, IOS (and upon request Linux, Macos and Windows) is packed with features, as for example: Geopackage and PostGIS editing support, Kalman filter on gps logs, geo-fences, native geotiff and shapefile visualization support, SLD styling for vector datasets.\n\nSMASH’s web counterpart is the Survey Server, a web application that allows groups of surveyors to centralize data collection. Users can synchronize the data from the app, but also download dedicated forms and projects, as well as basemaps and datasets. The server is built upon the same technology as the mobile app and visualizes the data with the same look and feel. Notes serverside-versioning has been introduced to enhance synchronization of data by teams. A redmine plugin is being developed by community members to create a geo-ticketing system.\n\nThis presentations gives an insight about the state of the art of the SMASH ecosystem and its current roadmap.",
        "description": ""
      },
      {
        "title": "From the field to the desk - end to end mobile data capture in an enterprise environment.",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "A demonstration of how to deploy a mobile data capture platform in an enterprise setting. In this example an environmental survey has been developed for a large organisation with a team of surveyors. Consideration is made to authentication, version control and synchronisation to an enterprise spatial database for wider consumption.\n\nQGIS is used to define base mapping, context layers and the data capture layers for the mobile application. In this project it is demonstrated how QGIS can be used to define survey forms to reduce input error. In this case a tree survey layer has been defined that adheres to the Individual Tree Standard from the UK’s Forest Research, the Open University and Treework Environmental Practice.\n\nThe QGIS project is then deployed to a dockerised Mergin service. Authenticated access is then granted to users of Lutra Consulting’s Input Android application. Field collected data can be created and synchronised with the Mergin service. Version control & merging allows multiple users to make asynchronous changes with the ability to rollback if required.\n\nThe Mergin service internal database is synchronised with an enterprise PostGIS database to allow other users to access and edit data via desktop. Media captured in the survey such as images is extracted through Mergin’s API and made available together with the survey data through a web GIS interface.",
        "description": ""
      },
      {
        "title": "Using QField to manage traffic sign inventory",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The new Road Traffic Act of Finland (effective since 2020) requires \nthe road management authorities to provide information about the existing road \nsigns, along with other similar infrastructure such as traffic lights and road paintings, to Traficom, \nthe Finnish Transport Infrastructure Agency. The data is stored in Digiroad, the national database of open \nstreet and road data, also hosted by Traficom. To help the different actors in public sector and elsewhere fulfill this legal obligation, \nas well as providing tools for infrastructure management and maintenance more generally, FOSS4G software can play an important role.\n\nIn this talk, we present results from our recent project related to this effort. In the project, we developed a \ntraffic sign inventory process using QField mobile data collection app and studied its suitability for the task.\nThere was a pre-existing conceptual data model for the road signs from Traficom, which was used as the basis for the \nphysical PostGIS database implementation. In addition, the data collection workflow was designed to make the \ndata collection as efficient as possible. This included configuring the data input forms and the traffic sign visualizations in the  related QGIS project file, as well as other aspects of usability of the app, such as further development of the geocoding functionality. \n\nThe process was then tested out in the field and improved upon in cooperation with employees from a few different-sized municipalities around Finland. The finished project report, along with the files needed to set up the data collection project are freely available in  the Github repository of the project: https://github.com/finnishtransportagency/digiroad-QField",
        "description": ""
      },
      {
        "title": "Analysing access to UK public rights of way with the QGIS Graphical Modeler",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The UK national walking organisation, Ramblers, are working to improve the public rights of way network, and in particular improve access to it for people who are less advantaged, and may not have access to vehicles. The research project described in this talk undertook an analysis of the national paths network using publicly available data supplied by hundreds of individual local authorities across the UK. This was done by setting up a series of models in the QGIS Graphical Modeler to generate six key indicators aggregated to census area level, including distance to nearest continuous path from each small area unit of population,  length of available path within a series of buffers, and access to paths of specific types – for example those passing through protected or designated areas. The talk will look at some of the challenges of the project, including scaling the modeller to work with millions of path features and tens of thousands of point locations, and building processes to combine path segments and then disaggregate them to an appropriate level. \n\nThe main goal of the project was to inform and support specific policy proposals, but it is also intended that the QGIS models should be passed on to Ramblers and used in the longer term, to monitor the impact of changes to the paths network and of population patterns over time, and also to support analysis of how additions to the network, for example by the inclusion of historic paths which are not yet official rights of way, could improve access. The intention is that these models could be run on smaller areas, and on hypothetical paths networks, to help build a case for extensions and rationalisation of the paths network at both national and local levels. Use of the Graphical Modeler rather than scripts or database processing will make it easier for Ramblers staff to run the models themselves in the future using inputs of their choice.",
        "description": ""
      },
      {
        "title": "Seamless fieldwork thanks to QFieldCloud",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "QFieldCloud's unique technology allows your team to focus on what's important, making sure you efficiently get the best field data possible.\n\nThanks to the tight integration with the leading GIS fieldwork app QField, your team will be able to start surveying and digitising data in no time.\n\nDiscover what QFieldCloud has to offer and how, thanks to seamless integration with your SDI, it can help make your teams' fieldwork sessions pleasant and efficient. And if you want to roll out your own customized version, nothing will stop you, QFieldCloud is open source!\n\nQFieldCloud is a SaaS (software as a service) solution built by OPENGIS.ch that allows your team to seamlessly integrate field data to your SDI.\n\nQFieldCloud is written in python using the Django Web framework that encourages rapid development and clean, pragmatic designs.\n\nQField is the mobile data collection app for QGIS with more than 120K active monthly users and 500K downloads. Discover how the seamless synchronisation with QFieldCloud can help make your teams' fieldwork sessions pleasant and efficient.",
        "description": ""
      },
      {
        "title": "How to join OSGeo (for projects)",
        "type": "General session talk",
        "track": "Transition to FOSS4G",
        "abstract": "Welcome to the Open Source Geospatial Foundation, proud hosts of FOSS4G, and advocate for free and open source geospatial software everywhere. This is a call out to open source software developers; please join OSGeo and help us help you!\n\nJoin OSGeo today:\n\n- Even just listing your project on the osgeo.org website is a great first step. Help us promote your technology so users can discover and enjoy your software.\n\n- The OSGeo “community program” gives project teams a chance to join the foundation with an emphasis on supporting innovation and new projects. The foundation provides some direct support, assistance along with endorsement and recognition from our board.\n\n- For established projects please join our “incubation program” to be recognized for excellence and as a full OSGeo committee.\n\nUnlike other foundations, OSGeo does not require that you give up or transfer any Intellectual Property; we simply ask that you be spatial, open-source, and open to participation.\n\nThis presentation gives clear instructions on how to join OSGeo, and representatives from recent successful projects will be on hand to answer your questions.",
        "description": ""
      },
      {
        "title": "OSGeo community interactions",
        "type": "General session talk",
        "track": "Transition to FOSS4G",
        "abstract": "OSGeo is an international organization, and the members are people from everywhere in the world .\n\nOSGeo is cares and loves the community. When you move from \"other\" G software to FOSS4G you become part of the community.\n\nThis community has a range of roles like users of the geo-spacial software, conference organizers, geo-spacial software developers, committee members, and some many more.\n\nWe are a complex organization.\n\nAs the second objective of this talk I would like to explain our organization.\n\nThe preface of the main objective:\n\nThese last two years have been hard times for everyone, we have been locked down, working remotely and unable to interact in person with our colleagues, and probably on this FOSS4G some members will not be able to be face to face during this FOSS4G 2022 in Florence, Italy.\n\nThe main objective:\n\nI would like to share the \"autographs & good wishes\" from the OSGeo community members that I've meet along these 7 years that I've been participating on the organization.",
        "description": ""
      },
      {
        "title": "Google Summer of Code with OSGeo",
        "type": "General session talk",
        "track": "Transition to FOSS4G",
        "abstract": "OSGeo's <b>Google Summer of Code </b> Initiative has been an inspiring and motivating platform for new contributors to join the OSGeo projects, community projects, guest projects, and incubating projects. In 2022, OSGeo is participating for the <b><i>16th year</i></b> in the Google Summer of Code, and it itself is a great achievement. With this talk, the OSGeo GSoC Administrators shall try to put forth the importance of GSoC with respect to the students and participating projects. The admins would focus on the development of projects with GSoC and encourage projects to be a part of the upcoming GSoC. <br/>\n\nOver the years, OSGeo's Google Summer of Code initiative has transformed into an initiative full of contributions towards geospatial software development. In the last 16 years, many OSGeo projects comprising incubating projects, community projects, and guest projects have progressed attributed to the contributions of student developers. Some of these contributors continue to participate as contributors for the projects and went on to take mentoring and organizing responsibilities. This is a true sense of FOSS4G in terms of the individual and collective growth of the developers and the OSGeo community. <b>In this talk, the OSGeo GSoC Admins team would try to appreciate the efforts of all the mentors and students involved till now and present the state of the GSoC 2022. The Admins would also present possibilities for new projects to be part of the GSoC with OSGeo as an umbrella organization.</b>",
        "description": ""
      },
      {
        "title": "OSGeoLive project report",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "OSGeoLive is a self-contained bootable DVD, USB thumb drive or Virtual Machine based on Lubuntu, that allows you to try a wide variety of open source geospatial software without installing anything. It is composed entirely of free software, allowing it to be freely distributed, duplicated and passed around. It provides pre-configured applications for a range of geospatial use cases, including storage, publishing, viewing, analysis and manipulation of data. It also contains sample datasets and documentation. OSGeoLive is an OSGeo project used in several workshops at FOSS4Gs around\nthe world.\nThe OSGeoLive project has consistently and sustainably been attracting contributions from ~ 50 projects for over a decade. Why has it been successful? What has attracted hundreds of diverse people to contribute to this project? How are technology changes affecting OSGeoLive, and by extension, the greater OSGeo ecosystem? Where is OSGeoLive heading and what are the challenges and opportunities for the future? How is the project steering committee operating? In this presentation we will cover current roadmap, opportunities and challenges, and why people are using OSGeoLive.",
        "description": ""
      },
      {
        "title": "Openness - a Strategic Choice",
        "type": "General session talk",
        "track": "Transition to FOSS4G",
        "abstract": "The National Land Survey of Finland has made a strategic decision to pursue increased use of open source solutions in its activities. We’ve been an active user of FOSS4G solutions for more than a decade. Further, we created an open source mapping framework Oskari, which has an active user and developer community.\n\nIn autumn 2020, the National Land Survey of Finland made a major decision to build our new topographic data production system on open source components, such as QGIS and PostgreSQL/PostGIS. The decision has raised a few eyebrows and a lot of interest among other national mapping agencies as well as other institutions using geospatial software. This talk will discuss\n-\ton what grounds we made such a decision\n-\thow we are progressing with the implementation\n-\thow we are looking at engaging in collaboration with open source communities\n-\tand most importantly, why every public sector organization should consider the benefits that can be gained by using and investing in open solutions.",
        "description": ""
      },
      {
        "title": "Implementation of INSPIRE in Lithuania: experience with the transition to FOSS4G",
        "type": "General session talk",
        "track": "Transition to FOSS4G",
        "abstract": "Since entering into force in May 2007, the Directive 2007/2/EC of the European Parliament and of the Council establishing Infrastructure for Spatial Information in Europe (INSPIRE directive) has been playing very important role in building spatial information infrastructures – both pan-European and at national level. Technical specifications and guidelines describe the key components of data content and of implementation of web services. However, each country decides individually how technically it will implement these requirements, what architecture and software it will choose to use. The roadmap of directive implementation has reached the last milestone in 21/10/2020 – all spatial data sets had to be provided to the INSPIRE geoportal ((https://inspire-geoportal.ec.europa.eu/). Now is the best time to share the experience how Lithuania started INSPIRE implementation path using commercial software but successfully ended with using only FOSS4G.\n\nImplementation of INSPIRE directive in Lithuania is centralized and state enterprise GIS-Centras is responsible for technical work. INSPIRE directive was implemented in 2 stages. The first stage started back in 2012 and it was dedicated to cover data sets from Annex I and Annex II (only orthophoto imagery). Back then the INSPIRE directive implementation was a new and little-known technical challenge. The decision to call a tender and use the commercial software for which there were not many viable alternatives at the time seemed quite logical. \n\nThe second stage of INSPIRE directive implementation started in 2018 and ended up in 2021. This stage was dedicated to cover data sets from Annex III and part of Annex II. Instead of just calling a tender to implement the requirements with commercial software we already had we decided to implement everything by ourselves and change the commercial software to FOSS4G. At the end of the project, we have not only prepared and published all the datasets and services, but also had a team of in-house specialists who were able to work with the FOSS4G. Even the results from the first stage was quickly changed to FOSS4G in order to unify the implementation architecture.\n\nWe have set three goals for the transition from commercial software to FOSS4G:\n1. Create a system that we can administer and develop ourselves;\n2. Create infrastructure that is cost-effective in the long run;\n3. Automate the workflow as much as possible.\n\nIn this presentation we will share our experience of transition from commercial software to FOSS4G both from technological and company/team perspectives. We will present the technological architecture we use to implement INSPIRE requirements. It consist of PostGIS, Geoserver, QGIS, Hale studio, GDAL and Geonetwork. Finally we will explain what we have learn as a company and GIS specialists during the transition process.",
        "description": ""
      },
      {
        "title": "Is it wrong to make money with FOSS4G technology?",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Spoiler alert, I believe the answer to the question posed in the title is a firm “no”. As such, this presentation will describe why a healthy commerce ecosystem is an essential component of the broader FOSS4G community. The presentation will describe several commerce models that both support open source initiatives and generate work and revenue for businesses. The commerce models presented will be complimented by real world examples of these models in action. The presentation will also describe the trend of large companies open sourcing some of the tools that they use to run their business and/or that support their products. The presentation will also describe the business importance of open source frameworks for both the broader Javascript development space (e.g., React, Angular, etc.) and the more specialized geospatial development arena (e.g., Deck.gl, etc.). Finally, while making money is appropriate within open source communities, it is always important that businesses contribute back to the open source ecosystem and best practices for being a good open source citizen will be discussed.",
        "description": ""
      },
      {
        "title": "Pedological Information System of Piedmont Region in Italy",
        "type": "General session talk",
        "track": "Transition to FOSS4G",
        "abstract": "Over the last 35 years, the IPLA has collected data on soils, both soil samples and cartographic data, to create a database (physical and digital) that represents knowledge for all stakeholders, public and private, of the composition and state of soils. Piedmontese.\nin the last 25 years the SIP (Pedological Information System) has been changed several times, to meet the new needs in terms of data collection but above all to update it to the new FOSS technologies.\nin fact, in the last 5 years we have gone from non-connected proprietary systems (DB FoxPro and Esri geoDB personal) to a single integrated system with FOSS tools: PostgreSQL / PostGIS and QGIS.\nAn information system has therefore been created that allows technicians on the one hand to be able to collect the field data by inserting them, with a web interface in the database, and to be able to see in real time, the cartographic themes in QGIS, based on the data.just inserted and taking advantage of the geographical component of the database.\nAlso the implementation of the survey points can be done, using in this case internal functions of the DB, both in alphanumeric way and in QGIS in a geographic way, with real-time modification of the points based on the coordinates entered by one of the two tools.\nAnd new implementations for field survey (QFiled) are under development, for the survey of the points and their visualization on the DB in real time.\nThe pedological observations are characterized by different levels of information among which more than 5000 soil profiles are the basic and fundamental data, subdivided into field data (descriptions and photos) and analytical data (physical-chemical determinations in laboratory). The elaboration of these data provides the main structure of the Pedological Information System, that is the description of more than 1200 Soil Types, used to build up the characterization of 7000 Geographical Units.",
        "description": ""
      },
      {
        "title": "Geo-commons in France : review of current initiatives",
        "type": "Lightning talk",
        "track": "Transition to FOSS4G",
        "abstract": "**\"Géocommun\"** `[ʒeokɔmɛ̃]` : is it the latest buzzword in France or a large movement towards more openness in the geospatial realm ?\n\nThe French National Geographical Institute (IGNF) recently started communicating its vision towards the development of Geo-commons. This sounds like a strategical change in the way the institute apprehends geo-data and geo-software production.\n\nIn this presentation, we first try to give a definition of what a \"geocommon\" is. Then we review the initiatives currently being deployed by various actors in France to transform this word into a reality.\n\nWe study the roots of these actions and their links to opendata, opensource and opengov movements.\n\nWe also try to provide a mindmap of involved actors, and how they interact together : administrations, data-oriented communities or software-oriented communities.\n\nThen we anticipate the impact on free and opensource software for geomatics, and how it could affect technologies and communities in this area.",
        "description": ""
      },
      {
        "title": "10 years in Georepublic and OSGeo",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Taro Matsuzawa has been working for Georepublic for 10 years now. He has had experience in many open source communities since his student days, but had not yet joined the OSGeo community until he joined Georepublic. He is now a FOSS4G specialist and has presented at FOSS4G conferences in Japan.\n\nFor a ten years he has been a committer for OpenMapTiles and several OpenSource projects, and has contributed more than 20 projects to Japanese companies and municipalities. In this issue, he would like to share some of the insights he has gained from his work and the OSS community.\n\nI will mainly talk about the basics and applications of Game Tile[1] technology, a small-scale map solution using pgRouting[2], and Python3 support for the ckanext-spatial plugin[3].\n\n1. https://leafletjs.com/SlavaUkraini/examples/crs-simple/crs-simple.html\n2. https://pgrouting.org/\n3. https://github.com/ckan/ckanext-spatial/pull/249",
        "description": ""
      },
      {
        "title": "Favoring Opensource Technologies for French Fire Brigades",
        "type": "Lightning talk",
        "track": "Transition to FOSS4G",
        "abstract": "The NexSIS project aims to create a digital rescue platform providing all civil protection actors in France with a complete set of cloud operational services. Open Source GIS solutions were chosen for this national project with strong technical requirements.\n\nThis has a direct impact on the way data will be exploited. Each Fire Department will have to adapt and create data that will be directly used by the NexSIS software (areas that require special equipment or specialized teams for example).\n\nCurrently in France each fire department has a budget depending on the size / number of people in the department. This budget is used to buy new fire extinguishers, computers, but also to hire new people, etc... Most departments currently use many different proprietary softwares for all GIS aspects. Historically, each department has made their own choices on what software they use.\n\nThis talk will show how we are helping fire brigades to make the switch to Open Source without losing any functionalities and without any extra work load.\n\nUsing the power of both QGIS and PostgreSQL, we will show how these tools can be used to share and publish common workflows (qgis expressions, model builders) that are often used in fire emergencies, build a common atlas (report module), edit spatial data (forms, and constraints) and so forth.",
        "description": ""
      },
      {
        "title": "The MAPME Initiative: Leveraging the power of open data and FOSS GIS to improve public expenditure in the development aid sector.",
        "type": "Lightning talk",
        "track": "Transition to FOSS4G",
        "abstract": "MAps for Planning, Monitoring and Evaluation (MAPME) is an initiative founded by Geo-geeks and FOSS enthusiasts from KfW Development Bank (KfW), French Development Agency (AFD) and MapTailor Geospatial Consultants.\nAid agencies such as KfW and AFD financially assist developing countries in fighting hunger, poverty, disease, illiteracy and environmental degradation around the world. Together with our partner countries we are key decision makers in the allocation of the so-called Official Development Assistance (ODA). KfW, for example, allocated 12.4 bn. EUR to assist developing countries achieving the Sustainable Development Goals (SDGs) in 2020.\nGeodata and geospatial technologies help us to take informed decisions to allocate funds responsibly and maximize public goods and benefits. Nevertheless, the uptake of open data and geospatial technologies within our institutions and decision-making processes is still relatively low. We think that one of the main reasons for this is missing openness in the way that we deal with data-analytic questions in our institutions. \nIn response we founded MAPME, an open community and open-source initiative to upscale and democratize the usage of geodata and geo-spatial technologies within our own institutions as well as our partners. With this initiative we promote cultural change in our institutions by prototyping small FOSS and open-data pilot projects that illustrate the power and usefulness of these technologies to improve development aid projects. One of our outputs is the mapme.biodiversity package, which offers R-users the possibility to automatically download and process several important open-data sources for conservation science using a parallelization approach to deal with large AOIs or global conservation portfolios (https://github.com/mapme-initiative/mapme.biodiversity). \nWe will offer a talk where we share our approach to FOSS application and development, what we see as barriers in our institutional and IT contexts and first successes stories that leveraged the power of geospatial data for learning about our projects and taking more informed decisions.",
        "description": ""
      },
      {
        "title": "Development of plugins in QGIS for the management of the Multipurpose Cadastre in the Government of Mexico City",
        "type": "Lightning talk",
        "track": "Transition to FOSS4G",
        "abstract": "Migration of the current tools that maintain the Cadastre of Mexico City to Open Source and free licensing technologies.\n\nThe objective of the project was the development of a plugin in QGIS that optimizes the functionalities of the licensed software and allows the maintenance and management of cadastral data.\n\nStages of the development process:\n\na. Requirements analysis: Based on the information provided by the client, the feasibility of integrating the database engine,  the API of workflow services with the transactional systems of the municipality is evaluated.\n\nb. Definition and implementation of PostGIS and QGIS: together with the client, for the integration of the tools and subsequent development.  \n\nc. Design of Python and QT tools: carry out the design and development of plugin components in QGIS.\n\nd. Design of Postgres security interfaces:  coordination with the permissions, automation routines with the plugin components.\n\ne. Development of functional tools from Bentley to QGIS: development of the previously detailed functionalities in the analysis and design stages of the project in order to replace the existing functionalities, accompanied by redesign. \n\nf. Test and implementation in Quality Assurance  (QA) environment \n\ng. Training and  technology transfer. \n\nh. Documentation of all strage of the process of the project, in order to carry out the complete transfer to our client.",
        "description": ""
      },
      {
        "title": "Maplandscape - an open-source geospatial workflow for agricultural landscape monitoring",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Maplandscape is a stack of open-source geospatial applications designed to enable mapping of agricultural landscapes and farm systems. It supports large team in-the-field mobile data collection, provides tools for data syncing and management, and easy visualisation and querying of spatial information for decision making and reporting. The workflow has been developed and deployed in Tonga through a collaboration between universities in Australia and the South Pacific, and Tonga’s Ministry of Agriculture, Food, Forests and Fisheries (MAFF). Maplandscape is currently being used in Tonga to map crop, livestock, agroforestry systems, and farm management practices; over 11,000 farms and four island groups have been mapped so far. This information has been used to inform agricultural planning and resource allocation, disaster response, land utilisation assessment, tracking land use changes, and monitoring of the condition of key commercial crops. \n\nThe Maplandscape workflow is based on QField for in-the-field data collection and QFieldCloud for data syncing, storage, and user authentication and management. Using QField mobile GIS, key agricultural landscape features (e.g. crop parcels, paddocks, fallow land) can be spatially mapped, and rich attribute information can be captured through various widgets and flexible and complex form logic, with support of reference geospatial layers. Three cloud-based applications have been developed that build on top of the QFieldCloud API to provide geospatial data visualisation and analytics tools. These applications provide differing and complementary functionalities, and facilitate quick analysis, publishing, and reporting of data collected using QField and stored using QFieldCloud. The apps are built using Shiny, Leaflet, ggplot, and DataTables software, and are deployed using ShinyProxy and docker containers in swarm mode. These applications use the QFieldCloud API to authenticate users and retrieve data and an R package has been developed to handle this interaction within Shiny apps. The goal of these applications is to speed up the process of data collected using QField informing agricultural monitoring, planning, and decision making activities. A summary of these geospatial data visualisation and analytics applications is provided below.\n\nmaplandscape-view is a web app that allows users to query and view spatial data on interactive maps and data tables and generate reports comprising cartographic outputs, charts, and summary tables directly from QFieldCloud data. maplandscape is a web app that allows users to specify and apply custom geospatial data querying, analysis, and visualisation tasks to their QFieldCloud data in a web browser and with a simple UI interface. This application enables users without a GIS background to extract information from, and analyse, rich datasets collected in-the-field using QField; for example, agricultural officials in Tonga used this application to generate village- and district-level crop area summary tables for their annual report. Finally, maplandscape-admin provides tools for users to manage their QFieldCloud projects and accounts.",
        "description": ""
      },
      {
        "title": "Early use of FOSS4G in a space start up",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "As a small but scaling new space company, Satellite Vu relies heavily on open source tooling for our image production pipeline, as well as storing our image assets and conducting experiments using thermal data sources. The company prides itself on being early adopters of emerging technologies, particularly as standardization of satellite imagery access and reproducible science are at the core of what we stand for.\n\nIn this talk, we’ll give an overview of the main projects we lean on for all of data engineering, data science and thermal science as well as outline the vision for Satellite Vu’s evolving role within the open source community. Specific tools we’ll comment on our use include:\n\n1. STAC, and the related stac-fastapi, for storing and serving image collections\n2. rioxarray and stackstac for scaling our use of both internal and external cloud native imagery datastores\n3. The pangeo stack for running experiments and scaling data processing\n4. pygeoapi, as a vector data server\n\nI’ll introduce the Satellite Vu public STAC, and talk through how we’re using FOSS4G tools to shorten the development time of new products as well as prepare for the first satellite launch in Q1 2023.",
        "description": ""
      },
      {
        "title": "Visualizing climate risks for disaster reduction and climate resilience programs – Interactive open-source tools for analysts and decision makers to utilize Earth observation data",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Climate and vegetation indicators created from Earth observation data provide timely information to analysts and decision makers implementing disaster risk reduction and climate risk mitigation programs. The United Nations World Food Programme’s (WFP) Climate and Earth Observation unit (ClEO) works with a number of Earth observation datasets to measure and monitor climate risks across all of the regions where we work, including 90+ countries globally. \n\nThe end users of this information include government institutions such as the meteorological and disaster management agencies, implementers of humanitarian assistance programs, as well as WFP field staff working on programs which build climate resilience through the development of community assets and livelihood support. \n\nTo enable the creation and dissemination of monitoring indicators, WFP is in the process of deploying an instance of Open Data Cube with nearly global coverage. Leveraging the power of data cubes to measure key climate and vegetation indicators over space and time, WFP’s Open Data Cube instance will provide free and open access to a wide range of analysis-ready data products. Utilization of this data requires user-facing applications with easy to use and intuitive interfaces. One of the tools developed by WFP to provide more direct access to climate and Earth observation data is PRISM – an open-source software solution which greatly simplifies the integration of geospatial data from various systems. PRISM has been developed to easily integrate data from Open Data Cube deployments using OGC standards – providing a quick tool to display time-series raster data in an interactive dashboard. \n\nDuring this talk, WFP will present a brief overview of the use cases we address with Earth observation data, the role of our Open Data Cube instance in the organization, and the development of tools and processes to disseminate data for visualization using OGC standards – including the PRISM platform.",
        "description": ""
      },
      {
        "title": "Developing with MapStore; creating a custom dashboard to map crime data",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "MapStore is an open source product developed for creating, saving and sharing in a simple and intuitive way maps, dashboards, charts and geostories directly online in your browser. You can use MapStore as a product to deploy simple geoportals by using the standard functionalities it provides but you can also use MapStore as a framework to develop sophisticated web gis portals by reusing and extending its core building blocks. MapStore is also integrated inside geOrchestra as well as GeoNode open source project.\n\nThe presentation will focus on the use of MapStore as web gis framework to create a modular, reproducible, simple yet powerful dashboard to visualize crime data (but in reality many different types of location based data) leveraging on GeoServer and PostGIS advanced functionalities. We will describe the main steps for creating such an infrastructure leveraging on the MapStore components and framework then we will cover how the existing dashboard template can be configured to work with your own data sources (eventually touching the needed processing steps for the data itself) including GeoServer and PostGIS advanced functionalities. \nWe will eventually discuss further improvements and  new features to evolve the current capabilities to capture new and emerging requirements.\n\nThe goal of this presentation is twofold, on one side we are addressing developers in order to show them advanced usage of MapStore to develop compelling applications on the other side we will be addressing power users and system administrators willing to deploy the Crime Mapping dashboard to make their own data available without writing code.",
        "description": ""
      },
      {
        "title": "Mastering Security with GeoServer and GeoFence",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The presentation will provide a comprehensive introduction to GeoServer's own authentication and authorization subsystems. \nThe authentication part will cover the various supported authentication protocols (e.g. basic/digest authentication, CAS, OAuth2) and identity providers (such as local config files, database tables and LDAP servers). \nIt will explain how to combine various authentication mechanisms in a single comprehensive authentication tool, as well as providing examples of custom authentication plugins for GeoServer, integrating it in a home-grown security architecture.\n\nWe’ll then move on to authorization, describing the GeoServer pluggable authorization mechanism, and comparing it with proxy based solution. We will explain the default service and data security system, reviewing its benefits and limitations.\n\nFinally we’ll explore the advanced authorization provider, GeoFence. The different levels of integration with GeoServer will be presented, from the simple and seamless direct integration to the more sophisticated external setup. Finally we’ll explore GeoFence’s complex authorization rules using:\n- The current user and its roles.\n- The OGC services, workspaces, layers, layer groups.\n- CQL read and write filters.\n- Attribute selection.\n- Cropping raster and vector data to areas of interest.",
        "description": ""
      },
      {
        "title": "Creating Maps in GeoServer using CSS and SLD",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The presentation aims to provide attendees with enough information to master GeoServer styling documents and most of GeoServer extensions to generate appealing, informative, readable maps that can be quickly rendered on screen. Examples will be provided from [GeoSolutions training material](https://docs.geoserver.geo-solutions.it/edu/en/), as well as from the [OSM data directory](https://github.com/geosolutions-it/osm-styles) we shared with the community.\n\nSeveral topics will be covered, providing examples in CSS and SLD, including:\n\n* Mastering common symbolization, filtering, multi-scale styling.\n* Using GeoServer extensions to build common hatch patterns,  line styling beyond the basics, cased lines, controlling symbols along a line and the way they repeat.\n* Leveraging TTF symbol fonts and SVGs to generate good looking point thematic maps.\n* Using the full power of GeoServer label lay-outing tools to build pleasant, informative maps on both point, polygon and line layers, including adding road plates around labels, leverage the labeling subsystem conflict resolution engine to avoid overlaps in stand alone point symbology.\n* Dynamically transform data during rendering to get more explicative maps without the need to pre-process a large amount of views.\n* Generating styles with external tools.",
        "description": ""
      },
      {
        "title": "Introduction to Spatial Data Outputs Platform - OpenStreetMap Galaxy",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "OpenStreetMap (OSM) Galaxy is a project that the HOT Tech Team launched in mid-April 2021 to optimise and improve availability and accessibility of OSM Data outputs for different user groups within the ecosystem. Through this project, we strive to address all the OSM data needs under one umbrella and ensure OSM data is available, accessible and ready to use for all kinds of users. We are trying to solve the high dependency on different data sources and uncontrolled platforms while focusing on fast queries and process optimisation by accessing data from HOT administered and controlled environment.\n\nAs a one-liner, the vision for OSM Galaxy is to provide a single platform to address all OSM Data Needs.\nIn OSM context, a data need is a broad term covering a variety of topics:\nRaw data exports\nAnalysing completeness of Data\nChecking the data quality in your neighbourhood\nUnderstanding your contribution to a mapathon, to name a few\n\nThrough this project we strive to:\nBring together all the data needs under one umbrella\nEnsure OSM data is available, accessible and ready to use for all kinds of users",
        "description": ""
      },
      {
        "title": "Geofolio: Making Environmental Data Understandable and Accessible for Everyone",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Geofolio is a project aiming to make environmental data understandable and accessible for everyone. Geodata is notoriously difficult to use for non-experts. It often gets hidden away in confusing data portals, and at least some GIS expertise is a common prerequisite to find and download data, extract your area of interest and to do some simple analysis. Geofolio lowers these adoption thresholds by letting users draw an area of interest, and then a factsheet with text summaries, maps, and charts is generated automatically from various open access geodatasets. The factsheets contain information on various environmental themes such as topography, land use, hydrology, climate, and agriculture. Users can download the source data, thereby providing an easy step up for further investigation and learning using open source GIS applications such as QGIS.\n\nGeofolio makes extensive use of open source software for geospatial applications. The front-end and factsheets use the Leaflet mapping library, and the back-end and processing framework depends on GDAL and Shapely. Geodata is stored for analysis and visualization using PostGIS and Cloud-Optimized GeoTIFF files. The actual data processing takes place \"on-the-fly\" using GDAL-enabled AWS Lambda functions.",
        "description": ""
      },
      {
        "title": "Just Enough GIS: Plugging Lightweight Mobile GIS into an Offline Field Data Collection Platform",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "In 2012, FAIMS project developed FAIMS Mobile, an open-source platform for minting Android applications for offline human-mediated data collection on multiple tablets. Originally intended for archaeology, this platform saw cross-disciplinary adoption including disciplines such as oral tradition, linguistics, ecology and geochemistry.  Mobile GIS (provided by Nutiteq, Estonia) was built into the core software from the start providing the most essential geospatial functionality from management and rendering user-owned raster and vector data, to manual data creation, editing, retrieval, and rendering. Automated data collection via onboard and bluetooth sensors was also implemented to support unique identifier generation and printing, and other key tasks for field sample tracking.  Navigation and spatial query facility existed. The simplified interface isolated end-users from administrators, with only the latter needing geospatial skills and domain knowledge, a division that facilitated data entry by unskilled volunteers. Many of the geospatial functions, however, required programming to customize. Given this barrier to entry, only clients with access to a programmer could create customisations for geospatially-tailored field data entry. Others had to run existing customisations, published on Github. Despite this bottleneck, FAIMS 2.6 clients created a variety of spatial data collection workflows, from simple offline shape mapping to manual map data digitisation.\nIn 2022, FAIMS project is rebuilding the FAIMS Mobile platform to equip it with a graphic user interface for customisation, to allow cross-platform deployment, and to implement ‘round trip’ data transfer to and from  existing desktop tools. We hope to retain a robust geospatial data creation capability but aim to strip away functionality that saw little use over the 10 previous years, taking a 'just-enough-GIS' approach. As the architecture of FAIMS Mobile is changing from sqlite with spatialite extension to CouchDB/ PouchDB supporting geojson, technical elaboration pointed to OpenLayers as the most appropriate and complete library for geospatial data collection and management. This paper will examine the challenges and considerations of ‘just enough GIS’ implemented with OpenLayers in a comprehensive mobile data capture application.",
        "description": ""
      },
      {
        "title": "GeoPrism Registry - Using Spatial Knowledge Graphs for Managing and Integrating Geographic Data Over Time Across Multiple Information Systems",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "A knowledge graph is a network that interconnects concepts, objects, or events according to domain specific relationships and terminology. Spatial knowledge graphs model locations and how they are spatially related to each other according to semantic properties and are useful for helping to automate the integration of geographic data across silos. Information systems used to make decisions often have different pictures of the geographies (i.e. people, places, and infrastructures) they respectively cover. Within a single area, different programs collect and store different geographic data in siloed systems at different times, leading to discrepancies and duplication of effort. This also results in decisions based on incomplete and out-of-date geographic data (e.g., spatial distribution of population and resources).\n\nGeoPrism Registry is an open-source Common Geo-Registry (CGR) implementation that utilizes spatial knowledge graphs to provide a single source of truth for managing geographic data over time across multiple information systems and data sources. It is used to publish, access, and manage changes over time to hierarchies and geospatial data for geographic objects such as administrative divisions, infrastructure and other relevant physical features. \n\nGeoPrism Registry uses geo-ontologies to define semantic properties and relationships that implement spatial knowledge graphs using a graph database. Changes to attribute values, relationships, and geographies are managed for different time periods. Historical views of data can be generated for any time period. The application has been released under the Lesser General Public License (LGPL) and was developed using only open-source components including OpenJDK, MapboxGL, PostgreSQL, OrientDB, Solr, GDAL, and GeoServer.\n\nThis talk will demonstrate how spatial knowledge graphs defined in GeoPrism Registry using FOSS4G tools can:\n1. contextualize data from different sources in both time and space,\n2. use geographic objects as the common link between data sources,\n3. facilitate trend analysis, and\n4. aggregate data according to different hierarchies\n\nSupport for the development of GeoPrism Registry was provided by the Bill and Melinda Gates Foundation via the Digital Solutions for Malaria Elimination (DSME) Project and the DSME Community. The DSME project uses geo-enabled information systems to improve the efficiency and effectiveness of malaria surveillance, program planning, and intervention.",
        "description": ""
      },
      {
        "title": "IOTA2: large scale land cover mapping operational chain",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "The use of remote sensing data operating in different observation domains is an undeniable asset for the realization of quality land cover products. \nIndeed, satellites allow to cover large areas of interest in a regular way with a durable quality. \nSatellite data can be of different but often complementary natures, which makes it possible to broaden the possible fields of application (water management, snow cover, crop yield, urbanization, etc.). \nIn addition to these new data, there are recent technological developments (or old but now usable due to the evolution of computing capacities, such as the use of neural networks), and means of service provision and dissemination that allow these applications to be carried out over a longer period of time (long time series that are computed more rapidly) and in a larger space at different scales, sometimes simultaneously (stationary, local, national, continental, global scale).\niota2, developed by CESBIO and CNES with the support of CS GROUP, is a response to the growing demand for the creation of an Open Source tool, allowing the production of land cover maps at a national scale that is sufficiently generic to be adapted to the different objectives of users. \nIn addition, this project ensures the production of an annual land use map of metropolitan France [REF https://doi.org/10.3390/rs9010095], with a satisfactory level of quality, thus proving its operational capacities.\n\niota2 integrates several families of supervised algorithms used for the production of land use maps. Supervised algorithms (e.g.,  Random Forests or Support Vector Machine) that process pixels that can be parameterised by the users through a simple configuration file. iota2 also offers the user the option of using a deep learning model.\nIn addition to the pixel approaches, contextual approaches are also proposed, with Autocontext [1] and OBIA (Object Based Image Analysis). Autocontext, based on RF, takes into account the context of a pixel in a window around its position. The OBIA approach exploits an input segmentation to classify objects directly.\n\nIn addition to the supervised classification approaches, iota2 is also able to produce indicator maps (biophysical variables) either by supervised regression or by using user-provided processors, diversifying the possibilities of using iota2.\n\nOne major interest in iota2 is it's ablility to deal with a huge amount a data, for instance the OSO product (https://theia.cnes.fr/atdistrib/rocket/#/collections/OSO/2327b748-a82c-5933-afb0-087bbfeff4cd) is generated using a stack of all available Sentinel-2 data over the France without any landscape discontinuity due to the Sentinel-2 grid. Another point of interest is its capability to produce a landcover map everywhere a Sentinel-2 data and a groundtruth are available (ie : https://agritrop.cirad.fr/597991/1/Rapport_Intercomparaison_iota2Moringa.pdf). \n\n1. Derksen, D., Inglada, J., & Michel, J. (2020). Geometry aware evaluation of handcrafted superpixel-based features and convolutional neural networks for land cover mapping using satellite imagery. Remote Sensing, 12(3), 513. http://dx.doi.org/10.3390/rs12030513",
        "description": ""
      },
      {
        "title": "Carto-OSC: Real Time Cartography in Performance",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "First there were DJs, then there were VJs. Clear the booth and make room for the MJ (map jockey).\n\nCarto-OSC leverages a handful of open source libraries, data, and protocols in order to activate performance spaces – theaters, galleries, nightclubs – with projected, real time cartography. The Open Sound Control (OSC) protocol, itself a mature successor to MIDI (Musical Instrument Digital Interface), is coupled with a touch surface to control layer coloring and patterning, zoom/pitch/bearing effects, label typography, transits along road networks and great circles, and a variety of transitions purloined from the history of cinema and visual music.\n\nCarto-OSC is written in modern JavaScript and relies on the solid foundation of Open Stage Control, MapLibre, D3.js, chroma.js, as well as OpenStreetMap and Natural Earth data.\n\nThis presentation describes the architecture of Carto-OSC, the impulses behind its creation, development, and future, and includes documentation of its use in performances of music, dance, and spoken word.",
        "description": ""
      },
      {
        "title": "The Carto 2 Project",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The Carto 2 project is part of the Geo-IDE program of the Ministries of Ecology and Agriculture in France. The objective of Geo-IDE is to provide stakeholders in the ministries with common data and tools in the field of geographic information. \n\nSince 2019, the Ministry of the Ecology and Camptocamp have been collaborating to create a new module, Carto2, that would allow data administrators to compose, publish and consult maps online and other users to search, consult or download published maps at the same time. \n\nCarto2 also had to offer a more modern and ergonomic platform, as well as possibilities to evolve the module and its functionalities in order to better meet the needs and expectations of the decentralized services.\n\nThe project is developed using free solutions such as Geoserver, QGIS Server and Openlayers. This is an example of the development of a large departmental spatial data infrastructure used by about 150 services in France.\n\nWe will present the software architecture and the strategies put in place to develop Carto 2 and we will describe the technical challenges we have faced during the development.",
        "description": ""
      },
      {
        "title": "EODAG (Earth Observation Data Access Gateway)",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Nowadays, we observe a rise in publicly accessible Earth Observation (EO) data. Together with it, there is more and more EO data providers, each potentially having a different data access policy. This difference is visible at various levels: in the data discovery (CSW, OpenSearch more or less custom, etc.), in the product access (object storage, downloads, direct file system access, etc.), in the storage structure and in the authentication mechanisms (OAUTH, JWT, basic auth,…). All these different technologies add a knowledge overhead on a user (end-user or application developer) wishing to take advantage of these data. EODAG was designed to solve this problem.\n\nEODAG (Earth Observation Data Access Gateway) is a command line tool and a Python package for searching and downloading remotely sensed images while offering a unified API for data access regardless of the data provider.\n\nIt gives you an easy way to access products from more than 10 providers, with more than 50 different product types (Sentinel 1, Sentinel 2, Sentinel 3, Landsat, etc.) that can be searched and downloaded.\n\nEODAG has the following primary features: search and download Earth Observation products from different providers with a unified API. It is both a Command Line Tool and a Python Package. It supports STAC API and Static STAC catalogs. It can run as a STAC API REST server to give access to a provider’s catalog. New providers can be added with a configuration file or by extending EODAG with plugins.\n\nEODAG ecosystem includes EODAG-Labextension, a Jupyterlab extension allowing to search and browse for remote sensed imagery directly from JupyterLab. It also includes EODAG-Cube for having a direct access to data and stream it to a Xarray Dataset.",
        "description": ""
      },
      {
        "title": "Geostack: a high performance geospatial processing, modelling and analysis framework",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Large geospatial data sets generated by modern remote sensing and environmental modelling provide new opportunities for analysts, scientists, and researchers. However, the size of these data sets can present challenges due to the computation and resource management required for analytics and processing. Current solutions for processing such data sets largely focus on horizontal scaling approaches on, for example, distributed systems such as the Cloud, without fully exploiting the opportunities offered by modern computing architecture. Furthermore, the variety of formats and types of geospatial data often result in complex processing workflows composed of multiple tools for reading and writing, transformation, processing, and resource management. We present an introduction, overview, and demonstration of the open-source Geostack framework (gitlab.com/geostack/library). This has been developed to help simplify many common operations, provide economy of code and to transparently take advantage of modern CPU/GPU hardware. We have aimed to provide three main routes to simplify and accelerate geospatial processing. These are: 1) a unified interface to read vector and raster data and interoperate between them, with no software dependencies for common geospatial data formats, 2) treatment of all data as objects independent of geospatial transforms, with transparent resource management through an underlying tile-based caching system, reprojection and interpolation carried out where needed, 3) extensive use of OpenCL to provide computational acceleration and automatic processing vectorisation on GPUs and multi-core CPUs as well as user-defined scripts to be executed over these objects. The framework also includes many common geospatial operations as well as several base geospatial solvers (including moving fronts, flow networks, particle modelling) accelerated using OpenCL. Geostack is a C++ API with Python bindings, the code examples and demonstrations are presented in Python. The Python bindings are available through conda and fully interoperable with common Python libraries including numpy, gdal, xarray, netcdf, geopandas and sqlite, allowing users to use as much or as little of the Geostack functionality as required. We present demonstrations of several common geospatial tasks with benchmark comparisons to alternate workflows.",
        "description": ""
      },
      {
        "title": "Geoinformation Production Management System - A QGIS based open-source solution to nationwide geospatial data production management",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "In the past seven years, the Brazilian Army Geographic Service has been putting effort into migrating the entire geospatial production chain into open-source. The first step was the use of PostgreSQL + PostGIS as the primary data storage solution, then the use of open standards such as OGC WMS and WFS for data sharing, then the use of QGIS as the main software for data acquisition, and finally, the development of DSGTools, QGIS plugin with several tools for data quality control and cartographic finishing.\n\nThe Geoinformation Production Management System (GPMS) comes as the latest addition to our open-source stack. It has two main goals: to manage the distribution of jobs between the staff automatically; and standardize workflows, layers, styles, tools, processes and parameters of each job. \n\nFor the first goal, the manager can create a profile for each team member, setting which parts of the workflow they are qualified to execute. When the user asks for a job, the system matches his qualifications with the available jobs on the current project and gives him the highest priority job. This, combined with the visualization of the live production status on QGIS, helps the manager improve resource allocation's decision management.\n\nFor the second goal, the manager can configure in detail each workflow, setting which database the job will be executed on, which layers the user needs to access, which styles are available for each layer, which QGIS Processes the user should run with pre-set parameters, which resources the user has access to, such as imagery and DEMs. GPMS also does the permission control in the PostgreSQL database based on the job the user currently is executing and allows spatial filtering, so the user only can work in a spatially defined subset of the data. In this way, when a user receives a job, he has everything he needs to complete it.\n\nAll the jobs information, such as user, start timestamp, end timestamp, and job parameters, are stored in the system, allowing the automatic generation of metadata compatible with Brazilian Standards and visualizations of the project's current state.\n\nAs the Brazilian Army Geographic Service has 5 Centers of Geoinformation, the use of a production management system helped with the standardization of procedures, since a standard configuration can be defined and replicated between centers to use the same consistent workflow, with the same database schema, same layers per job, same QGIS Styles, processes, parameters and so on.\n\nThe GPMS is available on Github on https://github.com/1cgeo/sap as a Node web service and requires a QGIS plugin for the client (https://github.com/1cgeo/Ferramentas_Producao) and a QGIS plugin for the manager (https://github.com/1cgeo/Ferramentas_Gerencia).",
        "description": ""
      },
      {
        "title": "On the road to 3D semantic segmentation",
        "type": "General session talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "A few years after the democratization of semantic segmentation advanced techniques in the 2D\ncontext (imagery), there are more and more initiatives for exploiting such algorithms with 3D\ndatasets. The context appears favorable: public and private initiatives are arising in terms of\nmassive 3D dataset collection, hence a huge amount of 3D point cloud data will become available in\na near future. As an example, the french cartography institute (IGN) is currently targetting a\nfull-coverage of the country with LIDAR data in the five next years.\n\nConsidering 3D point clouds is a really challenging task regarding semantic segmentation. Whilst\nthis data format allows to represent a scene with a high level of details, the unordered and\nunstructured nature of the data makes the standard convolution neural network approach\nineffective. However other deep learning algorithms exist to cope with these\ncharacteristics. Depending on the desired accuracy and the labelled data availability, some\n\"softer\" machine learning approaches may also complete the toolbox.\n\nLeveraging georeferenced data in such a context may be an interesting avenue in order to improve\nthe algorithm performances. In any case, these fairly innovative solutions can be applied in some\ngeographical use cases, e.g. cartography with street-views, Building Information Modelling (BIM),\n...\n\nThis presentation will provide some insights on these 3D semantic segmentation related topics:\n\n* the 3D semantic segmentation state-of-the-art will be flied over;\n\n* the BIM use case will be detailed through the presentation of an ongoing R&D project carried out\nby Bimadata.io, Oslandia and the LIRIS lab (CNRS);\n\n* the geo3dfeatures project (https://gitlab.com/Oslandia/geo3dfeatures) will be showcased in\norder to illustrate what the seminal component of a 3D point cloud segmentation software could\nbe.",
        "description": ""
      },
      {
        "title": "NASA’s Transform to Open Science (TOPS) Initiative",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Over the past few years we have learned some valuable lessons as a scientific community; we are stronger together; we must uplift one another in order to achieve scientific progress; and the best solutions are found when we work collaboratively. These lessons will not spread unless we begin to address the hesitancy in transitioning towards sharing data and results and the lack of applied experience with tools which make open collaboration easier. Transform to Open Science (TOPS) is a new NASA Science Mission Directorate initiative designed to spark a cultural shift to collaborative, inclusive science.\n\nNASA defines open science as a collaborative culture enabled by technology that empowers the open sharing of data, information, and knowledge within the scientific community and the wider public to accelerate scientific research and understanding. A system based on open science aims to make the scientific process as transparent (or open) as possible by making all elements of a claimed discovery readily accessible, which enables results to be repeated and validated.\n\nOut of this open science concept, an evolving paradigm called open-source science is emerging. Open-source science accelerates discovery by conducting science openly from project initiation through implementation. The result is the inclusion of a wider, more diverse community in the scientific process as close to the start of research activities as possible. This increased level of commitment to conducting the full research process openly and without restriction enhances transparency and reproducibility, which engenders trust in the scientific process. It also represents a cultural shift that encourages collaboration and participation among practitioners of diverse backgrounds, including scientific discipline, gender, ethnicity, and expertise.\n\nWithin NASA’s Open-source Science Initiative, TOPS provides the visibility, advocacy, and community resources to support and enable the shift to open-source science; NASA is designating 2023 as the Year of Open Science, a global community initiative to spark change and inspire open science engagement through events and activities that will shift the current paradigm. TOPS seeks to enable science that is accessible, inclusive and reproducible via four activities: providing high-level visibility to open science activities at NASA and beyond; developing an open science curriculum through a collaborative, community-driven model that will be fully and openly available virtually and at conferences; incentivizing open science activities via mechanisms such as partnerships, badging programs, and awards; and instigating a transformation of NASA’s culture to better recognize and reward open science activities.\n\nTOPS advocates for open science as it builds trust, advances understanding, and ultimately leads to new knowledge production and important new discoveries. To enable open science, the TOPS is developing resources and activities that will support and enable the scientific community to move forward. At FOSS4G we hope to share NASA's vision for open-source science, and invite this community to join us in the 2023 Year of Open Science as we challenge the old norms of scientific research and create a community which designs its scientific endeavors to be open from the start.",
        "description": ""
      },
      {
        "title": "OpenStreetMap Utopia: Exceptional data quality",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Many definitions and criteria available, explored and investigated for OpenStreetMap (OSM) data quality are pertinent to certain datasets, which are usually authoritative or controlled datasets. Several studies have used these measures successfully for OSM but this also makes the quality of the OSM dependent on the development of these datasets. This dependency exacerbates when such datasets are not available or condition to ephemerality. Unavailability and temporal non-reliability of controlled datasets are some of the reasons OSM data quality is still a concern. OSM is an ever-evolving digital space that has complex interconnectedness with physical space. To understand this interconnectedness we have to go back to the fundamentals and understand the genealogy of OSM with different qualitative and quantitative lenses. In this talk, I would like to present a research vision on how we can apply these quali-quantitative lenses to conceptualize the data quality of OSM to reduce the dependency on controlled or authoritative datasets. I will shed light on the layered conceptualization of OSM data quality with respect to different case study areas and projects that I am currently exploring. Layered conceptualization is based on the hypothesis that OSM data should be intertwined with the region and context-specific considerations. Understanding this intertwining will result in a better understanding of OSM data ethics and how it is related to the current data quality criteria already existing for OSM and other geo datasets. The aim is not to expunge the current data quality initiatives but to acknowledge and understand the exceptionality of OSM and its data quality. This talk will be a progress talk for the OSM Utopia project.",
        "description": ""
      },
      {
        "title": "State of Pghydro Project: PostgreSQL-PostGIS extension for Hydrographic Applications",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The PostgreSQL extension for Hydrographic Applications Project (PgHydro) represents the first innovation of intelligence as an extension on spatial database management systems for use in water resources management that uses the subcatchment network model and the logic elements present in the Pfafstetter basin coding system. PgHydro aims for an add-on implementation on a spatial database management system performed by a series of tables, queries, functions or views that can be used individually to assist in water resources decision making. These objects are the hydrography core of the intelligence system developed using free open source software that can be used by any person who deals with water resources management. To this end, this new conceptual model was implemented in the object-relational spatial database management system PostgreSQL/PostGIS respecting the integrity constraints related to the geometry of the mapped objects. These user-defined constraints respect the logical objects based on the Pfafstetter basin coding system and integrity constraints linked to the spatial relationships between objects, which follow the ISO SQL/MM specifications. The main advantage of using the pghydro extension is the possibility to process large datasets and complex queries using a more simple hydrography model and the tools and languages already available in spatial database management systems that work as a framework for the future development of new extensions related to water resources.\nThe pghydro functionalities can be run using a GUI developed in a QGIS plugin called PgHydro Tools. After the physical implementation of the pgHydro Scheme in the spatial database management system, the construction of the Pfafsteter hydrography dataset is started using the hydrography objects that make up the pgHydro Tools. The construction of this base is divided into seven stages: 1) Creation of the spatial database and creation of the pghydro extension; 2) insertion of the drainage lines and the drainage areas in the spatial database; 3) verification of the consistency of the drainage network geometries and topologies; 4) verification of the consistency of the drainage areas geometries and topologies; 5) verification of the consistency of topology between the drainage network and the drainage areas; 6) Pfafstetter basin coding and other information, finally; 7) export of the final Pfafstetter hydrography dataset. Optional steps are the systematization of river names and the multiuser edition management.\nThe pghydro project is officially and widely used by the National Water and Sanitation Agency of Brazil as a reference for the Water Resources Management of Brazil.",
        "description": ""
      },
      {
        "title": "From CAD to GIS: where are we?",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "From CAD to GIS: where are we?\n\nCAD and GIS are often considered as contradictory. However, we are regularly asked to help to perform a transition from CAD to GIS.\n\nWe would like to share with you an overview of situations where this transition succeeded. For this, we will go through issues and solutions we provided.\n\nWe will talk about the differences and similarities between those two worlds.\nWe will review the existing file formats.\nWe will make a list of available opensource tools that can facilitate this transition: gdal, QGIS and its plugins.\nWe will show that almost anything needed can be realized within QGIS today.\nFinally, we will introduce a verification tool that can validate data in order to facilitate their integration: QompliGIS.\n\nNot only will we talk about theory, but we will also give each case a concrete example of the work done for it, since several years.",
        "description": ""
      },
      {
        "title": "Location-based scheduling of VRP tasks using pg_scheduleserv",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "**Vehicle Routing Problems (VRP)** are a class of combinatorial optimization problems whose purpose is to design an optimal route for a fleet of vehicles, serving a set of customers, along with satisfying certain constraints. They are NP-complete (non-deterministic polynomial) and have been a challenging problem ever since.\n\n**[pg_scheduleserv](https://github.com/Georepublic/pg_scheduleserv) offers an API-based solution** for solving scheduling problems without the need to be an expert in algorithms or databases. It is a RESTful API written in Go, built on top of [vrpRouting](https://github.com/pgRouting/vrprouting). It uses [VROOM](https://github.com/VROOM-Project/vroom) as the schedule optimization engine to increase operational efficiency in location-based processes.\n\npg_scheduleserv lets you create jobs, shipments, or vehicles in the database by ***posting JSON objects*** through HTTP requests. It enables you to obtain the optimized schedule either as a JSON object or in ***iCal format*** so that the generated results can be directly used in your calendar applications.\n\npg_scheduleserv is easy to use and set up. By providing an API-based solution, it can be integrated into existing applications. **In this talk, we present pg_scheduleserv along with a simple demo application to demonstrate its efficiency and simplicity of usage.**",
        "description": ""
      },
      {
        "title": "ogc-client: OGC protocols made fun!",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "**ogc-client** is a library written in Javascript for interacting with OGC services. It is simple to use and provides a nice and consistent way to operate OGC protocols. The list of supported protocols now includes WMS and WFS and will hopefully soon expand to others including WMTS and OGC APIs, of course! Also **ogc-client** gives you great performance out of the box by keeping things in cache and using multi-threading.\nThe library is currently still quite small in scope but it has proven to be very efficient and helped us lower the complexity in various codebases. Great success!\nIf you are one of the poor souls having to reimplement for the zillionth time a GetCapabilities parser, or if you have given up trying to guess a data schema from a WFS service, this library is for you! Come join us for this lightning talk to learn more!",
        "description": ""
      },
      {
        "title": "Open source at the European Commission",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "This talk by Thomas Gageik, Director Digital Business Solutions (DIGIT.B) at the European Commission will get you up to speed on the most recent actions to encourage free and open source in and around the Commission. This is an introduction to the session organised by the EC, and we will show you what has changed since the adoption of the reinvigorated open source strategy in October 2020. \nOur topics include: what have we done to make it easier for the Commission to share software as open source, and how can we contribute to existing free/open source software tools. We will show you how the Commission is helping to strengthen the security of open source software, and how we are networking with other organisations to help open source to progress in public services across the EU. \n\nThe talk will also introduce you to the open source programme office. The EC OSPO, created in 2020, is here to help Commission projects with free/open source.",
        "description": ""
      },
      {
        "title": "Usage and contribution of FOSS at GISCO",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "GISCO, the ‘Geographical Information System of the COmmission’, is a permanent service of Eurostat that fulfils the requirements of both Eurostat and the European Commission for geographic information and related services at European Union (EU), Member State and regional levels. These services are also provided to European citizens at large. GISCO’s goal is to promote and stimulate the use of geographic information within the European Statistical System and the European Commission.\nOne of the main lessons learned over the last years is not only to provide ‘conventional’ GIS datasets, but add a variety of distribution channels like Application Programming Interfaces (APIs), Linked Open Data (LOD) plus Human Friendly Interfaces on top. API’s for example simplify software development and innovation by enabling applications to exchange data and functionality easily and securely into a digital ecosystem. Additionally, the implementation of API’s contributes to: a) an open government approach to modernise public administration b) a modernised use of the European Interoperability framework or c) the application of the Once Only Principle. The talk will describe some of GISCOs API’s supporting European Institutions in their daily work as well as the public. For that, we are using FOSS tools in production environments. Besides, GISCO team members develop or contribute to a wide variety of software tools (e.g. eurostat-map.js, gridviz, IMAGE tool, diff and generalization tool) which will be presented for further use by the FOSS4G community.",
        "description": ""
      },
      {
        "title": "A vision for INSPIRE: from a traditional SDI to a self-sustainable data ecosystem",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "Published in 2007, the INSPIRE Directive has established a pan-European Spatial Data Infrastructure (SDI) to support European Union (EU) policies related to or having an impact on the environment. The Directive requires Member States public organisations to make geospatial datasets in scope (i.e. belonging to 34 cross-sector categories known as data themes) interoperable, discoverable and accessible through view and download services. Fifteen years after the entry into force of the Directive, we assess the state of play, reflect on the lessons learned and, leveraging on these while also considering the current policy and technological context, elaborate a vision for the future evolution.\nThrough its Geoportal, which regularly harvests the EU Member States national catalogues, the INSPIRE infrastructure currently provides access to approximately 90 thousand datasets. The amount and update of those datasets is steadily increasing as is the fraction of datasets whose metadata, data models and view/download services are compliant to the legal requirements of the Directive. The INSPIRE infrastructure is currently based on three so-called central components, which in turn are implementations of reusable and mature open source software solutions: the INSPIRE Reference Validator makes use of the ETF testing framework, the INSPIRE Registry is based on the Re3gistry software (included in the OSGeo Live since 2021) and the INSPIRE Geoportal is currently being migrated to GeoNetwork  . INSPIRE has also played a key standardisation role in Europe by fully promoting and relying on open standards, mainly by ISO and OGC. Finally, an active and engaged community of stakeholders, meeting at the annual INSPIRE Conference and other related ad-hoc events, has highly favoured the policy and technological development.\nDespite many pros, lessons learned from INSPIRE also show some cons. These include e.g. overspecification in legislation (often leading to extensions to existing standards) which still limit implementation, and the lack of a common approach to data licensing. In addition, the current technological landscape is very different from the one from the INSPIRE dawn. New data sources (Internet of Things, citizen-generated and Earth Observation data, research data and data owned by businesses), new agile standards (e.g. OGC APIs for data sharing and modern standards for data encoding) and novel architectures (cloud, edge and fog computing) are creating an opportunity that INSPIRE shall leverage to remain relevant and fit-for-purpose. In parallel, driven by the recent European Strategy for Data, the current European policy context and related legislative instruments are strongly pushing for an increased, better and fairer use of all available data for the benefit of European economy and society.\nWithin this context, the talk will illustrate our vision to streamline and simplify the technological and organisational structure of INSPIRE towards a data-driven and self-sustainable ecosystem. We will mainly reflect on the key role played by open source software, open standards and open licenses, and on the need to redefine the governance of the infrastructure through the increasing involvement of open source communities, including OSGeo as a strategic partner.",
        "description": ""
      },
      {
        "title": "UN Open GIS Initiative",
        "type": "Keynote",
        "track": "Transition to FOSS4G",
        "abstract": "UN Open GIS Initiative, established in March 2016, is to identify and develop an Open Source GIS bundle that meets the requirements of UN operations, taking full advantage of the expertise of contributing partners (Member States, international organizations, academia, NGO’s and private sector).\n\nGeospatial Information Systems (GIS) has been played a substantial role in providing timely and effective geospatial information products (maps and dynamic tools) to ensure the United Nations operations are equipped with suitable information to support the UN mandates through informed planning and decision-making processes. The UN has been using proprietary GIS software for the past two decades. The rapid growth and development of open-source GIS solutions present the technological potential, operational flexibility and financial benefits as well as easy to access for UN operational partners and host nations. \n\nIn view of complexity and variety of UN operational demands and the outcome and lessons learned from the UN Open GIS Initiative, it is identified to develop a hybrid GIS platform that the users should be able to access the most suitable solutions to fulfil the operational demands in flexible and cost effective manner whether the solutions are open source or proprietary, combination of both and/or complement each other. The hybrid model lets coexist two software stacks, one open source based, the other proprietary, which renders different services to end users and applications. \n\nSignificant progress has been made so far in developing open source-based GIS solutions such as Hybrid Geospatial Database, GeoPortal, Analytical models/ applications, Data collection and Optimized/innovated applications for harmonizing open source technology with proprietary as well as open GIS trainings to the UN staff for smooth transition from proprietary to hybrid GIS platform technology.\n\nIn particular, it will provide an overall update on what has been achieved by the UN Open GIS Initiative during the past year, such as hybrid GIS architecture, mobile GIS solution, story map project, field application of OpenDroneMap and UN Vector Tile toolkit as well as the capacity building activities.",
        "description": ""
      },
      {
        "title": "Revamped INSPIRE Geoportal - Cooking the next generation of spatial data catalogues",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "In force since 2007, the INSPIRE Directive has established a European Spatial Data Infrastructure to support European Union (EU) policies relevant to the environment. The INSPIRE Geoportal (https://inspire-geoportal.ec.europa.eu) constitutes its main component, being the central point of access to all datasets published by EU Member States falling under the scope of the Directive. Using the INSPIRE Geoportal, users can search for, access, visualize and download datasets published by more than 7000 data providers from across Europe.\nIn line with the open source strategy of the European Commission and the ambition towards a sustainable evolution of the INSPIRE infrastructure based on open source components, since 2021 the INSPIRE Geoportal is has been revamped by using cutting-edge, open source applications and open standards, while redesigning the way in which information and services are offered to users. \nThe process comprises deep changes in the Geoportal backend, totally renovating the underlying catalogue application: management interface, powerful harvesting engine, set of metadata, data and service linking tests, search engine, automatic metadata translation capabilities, containerization and deployment in a cloud environment. In addition, a new frontend (user interface) is integrated with the mentioned backend using APIs, making both layers more independent in terms of technological stack and update cycles.  \nThe application selected for achieving these goals is GeoNetwork opensource, currently constituting the catalogue choice of around 80% of Member States national geospatial data portals in the EU. Since its inception in 2001, it has been developed with a strong focus on international standards and many new features have been added over the years.\nDuring the last year, the GeoCat staff and the INSPIRE team at the European Commission’s Joint Research Centre have closely collaborated in the development of a new powerful and versatile system for delivering the revamped INSPIRE Geoportal, contributing the improvements – to the maximum possible extent – to the core of GeoNetwork to maximize their exploitation by the geospatial community. \nMore in detail, the system encompasses a high-performance harvesting system based on a microservices, including link validation and reporting functionality. All these pieces have been developed as separate components, which can be scaled independently from other components within a GeoNetwork-based infrastructure. Performance and compliance to the INSPIRE Technical Guidelines formed part of the key requirements of the work. As a result, GeoNetwork capabilities for supporting the INSPIRE Directive will be substantially improved and completed in the toolkit. This, in turn, will also improve the transparency and collaboration between the national authorities and the European level.\nAt the end of this transitional phase for the INSPIRE Geoportal, foreseen by mid-2022, the project will deliver tangible benefits. Firstly, by providing EU Member States with an up-to-date and transparent toolkit for managing their geospatial metadata, data and services in compliance to the INSPIRE Directive monitoring and reporting obligations. Secondly, by making a direct valuable contribution to the core of GeoNetwork and to the whole open source geospatial community.",
        "description": ""
      },
      {
        "title": "FAIR-ization of INSPIRE datasets",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "Belgian federal authorities are working on PSI/INSPIRE conversion tool. We have produced an enhanced DCAT AP 2.0 profile. We have proposed to use ATOMFeed to instantiate dcat:Distribution classes because of their semantic completeness. We have tried to keep most of the INSPIRE metadata elements in order to keep the work that has been done for some years..\n\n\nNow we are working on its implementation through GeoNetwork 4.x microservices that would provide a consistent DCAT AP RDF/XML with many languages. By doing this we consider that our datasets will be more accessible through many platforms and open data portals and will become real FAIR data. \n\nThis work is the result of the strong collaboration between federal belgian authorities (e.g. Cadaster, National Mapping Agency, Office of federal statistics, ...).Moreover we are involving regional authorities in order to reach a certain harmonization. Now we would like to share these developments with the opensource community. You can find more information here https://github.com/belgif/inspire-dcat.",
        "description": ""
      },
      {
        "title": "MobiDataLab - Labs for prototyping future mobility data sharing solutions",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "MobiDataLab is the EU-funded lab for prototyping new mobility data sharing solutions.\nOur aim is to foster data sharing in the transport sector, providing mobility organising authorities with recommendations on how to improve the value of their data, contributing to the development of open tools in the cloud, and organising hackathons aiming to find innovative solutions to concrete mobility problems.\n\nThe project consists of following main pillars:\n\n1) Open Knowledge Base\n... a portal about open mobility data which provides informations about about\npractices and solutions related to legal and regulatory (s.a. licenses), governance,\ndata privacy, technical standards (for data interoperability and accessibility),\nand challenges for actors in the mobility domain. \n\n2) Transport Cloud\n... a cloud-based prototype platform for sharing mobility data. It facilitate users by\nseveral tool components to find, use and interact with mobility data in an open, interoperable\nand privacy-preserving way.\n\n3) Living and Virtual Labs\n... are the environments for the project to interact with the reference group (mobility data providers and users),\nb2b and endusers (s.a. data innovators, solution providers and further stakeholders in the mobility domain) to get\nfeedback on challenges and missing pieces in the mobility data and services assets. A set of mobility use-cases,\nset-up by the project stakeholders and the reference group will help to trigger practical execution, innovation,\nand further ideas within the labs.\n        \n4) Socio-economic impact\n... identifies the the current best practices in data sharing, analyses the market potential and elaborates new\ndata sharing services and business models on that.\n\nWith the heterogeneous experts project group, we are facing the challenge of mobility data sharing from different perspectives - research, privacy, data, mobility solutions, open data, services, ... .\nA close work with a large reference group (which is representing several mobility data providers - e.g. from the public and private sector and actors, s.a. start-up communities) and the implementation of virtual and living labs allows to get early real world feedback and help to identify challenges in interoperability and missing standards as well as findable and available data, conflicting licenses, accessibility and usability of data and services.\n\nSince the project started in February 2022, we will present the mid-term achievments and\nprovide an outlook on the second part of the project.\n\nFurther information on the project is available via https://mobidatalab.eu\n\nMobiDataLab is funded by the EU under the H2020 Research and Innovation Programme (grant agreement No 101006879).",
        "description": ""
      },
      {
        "title": "Group photo",
        "type": "General session talk",
        "track": "Unknown",
        "abstract": "The photo group for FOSS4G 2022",
        "description": ""
      },
      {
        "title": "Identifying new conservation areas: a web multi-criteria approach using Earth Observation and other spatial information",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "Today Protected Areas only partly cover important sites for biodiversity and are not yet fully ecologically representative and effectively or equitably managed. Improvement of the existing networks and further expansion of conservation areas will therefore require well-defined baselines that are comparable across countries for actions prioritisation.\n In recent years the availability of new earth observation imagery and advances in analysis and processing have significantly improved our capabilities in terms of mapping and monitoring biodiversity variables and ecosystem services. This event will introduce the Biodiversity Analyst, a web GIS tool, developed by the European Commission - Joint Research Centre, for identifying areas of potentially high conservation value based on the available datasets such as species distribution, ecosystems services and natural state.\nThe aim of the Biodiversity Analyst is to provide decision-makers with means to visualise and interact with the above datasets which are considered key for biodiversity conservation while allowing them at the same time to test different weighting schemes in terms of prioritisation to identify so-called \"biodiversity hot-spots\".",
        "description": ""
      },
      {
        "title": "openEO: Open Science for Earth Observation Research",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "The open standards, open source geospatial and open science communities still have a very limited answer to the question how researchers active in applied domains such as agriculture, ecology, hydrology, oceanography or land use planning can benefit from the large amounts of open Earth Observation (EO) data currently available. Solutions are very much tied to platforms operated and controlled by big tech (Earth Engine, Planetary Computer), particular programming languages, software stacks and/or file formats (xarray, Pangeo, ODC, GeoPySpark/GeoTrellis). The openEO initiative provides an API and a set of processes that separate the “what” from the “how”: users specify what they want to compute, and back-end processing engines decide how to do this. The openEO API is OpenAPI compliant, and has client interfaces for Python, R, and JavaScript, and in addition graphical user interfaces running in the browser or in QGIS. The underlying data model is that of a data cube view: image collections or vector data may be stored as they are, but are analysed as if they were laid out as a raster or vector data cube, e.g. for raster with dimensions x, y, band and time, or for vector with dimensions geometry, band and time. Because openEO assumes that imagery is described as STAC collections and the implementation is composed of open source components, it is relatively easy to set it up and compute on infrastructure where imagery is available through a STAC interface. Having a single interface to carry out computations on back-ends with different architecture makes it possible to compare results across implementations, to verify that EO processing is reproducible. So far, over 100 processes have been defined, and user-defined functions written in Python or R extend this ad infinitum. openEO was initially developed during a H2020 project (2017-2020). It is currently continued with ESA funding that has resulted in the “openEO Platform”, an implementation run by VITO and EODC where the general public can use the openEO interface for large scale computations. Several upcoming Horizon Europe projects will further support continued development of the API and openEO software ecosystem of clients and back-ends. Since the initiative is designed to be an open science, all users and developers are invited to engage. We will present the current state of the openEO ecosystem and give an outlook to forthcoming developments.",
        "description": ""
      },
      {
        "title": "EO Open Science Catalogue initiative by ESA",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "To enable sustainable and impactful Open Science in the long-term, ESA Earth Observation looks to design and implement a comprehensive Open Science framework, which includes a dedicated set of integrated tools and common practices for effective scientific data management, seeking to support Open Innovation, advance Science and increase community participation. The framework will build on and advance existing Open Science elements and will develop new capabilities to achieve the ambitions and vision set forth in the 2025 Agenda, supporting the European Green Deal. The four main pillars of the initiative are: i) EO Digital Platforms, interoperability and standardisation, ii) Accessible and Reproducible EO Science, iii) Inclusive and collaborative research and iv) Strategic Partnerships. Contributing to the second pillar, ESA is developing an EO Open Science Catalogue tool to enhance the discoverability and use of products, data and knowledge resulting from Scientific Earth Observation exploitation studies. Adhering by design to the \"FAIR\" (findable, accessible, interoperable, reproducible/reusable) principles, the Open Science Catalogue aims to support better knowledge discovery and innovation, and facilitate data and knowledge integration and reuse by the scientific community. \n\nThe Open Science Catalogue is based upon the [EO Exploitation Platform Common Architecture](https://eo4society.esa.int/2022/01/26/interoperability-sharing-your-application-where-the-data-sit/) (EOEPCA) and shares its basic Open Source components, but extends it with additional functionalities:\n - The Static Catalogue is a hosted STAC Catalogue, comprised of static Catalogue, Collection, and Items that represent the Themes, Variables, Projects, and Products\n - The Open Science Catalogue Frontend is a Vue.js based client application, that allows the efficient browsing of the Open Science Catalogue\n - The Backend API allows users to make submissions to create, update, and delete Themes, Variables, Projects, and Products. These submissions are then handled as GitHub Pull Requests, where they can be further reviewed, discussed, and finally accepted or denied.\n\nThe Open Science Catalog makes use of various geospatial Open Source technologies such as pycsw, PySTAC, and OpenLayers.\n\nIn this presentation we will review the EO Open Science Catalogue architecture, technology stack, and how this tool can be used to discover and publish Earth System Science products from ESA activities. We'll also look at future evolutions of the product and how it contributes to the overall ESA EO Open Science Framework.",
        "description": ""
      },
      {
        "title": "FOSS4G in the Solar System",
        "type": "Keynote",
        "track": "Unknown",
        "abstract": "It is well-known that Free Open Source Software is part of Space and Planetary Exploration, and the latest generation of rovers and drones on Mars embed FOSS components and frameworks.  But what about Free Open Source for Geospatial software and data access and availability?  We will travel the timeline of planetary cartography, from the first steps of remote and direct observation of the bodies of our Solar System to the era when Geographic Information Systems spread in Planetary Science and  FOSS4G starts to play an essential role in studies and missions to environments beyond planet Earth.",
        "description": ""
      },
      {
        "title": "The Copernicus Data Store (CopDS) - a reimagining of the Copernicus Climate Change Service (C3S) Climate Data Store (CDS)",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "The Copernicus Climate Change Service (C3S) Climate Data Store (CDS) is a single point of access to a wide range of free, quality-assured climate data, along with a suite of tools for performing cloud-based analysis and visualisation of very large datasets. Launched in 2018, the CDS provides over 100 datasets and 30 interactive applications for a global, interdisciplinary and intersectoral audience of over 100,000 users.\n\nThe Copernicus Data Store (CopDS) project aims to reimagine the CDS, making use of modern technologies and knowledge gained during the development of the existing system to expand and streamline its functionalities and improve its performance and scalability. We present a high-level blueprint of the in-development CopDS, with an emphasis on how we plan to overcome the limitations of the original CDS. We explore our plans for the development of a new suite of open-source Python tools for performing retrieval, analysis and visualisation of climate and atmospheric data under the CopDS project, along with our plans for offering free cloud-based infrastructure for processing and visualising very large datasets through an easy-to-use Python web interface. We also discuss the development of tools for transforming simple Python code into high-quality web applications for exploring CopDS climate and atmospheric datasets, providing tools for interactive mapping, graphical user interfaces and a results cache for responsiveness.",
        "description": ""
      },
      {
        "title": "Towards open and accessible weather forecast data",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "The European Centre for Medium-Range Weather Forecasts (ECMWF) is an independent intergovernmental organisation which is producing and disseminating numerical weather and environmental predictions to its users in national meteorological services as well as commercial customers. As of recently, ECMWF started the move towards serving data to users beyond operational forecasters in Member states and commercial customers for a charge, by adopting an open data policy which will be implemented in phases from 2020 to 2025. The first phase included opening hundreds of [web forecast charts](https://apps.ecmwf.int/webapps/opencharts) and making archived data available under a Creative Commons (CC BY 4.0) open licence in 2020. The next step was in January 2022 when the production of [open subset of real time medium range forecast](https://www.ecmwf.int/en/forecasts/datasets/open-data) began. \n\nThis phased move towards free and open data aims to support creativity and innovation in the field of scientific research as well as weather applications. It also represents a step towards more reproducible open science. However this can not be achieved by only opening the real time data. The users need to be able to find and easily use the data and integrate it into their own research work or application workflows. Reliable access to the data is achieved by making it available both through ECMWF https service and via the Microsoft Azure cloud, where the archived data is kept as well.\n\nIn order for the data to be more FAIR (Findable, Accessible, Interoperable and Reusable), additional development work is being done. This work includes the design of an [API](https://github.com/ecmwf/ecmwf-opendata) to easily download the geospatial data, and the development of open source Python libraries to [process](https://github.com/ecmwf/ecmwf-data) and [visualise](https://github.com/ecmwf/magpye) it. These open source libraries make use of open geospatial software, such as proj to deal with different projections. To present these new tools and help users understand how to retrieve and process ECMWF data, [a set of Jupyter notebooks](https://github.com/ecmwf/notebook-examples/tree/master/opencharts) was created, each of them reproducing one open weather forecast chart from the downloading the data to the visualisation. \n\nThis talk will give a short overview of which data is available in the open data set, and will then focus on the software and Jupyter notebooks development.",
        "description": ""
      },
      {
        "title": "10 years of open-source software in emergency management: the case of the European Flood Awareness Service",
        "type": "General session talk",
        "track": "A European approach to geospatial open source",
        "abstract": "The European Flood Awareness System and the Global Flood Awareness System (EFAS and GloFAS), are the two Early Warning Service for floods part of the Copernicus Emergency Management Service (CEMS), operated by the EU Joint Research Centre (JRC). EFAS and GloFAS aims to complement national and regional service by providing medium-range flood forecasts and hydrological outlooks for large, transboundary rivers. Data and products are accessible to eligible users through the Climate Data Store and dedicated web interfaces. ECMWF, having the role of the computational centre within CEMS, is responsible for running the forecasts and the post-processing, on top of co-developing and hosting the EFAS and GloFAS information systems.\n\nThese two information systems consist on back-end/front-end web services based on OGC standards and open-source software. As it is often the case, a web-based mapviewer allows to display different layers, produced by a WMS back-end. These layers are the graphical representation of the output of the hydrological models and meteorological observations, like flood probability, soil moisture, return period, observed precipitation etc. For most layers a new forecast is produced every 12 hours for EFAS and every 24 hours for GloFAS.\n\nUnlike many similar services, however, the aim of EFAS and GloFAS is not only to offer the latest forecasts or the latest observations but also to browse through data from previous days, so that older forecasts can be compared with actual observed events. This inherently means supporting the time dimension within the WMS standard, and managing large quantity of data that accumulates every day. In the case of EFAS, for example, an additional 1.5 Gb of data is produced twice a day.\n\nIt also means handling the inevitable changes in data formats and structures that arise as the service grows and new features are added, without breaking backward compatibility. New layers are added, old layers are removed, changes in the geographical domain or the projection for a certain layer must be supported from a certain date onward, etc. Not to mention increasing the number of forecast cycles from one per day to two or more.\n\nTo make matters worse, data access must be restricted on both front-end and back-end based on a matrix of user privileges, requested product and requested date. For example some layers are offered to all users with no time restrictions, while others are restricted to some users for the latest 30 days, and freely accessible to all users for dates older than 30 days ago.\n\nIn this talk we describe the challenges of developing and operating an authentication-aware web service heavily based on large geospatial datasets with a strong diachronic component.",
        "description": ""
      },
      {
        "title": "State of GRASS GIS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The new GRASS GIS version 8.2 is a special edition including all new features developed during Google Summer of Code 2021. One of the enhancements is the parallelization of several raster modules by means of OpenMP, an implementation of multithreading to speed up massive data processing. Another exciting new feature is much improved, the Jupyter notebook support. Here, a new python package (grass.jupyter) is available which allows to interactively visualise maps and time series given the integration with [folium](https://github.com/python-visualization/folium). \nThe graphical user interface in version 8.0 introduced faster and more streamlined startup without a need for a welcome screen. For even more convenience, version 8.2 adds an experimental single window layout with familiar look-and-feel.\nRelated to raster data, a new metadata class called semantic labels can now be added to raster maps. Examples of semantic labels are aerial or satellite spectral bands, dataset names in remote sensing products (ndvi, evi, lst, etc), or any custom names.\nAt community level, we have developed a student grant program and, thanks to the move to GitHub, we have welcomed numerous new contributors.",
        "description": ""
      },
      {
        "title": "Redesigning GRASS GIS graphical user interface in a community-driven way",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Aside from many new features, GRASS GIS 8 brings an improved graphical user interface focused on better user experience. Based on a broad community discussion involving several surveys and test sessions, we developed a new startup mechanism helping the users understand the data hierarchy and guiding them in their first steps. In addition, our surveys helped to identify a number of opportunities for improvements, including a need for a Single-Window mode that could fully replace the traditional Multi-Window GUI that has been in GRASS since the first GUI version in 1999. Therefore, during the GSoC 2021 project, the first steps towards the Single-Window GUI were established, eventually leading to the friedlier GRASS GUI in version 8.2. Come and listen to the presentation describing how a community-driven approach helped to steer the development direction of the GRASS graphical user interface to satisfy both GIS beginners and advanced users. You can also look forward to the brand-new screenshots of the GUI in version 8.2 that might eventually inspire you to try GRASS on your own.",
        "description": ""
      },
      {
        "title": "Take-Home Messages from Adding Code Quality Measures to GRASS GIS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The message is not surprising: You should quality check your code, too, even if you are writing a small script for your own needs! However, maybe you wondered if all the warning messages are relevant to you or got discouraged after getting a flood of messages from tools like Pylint. Perhaps you were even annoyed by it. This talk will help you get motivated and get started and how to automate that with continuous integration tools such as GitHub Actions.\n\nIn this talk, I will share my experience with adding various code and non-code checks to GRASS GIS which is primarily written in C, C++, and Python. Checking a mixed code base with over 30 years of development is not easy, but not impossible. The talk will cover code quality measures in GRASS GIS such as tests, Pylint, Black, GCC, CodeQL, and Super-Linter and how this compares to my experience with new and small organizational repositories.",
        "description": ""
      },
      {
        "title": "Orfeo ToolBox: open source processing of remote sensing images",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Orfeo Toolbox (OTB) is a free and open-source remote sensing software. It is available on multiple platforms, Linux, Windows and MacOs, and was developed primarily by CNES (French Space Agency) and CS Group in the frame of the development of the ORFEO program (French and Italian support program for Pleiades and Cosmo-Skymed).  \n\nOTB can process large images thanks to its built-in streaming and multithreading mechanisms. Its data processing schema is primarily based on ITK pipelines, and uses GDAL dependency to read and write raster and vector data. Many formats are supported by the library (at least those supported by GDAL) as CosmoSkyMed, Formosat, Ikonos, Pleiades, QuickBird,  Radarsat 2, Sentinel 1, Spot5, Spot 6/7, TerraSarX or WorldView 2.  \n\nOTB provides a lot of applications to process optical and SAR products: ortho-rectification, calibration, pansharpening, classification, large-scale segmentation and more. The library is written in C++ but all the applications can also be accessed from Python, command line launcher, QGIS and Monterverdi, a powerful satellite image visualization tool bundled in the OTB packages capable of manipulating large images efficiently. \n\nThe library also facilitates external contributions thanks to the remote module functionality: users can add new applications without modifying the core of the library. If this new remote module is relevant, it could be added as an official remote module, like DiapOTB (differential SAR interferometric processing chain) and OTBTensorflow (multi-purpose deep learning framework, targeting remote sensing images processing). \n\nMoreover, several operational image processing chains are based on OTB: their algorithms use the framework of OTB Applications while the orchestration is written in python. Some of the chains are also open source: Let It Snow (Snow cover detection), iota2 (Large Scale Land Surface Classification), WASP (Multitemp images fusion), S1Tiling (Sentinel-1 calibration and MAJA (Maccs-Atcor Joint Algorithm). The Orfeo Toolbox is also a part of the Sentinel 2 ground segment, being integrated in the S2 Instrument Processing Facility (IPF) module where it is used for radiometric corrections and resampling. \n\nIn the latest releases (from 7.x to 8.0), several features have been added as new SAR sensor models and new applications, and the OSSIM dependency - used for geometric sensor modelling and metadata parsing – has been removed in favor of functionalities available in GDAL. The aim of the presentation is to present the major features of OTB, the latest updates, the future features and architecture of the library and how OTB is used at CNES and CS Group to process data from scientific and developer points of view.",
        "description": ""
      },
      {
        "title": "OTB integration in operational processing chains",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The Orfeo ToolBox is used as development framework for satellite image processing over large dataset in several operational projects. Indeed, its image processing functionalities (multithreading, streaming, ram configuration) allow to process big images quite fast. The operational processing chains use OTB from the Python API and C++ API.  \n\nAmong the optical chain processing using OTB, we can list: MAJA, WASP, BIOPHY and IOTA2. MAJA (Maccs-Atcor Joint Algorithm) is an atmospheric correction and cloud screening software, based on multi temporal and multi-spectral processing. This chain uses L1C products to generate high quality L2A surface reflectance time series for Landsat8, Venus, and Sentinel 2 missions, it is mainly used by THEIA distribution center. The core algorithms of Maja are based on the Orfeo Toolbox. To process a product, the chain uses aerosol contents, cloud and shadow detection and various atmospheric effects to estimate accurate surface reflectance values. The main problem of the L2A products is the presence of clouds in time series which is why WASP was created. Indeed WASP (Weighted Average Synthesis Processor) delivers L3 products which provide monthly syntheses of cloud-free reflectance for Sentinel2 and Venus L2A products distributed by THEIA. This processor mainly includes a directional correction to normalize data and a weighted average of surface reflectance. Two other operational chains which uses OTB are BIOPHY - the goal of this processing chain is to create L2A products containing biophysical variables (FAPAR, FCOVER, LAI) related to the presence of vegetation in the image over a year – and IOTA2 - a soil occupation processor over a year of Sentinel1 and Sentinel2 data, the algorithms use the classification toolbox provided by OTB to process large areas, to determine the areas covered by buildings. \n\nOrfeoToolBox is also used for radar processing chain, like diapOTB or S1Tiling. S1TIling is a generic processing chain for Sentinel-1 time series developed with open-source software. Its main goal is to produce time series of Analysis ready data S1 images for large areas. The algorithms are using the SAR processing toolbox from OTB to take profit of its in-memory pipelining capabilities. DiapOTB is a differential SAR interferometry processing. It uses two SAR images of the same portion of the Earth’s surface taken at different time as input and aims to analyze potential events (earthquake, destruction …) by highlighting differences between SAR images. \n\nTo conclude, OTB is the central framework for a large scale of operational chains in remote sensing. Its genericity permits to cover a lot of use cases in one single tool. Note that it is also distributed in other projects like WorldCereal, SNAP, AI4GEO as toolbox or RUS as training and formation aim.",
        "description": ""
      },
      {
        "title": "Status of OTBTF, the Orfeo ToolBox extension for deep learning",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "OTBTF is a remote module of the Orfeo ToolBox enabling deep learning with remote sensing images.\nCreated in 2018, it aimed to provide a generic framework for various kind of raster-oriented deep-learning based applications.\nOriginally, OTBTF included user-oriented applications for patches sampling, model training, and inference on real world remote sensing images, and a few python scripts to help users with no coding skills to generate some ready-to-use models.\nA few years later, it has been used for a wide range of applications, like landcover mapping at country scale, super-resolution, optical image cloud removal, etc.\nThis talk will present a few selected IA based applications powered by OTBTF in the framework of research projects, public policies support, or teaching.\nWe will present the recent features added in OTBTF and we are very happy to introduce what is next!\nMore details on the project on the github repository: github.com/remicres/otbtf",
        "description": ""
      },
      {
        "title": "Aerial Images to Maps: Technological Advancements with OpenDroneMap",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "OpenDroneMap is an ecosystem of free and open source software to collect, process, analyze and display aerial data. In this talk we will present an exciting overview of what's new in the ecosystem, where the project is headed and how you can benefit from using it. In particular, will first provide a brief overview of the ecosystem, what the tools are and how you can start making maps in minutes. A short introduction to the \"magic\" of the processing pipeline will be presented. We will then touch on state-of-the-art advancements in photogrammetry technology within ODM, how we benefit from a global team of researchers and how that has allowed us to match (and often times exceed) proprietary software results. After a presentation of the technological advancements, we will discuss the importance of people, or how prioritizing people over code and investing into the community has affected both participation and adoption.",
        "description": ""
      },
      {
        "title": "Effortless Aerial Data Management and Sharing: The DroneDB Ecosystem",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "DroneDB is free and open source software for geospatial data storage. It provides a novel approach to store point clouds, textured models, aerial images, orthophotos and elevation models via a dynamic filesystem index. In this talk we will present DroneDB's storage approach and how you can start using it right away. We will discuss the project's architecture and roadmap. We will also perform a showcase of the project and demonstrate its most effective use cases. \n\nWe will cover how DroneDB's dynamic index can be published on the web using Registry, a cross-platform open source application which provides both a friendly user interface and a RESTful API. This enables users and GIS developers alike to access and manage the underlying data. We will showcase the ddb client, a command-line interface that enables power users to manage the index and can be used to sync, share and download remote datasets in a manner inspired by git workflows.\n\nWe will show how WebODM and Registry can integrate together to create a powerful and versatile workflow for 3D reconstruction using a full opensource stack.",
        "description": ""
      },
      {
        "title": "OpenAerialMap V2 Design and Development",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "OpenAerialMap.org (OAM) was built in 2015 to serve as a platform and tools for sharing openly licensed satellite and aerial imagery. For Humanitarian OpenStreetMap Team (HOT) and its partners, open imagery has been critical for disaster response and preparedness mapping projects. Those images have traditionally been difficult to share and access because of the large file sizes and technical skills required to publish them online. Since its inception, OAM provides an easy means of contributing to and accessing a large repository of open imagery, with over 11,000 images added. The OAM browser is designed to easily index, visualize and filter images, while the data itself is stored in Cloud Optimized GeoTIFF (COG) format in the Open Imagery Network, a federated network of highly available imagery buckets from different cloud providers. \n\nOpenAerialMap is the only platform built on open-source software that allows anyone to upload and share aerial imagery of anywhere on Earth. With advances in drone mapping technologies and their proliferation in places where cost and access used to be a limiting factor, there are now massive amounts of images that can be easily made available through OAM. Once uploaded, all imagery becomes instantly accessible via scalable TMS and WMTS services for mapping in OpenStreetMap or for any other use. Since its creation, OAM has been democratizing high resolution Earth observation and promoting the sharing of aerial imagery through open data licenses.\n\nThis year HOT joined with Kontur to take a fresh look at OAM and redesign the platform. Using modern, equitable, human-centered design principles, we evaluated how this tool could be used to better support HOT’s vision that everyone has access to high quality map data and can use that data responsibly to improve their lives and their communities. The development will build on and integrate emerging standards for geospatial data such as the Spatio-Temporal Asset Catalog (STAC) specification. A broad range of users and stakeholders will be involved in the design process, to ensure that OAM v2 will result in a modern and accessible platform. In this talk we will present an update on the development of OAM as informed by the results of that design work and share a preview of its implementation using open-source geospatial software. \n\nThe need for access to open imagery has never been greater, with advances in UAS imaging and processing technologies and their proliferation in places where cost and access are a major factor. This year HOTOSM joined with Kontur to take a fresh look at OAM and redesign the tool with modern, equitable, human-centered design principles to better support HOTOSM’s vision that everyone has access to high quality map data and can use that data responsibly to improve their lives and their communities. In this talk we will present an update on the development of OAM as informed by the results of that design work.",
        "description": ""
      },
      {
        "title": "State of PDAL",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "PDAL is Point Data Abstraction Library. It is a C/C++ open source library and applications for translating and processing point cloud data. It is not limited to LiDAR data, although the focus and impetus for many of the tools in the library have their origins in LiDAR.  PDAL allows you to compose operations on point clouds into pipelines of stages. These pipelines can be written in a declarative JSON syntax or constructed using the available API. This talk will focus on the current state of the PDAL Pointcloud processing library and related projects such as COPC and Entwine, for pointcloud processing. Coverage of the most common filters, readers and writers along with some general introduction on the library, coverage of  processing models, language bindings and command line based batch processing. First part will be covering new features for current users. Some discussion of installation method including Docker, binaries from package repositories, and Conda packaging. For more info see https://pdal.io",
        "description": ""
      },
      {
        "title": "MDAL: mesh data in QGIS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Mesh Abstraction Library (MDAL) has become an integral part of QGIS over the recent years. MDAL is used in QGIS to parse meteorological and hydrological data. MDAL is an open source library and recently has joined the OSGeo family as a Community project.\nMDAL data can be 1-dimensional, 2D or stacked 3D data. QGIS has been extended to render all those types of data in 2D and 3D map canvases. Once data are loaded in QGIS, users can easily style and explore temporal dimension of the data using QGIS generic tool. Additional plugins have been developed to leverage on mesh data in QGIS to slice and dice the mesh data.\nIn addition to visualising the data, new tools have been developed to directly edit the unstructured mesh data in QGIS. Users can edit geometries and values of the faces and vertices of the mesh data. The built-in validation tools for mesh editing, ensure the resulting mesh is topologically correct during and after mesh editing operations.",
        "description": ""
      },
      {
        "title": "Lidar classification, accuracy and change detection using the Norwegian open lidar data archive.",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Three dimensional representations of surface terrain and structure is essential for a range of widespread applications and forms a base dataset that underlies many decision making processes. A few examples include land use planning, areal overview, operational analysis, emergency handling, route and transport planning, geographical and meteorological modelling etc. Recently, the Norwegian Government and the Norwegian Mapping Authority tasked the acquisition of high resolution Light Detection and Ranging (LIDAR) data covering the entire mainland with a minimum of 2 point measurements per meter. In addition, all aerial lidar acquisitions that were tasked by the government since the early 2000s are also publically available for download. In this work using FOSS, we discuss the height accuracy of ground classified datasets (i.e. Digital Terrain Models, Digital Surface Models) with varying original acquisition ground point densities. We create classification pipelines that allow us to calculate derivative products such as a “normalized” vegetation density and further compare these over time. This work in progress discusses our experience with open source tools on open source data and some of the challenges we encountered scaling our methods for big data.",
        "description": ""
      },
      {
        "title": "State of OSM in QGIS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "QGIS is one of the most used Open-Source GIS Software. It is possible to display, edit, analyse, process different kind of data such as vector, raster, mesh, point clouds etc.\n\nQGIS has some native functionalities to work with OSM data. Either with raster layer as a basemap, or with vector, QGIS can deal with OSM data. Depending on the amount of data to work with, the need to \"refresh\" the data (from the main OSM database), the extent of the coverage, different plugins or technologies are possible.\n\nThis presentation will try to give an overview how it's possible to use OpenStreetMap data within QGIS according to different situations (Geocoding, TMS/WMS, Overpass-API, Docker, PostgreSQL…).\nThe presentation will show how you can contribute to QuickOSM to add some default « map preset » to QuickOSM core on GitHub. This feature in QuickOSM allows users to have a set of vector layer with styles in QGIS which are ready to be used, with a symbology.",
        "description": ""
      },
      {
        "title": "Surveying amenities for OSM at scale",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "OpenStreetMap editing transitions to mobile devices. There are few editing apps, and the best ones are thematical. This year I've published \"Every Door\": an app specifically designed to collect hundreds of shops and amenities. I've made it with the experience of mapping in OSM, making a Telegram bot of a similar purpose, and studying geospatial UX design. I've surveyed half a thousand amenities with the bot, and even more — with this new app.\n\nIn this talk we'll briefly touch on the app itself and the OSM tagging model. The main attraction would be map UX design: why you should remove the most interactivity from your maps. These are hard to use even on desktop, and a small screen provides an even bigger challenge. Can we get rid of them altogether? Let's see how working with maps can be made efficient, and how the ideas behind this app can make geodata collection apps better.",
        "description": ""
      },
      {
        "title": "Political Reapportionment: Drawing Boundaries with QGIS",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The process of drawing new political boundaries in representative democracies has generally been done with closed source software. However, a number of open source products are changing the way governments draw their jurisdictions. The QGIS Redistricting Plugin has been used to redistrict communities in the United States, Canada, and Australia, and other open source software such as DistrictR has been used to redistrict the United States in their previous cycle, significantly cutting the cost needed to participate in this activity and allowing individuals to make better contributions. At its core, the software is simple but powerful: it allows users to change attributes in an attribute column using selection tools and displays aggregate statistics for other selected columns. Join John Holden, the plugin's developer, and Blake Esselstyn, a geographic and political consultant, for a plugin demonstration and a discussion of how governments and citizen groups have transitioned to using open source software in this important political area.",
        "description": ""
      },
      {
        "title": "Manipulating text with PostgreSQL - lesser known PG jewels",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "PostgreSQL is the most advanced opensource RDBMS. As GIS folks, you most probably use it in combination with PostGIS, its Geospatial plugin.\n\nWhen dealing with Geospatial data, we usually focus on geometries. But most of feature attributes are text data. Of course, filtering on these text data with standard SQL capabilities is a day-to-day operation for database users.\n\nBut PostgreSQL provides much more capabilities when it comes down to text data management. In this presentation, we will go through a few of them.\n\nAfter a quick look at standard text functions in PostgreSQL, we will discover the lesser known fuzzy matching modules : \n- `pg_trgm` extension allows for string searches using trigraphs to determine a similarity rank between text items\n- `fuzzystrmatch` extension provides fuzzy matching functions like soundex, Levenshtein, metaphone\n\nThen, we will explore **Full Text Search ( FTS )** PostgreSQL capabilities.\n\nLast but not least, we will peek inside PostgreSQL collation concept, which has nothing to do with your lunch. Collations are a powerful feature in PostgreSQL allowing to adapt the way you deal with text data according to the localization. Like trying to answer this - apparently - obvious question : is '12' before or after '2' ?\n\nAnd, because we can, display all of this on a map :-)",
        "description": ""
      },
      {
        "title": "Postgis Topology to secure data integrity, simple API and clean up messy simple feature datasets.",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "In Postgis Topology a merge of two surfaces does not involve spatial operations, since \nthe surface to border relation has foreign key structures in the database. This means that the border of the new object is spatially not touched/changed when two surfaces are merged. With simple feature the common border must be computed on the fly, which again may involve snapTo and cause tiny overlaps and gaps. \n\nWith Postgis Topology you can easily make an API where the client only sends new borders which is a key issue to secure data integrity. This secures that old border are not are not moved by a client error or the by simple transport format, because existing points are never not passed back to the server. Postgis Topology makes it easy for the server to work with those new borders(delta), because there are standard methods for this in Postgis Topology and all relations between border and surfaces are stored in the database. Postgis Topology also has validation routines in addition to using standard database constraints to secure a healthy system. \n\nThe principles that Postgis Topology is based on was used in spatial system many years ago, but one problem was to keep the border line work nice and clean and not end up in a spaghetti.  So one of the first things we did together with Sandro Santilli was to create methods on top of Postgis Topology to avoid this, by throwing away any border parts that does not contribute to a new “valid” surface.\n\nPostgis Topology is built on a relational database model that is based on SQL-MM part 3. Your own domain data are easily linked to border, surface objects with more. For instance to check domain attributes on a surface on the other side of a border is not spatial query but a standard relational query. \n\nThe following projects will also be touched in this talk:\n\nhttps://gitlab.com/nibioopensource/pgtopo_update_sql (Functions using Postgis Topology to make it easy to create spatial update clients.)\n\nhttps://github.com/strk/qgis_pgis_topoedit (Postgis Topology is very well integrated with QGIS.)\n\nhttps://github.com/larsop/resolve-overlap-and-gap (Show how we clean up,  simplify, generalize  simple feature tables with millions of rows using Postgis Topology)\n\nIs relational database structure a good choice for Postgis Topology? Yes I will mean and since it’s also linked up SQL-MM part3 and not a random private structure and with all great Postgis functions available this is very good combination. You may take take glance at https://www.ibm.com/ibm/history/ibm100/us/en/icons/reldb/ and other articles about relational databases. \n\nThe plan now is to build a full ecosystem around Postgis Topology with a generic client to support declarative rules, where you can define attributes, rules for attribute handling and how to deal with overlap and gap. \n\nAll the work NIBIO has done/is doing here would not have been possible with out the great support from Sandro Santilli.",
        "description": ""
      },
      {
        "title": "Spatio-temporal Database - Creating a high availability easily scalable Spatio-temporal database cluster with Postgres, PostGIS, and timescaleDB!",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "I am working as the Technical Lead at Blue Sky Analytics, a climate-tech startup empowering the world’s decision-makers with accurate, real-time, and standardized climate data. \n\nAll datasets that we are building here at Blue Sky Analytics, technically have one similarity - they all have a space and time component. We tried to build solutions like filling empty values in inconsistent temporal data, and dividing the data in specified time period chunks for faster queries, while these worked as POC, they were not easy to scale up. Working with structured data was much easier to understand, working on postgres with the addition of timescale and PostGIS gave us exactly what was needed. Building the solution at the database level with the existing open-source technologies has been an exhilarating experience. \n\nImagine a dataset with hourly frequency going back years on a global level, with frequent inconsistencies, that not only you have to efficiently store but that should also be highly accessible in combination with other such datasets. If not for the open-source, we would not have been able to answer questions like:\n - How much have the lakes shrunk between the years 2010-2020 on a yearly basis?\n- Finding GHG emissions from biomass burning of \"*all US states, for the last 10 years on a monthly, weekly, daily basis***\".**\nLeveraging other open source solutions like h3-pg indexing also helped us to reduce the query time by an exponential factor for global level queries! \n\nWhile the database sounds pretty amazing, another challenge was putting it all together and deploying it on the cloud, which was a whole another challenge. The most intuitive solution was to deploy a bunch of Postgres instances. While it was not so hard to implement the basics, it became almost impossible to scale up or down, install rolling updates, account for failures. \n\nKeeping up with the tech, Kubernetes seemed like a great solution for building a high availability cluster service, and finding the postgres-operator (PGO) by crunchydata was exactly what we needed. It combined all the right tools like pgBouncer, pgBackRest, and monitoring solution using grafana and Prometheus all in one packaged easily to deploy service. While the learning curve with Kubernetes was a little steep, it lead to building a highly scalable and resilient database cluster.\n\nThe PostgreSQL + PostGIS + Timescale + H3 stack helped us simulate the temporal and spatial nature of the world at the database level and gave a universal approach to store and query all our datasets. It can handle textual data like fires with time (recorded time) and spatial information (lat -long) or shapes of counties, water bodies, etc., and combine them with each other using few joins giving us a very powerful geospatial-temporal query engine. \n\nWithout FOSS it would have been impossible to even imagine any of this but as of now, we are quantifying climate change!",
        "description": ""
      },
      {
        "title": "Web mapping at any scale: Bite-size, full-stack cartography with Protomaps + PMTiles open source tools",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Protomaps is a new, open source set of tools for vector cartography on the web. It’s designed to enable projects of any scale - from hobby projects of a neighborhood, to dense datasets covering the entire planet. It finally makes it simple to both host tiles and render them using web standards, and accomplishes it in the most affordable way possible. \n\nThis talk will be an overview of the entire mapping stack, driven by an ethos of simplicity. Component projects include: \n\n* The OSM Express database for syncing and querying fresh OpenStreetMap data\n* The PMTiles cloud-optimized archive format for serverless hosting on platforms like S3\n* The Protomaps JS renderer for custom cartography on the web using Canvas 2D\n* The relationship to complementary projects like GDAL, Leaflet, MapLibre, Tippecanoe and FlatGeobuf\n\nI’ll also describe successes and failures in adoption among users over the past two years, as well as future development plans.",
        "description": ""
      },
      {
        "title": "Gleo: Reinventing WebGL maps",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "WebGL has enabled fast rendering of maps on the web (including MapLibreGL and OpenLayers renderers), but from the software development point of view, is a notoriously cumbersome technology to work with. \n\nThis session introduces Gleo, a JavaScript+WebGL map display library aiming to cover similar use cases than Leaflet, OpenLayers, MapZen and MapLibreGL.\n\nA few architectural features of Gleo will be outlined, including:\n- \"One GL shader per type of cartographic symbol\" rendering & framebuffer compositing approach\n- Object-oriented design: symbols as instances; allocation/deallocation of GPU resources for each symbol\n- ES6 javascript features: classes, modules, private fields; symbol as DOM EventTarget; deprecation of mouse/touch events in favour of pointer events\n- Sliding window algorithm in a wrapped WebGL texture for tile caching\n- On-the-fly reprojection enabled by updating just one WebGL data structure\n- On-the-fly CRS offsetting to prevent floating-point precision artifacts\n- Coordinate wrapping and display tessellation to avoid antimeridian artifacts",
        "description": ""
      },
      {
        "title": "MapComponents, a new component framework for developing web map applications",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "MapComponents is a new component framework to quickly and easily build dynamic geospatial web apps. It includes React front-end components for all kinds of projects, from small apps with a narrow and specific focus up to complex geospatial suites for the web. Server-side components are also planned to aid the development of flexible backend services.\n\nMapComponents\n- is a modular framework to create tailored geospatial web apps built upon modern webbrowser technology\n- can be used to visualise and analyse geo data\n- can be used for desktop and mobile applications (online and offline; progressive web apps (PWA))\n- provides independent components which can be combined into full-fledged geospatial web applications (e.g. dashboards, WebGIS, ...)\n- provides a catalog of components and example applications\n- uses a flexible core which theoretically supports any kind of mapping library (currently supported are MapLibre, Mapbox GL JS and OpenLayers)\n- is easily integrated into existing stacks\n- makes it easy to rapidly design and deploy a map centric web app\n\n\nMapComponents is developed by WhereGroup GmbH and is available under the MIT license.\n\nhttps://www.mapcomponents.org/\n\nWe will present the project, with its current state and goals, and will show practical examples.",
        "description": ""
      },
      {
        "title": "EVRYMAP - An extensible web mapping framework based on Angular, NodeJS, Leaflet and Mapserver.",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "It started as a way to help us publish geospatial data. It quickly morphed to something quite different. You can call it scope creep. And despite this term being close to a swear word in ICT, it turned out to be very good thing. And that's because while still serving its main purpose, which is to provide an out-of-the-box web based mapping app with all the trimmings (navigation, measure, layer control and search tools), EVRYMAP also:\n\n- Provides client-side editing tools\n\n- Provides a modular design that allows you to implement custom business logic by simply writing your own apis. EVRYMAP will consume these APIs automatically by defining them in configuration as 'modules'\n\n- Implements 1-n relationships between your spatial data and other related data. Which may come from the same or external databases\n\n- Can be run as standalone or within an iframe to spatially enable third party applications (and provides the communication mechanism)\n\nUsing EVRYMAP at the core, we have also deployed a few systems in production environments as commercial apps, namely:\n-Landify, a mini-cadastre for organizations with a real-estate portfolio. It allows users to easily review, catalogue, and manage real estate data (land parcels and buildings).\n-MapTheYA, a map-based information system for the management of water networks including topology checks.\n-Building permits/Expropriations Management\nExamples of not \"map-first\" systems, meaning that while the bulk of their functionality are text/form based (applying for electronic copies of documents) they also include embedded maps to improve user experience.\n \nThis presentation will provide a brief introduction to EVRYMAP, the way it works, how you can configure and extend its functionality and what we plan for the future. And being the new kid on the block, ask the community for input and feedback!",
        "description": ""
      },
      {
        "title": "Web Mapping with Global Map Projections",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Web Mapping as a technology and a method is now twenty years old. Within the \nOSGeo Community, it has been fostered by projects such as OpenLayers \nand Leaflet. They evolved tightly intertwined with the framework imposed by  \nfree data providers, initially around commercial efforts like Google and later \nOpenStreetMap. While useful in providing an easy entry to web mapping, and \nconvenient background layers, these data providers also triggered a regression \ntowards centuries-old cartography techniques, in particular the Mercator projection. \nThis has become a major hurdle to web mapping, particularly concerning global \ndata.    \n\n \n\nThe Mercator map projection was created to aid sea faring in the XVI century and \nwas rendered useless with the advent of global positioning systems. Its use in \ncartography may still be acceptable at large scales, neighbourhood or city \nlevel, but at smaller scales it imposes severe distortion to distances and areas. \nFor global datasets in particular, the Mercator projection is unusable, for it \ncannot represent the full surface of the planet.   \n\n  \n\nWeb mapping developers may work around this framework with libraries such as D3 \nor proj4js, and by setting up bespoke base layer services. But in doing so they \nface a different problem: the deep dependence on the CRS index created by the \nEuropean Petroleum Survey Group (EPSG). Primarily concerned with the survey and \nextraction of fossil fuels, the EPSG leans heavily on local or regional CRSs, \nlargely ignoring global CRSs.  Hardly any of the more than 100 map projections \nand coordinate systems developed since the beginning of the XX century feature \nin the EPSG index.  Landmark projections such as the Eckert series, the \nHomolosine, Eumorphic, Dymaxion or the Snyder series were never included in the \nEPSG index.  Not even the classical Mollweide projection (one of the turning \npoints towards modern cartography) appears in the EPSG index. With a FOSS4G \nstapple such as MapServer, this forces the leveraging of map (re-)projections to \nthe client, which is not always possible.     \n\n  \n\nWeb mapping with global data thus remains a technical challenge with FOSS4G. \nThis address reviews several techniques and work-arounds making global web \nmapping possible with familiar FOSS4G technologies. Starting with the \nappropriate configuration of CRS managing software, going through the set-up of \ndata servers and finally providing examples with web mapping clients.",
        "description": ""
      },
      {
        "title": "Liven up your webmaps with custom microanimations",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Micro animations are small animations on a website that support the user by attracting focus to where we want their attention. They can also be used to support relationships between elements in a web application, for example a list element and a map feature, or simply to spark a little joy. Users today have come to expect these animations in their online experiences. How can we provide these features in a web map? Map libraries gives you some animations out of the box today, but what if you want something custom?\n\nThis presentation will give examples on how small animations can be used in web maps to support interactivity. We will walk through building our own, custom animation that can be used as a starting point for many types of animations in web maps. The technique is library-agnostic, so we’ll show examples in both MapLibre GL JS, Leaflet and OpenLayers.",
        "description": ""
      },
      {
        "title": "Joint ESA-NASA Multi-Mission Algorithm and Analysis Platform (MAAP)",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The scientific community is faced with a need for greatly improved data sharing, analysis, visualization and advanced collaboration based firmly on open science principles. Recent and upcoming launches of new satellite missions with more complex and voluminous data, as well as the ever more urgent need to better understand the global carbon budget and related ecological processes, provided the immediate rational for the ESA-NASA Multi-mission Algorithm and Analysis Platform (MAAP). \n\nThis highly collaborative joint project of ESA and NASA established a framework between ESA and NASA to share data, science algorithms and compute resources in order to foster and accelerate scientific research conducted by ESA and NASA EO data users. Presented to the public in October 2021, the current version of MAAP provides a common cloud-based platform with computing capabilities co-located with the data, a collaborative coding and analysis environment, and a set of interoperable tools and algorithms developed to support the estimation and visualization of global above-ground biomass. \n\nData from the Global Ecosystem Dynamics Investigation (GEDI) mission on the International Space Station and the Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) have been instrumental in the first products of MAAP including the first comprehensive map of Boreal above-ground Biomass and a current Global Biomass Harmonization Activity, but the platform is also being specifically designed to support the forthcoming ESA Biomass mission and incorporate data from the upcoming NASA-ISRO SAR (NISAR) mission. While these missions and the corresponding research which includes airborne, field, and calibration/validation data collection and analyses, provide a wealth of data and information relating to global biomass estimation, they also present data storing, processing and sharing challenges. The NISAR mission alone will produce about 80TB/day. These large data volumes present a challenge that would otherwise place accessibility limits on the scientific community and impact scientific progress. \n\nOther challenges being addressed by MAAP include: 1) Enabling researchers to easily discover, process, visualize and analyze large volumes of data from both agencies; 2) Providing a wide variety of data in the same coordinate reference frame to enable comparison, analysis, data evaluation, and data generation; 3) Providing a version-controlled science algorithm development environment that supports tools, co-located data and processing resources; and 4) Addressing intellectual property and sharing challenges related to collaborative algorithm development and sharing of data and algorithms.\n\nMAAP products can be explored on the MAAP Dashboard at https://earthdata.nasa.gov/maap-biomass or the joint platform entrance at scimaap.net. MAAP also can be accessed through individual NASA (https://maap-project.org) and ESA (https://esa-maap.org/) landing pages.",
        "description": ""
      },
      {
        "title": "Write once, run anywhere: safe and reusable analytic modules for WebAssembly, Javascript, or more!",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The proliferation of client-side analytics and on-going vulnerabilities with shared code libraries have fueled the need for better safety standards for running executables from potentially unknown sources. WebAssembly (WASM), a compilation target that allows lower-level languages like Rust, C, and Go to run in the browser or server-side at near-native speeds. Much like Docker changed the way we run virtualized workflows, WASM runtimes create safe virtual environments where access to the host system is limited. \n\nIn combination with a new free and open source full-stack geospatial platform, Matico, efforts are underway to enable portability across workflows and applications to more easily use WASM modules. WASM implementations of GDAL are in the works, and powerful open source Rust geospatial libraries are easily packaged for web usage through Wasm-Pack. Additional geo WASM libraries like jsgeoda provide spatial indices, binning, and autocorrelation functions. Shareable code can be a recipe for security vulnerabilities and attack vectors, potentially exposing personal or critical information, particularly if there is the opportunity to run code server-side. WASM implementation alleviates this by requiring access from the Virtual Machine (VM) to be limited and explicit, and for Javascript developers the lightweight AssemblyScript language is relatively familiar. \n\nAn upcoming Javascript feature called ShadowRealms may enable even simpler and more familiar implementations to safely run Javascript code shared between module authors. These developments lay the groundwork for a hybrid front- and backend geospatial ecosystem of shareable code snippets and analytic functions, much like have emerged in the UI component Javascript ecosystem. The combination of emerging features positions web geospatial analytics and This talk explores the implementation and performance of running geospatial analytic modules through a WebAssembly virtual machine and through the upcoming Javascript ShadowRealm specification.",
        "description": ""
      },
      {
        "title": "OpenPlains - Is it the new web GRASS?",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "OpenPlains - Is it the new web GRASS?",
        "description": ""
      },
      {
        "title": "Scaling down web maps for the final user",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "WebGIS publishing platforms like MapStore, are usually very feature-rich, to cover a lot of different  scenarios, from the QGIS-like do-it-all web application, to a simple interactive map for a company website.\n\nThis comes with an important trade-off: even when removing most of the unneeded functionalities for the simplest use case, the cost of the platform needed to run your maps can be overwhelming.\n\nThe problem here is that a single platform is not always the best choice for every kind of usage.\n\nThis talk shows how to use the popular MapStore platform as an application builder, to publish interactive maps that can run on a very light engine, de facto scaling down a quite heavy platform to the needs of performance and simplicity that better suit a lot of general user oriented applications and websites.\n\nWe will start by creating a map from different data sources, using MapStore, then we will export the map and publish it to our alternative light engine.\n\nWe will then highlight all the advantages this approach can offer.\n\nFinally we will give some insights on the technical aspects of this project.",
        "description": ""
      },
      {
        "title": "Automating Generating a Web Map from Online Tabular Data: UC Davis Potential Worksite Exposure Interactive Web Map",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Tables are a great way to store data and this format is often used to make data available for the public on websites. While these tables technically meet their intended goal of sharing data, they do not make it easy to understand the spatial and temporal patterns in the data they contain. In this talk, I will demonstrate how an automated toolchain of web scraping and text processing in R, and interactive visualization in Leaflet is automated with GitHub Actions and applied to aid data interpretation and generate new insights from a daily-updated online tabular dataset using a case study of the University of California Davis’ Potential Worksite Exposure Reporting data for COVID-19. \nIn the United States, California Assembly Bill 685 (AB685) requires employers in the state of California to notify employees of potential worksite exposures to COVID-19 to the geographic scale of individual buildings. The University of California Davis meets this requirement by listing any potential exposures on a website, giving the date reported, the dates of the potential exposure, and the building name as reported by the employee. To make a map from this data, the dates and building names had to be standardized and joined to a vector layer of campus buildings before they can be added to an interactive Leaflet map. Because the data updates daily, the whole process needed to be automated so no one had to run the scripts every day to update the map. The result is a map that gives uses a much clearer understanding of the spatial and temporal distribution of potential exposures to COVID-19 on campus.",
        "description": ""
      },
      {
        "title": "MapStore and geOrchestra, a match made in heaven",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "Work started at the end of 2019 to integrate [MapStore](https://mapstore.readthedocs.io/en/latest/) as a [WebGIS viewer](https://mapstore.geosolutionsgroup.com/mapstore/#/) for the [geOrchestra SDI](https://www.georchestra.org/) (a free, open source, modular and interoperable Spatial Data Infrastructure software born in 2009 to meet the requirements of the INSPIRE directive in Europe). The work, led by [GeoSolutions](https://www.geosolutionsgroup.com/), was funded by [Rennes Mètropole](https://metropole.rennes.fr/) with the  goal to meet the expectations of the large geOrchestra community for a new, more ergonomic, modular and customizable WebGIS based on updated technologies.\n\nThe project also triggered a significant evolution of the MapStore product by developing several interesting new tools and enhancements to the MapStore framework. Thanks also to this powerful integration MapStore significantly increases its strengths by opening the door to further and more advanced developments and evolutions. Below is a list of main enhancements and new features that have been part of the integration:\n\n- Application Context Manager: an administrative tool designed to build and configure MapStore's viewers\n- General evolutions of common existing tools in MapStore to enrich the user experience: Map viewport enhancements, CRSs management, TOC, translations, styling of layers, advanced measure tool, layer metadata, various catalog tool extensions to support additional data sources (like TMS, WFS etc), Attribute Table enhancements for the editing mode and more\n- Enhancements on the MapStore security tier aimed to the integration\n- Extension Manager: extensions are plugins that can be distributed as a separate package (a zip file), and be installed, activated and used at runtime in an existing MapStore installation\n- MapStore Data Directory: to make more portable and manageable the MapStore configuration and installed extensions\n\nThe first integration received positive feedback from the geOrchestra community but also from those who were already using MapStore as a standalone application as well as from GeoNode users. Thanks to MapStore, some application flows have also been strengthened and consolidated in geOrchestra; further developments made after the first integration work also had the aim of migrating other custom tools (such as Cadastrapp and Urbanisme) leveraging on the MapStore Extensions system and also the geOrchestra community has taken its own steps in this direction with the inclusion of further custom extensions for MapStore.\n\nThis great collaboration is progressing fruitfully even today where further evolutions and developments are expected for 2022, including new features and functionalities for MapStore.",
        "description": ""
      },
      {
        "title": "Styling Natural Earth with GeoServer and GeoCSS",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Natural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth one can build a variety of visually pleasing, well-crafted maps with cartography or GIS software.\n\nGeoServer GeoCSS is a CSS inspired language allowing you to build maps without consuming fingertips in the process, while providing all the same abilities as SLD.\n\nIn this presentation we’ll show how we have built a world political map and a world geographic map based on Natural Earth, using CSS, and shared the results on GitHub. We’ll share with you how simple, compact styles can be used to prepare a multiscale map, including:\n* Leveraging CSS cascading.\n* Building styles that respond to scales in ways that go beyond simple scale dependencies.\n* Various types of labeling tricks (conflict resolution and label priority, controlling label density, label placement, typography, labels in various scripts, label shields and more).\n* Quickly controlling colors with LessCSS inspired functions.\n* Building symbology using GeoServer large set of well known marks.\n\nJoin this presentation for a relaxing introduction to simple and informative maps.",
        "description": ""
      },
      {
        "title": "pycsw project status 2022",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "pycsw is an OGC CSW server implementation written in Python and is an official OSGeo Project. pycsw implements clause 10 HTTP protocol binding - Catalogue Services for the Web, CSW of the OpenGIS Catalogue Service Implementation Specification, version 3.0.0 and 2.0.2. pycsw allows for the publishing and discovery of geospatial metadata, providing a standards-based metadata and catalogue component of spatial data infrastructures. The project is certified OGC Compliant, and is an OGC Reference Implementation.\n\nThe project currently powers numerous high profile catalogues such as IOOS, NGDS, NOAA, US Department of State, US Department of Interior, geodata.gov.gr, Met Norway and WMO WOUDC. This session starts with a status report of the project, followed by an open question answer session to give a chance to users to interact with members of the pycsw project team. This session will cover how the project PSC operates, the current project roadmap, and recent enhancements focused on ESA's EOEPCA, Open Science Data Catalogue and OGC API - Records.",
        "description": ""
      },
      {
        "title": "pygeoapi project status 2022",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "pygeoapi is an OGC API Reference Implementation. Implemented in Python, pygeoapi supports numerous OGC APIs via a core agnostic API, different web frameworks (Flask, Starlette, Django) and a fully integrated OpenAPI capability. Lightweight, easy to deploy and cloud-ready, pygeoapi's architecture facilitates publishing datasets and processes from multiple sources. The project also provides an extensible plugin framework, enabling developers to implement custom data adapters, filters and processes to meet their specific requirements and workflows. pygeoapi also supports the STAC specification in support of static data publishing.\n\npygeoapi has a significant install base around the world, with numerous projects in academia, government and industry deployments. The project is also an OGC API Reference Implementation, lowering the barrier to publishing geospatial data for all users.\n\nThis presentation will provide an update on the current status, latest developments in the project, including new core features and plugins. In addition, the presentation will highlight key projects using pygeoapi for geospatial data discovery, access and visualization.",
        "description": ""
      },
      {
        "title": "pygeofilter: geospatial filtering made easy",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "## Abstract\n\n[pygeofilter](https://github.com/geopython/pygeofilter/) is a library to support the integration of geospatial filters. It is split into frontend language parsers (CQL 1 + 2 text/JSON, JFE, FES) , a common Abstract Syntax Tree (AST) representation and several backends (database systems) where the parsed filters can be integrated into queries.\n\n## Parsers\n\nCurrently pygeofilter supports CQL 1, CQL 2 in both text and JSON encoding, OGC filter encoding specification (FES) and JSON filter expressions (JFE) as input languages. Additionally pygeofilter provides utilities to help create parsers for new filter languages.\nThe filters are parsed to an AST representation, which is a common denominator across all filter capabilities including logical and arithmetic operators, geospatial comparisons, temporal filters and property lookups. An AST can also be easily created via the API, if necessary.\n\n## Backends\n\npygeofilter provides several backends and helpers to roll your own. Built-in backends are for Django, SQLAlchemy, raw SQL, (Geo)Pandas dataframes, and native Python lists of dicts or objects.\n\n## Usage\n\npygeofilter is used in several applications, such as [PyCSW](https://pycsw.org/), [EOxServer](https://github.com/EOxServer/eoxserver/) and [ogc-api-fast-features](https://github.com/microsoft/ogc-api-fast-features/)",
        "description": ""
      },
      {
        "title": "QGIS Server into the wild",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "With our Lizmap hosting service, we provide and monitor several hundred of QGIS servers. These QGIS Servers receive and process 3.5 million requests per week, including 3 million WMS GetMap requests.\n\nWe do not control the content of these QGIS projects, which are sent by our customers on our servers. Therefore, we need to deal with projects having some various kind of issues. Some QGIS projects can have very heavy SQL views which are slow to load. Our infrastructure may host projects having hundreds of layers with complex symbology. Users can publish QGIS PDF layouts (A4 and A3) with custom logos etc. This can lead to memory problems.\n\nGIS technicians can add different data sources : vector and raster files, PostgreSQL / PostGIS database, OGC WMS, WFS and WMTS web services into these QGIS projects. We need to ensure that QGIS Server is working properly, for all customers, to execute incoming requests when some external Web Services providers are too slow to respond or are temporarily offline.\n\nWe need to take care of possible errors propagated by these projects. In some circumstances, we have about 10 thousand errors per week coming from QGIS server.\n\nThe goal of this presentation is to give an overview of what QGIS Server can experience into the wild and what we need to do to make the Lizmap user experience the best possible: monitoring, proxy, caching.",
        "description": ""
      },
      {
        "title": "PgMetadata - A QGIS plugin to store the metadata of PostgreSQL layers inside the database, and use them inside QGIS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "PgMetadata is made for people using QGIS as their main GIS application, and PostgreSQL as their main vector data storage.\n\nThe layers metadata are stored inside your PostgreSQL database, in a dedicated schema. Classical fields are supported, such as the title, description, categories, themes, links, and the spatial properties of your data.\n\nPgMetadata is not designed as a catalog application which lets you search among datasets and then download the data. It is designed to ease the use of the metadata inside QGIS, allowing to search for a data and open the corresponding layer, or to view the metadata of the already loaded PostgreSQL layers.\n\nBy storing the metadata of the vector and raster tables inside the database:\n\n* QGIS can read the metadata easily by using the layer PostgreSQL connection: a dock panel shows the metadata for the active layer when the plugin detects metadata exists for this QGIS layer.\n* QGIS can run SQL queries: you can use the QGIS locator search bar to search for a layer, and load it easily in your project.\n\nThe administrator in charge of editing the metadata will also benefit from the PostgreSQL storage:\n\n* PostgreSQL/PostGIS functions are used to automatically update some fields based on the table data (the layer extent, geometry type, feature count, projection, etc.).\n* The metadata is saved with your data anytime you backup the database\n* You do not need to share XML files across the network or install a new catalog application to manage your metadata and allow the users to get it.\n\nThe plugin contains some processing algorithms to help the administrator. For example:\n\n* a script helps to create or update the needed \"pgmetadata\" PostgreSQL schema and tables in your database\n* a algorithm creates a QGIS project suitable for the metadata editing. This project uses the power of QGIS to create a rich user interface allowing to edit your metadata easily (forms, relations). Why use another interface when QGIS rocks ?\n\nMore PgMetadata features will be shown during the presentation:\n\n* Modification of the template to tune the displayed metadata\n* Export a metadata dataset to PDF, HTML or DCAT\n* Publish the metadata as a DCAT catalog with Lizmap Web Client module for PgMetadata. It can then be harvested by external applications (Geonetwork, CKAN)\n* The data model is very close to the QGIS metadata storage and the DCAT vocabulary for compatibility.\n\nWe will also show the last features such as the new support of the PostgreSQL rasters",
        "description": ""
      },
      {
        "title": "QGIS MapTiler plugin v3- vector basemaps & global DEM for 3D terrain",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The MapTiler plugin is the easiest way to load styled vector tiles into QGIS. The plugin allows anybody to easily load map data of the entire planet (from OpenStreetMap project), with details down to the street level from Cloud or any other URL.\n\nThe version 3.0 of MapTiler plugin brings several new features, maps and datasets. \nA new global DEM of the entire planet is ideal for terrain spatial analysis. New maps - both in vector and raster - OpenStreetMap (popular OSM Carto finally in vectors!) and a Winter map for all wintertime activities. A new Satellite map based on our new 2021 cloudless satellite imagery with 10m resolution for the entire planet. \n\nThe plugin offers maps of the entire world in vector or raster tiles, but can also open maps from any other URL. You can load high-resolution aerial imagery, hillshading, global terrain data and contour lines for outdoor maps or official government open data from various countries.\n\nA ready-to-use list of beautiful map styles is available to QGIS users. Those who prefer customized maps can make their own map design in a few clicks using the Customize tool. Users can set their own colors, fonts, or choose the language of map labels.\n\nUse the power of QGIS and reproject, rotate and export vector tiles to various formats (including PDF, SVG or DWG) or use Print Composer to create beautiful high-detailed maps to fit your needs.\n\nThe plugin is an open-source project with code available at GitHub repository and open to any contribution from developers and users.",
        "description": ""
      },
      {
        "title": "How to get a good response on stackexchange",
        "type": "General session talk",
        "track": "Education",
        "abstract": "I often hear complaints that the stackexchange sites are too mean to new users, that moderators are too quick  to close a question that doesn't fit the guidelines or nitpick the questions to death. Also, that they didn't get a good answer anyway so what is the point.                                                                          \n                                                                                                                         \n This talk will give you an introduction on how to ask a “good” question on gis.stackexchange.com from an experienced moderator of the site. As a bonus, you will also find out how to file a useful bug report (either internally or to an external project). This talk will cover the things that you might not think are useful but are in fact vital to someone who is trying to help you.                                                                 \n                                                                                                                         \n I will discuss how the site works and how to make it work well for you, what sort of questions are “good” questions and which ones are better asked somewhere else. I will also cover what the difference between closing and deleting a question is and how to get your question reopened if it is closed. How the review queues work and how you can help improve the site for other users.",
        "description": ""
      },
      {
        "title": "A Moodle based complex system to teaching spatial data processing",
        "type": "General session talk",
        "track": "Education",
        "abstract": "Open-source solutions in geoinformatics have gradually come into the focus of attention over the past decades, becoming of the most well-promising opportunities in tertiary education. It is  indubitable that students have to develop their skills to find, apply and contribute to the existing open-source opportunities, besides developing skills regarding coding and programming logic. \nAt the Budapest University of Technology and Economics, a complex system has been developed to support students on this matter. This system is fully based on open-source software and free cloud services. Consequently, all  the teaching materials have creative common licenses supported by the use of open source software. \nThe main entry point of the educational materials is Moodle, considered as one of the most popular learning management systems. The majority of the source codes and explanation texts/notes used for teaching are published in Jupyter notebooks, stored on personalized GitHub pages. For opening and testing the Jupyter notebooks, the students can use Google Colab. \nAnother challenge worth mentioning is the continuous assessment evaluation format. Moodle supports the creation of tests based upon a wide variety of question types (e.g. multiple choice, true/false, drag and drop markers, etc), which are stored in a question bank. The test is generated by randomly selecting a given number of questions; therefore, taking the test a couple of times is highly recommended. As stated by many, this way of self-studying is popular among students these days and efficient in achieving remarkable progress. \nOur presentation shares either the developed system or the gained experience over the recent years.",
        "description": ""
      },
      {
        "title": "Teaching GIS Through Geospatially Aware Agent-Based Modeling",
        "type": "General session talk",
        "track": "Education",
        "abstract": "Teaching GIS Through Geospatially Aware Agent-Based Modeling.\n\n\n\nAgentScriptGIS is a web-based platform that provides a geospatially aware agent-based modeling programming environment. The goal is to enable programmers to generate geo-agent-based models with minimal barriers to entry. The platform provides a programming environment that includes an agent-based modeling library (agentscript.org), a geo-aware programming context, and a map display (leafletjs.com). \n\nThe platform was designed to reduce the overhead needed for programmers to begin modeling. We want to empower modelers who come from a wide array of backgrounds with the ability to write and animate geospatial models with minimal time and effort. The intended audience of this platform are users who want to explore geospatial and agent-based modeling but may have little to no experience interacting with these types of models.\n\nWe use agent-based modeling as a basis for our platform since it is a popular way to teach programming and can model a wide spectrum of problems. Popularized by NetLogo and StarLogo, agent-based modeling is used in a variety of educational contexts from elementary school studies to graduate level research. To maximize deployability, our agent-based modeling library AgentScript was written in JavaScript and built to be leveraged by the web browser.\n\nGIS software is typically professional in nature and leans on being sophisticated and precise, but is often overburdened with complexity. The hobbyist GIS programmer faces a steep learning curve when starting, including choosing appropriate tools and information sources, deciding on data formats and understanding projections. Our platform intends on removing these burdens on the user by trading versatility for simplicity and ease of use.\n\nWe are preparing our platform to be used initially in academia, but can see it being applied in a variety of settings. AgentScriptGIS focuses on facilitating new ways to engage students, teachers and modelers with geospatial issues. This platform provides a low barrier of entry to users and promotes the modeling of local and regional problems by leveraging real-world data and empowering low skill users with the ability to model various geospatial phenomena.",
        "description": ""
      },
      {
        "title": "Using GRASS GIS in Jupyter Notebooks: An Introduction to grass.jupyter",
        "type": "General session talk",
        "track": "Education",
        "abstract": "Although integration of GRASS GIS with Python has been well supported for several years, using GRASS with computational notebooks such as Jupyter Notebooks was inconvenient up until recently. Computational notebooks allow users to share live code with in-line visualizations and narrative text, making them a powerful interactive teaching and collaboration tool for geospatial analytics. In this talk, we’ll introduce a new GRASS GIS package, grass.jupyter, that enhances the existing GRASS Python API to allow Jupyter Notebook users to easily manage GRASS data, visualize data including spatio-temporal datasets and 3D visualizations, and explore vector attributes with Pandas. We’ll demonstrate how to create interactive maps through integration with folium, a leaflet library for Python, and we’ll look at an example use case: using notebooks to teach an advanced geospatial modeling course for graduate students at NC State University.\nGrass.jupyter is still under active development but is available experimentally in GRASS version 8.0 and officially with GRASS version 8.2.",
        "description": ""
      },
      {
        "title": "Geospatial data science for planning education systems",
        "type": "General session talk",
        "track": "Education",
        "abstract": "The presentation will share how UNESCO’s International Institute for Educational Planning (IIEP) applies FOSS4G technologies to advance Ministries of Education’s use of geospatial data in planning better educational results among school children. Our work here at the IIEP-UNESCO is to design tools for educational planners all around the world, and FOSS4G has been the cornerstone of our work. \n\nEducational planners are the professionals who work in Ministries of Education –in district offices or in the central office-- that are tasked with designing the best possible strategies and interventions to make sure that all learners will get good quality access to relevant and efficient educational services. For decades, planners have been using geospatial insights with minimal computing capacity and- to be honest- very little spatial data.\n\nOver the last few years, we have been completely refurbishing the methods and the data that we use as planners, and working with the FOSS4G community has been instrumental in fulfilling our mission.\nThis talk is about sharing concrete applications and use cases of geospatial data in educational planning. For example, we spatialize the number of students that will enrol in each grade in different communities, we plan for the training, recruitment, deployment, and retention of the teaching staff, we lead suitability analyses to check where to best build a new school or where to refurbish existing ones. \n\nSo in this presentation we will show you examples of application of tools and methodologies all built on FOSS:\n   -\tSpatialized school-age populations in Jamaica\n   -\tRouting optimization of inspection circuits in Finland\n   -\tGeographically-weighted regressions for improving learning in Colombia\n   -\tSchool infrastructure and natural hazard risk model in Indonesia\n   -\tSea level rise and historical floods in Viet Nam \n   -\tSchool catchment areas based on travel time (check out the presentation submitted by Riku Oja from GISPO, it’s our joint work!) \n\nAs an institution here at IIEP we have sought to advance this line of work by (1) making a complete switch to free and open source software (FOSS) and open access documentation and data sources (OpenScience), (2) bringing geospatial approaches and big, small, and thick data, to update EDplanning processes, (3) creating technical partnerships with instances such as GISPO, UNOSAT, among others, and (4) collaborating on informing education policy-making with geospatial insights.\n\nThis talk is an invitation to all geospatial data geeks to join us in shaping the future of educational planning.",
        "description": ""
      },
      {
        "title": "Rockfall Quantitative Risk Assessment at a medium-large scale based on FOSS4G tools. An example of applications in the North-Western Alps",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Rockfall risk analysis and mitigation activities are key points in land management in mountain areas and along coastal cliffs, aimed at the protection of population, structures, infrastructures and involved economic activities such as viability, industry and tourism. \nRockfall is a complex landslide phenomenon, widespread over large areas and characterised by high variability. As a function of the amount of available data to describe such variability, the risk analysis can be carried out at different levels of detail, i.e. at different reference scales, each one characterised by specific objectives, procedures, and input data (Fell et al, 2008).\nAt the detailed scale (> 1: 5000), in order to design risk mitigation works, it is necessary to analyse localized rockfall phenomena through specific methodologies requiring a careful identification of danger scenarios, a statistical description of the parameters, and sophisticated probabilistic calculation tools.\nAt the medium-large scale (1: 5000 - 1: 25000), on the contrary, due to the difficulty in finding detailed information over larger slope portions, it is possible to analyse widespread instability sources based on simplified mechanical considerations and several spatial approximations. Such large scale analyses can be used as a management tool for territorial planning and can be easily implemented in GIS software.\nThis work presents a medium-large scale Rockfall Quantitative Risk Assessment procedure fully developed within the QGIS environment. The procedure is based on the IMIRILAND methodology (Castelli and Scavia, 2008), which allows to obtain risk maps through integrated and consequential phases and simple raster calculations. The main steps of IMIRILAND methodology are:\n•\thazard analysis, aimed at defining, for a given rockfall scenario, the potentially involved area, the intensity of the damaging phenomenon and the temporal probability of occurrence;\n•\tidentification of the elements at risk and definition of their value and their exposure with reference to  physical, social, environmental and economic considerations;\n•\tanalysis of the vulnerability of the elements at risk, i.e. the degree of loss of the element as a consequence of the impact with the falling block;\n•\tcalculation of the risk, combining the hazard with value, exposure and vulnerability of the elements at risk.\nThe IMIRILAND QRA procedure was applied to the mountain site of Sorba Valley (VC), North-Western Alps. The site involves an area of about 10 km2 with altitudes ranging from 750 m up to 2035 m a.s.l. The site is prone to rockfall events, which historically involved some hamlets and some sections of the valley main road. However, very little information on such events is available, and no indication can be obtained in terms of rockfall recurrence and involved volumes. Due to this, it was not possible to take into account temporal aspects and relative (spatial) risk maps were produced in this work.\nAll the analysis was carried out using open data available as web services and datasets from the Regione Piemonte GeoPortal:\n•\tDTM with 5 m x 5 m raster resolution – GeoPortale Piemonte;\n•\tOrthophoto AGEA 2018 – GeoPortale Piemonte;\n•\tPiemonte Land Cover BDTRE (Base Dati Territoriale di Riferimento degli Enti) – GeoPortale Piemonte;\n•\tvehicular mobility TGM (Traffico Giornaliero Medio) – GeoPortale Piemonte.\nThree rockfall design scenarios were identified regarding homogeneous rockfall source areas associated with different design block volumes. Each scenario included more than 3600 source points, extracted through the analysis of the DTM (slope and aspect) and the observation of the orthophoto for the identification of rocky outcrop zones. For each scenario, a quick estimation of a time-independent hazard was performed using the QGIS QPROTO plugin (Castelli et al, 2021). The plugin is based on the Cone Method (Jaboyedoff and Labiouse, 2011) and runs a visibility analysis through the r.viewshed GRASS GIS module, combined with simplified topographic, geomorphological and mechanical considerations. The result of the analysis is a series of raster maps with the distribution of computed values of velocity, energy, and relative spatial hazard. \nThe following step of the IMIRILAND procedure is the analysis of the damage, based on the collection of information on the exposed elements. To this aim:\n•\tthe elements at risk were classified according to various Land Cover categories from BDTRE, associated with relative hierarchical values. Physical and social values were taken into account for each element. Physical value is mainly linked with the type of element and with the reconstruction costs while social value is linked to the presence of persons and the social utility of the asset;\n•\tthe physical exposure of the elements at risk was defined for each hazard scenario with reference to the computed runout area. The social exposure was defined taking into account the time spent by people inside buildings or on the roads. \n•\tthe physical vulnerability of the elements at risk was defined on the basis of the intensity of the phenomenon in terms of rockfall energy and the type of element. The social vulnerability is the same as the physical one inside buildings and is 100% outside buildings. \nPhysical and social damage maps were then obtained for each hazard scenario through the product of the value, the exposure and the vulnerability of the elements located in the involved area. Due to the lack of information on the temporal probability of occurrence of the scenarios, damage maps correspond to relative, time-independent, risk maps. \nThe results show that the highest risk is concentrated in the inhabited areas and some portions of the valley road, according to the few historical information available on the site. \nThe QPROTO plugin is available at the GIT repository of FAUNALIA (gitlab.com/faunalia/QPROTO) and can be easily used by professionals, public administrators, managers of roads, railways or infrastructures for land planning purposes or for preliminary analyses aimed at defining the most critical zone of a wide area, where resources and more in-depth analyses can be focused for mitigation purposes.",
        "description": ""
      },
      {
        "title": "DistrictBuilder, or how TopoJSON was the cause of and solution to all of our problems",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "DistrictBuilder (districtbuilder.org) is a web-based, open-source tool for collaborative political boundary redistricting or redistribution.\n\nIn order to support creating legally valid districts, DistrictBuilder allows advocates and legislators to define districts using geometries as small as a single census block, which are very numerous – a medium-sized state will have hundreds of thousands of them. Users can create districts from any combination of geometries, and we need to be able to generate statistics and dissolve them into district geometries in near real-time.\n\nBy reformatting our data as TopoJSON, a file format and Node.js library for working with topological data, we are able to dissolve over half a million census blocks into legislative districts in only a few seconds!\n\nI’ll discuss how we use TopoJSON in DistrictBuilder; the issues we encountered when using it at scale in production and how we were able to overcome them; and the other tools we considered instead of TopoJSON and how they compared in terms of performance.\n\nI’ll also go over our strategy for displaying and calculating metrics in real-time in the browser, using typed arrays and web-workers in combination with Mapbox vector tiles to do real-time aggregation of statistics from hundreds of thousands of features.",
        "description": ""
      },
      {
        "title": "Geospatial Indexing with Apache Lucene and OpenSearch",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Come have a look under the covers at the data structures that enable geospatial and multi-dimensional indexing and search at massive scale in Apache Lucene and OpenSearch. This talk will cover not only the indexing structures considered and ultimately implemented in the Apache Lucene Open Source Project but the exceptional performance improvements and centimeter spatial accuracy obtained in the latest release. As a bonus, this talk will cover new and upcoming Spatial Analysis Aggregations and Processing available in the OpenSearch Open Source project.\n\nFrom tessellation to multidimension encoding and block KD trees this talk will cover the algorithms and data structures written and committed to the following open source projects:\n\nApache Lucene (specifically the release of BKD based geo indexing https://issues.apache.org/jira/browse/LUCENE-8396)\nPerformance benchmarks for Lucene Spatial Indexing: https://home.apache.org/~mikemccand/geobench.html\n\nFinally, we will discuss the future of the project including existing and evolving support for custom coordinate reference systems and projections, spatial regression modeling and statistics, and spatial visualizations with OpenSearch Dashboards.",
        "description": ""
      },
      {
        "title": "Geospatial and Apache Arrow: accelerating geospatial data exchange and compute",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "The Apache Arrow (https://arrow.apache.org/) project specifies a standardized language-independent columnar memory format. It enables shared computational libraries, zero-copy shared memory, streaming messaging and interprocess communication without serialization overhead, etc. Nowadays, Apache Arrow is supported by many programming languages. \n\nGeospatial data often comes in tabular format, with one (or multiple) column with feature geometries and additional columns with feature attributes. This is a perfect match for Apache Arrow. Defining a standard and efficient way to store geospatial data in the Arrow memory layout (https://github.com/geopandas/geo-arrow-spec/) can help interoperability between different tools and enables us to tap into the full Apache Arrow ecosystem:\n\n- Efficient, columnar data formats. Apache Arrow contains an implementation of the Apache Parquet file format, and thus gives us access to GeoParquet (https://github.com/opengeospatial/geoparquet) and functionalities to interact with this format in partitioned and/or cloud datasets.\n- The Apache Arrow project includes several mechanisms for fast data exchange (the IPC message format and Arrow Flight for transferring data between processes and machines; the C Data Interface for zero-copy sharing of data between independent runtimes running in the same process). Those mechanisms can make it easier to efficiently share data between GIS tools such as GDAL and QGIS and bindings in Python, R, Rust, with web-based applications, etc.\n- Several projects in the Apache Arrow community are working on high-performance query engines for computing on in-memory and bigger-than-memory data. Being able to store geospatial data in Arrow will make it possible to extend those engines with spatial queries.",
        "description": ""
      },
      {
        "title": "What’s new in geospatial Elasticsearch",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "[Elasticsearch] is a well-known and mature NoSQL database providing search and analytics services for big datasets. The “elasticity” of its name comes from the distributed design and easy scalability capabilities that have made it an industry leader for more than ten years. In this talk we will present two exciting new features that have been added recently to the product related with the geospatial topic: vector tiles support and line and hexagon aggregations.\n\nVector tiles have become an industry standard to encode large amounts of data to be displayed in the browser by web mapping libraries like MapLibre or OpenLayers. Elasticsearch analytics & geo team has added a [new API endpoint] that renders search and aggregation queries as zipped [protobuffers], allowing developers to retrieve right from the datastore assets that are ready to be sent to the user's browser without much further processing. This will speed up the rendering of large datasets by avoiding transferring JSON assets from Elasticsearch to application middleware.\n\nElasticsearch geospatial aggregation capabilities have been extended recently by two new methods, one is to allow combining related points into a new [line geometry] (think of a vehicle track) and the other is to aggregate geometries into an [hexagon grid]. The new geo-line aggregation will be very useful for asset tracking use cases where the second enables Elasticsearch to perform powerful analytics combined with the extensive support for metric aggregations.\n\nIn this talk we will present this project, going through the different use cases with some examples and demonstrations using both Kibana [Elastic Maps] and a simple ad-hoc web project that leverages this new feature.\n\n[Elasticsearch]: https://www.elastic.co/elasticsearch/\n[protobuffers]: https://developers.google.com/protocol-buffers\n[line geometry]: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-geo-line.html\n[hexagon grid]: https://www.elastic.co/guide/en/elasticsearch/reference/8.1/search-aggregations-bucket-geohexgrid-aggregation.html\n[new API endpoint]: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-vector-tile-api.html\n[Elastic Maps]: https://www.elastic.co/maps",
        "description": ""
      },
      {
        "title": "Spatial data processing with workflow engines",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Workflow engines like Apache Airflow are commonly used in data engineering nowadays. They provide an infrastructure for setting up, executing and monitoring a defined sequence of tasks, arranged as a workflow application. Tasks and dependencies are defined in a declarative way or in a programming language like Python. Airflow established using directed acyclic graphs (DAGs) to manage workflow orchestration.\n\nThis talk compares a selected subset out of the huge number of available Open Source workflow engines, which are especially suited for workflows containing spatial data processing. It compares the well known Apache Airflow engine with Dagster, an other solution using DAGs and a BPMN-based workflow engine using Celery as distributed task queue.\n\nIn the same space there is the new OGC API - Processes standard which is a modern REST API for wrapping computational tasks into executable processes. This talk gives an overview of the API and shows possible integrations with available workflow engines.",
        "description": ""
      },
      {
        "title": "A new SQL library to enable spatial analytics in Spark",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "In this talk, we'll review the major milestones that have defined Spatial SQL as the powerful tool for geospatial analytics that it is today. \n \nFrom the early foundations of the JTS Topology Suite and GEOS and its application on the PostGIS extension for PostgreSQL, to the latest implementation in Spark SQL using libraries such as the CARTO Analytics Toolbox for Databricks, Spatial SQL has been a key component of many geospatial analytics products and solutions, leveraging the computing power of different databases with SQL as lingua franca, allowing easy adoption by data scientists, analysts and engineers. \n \nThe CARTO Analytics Toolbox is a comprehensive library that provides advanced geospatial functionality through Spark SQL. It enables Spatial SQL analytics at scale providing the foundational tools for analyzing and visualizing geospatial data. \n \nIn this talk we'll cover the technical aspects of the library implementation using Open Source technologies, as well as demonstrating the installation and practical usage with a real-life example. \n \nOur talk will go through some of the geospatial operations that can be performed directly in Spark and we will demonstrate how users of the Analytics Toolbox can create beautiful map visualizations leveraging the latest Open Source rendering tools; and how to address a wide variety of spatial use cases using other products built on top of open source technologies, like CARTO and Databricks.",
        "description": ""
      },
      {
        "title": "State of GeoBlaze: A Blazing Faster Raster Analysis Engine in Pure JavaScript",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "**[GeoBlaze](https://geoblaze.io)** is a blazing fast raster analysis engine written in pure JavaScript.  With geoblaze, you can run computations ranging from basic statistics (min, max, mean, median, and mode) to band arithmetic and histogram generation in either a web browser or a node application.\n\n### presentation\nThis presentation will go over recent updates to GeoBlaze, including the addition of support for Cloud-Optimized GeoTIFFs.  We will also discuss the roadmap for the next couple years.\n\n### use cases\nGeoBlaze can be used wherever vectors and rasters meet.  You can use it to calculate the hectares of wheat in a country, the change in daily median earth temperature, and identify wildfires in satellite imagery.\n\n### environment\nBecause GeoBlaze is written in pure JavaScript it can be run in various environments, on an EC2 server, Lambda function, Cloudflare worker, or in the browser.  It performs calculations using the CPU, so it is not restricted only to environments where a GPU is available.\n\n### notable dependencies\nGeoBlaze is built on top of the following open-source projects: [dufour-peyton-intersection](https://github.com/GeoTIFF/dufour-peyton-intersection), [georaster](https://github.com/geotiff/georaster), [geotiffjs](https://github.com/geotiffjs/geotiff.js),  and [calc-image-stats](https://github.com/danieljdufour/calc-image-stats).\n\n### sample notebooks:\n- Time Series Analysis with GeoBlaze: Mean Daily Air Temperature for the Month of May: https://observablehq.com/@geosurge/time-series-analysis-with-geoblaze-mean-daily-air-temperat\n- Identifying Carr Wildfire with Landsat 8: https://observablehq.com/@geosurge/identifying-carr-wildfire-with-landsat-8\n- Hectares of Rainfed Wheat in Ukraine: https://observablehq.com/@danieljdufour/hectares-of-rainfed-wheat-in-ukraine",
        "description": ""
      },
      {
        "title": "The benefits of COG (Cloud Optimized GeoTIFF) outside the cloud",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "This is a technical feedback about why the COG (Cloud Optimized GeoTIFF) format is valuable outside the cloud and can speed up productivity in many ways.\n\nDuring first months, remote work and COVID, IT department was overbooked and has to face to many issue such bandwidth limitation. Images display was suffer in GIS client. Of course, webservice was always available but user has to control on band order or radiometry settings. WCS is supposed to be the solution. Unfortunately, it offered degraded performance.\n\nCOG is supposed to be serve from HTTP server or S3. But we’ve simply test from a network drive / mount point and it offer great performance. Depending internet connection, it could be as fast as it is in local ! \nCOG advantage must be consider outside of the cloud as remote work tends to develop more and more. It could avoid to deploy heavy webservice infrastructure for only raster visualization. \n\nFrom other side, benchmark between publish some other format compare to COG in GeoServer. From Regional Data Infrastructure, it’s streamline storage data between raster format as input file for webservices and opendata raw downloading services as open archives.\n\nFinally, I will give some feedback and tips and tricks to find best parameters to convert orthophotography, DEM or DSM, etc. to COG.",
        "description": ""
      },
      {
        "title": "Update on Modular OGC API Workflows specifications",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Update on the status of OGC API standards and draft specifications enabling client-driven execution of processing workflows, supporting on-demand and ad-hoc selection of data and algorithms. Overview of the capabilities enabled by OGC API - Tiles, OGC API - Coverages and Processes – Part 3: Workflows and Chaining. Demonstration of both a server and a client implementing these specifications.\n\nThe Workflows and Chaining draft extension specification to OGC API – Processes enables ad-hoc execution of workflows integrating processes and data available from one or more OGC API instances. The specification allows triggering processing as a result of requesting results for a specific area and resolution of interest, which provides a simple mechanism to chain geospatial data inputs and outputs. \n\nBy referring to a collection of geospatial data irrespective of a particular area, resolution or date/time of interest, workflows can be defined in a generic, re-usable manner, and processing can be performed on-demand rather than (or in addition to) as a batch execution. Such on-demand processing has the advantage of optimizing the use of computing resources and speeding up the availability of the latest available data, such as for continuously captured Earth Observation satellite imagery.\n\nThe initial version of the Workflows and Chaining specification was a result of a GeoConnections 2020-2021 project funded by Natural Resources Canada, which also supported the development of a unified OGC API driver in GDAL allowing to directly visualize the results of such workflows in QGIS.\n\nOGC API – Tiles is the specification succeeding to WMTS in the OGC API family, leveraging the concept of 2D Tile Matrix Sets. In addition to providing tiles of maps or imagery, Tiles can also be used to distribute raw data tiles, including coverage and vector tiles. Using tiles to deliver results and trigger execution of processing workflows can facilitate caching while allowing to efficiently select an area and resolution of interest.\n\nOGC API – Coverages is the specification suceeding to WCS in the OGC API family, and provides a simple mechanism to request an optionally down-sampled subset of a coverage. Specific fields (e.g. imagery band) can be selected as needed. The Coverages specification can also be used to request results while triggering execution of a workflow.",
        "description": ""
      },
      {
        "title": "Introducing WIS 2.0 in a box: an open source and open standards platform for international weather, climate and water data discovery, access, and visualization",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The World Meteorological Organization (WMO) Information System (WIS) is a coordinated global infrastructure responsible for telecommunications and data management functions and is owned and operated by WMO Members.\n\nWIS 2.0 will provide users with seamless access to diverse information from a wide range of sources and will enable weather, water and climate information to be related to socioeconomic and other contexts. Through an open ecosystem of tools, applications and services, WIS 2.0 will allow all information providers to manage, publish and share their data, products and services, and will allow all users to develop value-added services and new products.\n\nThe WIS 2.0 principles highlight and promote the value of standards, interoperability and the Web/mass market. This will extend the reach of weather/climate/water data for a number of societal benefits.\n\nWIS 2.0 is being designed to have a low barrier to entry for data providers. This will also result in enabling infrastructure and provide great benefit for less developed countries (LDCs). There is a strong motivation to provide LDCs easy to use tools and sustainable workflow for data exchange to 1./ ease the burden of exchanging data 2./ continue to provide valuable weather/climate/water data in WIS 2.0 over time.\n\nThe WIS 2.0 in a box (wis2box) project enables LDCs free and open source onboarding technology to integrate their data holdings and publish them to WIS 2.0 in a manner consistent with the architecture for plug and play capability, supporting discovery, access and visualization.  \n\nThis presentation will provide an overview of the project and current capabilities highlighting the use of numerous FOSS4G tools and PubSub driven implementation of OGC API standards.",
        "description": ""
      },
      {
        "title": "Designing dynamic forms in QGIS Desktop with expressions",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "This presentation will show tips and tricks how to design dynamic and relatively complex forms in QGIS desktop - with the help of the drag and drop form designer, widget configurations, dynamic expressions, data-defined widget visibility, default values, constraints, embedded forms, relations, actions and more. In addition, we will show how you can use spatial joins from other layers to automatically fill in data from independent but spatially related layers.\n\nYou will be walked through an application developed for the management of biodiversity subsidies in the Kanton of Solothurn, Switzerland. The application allows to collect data from eligible areas in the canton's biodiversity programme. Farmers and foresters can apply for separate subsidies for biodiversity support if the areas and their management methods meet certain criteria. The QGIS based application allows to collect data, automatically assigns parcel numbers, place names, community names, etc. and allows to define usage restrictions and record maintenance measures. Interfaces exist for a reporting generator (contract generation) and an SAP based disbursement system for the payment of subsidies.\n\nIn the presentation we will present the result of the development work and show some tips and tricks with forms, widgets, expressions and actions and how we stitched everything together.",
        "description": ""
      },
      {
        "title": "QGIS Temporal Controller with WMS-T layers",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "QGIS is a freely downloadable open source GIS software suite that contains a desktop option, mobile, and web component. QGIS is free to download and use, it is released with a GPL v3 license which is a non commercial license allowing users to download and use it without concerns compared to other commercial GIS software.\nUp to QGIS version 3.12 there was no core support for temporal data, users were required to install a plugin called TimeManager in order to visualize temporal data inside QGIS. Through a collaboration between the Canadadian Government, Kartoza and North Road, efforts were made to add core support for temporal data inside QGIS.\nAs a result the QGIS version 3.14 was released with a Temporal Controller feature which was now responsible for handling all the temporal layers inside QGIS. The initial role out of the Temporal Controller contained support for raster, vector and WMS-T layer providers.\nThis session will explore how to use the QGIS Temporal Controller to do animation and visualization of the WMS-T layers, this will include how to setup a standard WMS server that will be serving time based layers. \nIn the session we will also learn about the Temporal Controller API, how to use it through QGIS python bindings and create a simple QGIS plugin that will show the API in action.",
        "description": ""
      },
      {
        "title": "A Python tool for QGIS for gender recognition in street directories",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "In the last years the attention for gender equality in all context has increased all over the world.\nNowadays the sensibility of Public Administrations towards the naming of streets, roads, squares and monuments after women has highlighted that, instead, the toponymy has always been oriented to the choice of male figures.\nIn this work we present a Python script for QGIS, that allows to verify if a proper name, contained in a street directory, is of male or female gender.\nThere are other Open Source projects that, starting from an address, verify the gender of the represented person; the most famous is the GeoChicas Project [1]; in Italy it is worth mentioning the \"Toponomastica Femminile\" Association [2] that manually verifies the streets dedicated to women, according to a predefined taxonomy (religious women, artists, etc.).\nThe goal of the present work is to automate the gender reconnaissance starting from a list of names; however, unlike GeoChicas that use as a base parameter a dictionary of names with which to compare the list, we propose to make a query of DBpedia via SPARQL in order to identify the subject and derive its gender. \nIf the address is the attribute of a spatial dataset, then it is possible to add a new attribute (the gender) to the vector layer table as a result of DBpedia query.\nThis approach overcomes language limitations (which would require differentiated dictionaries) and the ambiguities that some names would have (for example the nome \"Andrea\" is used as both a masculine and feminine name).\nThe script is created using the SPARQL language with a very simple structure, in which the triplet of data is constructed in order to obtain the gender from the name of a person through the query of DBpedia.\nThe script can be run in QGIS environment associating the data outputs directly to the geometry or even outside of QGIS and as a result you will have a list of \"genders\".\nThe process of relying on Wikipedia/DBpedia has the twofold advantage that, where the name dedicated to street exists, then the desired information is taken, the gender in our case, while if it missing it can be added or enriched. \nThe script is currently under validation and will be published in the dedicated git repository [3].\n\n[1] https://github.com/geochicasosm \n[2] https://www.toponomasticafemminile.com \n[3] https://github.com/skampus/toponomasticafemminile",
        "description": ""
      },
      {
        "title": "Advanced QGIS forms into the web with Lizmap",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "You would like many people from your team or crowdsourcing to fill data in your geodatabase. One way to do that is to make appealing, easy to use and well-constructed forms avoiding wrong inputs. Also, you do not want people to give up filling because the form is too long while in the same time you could automatically fill some entries based on others. With QGIS Desktop, it is possible to make great maps but also advanced forms by using expressions to control field visibility, default values, proposed values, constraints and more. It is very powerful but now how to share those forms to anybody whatever their device or operating system? Could it be possible to share a link for people to open and fill those forms in their web browser?\nLet’s see how you can get most of these features for your forms in web browsers thanks to QGIS Server and Lizmap.",
        "description": ""
      },
      {
        "title": "Dataviz in QGIS and on the web",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "#### QGIS and Dataviz\n\nCreating plots is out of the main scopes of QGIS but thanks to the simple Python API, it is easy enough to create additional scripts and plugins. The DataPlotly plugin has been developed for QGIS(the first release was created in 2017 while nowadays the plugin has been downloaded more than 100,000 times). It's today a well maintained Python plugin with a growing community of developers, users and testers. \n\nDataPlotly allows creating D3 like plots from spatial data. It is build on top of [Plotly.com](Plotly.com), a JavaScript library which offers an easy API for many languages such as Python, R, Matlab etc.\n\nThe plots are completely interactive so that plot elements are directly linked with map items; therefore the user is able to query map items from the main plot canvas. Thanks to a crowdfunding campaign, the functionalities of DataPlotly were extended: a complete refactoring of the code, more plots but especially the creation of plots in the layout composer, also for atlas layouts.\n\nThe plugin is also compatible for QGIS server. Lizmap Web Client is an opensource server application to publish QGIS project on the web without any coding skills needed. It’s using QGIS Server in the backend so users have the same rendering between their QGIS Desktop and the web version of their project.\nThanks to the DataPlotLy plugin installed on QGIS Server and to the Lizmap application, it allows users to print PDF with plots from in their web-browser.",
        "description": ""
      },
      {
        "title": "QGIS and Community: The QGIS Open Day",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "The QGIS Open Day are organised on the principle of self-organisation and community participation. The monthly sessions are open to anyone in the opensource community and cover various topics from presenting new developments and releases, tutorial style work-flows and interactive open sessions. \nIn the year the channel has been active, QOD has generated over 50 videos obtained 4000 subscribers and on average QOD channel receives Approximately 5000 views each month. Most QOD viewers are from the United States, Germany, India, France and the UK and 94% of QOD viewers are male. The most popular video on the channel is “A geological map work-flow in QGIS with Chris Lambert with” 5277 views.\nLooking forward, the QOD channel aims to be the official platform to show the functionality of the new QGIS releases, plugins, work-flows, and opensource GIS platforms. The channel aims to increase support, viewership and participation from a wider, more diverse audience and encourage different regions to contribute. Join the QOD community and let’s learn from each other.",
        "description": ""
      },
      {
        "title": "QGIS Data Versioning with Kart",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "Maybe you've heard of [Kart](https://kartproject.org), the great new geodata versioning tool from the team at Koordinates? But did you know that Kart also has a QGIS plugin so you can do *real* data versioning without needing to leave QGIS?\n\nIn just 5 minutes we'll demonstrate how to import data into a new Kart repository, make and review some changes, merge a branch, and push everything to a remote server. All from QGIS!\n\n—\n\nWe’re drowning in data, but the geospatial world lags badly behind in versioning tools compared to our software counterparts. Kart (https://kartproject.org) is solving this with a practical open tool for versioning datasets, enabling you to work more efficiently and collaborate better.\n\nKart allows you to quickly and easily manage history, branches, data schemas, and synchronisation for large & small datasets between different working copy formats, operating systems, and software ecosystems.\n\nModern version control unlocks efficient collaboration, both within teams and across organisations meaning everyone stays on the same page, you can review and trace changes easily: ultimately using your time more efficiently.",
        "description": ""
      },
      {
        "title": "Creating GIS Rest APIS using Geodjango under 30 minutes",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "We're living in the world of APIs. CRUD operations are base of lot of operations. Many smart frameworks such as Django, Flask, Laravel provides out of the box solutions to filter the data, which covers almost all needs to separate data based on column values. \nWhen it comes to Geospatial data, we expect to filter data based on their location property instead of metadata. This is where things get complicated, if you are using framework that doesn't have package, library built to handle such use cases, you are likely to be dependent on either database or any external package to handle it.\n\nFortunately Geodjango[https://docs.djangoproject.com/en/4.0/ref/contrib/gis/] (Django's extension) allows us to create databases which understands geometry and can process it[https://docs.djangoproject.com/en/4.0/ref/contrib/gis/geoquerysets/#gis-queryset-api-reference]. It also provides support to write APIs using Rest Framework extension [https://pypi.org/project/djangorestframework-gis/] which takes this to next level allowing user to output the data in various formats, creating paginations inside geojson, create TMSTileFilters, etc.\n\nIn this talk we'll scratch the surface of this python package and see how to build basic CRUD APIs to push, pull GIS data along with filtering it to the PostgreSQL database",
        "description": ""
      },
      {
        "title": "Agile Geo-Analytics: Stream processing of raster- and vector data with dask-geomodeling",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "We present *dask-geomodeling*: an open source Python library for stream processing of GIS raster and vector data. The core idea is that data is only processed when required, thereby avoiding unnecessary computations. While setting up a dask-geomodeling computation, there is instant feedback of the result. This results in a fast feedback loop in the (geo) data scientist’s’ work. Big datasets can be processed by parallelizing multiple data queries, both on a single machine or on a distributed system. \n\n### Abstract \nIn geographical information systems (GIS), we often deal with data pipelines to derive map layers from various datasets. For instance, a water depth map is computed by subtracting the digital elevation map (DEM) from a water level map. These procedures are often done using open source products such as PostGIS and QGIS. However, for medium to large datasets (> 10 GB) the extent of these analyses are costly due to memory restrictions and computational cost. As a rule, these issues are tackled by manually cutting the dataset into smaller parts. However, this is a tedious and time-consuming task. In case one needs to this regularly, this is not feasible. \n\nWe present the open source Python library *dask-geomodeling* [1] to solve this issue. Instead of a script, dask-geomodeling requires a so-called “graph”, which is the definition of all operations that are required to compute the derived dataset. This graph is generated by plain Python code, for instance: \n\n```\nplus_one = RasterFileSource('path/to/tiff') + 1 \n```\n\nNote that these operations are lazy: there is no actual computation done and therefore the above line executes fast. Only when actual data is requested: \n\n```\nplus_one.get_data( \n    bbox=(155000, 463000, 156000, 464000), \n    projection='epsg:28992', width=1000, height=1000 \n)\n```\n\nAn array containing the data is computed. No need to load the whole TIFF-file in memory if you only use a small part! \n\nThe computation occurs in two steps. First, a computational graph is generated containing the required functions. While generating the computational graph, the operations may be chunked into smaller parts. Second, this graph is evaluated by *dask* [2], using any scheduler (single thread, multithreading, multiprocessing, distributed) that is provided dask. \n\nThis library is open source under the name “dask-geomodeling” and is distributed on Github, PyPI, and Anaconda. A hosted cloud version is also available under the name Lizard Geoblocks [3]. Currently, we have implemented a range of operations for rasters, vectors, and combinations. The community is welcome to use our library, benefit from it, and expand it! \n\nReferences \n\n1. dask-geomodeling, https://github.com/nens/dask-geomodeling, https://dask-geomodeling.readthedocs.io/ \n2. dask, https://dask.org/\n3. Lizard Geoblocks, https://lizard.net/",
        "description": ""
      },
      {
        "title": "pyotb: a pythonic extension of Orfeo ToolBox",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "Orfeo ToolBox (OTB) is an open-source project for state-of-the-art remote sensing, made for large-scale image processing. It is written in C++ and a Python interface is available. However, the use of plain OTB in Python requires a lot of code; more than what a Python user is used to!\n\npyotb aims at making the use of Orfeo ToolBox easy in Python. In this talk, discover:\n- how to run any application of OTB in just one line of code\n- how to build complex processing chains containing several applications in an intuitive way.\n- how to interact easily with NumPy and Tensorflow.\n- some pythonic features made for user convenience.\n- some functions written to mimic the behavior of some well-known NumPy functions: `pyotb.where`, `pyotb.clip`, `pyotb.all`, `pyotb.any`... and counting!\n\nOTB has an amazing pool of applications and can run on all types of computers: from resource limited laptops to high performance clusters. With pyotb, unleash the power of OTB in Python! \n\nWe will make you love the way you can use OTB in Python. You can find more info on the project on the pyotb repository: https://gitlab.orfeo-toolbox.org/nicolasnn/pyotb",
        "description": ""
      },
      {
        "title": "Building a data analytics library in Python",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The Data Operations Systems and Analytics team at NYC DOT’s primary mission is to support the data analysis and data product needs relating to transportation safety for the Agency. The team’s work producing safety analysis for projects and programs typically involves merging data from a variety of sources with collision data, asset data, and/or program data. The bulk of the analysis is performed in PostgreSQL databases all with a geospatial component. The work necessitates ingesting input data from other databases, csv/excel files, and various geospatial data formats. It is critical that the analysis be documented and repeatable. \n\nMoving data around, getting external data into the database, transforming it, geocoding it etc., previously occupied the bulk of the team’s time before, reducing capacity for the actual analysis. Additionally the volume of one-off and exploratory analyses resulted in a cluttered database environment with multiple versions of datasets with unclear lineage and state of completeness. \nModeled on the infrastructure as code idea, we began building a python library that would allow us to preserve the entire analysis workflow from data ingestion to analysis and to output generation in a single python file or Jupyter notebook.  The library began as a way to reduce the friction and standardize the process of ingesting external data into the various database environments utilized. It has since grown into the primary method to facilitate reproducible data analysis processes that includes the data ingestion, transformation, analysis, and output generation.\n\nThe library includes basic database connections, and facilitates quick and easy import and export from flat files, geospatial data files, and other databases. It provides both inferred and defined schemas, to allow both quick exploration and more thoroughly defined data pipeline processes.  The library includes standardization of column naming, comments, and permissions. There are built in database cleaning processes, geocoding processes, and we have started building simple geospatial data display functions for exploratory analysis. The code is heavily reliant on numpy, pandas, GDAL/ogr2ogr, pyodbc, psycopg2, shapely, and basic sql and python. The library is not an ORM, but occupies a similar role, but geared towards analytic workflows.\n\nThe talk will discuss how the library has evolved over time, the functionality and use cases in the team’s daily workflows as well as where we would like to extend the functionality and open it up for contributions.  While the library is not currently open source, we are actively working on creating an open version and migrating to Python 3.x. This library has greatly improved the speed and simplicity of conducting exploratory analysis and enhanced the quality and completeness of the documentation of our more substantial data analytics and research.\nThe library should be of interest and utility for anyone working with data without the support of a dedicated data engineering team to facilitate the collection of multiple datasets from a variety of formats, as well as anyone looking to standardize their data analysis workflows from beginning to end.",
        "description": ""
      },
      {
        "title": "Not too big, not too small: open source geospatial units that are just right",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Publicly available data tends to be spatially aggregated to administrative units, limiting the feasibility of nuanced analyses that reflect the natural state of communities and provide actionable insights for a wide range of stakeholders. While higher resolution data is generally available within government agencies, access for external researchers is limited due to well-established privacy concerns. Inspired by our own use case of developing a regional quality of life metric for neighborhoods in Denmark, our team at Aalborg University’s Department of the Built Environment, in collaboration with data.org’s Growth and Recovery Challenge, and Data Clinic, set out to develop and open source not only foundational granular spatial units and data that adhere to privacy laws, but also the accompanying methodology that has the potential for broad applicability in other countries. \n \nIn this presentation, we will demonstrate the methodology’s generalizability, particularly across common European land use and geographical features, and show how the resulting high-resolution shape files and community data can become crucial tools for government decision-makers, community organizations, and researchers in their efforts to increase transparency and engage in practical, actionable research.\n \nFocused initially on our Denmark use case, we algorithmically create spatial units with minimum household and population counts from country-wide hectare cell level data. Our approach uses data on road networks and administrative boundaries to create socially meaningful component polygons. This is achieved by developing tools based on already existing open source packages available in R and Python. The hectare cells are then mapped onto the polygons and clustered using the max-p regionalization algorithm with constraints on the minimum population and household counts to arrive at the final set of spatial units. \n \nTo improve the accessibility of this data to not just researchers but also administrative decision-makers, community organizations, and the general public, we are developing an online tool to explore and visualize indicators within the resulting fine-grained regions such as disposable income, educational level, housing prices, migration rates, distances to public institutions, and labor market attachments in Denmark. Regional inequality in Denmark has increased over time, and with the help of this tool, we hope to provide the ability to study these key metrics both within and across municipal regions. In the development of the tool, we prioritize user feedback and common use cases to ensure both applicability and longevity.\n \nThis project has been developed with an open-source mindset by: 1) creating flexible open data resources that can adapt to a wide range of public use cases 2) open sourcing the methodology for use in other countries/regions and 3) enabling the use of existing open data and tools such as Open Street Maps, R and Python in the pipeline.\n \nWe firmly believe that the project has the potential to improve knowledge sharing and collaboration between GIS experts, decision-makers, researchers and the general public not only in Denmark, but also in Europe and beyond.",
        "description": ""
      },
      {
        "title": "Power Grid Mapping In West Africa (Sierra Leone)",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "This is a presentation about mapping of electricity poles in Sierra Leone with the use of open source geospatial tools. It highlights the use of community members to create and use data for social and economic purposes. It delves into how YouthMappers are sharing and using acquired skills within their chapters and communities as a whole. \nThe project starts with the skills assessments of existing YouthMappers and development of strategies to train or capacitate new mappers with the required skills, this includes the . It entails the processes of community engagements and working with stakeholders and relevant authorities in project implementation. It highlights the workflow with the use of Open Source Geospatial tools and  their outcomes. It also talks about the levels of collaboration between different organizations to achieve a common goal. The presentation will be an engaging with questions and answers and live showcase of tools to make the presentation clear to those that would be participating.",
        "description": ""
      },
      {
        "title": "Aggregating risk with H3 and PostGIS",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "In this talk we will look at how PostGIS and Uber's H3 index can be used for aggregating large amounts of data, in our case property insurance risk, in real-time.  We will explore a number of different techniques from the H3 PostGIS extension generating GeoJSON, to generating MVTs from the database to pre-caching the H3 index and painting a vector tile layer client side.  For our client side layer will use a React JS interface, Maplibre and will also look at Deck.GL for more advanced use cases.  We will discuss how the stack can be deployed using a serverless architecture running on AWS Lambda and Aurora Serverless Postgres.\n\nThis talk requires no prior knowledge however some experience with PostGIS and vector tiles will be useful.  You will learn techniques which can be applied to any problem domain where there is the need to work with data volumes where processing individual points would not be practical.",
        "description": ""
      },
      {
        "title": "Building a Geocoder on top of PostGIS - a Field Report",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "It seems to be conventional wisdom that a search engine for geodata is best implemented with a text search engine like OpenSearch or Solr. Most of available open-source geocoders follow that wisdom. Nominatim is the odd one out. OpenStreetMap's main geocoder was originally developed 12 years ago as a proof of concept that a geocoder can be efficiently implemented on top of a PostgreSQL/PostGIS database. Since then it has grown into mature project. And so have the PostgreSQL database and the OpenStreetMap project.\n\nIn this talk, I will share some of the experiences of working with PostGIS on a growing OpenStreetMap dataset. The talk starts with a quick overview about what the Nominatim database looks like under the hood. It then goes on to present some of the lessons we have learned over the last 10 years on managing a PostGIS database with more than 270 million searchable places. We talk about features that improved performance and about some that are best avoided. The talk concludes with some general observation about implementing search on top of an SQL database.",
        "description": ""
      },
      {
        "title": "United Nations Mission in South Sudan GeoStories",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The United Nations Mission in South Sudan (UNMISS) is a United Nations peacekeeping mission for South Sudan, which became independent on 9 July 2011. UNMISS was established on 8 July 2011 by United Nations Security Council Resolution 1996 (2011) and as of March 2021, it is composed of 19,075 total deployed personnel including 14,222 troops; 217 experts on mission; 1,446  police personnel; 2,228 civilians; 387 staff officers and 388 UN Volunteers, where, it is headquartered in the South Sudanese capital of Juba. \n\nUnder Chapter VII of the Charter of the United Nations, UNMISS is therefore authorized to use all necessary means to implement its mandate which includes:\n(a) Protection of civilians\n(b) Creating conditions conducive to the delivery of humanitarian assistance\n(c) Supporting the Implementation of the Revitalized Agreement and the Peace Process\n(d) Monitoring, investigating, and reporting on violations of humanitarian and human rights law\n\nThe mission has decided to extend its public outreach activities in a different method by utilizing geospatial information and using open geospatial tools and data for showcasing some of its important activities in support of above-mentioned mandates, and for this purpose contracted a service provider through bidding exercise and procurement protocols. \n\nIn this general session talk, speaker(s) will give their presentations on below topics:\n- UN Open GIS Initiative Background \n- UNMISS GeoStories architecture, FOSS4G tools and data\n- Preventing mis/dis-information by extending public outreach\n- Review selected Geostories in support of UNMISS mandate",
        "description": ""
      },
      {
        "title": "Geo-Infographics created dynamically from PostGIS using ST_AsSVG",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "## The Problem\nLet's assume you have an attribute-focused table, but you would still like to see a thumbnail of the associated geometry. Or more generally: How to dynamically render polygon geometries in a HTML page without any mapping library. **Enter ST_AsSVG (PostGIS function)!**\n\n## Context\nLast year [I showed how we display geo data in our webapps using vector tiles (ST_AsMVT)](https://www.youtube.com/watch?v=s_dWBOiuFiY&t=139s). This year I will explain how we apply ST_AsSVG of PostGIS on database records to **create beautiful geo-infographics** in pure HTML. The result is a geo-visualization similar to this one: [Comparison maps of Australian Cities (Size, Population)](https://imgur.com/OQClpbc). The trickiest part will be the sizing of the SVG objects ([viewport vs. viewBox](https://webdesign.tutsplus.com/tutorials/svg-viewport-and-viewbox-for-beginners--cms-30844)).\n\n## Content\nThe talk will contain some theory on SVG. It will then show basic setups for FastAPI, SQLModel, Jinja2 and, of course, PostGIS. All code will be made available via GitHub. \n\n## Aim\nAfter the talk you will master sizing of SVG and be capable of creating your own dynamic geo-infographics directly from data stored in your PostGIS database.",
        "description": ""
      },
      {
        "title": "Data integrity risks when using simple feature",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "When you care about data integrity of spatial data you need to know about the limitations/weaknesses of using simple feature datatype in your database. For instance https://land.copernicus.eu/pan-european/corine-land-cover/clc2018 contains 2,377,772 simple features among which we find 852 overlaps and 1420 invalid polygons. For this test I used “ESRI FGDB” file and gdal for import to postgis.  We find such minor overlaps and gaps quite often, which might not be visible for the human eye. The problem here is that it covers up for real errors and makes difficult to enforce database integrity constraints for this.  Close parallel lines also seems to cause Topology Exception in many spatial libraries.\n\nA core problem with simple features is that they don't contain information about the relation they have with neighbor features, so integrity of such relations is hard to constraint. Another problem is mixing of old and new data in the payload from the client. This makes it hard and expensive to create clients, because you will need a full stack of spatial libraries and maybe a complete locked exact snapshot of your database on the client side. Another thing is that a common line may differ from client to client depending on spatial lib, snapTo usage, tolerance values and transport formats.\n\nIn 2022 many system are depending on live updates also for spatial data.  So it’s big advantage to be able to provide a simple and “secure” API’s with fast server side integrity constraints checks that can be used from a standard web browser. When we have this checks on server side we will secure the equal rules across different clients.\n\nIs there alternatives that can secure data integrity in a better way? Yes, for instance Postgis Topology. The big difference is that Postgis Topology has more open structure that is realized by using standard database relational features. This lower the complexity of the client and secures data integrity. In the talk “Use Postgis Topology to secure data integrity, simple API and clean up messy simple feature datasets.” we will dive more into the details off Postgis Topology\nBuilding an API for clients may be possible using simple features, but it would require expensive computations to ensure topological integrity but to solve problem with mixing of new and old borders parts can not be solved without breaking the polygon up into logical parts. Another thing is attribute handling, like if you place surface partly overlapping with another surface should that have an influence on the attributes on the new surface.\n\nWe need to focus more on data integrity and the complexity and cost of creating clients when using simple feature, because the demands for spatial data updated in real time from many different clients in a secure and consistent way will increase. This will be main focus in this talk.",
        "description": ""
      },
      {
        "title": "FunctionalScope - Interactive real-time simulation tool for neighborhood planning",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The FunctionalScope software builds on the concept of the CityScope, developed by the MIT Media Lab City Science Group. The FunctionalScope supports urban planners in the functional planning phase of new neighborhoods, the phase in which a competition design proposal is refined in preparation for creating a binding land use plan (Bebauungsplan). \n\nThe tool offers a 3D view of the new urban design, vector(ized) data of the architectural  designs, embedded into a MapLibre based application in the browser. Several near-to-realtime APIs offer the opportunity to evaluate a neighborhood’s design performance in terms of pedestrian flows, wind-comfort and traffic noise. Each simulation allows the user to set custom scenario criteria to enable to, for example, assess different policy and design strategies for the neighborhood such as pedestrian access to private land, speed limits on city streets, or simulate wind-comfort in for various wind conditions.\nIn addition to the web-interface for detailed planning stages, we have developed a tangible table, which allows users to iteratively generate new spatial configurations using 3D-printed buildings. Simulations are run for the designs created on the table, too.\n\nThe entire stack is built on open-source software.\n\nWe have used this tool in cooperation with the City of Hamburg (HafenCity GmbH) during the planning process of a new waterfront-neighborhood, Grasbrook. The FunctionalScope is designed to in a generic manner and twill be used in the planning of at least one new neighborhood-scale urban development project in Hamburg.\n\nThis talk will present the tech stack behind the tool: starting from the translation of architectural into geospatial data (geojson), covering the 3D neighborhood visualization in MapLibre and presenting our open-source near-to-realtime simulation APIs. Moreover, the technology behind the tangible planning table, based on an infrared camera, ArUco markers and Unity will be explained.\nThe talk concludes with lessons learned when developing and applying such an innovative tool to support a new neighborhood-scale development project.",
        "description": ""
      },
      {
        "title": "State of MapServer 2022",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "MapServer is one of the founding OSGeo projects, and is used for publishing spatial data and interactive mapping applications to the web [1]. \n\nThis talk provides an overview of new developments for existing users, and to show the potential of MapServer for those yet to try the software. \n\nWe'll review migrating to the new MapServer 8.0 release [2], using the new OGC API, highlighting lesser-known features, optimizing performance, and reporting news from the MapServer ecosystem. \nMapScript [3], a scripting interface to MapServer provided in several languages such as Python, PHP, and C#, will also be covered. \n\nThis talk will give an overview of current and planned development for MapServer and its related project MapCache, a tile server that speeds up access to map layers [4]. \n\nFinally, we'll look at how to become involved in the MapServer community both as a user and as a developer. \n\n[1] https://mapserver.org/\n\n[2] https://github.com/mapserver/mapserver/wiki/MapServer-8.0-Release-Plan\n\n[3] https://mapserver.org/mapscript/index.html\n\n[4] https://mapserver.org/mapcache/",
        "description": ""
      },
      {
        "title": "MapServer - Make beautiful maps",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "This talk will be about the art of beautiful digital cartography.  Some of the features in Mapserver can contribute to making maps that stand out a little extra.  We will focus on advanced line symbology, the layer composition pipeline and the newly added GEOMTRANSFORM \"centerline\". \nCreating very complex line symbology can be tricky.  We will go into detail about how to build such symbology. The layer composition pipeline offers many exciting possibilities.  We will show various examples how to achieve some stunning symbology for different feature types.   The geomtransform centerline function can produce beautiful labeling possibilities.  My first experiences and lessons will be shared.  Other things that could come up is possibilities with “Named Styles”\nTo summarize it will be a talk about some new features and some older features in MapServer that are described in more detail. The talk is based on practical experiments and real problems that the author has experienced.",
        "description": ""
      },
      {
        "title": "Mapserver layer handling, production, and management in larger scale environment",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Managing hundreds of layers from different sources in a Mapserver production is extensive work. Keeping them up to date, scalable and in constant deployment takes time and effort. Not to mention monitoring all of it.\n\nBy combining a configuration management tool (open-source Progress Chef in our case) and Mapserver, we have a continues deployment cycle. Mapserver’s map file is divided into pieces that Chef puts together. All the layer files are separate entities which are easily manageable and changeable. Different map files can be produced combining different layers to keep map files smaller but still all in one place for management. It also enables to switch off or turn on layers easily.\nThis also gives the benefit of keeping development environment different from production.\n\nThrough MapProxy seeding process we also provided our thousands of users with their base map services and serve them WMS, WFS and our own produced Vectortiles.\n\nAll of it is also under constants monitoring and the logs are processed to produce simple statistics to see which applications are requesting, which layers are being accessed the most. We have built a notification system that notifies us immediately through hooks if our services are down or there are errors in any of the Mapserver layers requests.\n \nIt brings us back to the point of how to make your Mapserver layer handling, production, and management smoother and more straightforward. Let us share our insight!",
        "description": ""
      },
      {
        "title": "A crawler for spatial (meta)data as a base for Mapserver configuration",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "At our institute we manage a lot of input data and model outcomes of soil data to be shared online. We experienced that updating service configurations and metadata records can be quite a challenge, when managed manually at various locations. We've been working on tooling to help us automate the publication processes. These days data publications are set up as CI-CD processes on Gitlab/Kubernetes.\nThese efforts resulted in a series of tools which we call the Python Data\n Crawler. The crawler spiders a folder of files, extracts and creates metadata records for the spatial files, as well as generates a Mapserver configuration for the data to be published as OGC services. Underneath we're building on the tools provided by the amazing FOSS4G community, such as GDAL, Mapserver, pygeometa, owslib, mappyfile, rasterio and fiona.\nA typical use case for this software is with many organizations maintaining a file structure of project files. The crawler would index all the (spatial) data files, register the metadata records in a catalogue and users would query the catalogue from QGIS Metasearch to find and load relevant data.\nWe will present our findings around the project at the conference and hope to talk to institutes with similar challenges, to see if we can create an open source software project around the Python Geodata Crawler.",
        "description": ""
      },
      {
        "title": "Sharing EO data with farmers and herders in the West African Sahel: Lessons from the GARBAL program",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Farmers and herders in the West African Sahel are critically vulnerable to climate shocks and need access to climate information to secure their livelihoods. Herders use data on pasture and water availability to move their livestock and farmers need weather predictions to plan their planting. While satellite imagery has made much of this information readily accessible to the spatial community, few channels exist to transmit this information to herding communities. As a result, climate data has become more powerful than ever before, yet mostly inaccessible to those who depend on this information for their livelihoods.\n\nThis talk goes over the lessons of a programme that seeks to bridge this gap. GARBAL is a call center that uses Copernicus Earth Observation imagery and field data to provide farmers & herders with information on pasture, water and markets in Mali, Niger and Burkina Faso. GARBAL was first developed in 2015 and this talk will provide lessons from several years of practice.\n\nThe GARBAL interface is built on mapserver and uses automated scripts to download and treat imagery from Sentinel 2 and Meteosat which then display information on pasture conditions and water availability. Field data is routed through a network of local data collectors who provide weekly updates on livestock conditions and market prices. In addition to an interactive map, the interface provides user-friendly textual outputs that summarize all the layers for any area of interest on the map, which allows call center agents to quickly provide data to callers. \n\nThe talk will share lessons from the technical and programmatic aspects of the project. The technical side will go over the architecture of the data treatment, demo the interface, talk about successes and failures and show how you can play with the data yourself. The programmatic side focuses more on how the user needs evolved over the years, techniques for translating GIS data into information useful to farmers and herders, operating in areas of active conflict and how EO data fits into existing centuries-old traditional data collection systems in the Sahel.",
        "description": ""
      },
      {
        "title": "Calculating school catchment areas - an open source solution",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "In many countries, access to schooling is one of the key measures of performance of the education system. It is not always known how long learners walk to school, even if the buffer distance is set by policy. GISPO teamed up with the UNESCO International Institute of Educational Planning (IIEP) to study the problem.\n\n\nThe result is a new QGIS plugin (“Catchment”) which allows easily calculating catchment areas based on travel time (isochrones), for all schools across a whole territory. The plugin uses the open source Graphhopper routing server and OpenStreetMap data across the globe. This allows us to easily find out how many people live e.g. 15, 30 or 60 minutes away from education in different parts of a country. \n\nFurther, the development of the plugin triggered a campaign of local OpenStreetMap mapping in Madagascar, which was one of the first countries to pilot the plugin. Having more roads mapped on OpenStreetMap has an impact far beyond educational planning.\n\nNaturally, the same plugin may also be used for calculating all kinds of service catchment areas in QGIS; it was also employed to e.g. calculate access to rail transit across Helsinki metropolitan region.",
        "description": ""
      },
      {
        "title": "TOSCA – A Novel GIS-Toolkit in Support of Sustainable Urban Development",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "TOSCA, also known as Toolkit for Open and Sustainable City planning and Analysis, was implemented by the Digital City Science group at Hafencity University Hamburg (HCU) as a joint venture with the German Association for International Cooperation (GIZ). The project – which has won the Hamburg Open Science Award in year 2020 –works very closely together with academic and local governments in India and Ecuador in order to develop use cases in the context of urban upgrading, disaster prevention, and participatory planning. The WebGIS application uses modern state-of-the art technologies like Docker, View.js, PyWPS, GrassGIS and Geoserver. The source code of the open source solution is hosted on Git repo. Moreover, user and admin manuals plus several step-by-step video tutorials were uploaded on the Vimeo video portal. In terms of analysis functionalities, TOSCA is equipped with buffer area, time map (service area analysis), query module (filter by categorical and numeric attributes) and volcanic eruption scenario analysis (equivalent with intersect: select features by geolocation).\nThis project has been successfully implemented in India and Ecuador since October 2019. It supports investigations not only in regards to Indian slum upgrading issues, but also volcanic disaster mapping challenges in Ecuador. Further applications of TOSCA in Palestine has been kick-start in May this year. TOSCA can be deployed on multi-touch table or on virtual machine - through cloud hosting, and is designed for usage by non-GIS specialists. It targets diverse user groups ranging from local citizen to experts, the former implying participatory workshops and the later focusing on urban scenarios decision-making processes.\nTOSCA Git Repo: https://github.com/digitalcityscience/TOSCA\nVimeo Tutorial Site: https://vimeo.com/user127753830\nIn order to promote the TOSCA Toolkit further, we encourage developers co-work with us to further develop on modules of the Toolkit.",
        "description": ""
      },
      {
        "title": "Tips for parallelization in GRASS GIS in the context of land change modeling",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Although GRASS GIS has been used for big data processing for a while now, you may think that some esoteric knowledge is needed to take full advantage of its computational power. The purpose of this talk is to demonstrate simple ways to parallelize your computations in GRASS GIS, that are applicable whether you are working on your laptop or HPC. I will give an overview of the state of parallelization of individual tools, show benchmarks, and introduce you to other GRASS GIS parallelization tricks. I will use examples relevant to land change modeling and share our experience with simulating urban growth at 30m pixel across the contiguous United States (16 billion cells) using FUTURES simulation implemented in r.futures addon. This talk is for all levels of expertise, although basic Python or GRASS GIS knowledge will be advantageous. \n\nGRASS GIS is a well established, all-in-one geospatial number cruncher with Python interface, command line, and GUI, with new major version 8.0 released in spring 2022.\n\nFUTURES is an open source urban growth model specifically designed to capture the spatial structure of development. It can accommodate the input of a variety of datasets with different spatial extents and can be coupled to other models. FUTURES is implemented in r.futures GRASS GIS addon.",
        "description": ""
      },
      {
        "title": "Publishing INSPIRE datasets in GeoServer made easy with Smart Data Loader and Features Templating",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "GeoServer is a well-established multiplatform, open-source geospatial server providing a variety of OGC services, including WMS (view services), WFS and WCS (download services) as well as WPS (spatial data processing services). Among the open-source GIS web servers, GeoServer is well known for the ease of setup, the web console helping the administrator to configure data and services, the variety of OGC services available out of the box, as well as the rich set of data sources that it can connect to (open source, such as PostGIS as well as proprietaries, such as ArcSDE, Oracle or ECW rasters). GeoServer also provides several OGC APIs, including the OGC API - Features which recently attracted the interest of the INSPIRE community.\n\nAs far as the INSPIRE scenario is concerned GeoServer has extensive support for implementing view and download services thanks to its core capabilities but also to a number of free and open-source extensions; undoubtedly the most well-known (and dreaded) extension is App-Schema which can be used to publish complex data models (with nested properties and multiple-cardinality relationships) and implement sophisticated download services for vector data. Based on the feedback of App-Schema users collected over the years, a new generation of open-source mapping extensions have been implemented in GeoServer: Smart Data Loader and Features Templating, these extensions are built on top of App-Schema and ease the mapping of the data models by allowing us to act directly on the domain model and target output schema using a what you see is what you get approach.\n\nThis presentation will introduce the new GeoServer Smart Data Loader and Features Templating extensions, covering in detail ongoing and planned work on GeoServer. We will also provide an overview about how those extensions are serving as a foundation for new approaches to publishing complex data: publishing data models directly from MongoDB, embracing the NoSQL nature of it, and supporting new output formats like JSON-LD which allows us to embed well-known semantics in our data. Eventually, real-world use-cases from the organizations that have selected GeoServer and GeoSolutions to support their use cases will be introduced to provide the attendees with references and lessons learned that could put them on the right path when adopting GeoServer.",
        "description": ""
      },
      {
        "title": "Liberate your QGIS projects out of office with Mergin Maps",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Mergin Maps (Mergin synchronization server and the Input app) is a package of free and open-source components developed by Lutra Consulting since 2017. It allows users to seamlessly share QGIS projects with others and keep a history of the geo-data. Moreover, it allows collecting data in the field with the mobile application Input, fully based on the QGIS core engine. No more paper for the collection of vital data in the field! We will briefly present published case studies to show the capabilities and features of the solution. \n\nWe will talk about the recent development of the product.  In the Input app, where we focused on improving the field survey experience by allowance to use of precise external GPS receivers, stake-out navigation mode or attaching multiple photos to a single feature. \n\nOn the server-side, in the Mergin, we will demonstrate the ability to store, version and share your geo-data with your team. You will see the new feature to show a map overview of your Mergin project on the dashboard.  To fully integrate into CDI, the DB-sync tool for two-way synchronization between Mergin and PostgreSQL will be presented. Advanced features for usage in large teams, such as selective synchronization and work packages (subprojects for teams within companies) will be explained.\n\nAt the end of the talk, we will uncover the upcoming roadmap for the new features coming in the second half of 2022.",
        "description": ""
      },
      {
        "title": "Kart: an introduction to practical data versioning for rasters, vectors, tables, and point clouds",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "We’re drowning in data, but the geospatial world lags badly behind in versioning tools compared to our software counterparts. [Kart](https://kartproject.org) is solving this with a practical open tool for versioning datasets, enabling you to work more efficiently and collaborate better.\n\nWe will introduce you to Kart and demonstrate some of the key features, including our QGIS plugin. And we'll highlight what’s coming next on our roadmap.\n\nSince 2021 we have added support for Raster and Point Cloud datasets, and we'll be showing how we build on Kart's versioning and spatial filtering techniques to efficiently navigate, access, and use large and small datasets.\n\nKart allows you to quickly and easily manage history, branches, data schemas, and synchronisation for large & small datasets between different working copy formats, operating systems, and software ecosystems.\n\nModern version control unlocks efficient collaboration, both within teams and across organisations meaning everyone stays on the same page, you can review and trace changes easily: ultimately using your time more efficiently.",
        "description": ""
      },
      {
        "title": "Elevation data support in QGIS: 3D, profiles, point clouds and more!",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "For many years QGIS has been focused on 2D spatial data and support for\n3D data was very limited. This has changed in the last couple of years\nand QGIS is getting a full suite of tools to work with 3D data.\n\nQGIS development team has been actively working on better support for\ndata with elevation - such as point clouds, raster digital elevation models,\n3D vectors or meshes. This has been possible mainly thanks to the successful\ncrowdfunding campaign run in autumn 2021:\nhttps://www.lutraconsulting.co.uk/crowdfunding/elevation-pointcloud-enhancements-qgis/\n\nIn this talk, we will show outcomes of these development efforts including:\n- a brand new profile tool for detailed inspection of elevation data\n- new 2D/3D visualization options for data\n- great improvements to the usability of 3D map views\n- support for Cloud Optimized Point Cloud (COPC) format\n\nWe will also discuss the plans for future releases and how QGIS can even\nbetter fit requirements of users with the ever increasing supply of 3D data.",
        "description": ""
      },
      {
        "title": "TiTiler: Not just a tile server",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "During the last 2 years we've been working on TiTiler (https://developmentseed.org/titiler/), a dynamic raster tile server. Built on top GDAL/Rasterio, TiTiler is written in python and use FastAPI (https://fastapi.tiangolo.com) framework. TiTiler is an application that let you create raster tiles dynamically from raster datasets (e.g Cloud Optimized GeoTIFF) but also from Spatial Temporal Asset Catalog (STAC) or Mosaic (using MosaicJSON). It is also a set of python modules which can be used independently to create custom services.\n\nDuring this talk we'll explain the concept of dynamic tiling, what is TiTiler (and the libraries powering it), how it works and more important how users can customize and built their own dynamic tile server.\n\nWe will also present project like TiTiler-PgSTAC (https://github.com/stac-utils/titiler-pgstac) which enables the creation of Mosaic tiles dynamically from a Spatial Temporal Asset Catalog (STAC) database, or eoAPI (https://github.com/developmentseed/eoAPI) which is a full Earth Observation data service combining STAC database, STAC-FastAPI and a TiTiler in one easily deployable project.",
        "description": ""
      },
      {
        "title": "ZOO-Project: News about the Open WPS Platform",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "ZOO-Project is a WPS (Web Processing Service) platform which is implemented as an Open\nSource project and following the OGC standards, it was released under an MIT/X-11 style license and\nis currently in incubation at OSGeo. It provides a WPS compliant developer-friendly framework to\neasily create and chain WPS Web services. This presentation gives a brief overview of the platform\nand summarizes new capabilities and enhancement available in the new version. A brief\nsummary of the Open Source project history with its direct link with FOSS4G will be presented. The new release comes up with a brand new ZOO-Kernel Fast Process Manager and, with the approved standard OGC API - Processes part 1: core. The new functionalities and concepts available in the latest release will be presented and described, also highlight their interests for applications developers and users. Apart from that, various use of OSGeo software, such as GDAL, GEOS, PostGIS, pgRouting, GRASS, OTB, SAGA-GIS, as WPS services through the ZOO-Project will be presented. Then, the ongoing developments and future innovations will be explored.",
        "description": ""
      },
      {
        "title": "MapMint: The service-oriented platform",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "MapMint is a comprehensive task manager for publishing web mapping applications. It is a robust\nopen source geospatial platform allowing the user to organize, edit, process and publish spatial data\nto the Internet. MapMint includes a complete administration tool for MapServer and simple user\ninterfaces to create mapfiles visually.\nMapMint is based on the extensive use of OGC standards and automates WMS, WFS, WMT-S, and\nWPS. Most of the MapMint core functions are run through WPS requests which are calling general or\ngeospatial web services: vector and raster operations, mapfiles creation, spatial analysis and queries\nand much more. MapMint server-side is built on top of ZOO-Project, MapServer and GDAL and its\nnumerous WPS services are written in C, Python and JavaScript. MapMint client-side is based on\nOpenLayers and Jquery and provides user-friendly tools to create, publish and view maps.\nMapMint architecture and main features will be introduced in this presentation, and its modules\n(dashboard, distiller, manager, and publisher) will be described with an emphasis on the OGC standards and OSGeo software they are using. Some short but relevant case studies and examples will finally\nillustrate some of the key MapMint functionalities.",
        "description": ""
      },
      {
        "title": "EOEPCA - An Open Source Exploitation Platform",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Exploitation platforms offer a cloud-based virtual work environment where expert users can access data, develop algorithms, conduct analysis close to the data and share their value-adding outcomes. We now have a complementary ecosystem of platforms, data sources and cloud services. To fully exploit the potential of these complementary resources we anticipate the need to encourage interoperation amongst the platforms, such that users of one platform may consume the services of another directly platform-to-platform.\n\nThe goal of the EO Exploitation Platform Common Architecture (EOEPCA) project is to define and agree a re-usable exploitation platform architecture by identifying a set of common building blocks that provide their services through open interfaces (e.g. OGC), to encourage interoperation and federation within this Network of Resources. We are also developing an open source Reference Implementation, to validate and refine the architecture, and to provide an implementation to the community.\n\nThe Reference Implementation comprises a set of open source components that are available on GitHub, and provided with helm charts for Kubernetes deployment. The components can be used together as an integrated platform, or individually for specific capabilities - which include:\n\n* Application Deployment and Execution Service (ADES) - processing engine for execution of user defined applications via OGC API Processes interface\n* Processor Development Environment (PDE) - integrated web tooling to develop, test and package apps for ADES execution\n* Resource Catalogue - metadata catalogue for data/applications which provides OGC CSW, API Records, STAC and OpenSearch interfaces\n* Data Access - standards-based access to both platform and user-owned data (OGC WCS, WMS, WMTS)\n* Workspace - centralises the user’s management of owned resources through personal Resource Catalogue and Data Access services, integrated with platform S3 object storage\n* Identity and Access Management - OpenID Connect (OIDC) for authentication (OIDC) and User Managed Access (UMA) for authorization, with integrations for external identity providers\n\nWe provide an introduction to each of the building blocks and the open source projects that underpin their development.\n\nAll of our work is available on GitHub (https://github.com/EOEPCA), via our website (https://eoepca.org/) and through our helm chart repository (https://eoepca.github.io/helm-charts).",
        "description": ""
      },
      {
        "title": "Development of Environmental Impact Assessments(EIA) Data Visualization System using FOSS4G - Phase I",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "This talk is about the development of an Environmental Impact Assessments(EIA) data visualization system using FOSS4G. The system is being developed by Gaia3D utilizing several open source projects such as PostGIS, GeoServer, Cesium, and mago3D. \n\nAlthough EIA has played an important role for environmental decision-making and sustainable development, most EIA statements are published as a mix of text and tabular data that is not\neasily accessible to or understandable for the public. The system was designed to improve the public’s understanding of stakeholders before and after a construction project by providing visualization of key environmental elements. The final goal of the system is to improve the EIA process so that not only experts but also non-experts, citizens can participate in the EIA process and easily understand the meaning of the EIA statements with help from 3D GIS, Easy Finger real-time simulation technology. \n\nThis system development is 5 years long project funded by Ministry of Environment(MOE-2020002990005), South Korea. This talk will focus on the research outcome of Phase I and future plans. The final system will be opened as an open source with permission from Ministry of Environment.",
        "description": ""
      },
      {
        "title": "yeoda - providing low-level and easy-to-use access to manifold earth observation datasets",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "In recent years, several Python packages (e.g. xarray, rasterio) have evolved around more basic software libraries such as netCDF4 or GDAL for accessing geospatial data. These packages allow to work with all kind of data formats (e.g. GeoTIFF, NetCDF, ZARR) providing the data in array format (NumPy, xarray) and constitute a fundamental part of any scientific analysis or operational task. However, they do not offer full flexibility when working with Earth Observation (EO) datasets. The multidimensional complexity of EO data (i.e. space, time, bands) is often resolved by distributing dimensions across many files and thus not always easy to access. An important step forward to streamline EO data access has been the Open Data Cube (ODC) toolbox, which utilizes predefined dataset configurations and file-based indices stored in a database. With this setup, ODC enables an easy and uniform access to multidimensional geospatial datasets. Still, users are often confronted with a great variety of data formats, and files being distributed over different systems. This can pose a hurdle when working with ODC, especially if one wants to process a new stack of geospatial data, where the extra overhead of a database can stall swift progress.\n\nIn order to close this gap, the yeoda (''your earth observation data access'') Python software package aims to resolve this shortcoming by offering a similar interface as ODC, but allowing to interact with geospatial data on a lower level. It relies on two other Python software packages developed by TU Wien: geospade (definition of geospatial properties of a dataset, e.g. geometries), and veranda (read/write access to a variety of raster and vector data formats, e.g. GeoTIFF). This modular setup ensures a clear separation of concerns, specifically between geospatial operations and I/O tasks, yielding a homogenized interface independent from the actual data format. For example, geospatial operations based on tiled EO raster datasets can be easily performed across tile or file boundaries. Data access is then realised in veranda, which combines geometric properties with I/O objects listed in a table. On top of geospade and veranda, yeoda acts as a communication layer between files stored on the file system and data objects by adding additional dimensions to the data table, such as common metadata or file name entries. Thus, one can filter multiple files by their attributes (e.g. time, bands, variable names, satellite platform) before accessing the data.  \n\nHence, yeoda guarantees the necessary freedom to apply arbitrary algorithms on manifold data formats, while simultaneously supporting scalability by means of parallelised I/O operations. Despite ODC's tremendous value for accessing EO datasets through large scale operational services, yeoda introduces a new level of data interaction making it an indispensable tool for the EO user community. When taking a look on recent advancements in interoperable cloud-based processing via the openEO API, yeoda could be utilized as a slim back-end library to lower the hurdle of sharing new EO datasets and to foster scientific exchange.",
        "description": ""
      },
      {
        "title": "GC2/Vidi: What’s new in spatial data infrastructure project",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "GC2/Vidi: What’s new in spatial data infrastructure project \n\nThe GC2/Vidi platform helps you build a spatial data infrastructure quickly and easily. Powered using open source components for a scalable solution focused on freedom rather than fees. \n\nGC2/Vidi comprises two software projects: \n\n* GC2 – makes it easy to deploy PostGIS, MapServer, QGIS Server, MapCache, Elasticsearch, GDAL/OGR. And offers an easy-to-use browser application to configure the software stack. \n\n* Vidi – a modern take on browser GIS. It is the front-end client for GC2. \n\nThe GC2/Vidi project is released under GPL and accepted as an OSGeo Community Project in 2018.  \n\nThe talk gives a brief overview of the platform and summarizes the capabilities it has to offer. A new CLI tool (Command Line Tool), which enables administration, import/export of data, starting MapCache seed jobs, running SQLs and more will be introduced. \n\nIn addition, the new \"GC2/Vidi User Group\" will be introduced. It is a non-profit organization whose mission is to promote the adoption of GC2/Vidi and the underlying technologies as well as knowledge sharing. The organization was founded in 2020 and has about 15 members, including municipalities, public transport and private companies.",
        "description": ""
      },
      {
        "title": "Fast rendering from vector tiles in deck.gl",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The shift to using vector rendering has enabled maps to take a leap forward compared to using raster data. It is now possible to offer a much richer experience by performing styling, processing and filtering directly in the client. Coupled with tiled rendering, it is now feasible to work with huge datasets directly in the web browser.\n\nThis presentation will look at how applications can be built using the open source deck.gl library, with a focus on displaying vector tilesets, styling and filtering data on the client, with acceleration provided by the GPU. We will look at how deck.gl elegantly works with vector tiles and show how maps and visualisations can be styled using a few lines of code. We will also explore tools provided by the CARTO platform, which bring these features to those without programming experience, via a web-app.\n\nA brand new feature of deck.gl will be presented: the MaskExtension is a powerful tool that allows one dataset to act as a geospatial mask for another. For example this can be used to let the user select features on a map using a lasso tool, or to select map features based on a geospatial bound. All at 60fps on the client.",
        "description": ""
      },
      {
        "title": "geojson-vt for Highly Efficient Geojson Rendering in Open Layers",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "GeoJSON is one of the most common geospatial data formats. In simple terms, it is an extension of JSON with geometry property. It is text-based and designed with human readability in mind. For the sake of being eye-convenient, there is a performance trade-off when the browser renders it. GeoJSON consists of features containing redundant property keys, causing the size to be bloated as the feature size goes up. Commonly, drawing GeoJSON with the size of tens megabytes would be slow. Showing a hundred megabytes of GeoJSON data on the browser would most likely crash the browser.\n \nWhen we are in complete control of the system: back end, front end, or anything in between, we could probably change the source format to something more efficient like Vector Tiles. But what if we can only tweak the front end?\n \nWhen we can only tweak the front end, geojson-vt comes to the rescue. Initially designed for Mapbox, we can pair it with OpenLayers to render GeoJSON on the fly as Vector Tiles. We will compare the performance between direct GeoJSON rendering versus geojson-vt for different types of GeoJSON. The usage is straightforward, making it a pretty easy solution to improve our map’s performance. On top of that, we could still use Vector-specific Open Layers function like getFeatures when needed.",
        "description": ""
      },
      {
        "title": "Vision and Challenge of Re:Earth - an open source WebGIS platform using Cesium",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "The Re:Earth project grew from the idea of, \"What would be possible if anyone, anywhere could access the digital Earth's potential?\". To make this a reality, we knew Re:Earth needed to be a no-code solution. But more than that, we needed to make sure hardware and OS requirements wouldn't get in the way, which is why Re:Earth is a fully web-based application. \n\n***Re:Earth allows you to manage, edit, compute and visualize a multitude of geographic information including 3D data with no coding required.***\n\nWe knew projects as well as data would need to be shareable so we have both project publishing and data exporting. \n\nPublishing a project is easy and gives users the chance to opt-in or out of SEO, change their URL and setup publishing to their own domain. Exporting data is easy and supports many of the most common file formats seen in GIS.\n\nIt is also the first WebGIS to feature a plug-in system that runs in the browser.\n\nToday, we are focused on solving a problem people face in maintaining, organizing, and managing a wide variety of data, by developing Re:Earth into a general-purpose data management system that can handle all types of data, and one that can be integrated with the user's existing systems.\n\nOur desire has always been to open Re:Earth to the OSS community, to build a global community around the vision of Re:Earth, and to provide and disseminate the value we create with our contributors to the wider society.\n\nThe first step to making this happen was Resium, a popular OSS package that allows developers to use Cesium with React. With Resium we have been able to write Re:Earth's codebase with React and Typescript on the front end. As the main backend language we chose Go. By using these modern languages we have kept Re:Earth highly maintainable and scalable and hope that other developers will find contributing to it easy.",
        "description": ""
      },
      {
        "title": "Enrich styles and enhance styling process with OpenMapTiles",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Vector tile map is now industry standard and general-purpose schema is available on OpenMapTiles project. But if the features that people focus on in your own country are different from the default schema? We developed styles to cover and highlight Japanese authentic geographic attributes such as railways, hot springs, and religious facilities with an improved OpenMapTiles schema. The styles are available on MapTiler Cloud as MIERUNE styles globally. In the process, we developed some useful tools for vector tile styling. One is a style-competing tool that makes it cartographers easy to compare two versions of styles interactively. Another is a kind of style management tool using git that visualizes diff of style.json and takes screenshots automatically. Structured approaches of planning and implementation of vector tile styling are not much shared. In this talk, we will speak about how to enrich the styles for your own country and enhance the styling process for vector tile cartographers.",
        "description": ""
      },
      {
        "title": "Baremaps studio: dynamic vector tiles map rendering",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Baremaps is a blazing fast vector tile server which makes your life easier regarding the publication of OSM data: import, generation and cloud storage.\nBut Baremaps also shines and differentiates from solutions like pg_tileserv in the way you can customize your tileset and merge custom datasets.\nBased on this advantage, we turn Baremaps out to be a vector tiles studio api, allowing the user to easily customize the content of the vector tiles.\nWe adopted the OGC api specification for tileset, layers and styles. Baremaps offers various entry points to manage the datasets and serve them as vector tiles. As an exemple, you can dynamically import different kinds of data sources (geojson, SHP, database) to the server which will expose them as datasets, then you can use any kind of dataset within the same tileset. You can also bring value to your data by doing aggregations (spatial, attribute, hexbin) or computation. It leverages the power of postgis functions and vector tiles specification into one solution. You can attach a style for your dataset and baremaps will serve both Mapbox style file and Vector tiles stream to render the map the way you expect. \nTo illustrate this concept, we will showcase a studio UI which literally provides a tool to quickly create valuable maps and publish them to the web.\n\nBaremaps Studio is the solution to handle dynamic rendering and styling of your vector datas.",
        "description": ""
      },
      {
        "title": "Building a scalable tiling service using Amazon API Gateway",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Recent advancements in both raster and vector tile generation mean that TileJSON services can now serve tiles from on-the-fly sources as well as pre-built caches. Currently, Addresscloud uses CloudFront backed by S3 buckets to serve tile caches for its customer-facing applications. Whilst this configuration worked well for pre-built tile caches, it does not readily support on-the-fly generation and is limited by CloudFront's requirement for cookies or signed URLs for private tilesets. In this presentation we will look at the use of Amazon's API Gateway to provide a scalable interface for multiple TileJSON sources. This approach benefits from providing on-the-fly generation tile in a serverless manner and supporting multiple authorization configurations. The presentation will demonstrate the integration of API Gateway with three tile sources: (1) a Lambda function using rio-tiler for on-the-fly generation of raster tiles from a Cloud Optimised GeoTiff. (2) a Lambda function using Amazon Aurora's HTTP API for MVT generation from PostGIS. (3) a proxy interface to a pre-built cache of tile objects stored in an S3 bucket. The presentation will include publication of source code under an open license, which will be available to the community as a reference architecture. This presentation is of interest to anyone developing tiling services in the cloud.",
        "description": ""
      },
      {
        "title": "New OS: @vcmap/core",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "In the current web GIS ecosystem, 3D is nothing new. It is currently fairly simple to create a 3D web application for rendering geospatial data using open source software, the same can be said for 2D GIS. But in some use cases, you do not wish to have to decide between one or the other. Enter @vcmap/core, a new OS project developed by virtualcitySYSTEMS GmbH of Berlin. With a number of high level abstractions, this slim open source library allows you to create web applications which are able to represent the same data in 2D, 3D and even oblique imagery.  \n\nBy abstracting layers, maps, interactions and styling, your data becomes renderer agnostic. Additionally, a parameterized approach to 3D allows you to easily create cuboid 3D representations from simple 2D representations. A feature which has proven useful in urban planning scenarios. \n\nFurthermore, the @vcmap/core comes with a powerful serialization mechanism. All runtime objects can be serialized and stored using JSON. This way, you can easily develop a web gis framework which allows a quick deployment of multiple applications which only differ in data. \n\nAnd this is not all, the @vcmap/core is still not finished, with geometry editors on the roadmap and a further open source project, the @vcmap/ui to follow this year. The @vcmap/ui is an accompanying UI which integrates smoothly with the @vcmap/core and provides a powerful plugin API. This plugin API allows for fast development of custom tools with which to enhance, analyze and use your geospatial data without the need to fully implement an entire web GIS.",
        "description": ""
      },
      {
        "title": "Open Data in OpenStreetMap’s RapiD Editor",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "The MapWithAI RapiD editor for OpenStreetMap offers a variety of open data to improve OpenStreetMap. This web-based map editor presents the user with various sources of open data to validate and add to OpenStreetMap, including MapWithAI roads, Microsoft buildings, and various open datasets shared via Esri. \n\nIn addition to these past data offerings, the user can now validate and add sidewalks and crosswalks derived from both Mapillary street-level imagery, as well as derived from various organizations who provide footway open data. Finally, Mapillary point data derived from imagery can also now be verified and directly converted into map data, thanks to a more efficient and rapid workflow. \n\nWe will explore all that open data available in the RapiD editor, with a specific focus on how footways are generated from Mapillary, validated from open datasets, conflated against existing OpenStreetMap data, and presented to the user for improved maps of pedestrian walkability.",
        "description": ""
      },
      {
        "title": "Speeding up the RapiD map editor with WebGL and PixiJS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "RapiD is an advanced Open Source map editor for OpenStreetMap built by the MapWithAI team at Meta.  RapiD makes it simple to work with openly licensed geodata and AI-detected road, building, and landform features.  \n\nFor years the RapiD editor was based on a SVG rendering engine built with D3.js.  As our users map the world in increasing levels of detail, and as more open data sources become available, our rendering tech has struggled to keep up with the massive amounts of data that we’re asking it to display in a browser-based JavaScript application.\n\nOur team recently converted this legacy rendering engine to instead use WebGL technology by leveraging the popular Open Source PixiJS game engine.  The conversion from SVG to WebGL yielded a considerable performance boost, and the new WebGL-based renderer is up to the task of working with massive world-scale datasets and handling the increasing data density of OpenStreetMap.\n\nIn this talk we share our progress on bringing new datasets into RapiD, tell the story of how we built a modern map editor on top of an Open Source game engine, and share our roadmap for the future of mapping.",
        "description": ""
      },
      {
        "title": "Use of FOSS4G at Gojek to automate map error detection at scale",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Our digital maps are not always up to date with the real world. New road constructions and road blockages could reduce the accuracy of the map data. In a logistics company like Gojek that serves millions of users per day in South East Asia, the core undertaking revolves around routing and ETAs. Any inaccurate local map data can lead to a direct negative impact on business metrics.\n\nSo how do we ensure that map inconsistencies are detected and fixed promptly to minimise interference of our services? When manual detection is labor intensive and not scalable to millions of road networks in vast regions, how can we effectively automate this at scale? \n\nThis talk is a story of how we, at Gojek, built a pipeline that uses bad customer experience as the trigger to identify potentially faulty data in OpenStreetMap. Our solution makes use of noisy GPS traces and Overpass, an open source tool, to automate this detection. \n\nThis solution enabled us to identify 100s of potential issues per day, categorise them, associate business impact to each map issue and allow our map analysts to fix them seamlessly.",
        "description": ""
      },
      {
        "title": "Matico a new federated FOSS platform for spatial analysis, data management, visualization, and app building",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Geospatial data and analysis is more central than ever to data science, research, and policy analyses. This is especially evident in the explosion of tools, both open source and proprietary that have been developed over the past 5 years to help users manage and gather insights from their data. However many of these powerful tools, like geopandas (analysis and modeling) and deck.gl (visualization)— are technically inaccessible to analysts and researchers without the available time or skills for advanced coding. A number of commercial ventures (Carto, ESRI etc) attempt to overcome this limitation by bringing these tools together as part of polished, graphical user interface driven platforms. While these platforms offer ease of use, they raise concerns about longevity, data ownership, and academic support.\n\nMatico is a new free and open-source platform we are developing at the Spatial Data Science center that seeks to fill the gap between open but technically focused tools and commercial platforms. Consisting of a suite of interoperable components, Matico enables organizations and individuals to manage and visualize their geospatial data while easily maintaining their own infrastructure. A backend server allows users to easily load, clean, analyze, and distribute data through APIs, queries, and in-browser data editing tools while a powerful app builder allows users to develop their own rich applications that target diverse audiences.\n\nThis talk will demonstrate the current features of Matico, our future roadmap , and demonstrate relevant use cases. Matico is now and will forever be open through a permissive MIT open-source license. Learn more at https://matico.app/",
        "description": ""
      },
      {
        "title": "Kaoto: Integrate your Architecture without coding",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Kaoto is an graphical tool to **orchestrate components** in a **visual**, low-code and no-code editor. Once you have your workflows defined, you can deploy them directly to any kubernetes compatible cloud. Kaoto both be deployed as a SaaS platform or used as a standalone application.\n\nThe user interface have both a source code text editor and a drag and drop graphical space. This way users can work both no-code and low-code at the same time, simplifying the learning curve of Apache Camel to create integrations. \n\nKaoto is **highly customizable**. It supports custom views for your specific needs, like showing manuals and helpers for your specific use cases. You can also add **your own domain specific languages** and extensions to use different underlying frameworks with the same user interface. This helps your non tech savvy users adapt to new environments.\n\n Kaoto augments your productivity, accelerating new users and helping experienced developers to build complex integrations.",
        "description": ""
      },
      {
        "title": "The most accurate cameras to generate map data from street-level imagery",
        "type": "Lightning talk",
        "track": "Open Data",
        "abstract": "Mapping is time-consuming and requires a high volume of a workforce when it comes to keep maps up to date periodically. This brings the need of finding alternative approaches to keep maps up to date. Mobile mapping is the process of collecting geospatial data from a mobile vehicle using a 360º camera, laser scanner, GPS/IMU positioning system, and other sensors. \n\nMany devices now include a geotag for every photo captured, and GPS accuracy can\thave major effects on the quality of street-level imagery and derived data. Join us in an exploration of the different accuracy levels of GPS-enabled cameras, where we will take a look at how different devices compare, and what varied levels of GPS accuracy look like both for image location and for data extracted using computer vision and structure from motion.\n\nUnderstanding the differences between devices is an important step in planning street-level imagery capture, as it will align your expectations with the advantages and limitations of the hardware you use. We tested various devices and will share the results of our investigation, with the aim of equipping you to capture street-level imagery with the tools and methods that fit your needs.",
        "description": ""
      },
      {
        "title": "Analysis Ready (Meta)Data",
        "type": "Lightning talk",
        "track": "Open Data",
        "abstract": "The term Analysis Ready Data started as a way to describe a Landsat product that would efficiently allow time-series based analysis by providing a consistent, grid and pixel-aligned product corrected to surface-based measurements. Since then it has come to mean a wide range of things, but without a clear set of standards on how to characterize ARD there is little to no interoperability among datasets that call themselves ARD.\n\nThe Analysis Ready Metadata initiative uses the SpatioTemporal Asset Catalog (STAC) spec as the vehicle for describing well-characterized data. This goes beyond the basic geospatial and temporal characteristics captured in the core STAC spec and into detail about the processing level of the data, corrections that have been applied, as well as spatial and measurement uncertainties.  Having well-characterized data through it’s STAC metadata enables discovery of usable data, automated processing using interoperable workflows, and tracking of data provenance of derived products.\n\nThe CEOS ARD (previously CARD4L) specifications require certain metadata and processing to be done for it to be compliant and can use this STAC metadata to automatically assess the potential for a dataset to be compliant with the needed requirements. This talk will cover elements of STAC, ARD, and the CARD4L family product specifications.",
        "description": ""
      },
      {
        "title": "Using COMTiles to reduce the hosting costs of large map tilesets in the cloud",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "COMTiles (https://github.com/mactrem/com-tiles) is a streamable and read optimized file archive for hosting map tiles at global scale on a cloud object storage. Currently most geospatial data formats (like MBTiles or Shapefiles) were developed only with the POSIX filesystem access in mind. COMTiles in contrast is designed to be hosted on a cloud object storage like AWS S3 or Azure Blob Storage without the need for a database or server on the backend side. The map tiles of a COMTiles archive can be accessed directly from a browser via HTTP range requests. COMTiles are already successfully used in some projects to significantly reduce the hosting costs and simplify the handling of large tilesets in the cloud.  \nStructure of the talk:\n- Basic concepts of COMTiles like the structure of the streamable index table (pyramids vs space-filling curves vs fragments)\n- Comparison of COMTiles to existing cloud native geospatial formats regarding the visualization of large datasets in the browser\n- Advantages of using a streamable archive format like COMTiles over directly hosting the map tiles in the cloud",
        "description": ""
      },
      {
        "title": "GeoMapFish status: vector tiles!",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "GeoMapFish is an open source WebGIS platform developed in close collaboration with a large user group. It targets a variety of uses in public administrations and private groups, including data publication, geomarketing and facility management. OpenLayers and an OGC architecture allow to use different cartographic engines (MapServer, QGIS Server). Recently new features have been added such as vector tiles integration, from raw data to visualization. In order to get rid of AngularJS dependency, a roadmap has been established for a migration to a web components architecture. Everything has been planned so that our users can continue to develop their projects during this process. K8S support is evolving with the implementation of the necessary tools for Azure environments. Highly integrated platform, large features scope, fine grained security, reporting engine, top performances and excellent quality of service are characteristics of the GeoMapFish solution. In this talk we ll present the key usages, web components migration process and latest developments, including vector tiles support.",
        "description": ""
      },
      {
        "title": "Open Back-End for Vector Tile Based Web Apps",
        "type": "General session talk",
        "track": "Transition to FOSS4G",
        "abstract": "The Mapping Service at the Center for Urban Research at the City University of New York (CUNY) Graduate Center engages with foundations, government agencies, businesses, nonprofits, and academics to use spatial information and analysis to develop research projects. Our most recent set of web maps focus on the decennial redistricting process in the United States. Redistricting is often a complex and complicated process. Delays in publishing data from the 2020 Census due to COVID-19 shortened the time frame for redrawing legislative lines in many states. Given the often rushed nature of redistricting it was crucial to provide fair district advocates, journalists, and lawmakers with accurate maps and data shortly after the proposed districts became publicly available. \n\nIn previous projects, we relied on proprietary back-end stacks using ArcGIS, Microsoft SQL, and the .NET framework. These products afforded a viable but inflexible solution to our GIS needs. The online mapping platform for ArcGIS is not as elegant as its open source counterparts, Microsoft SQL did not provide a solution for directly serving vector tiles, and each upgrade of Windows, IIS, and Visual Studio presented unique challenges. \n\nLast year we implemented a new back-end stack to connect our spatial databases to our web sites using FOSS solutions: QGIS, PostGres with PostGIS, Mapbox, and Nodejs. The result is a free, fully customizable solution that is easy to update, maintain, and migrate. We are currently using it in about a dozen applications to serve vector tiles and query demographic and other data. With our new workflow we were able to quickly upload dozens of map proposals, calculate metrics to analyze the potential impacts of each one, and present them on our website within hours of the data being made available to us.",
        "description": ""
      },
      {
        "title": "Building Germany’s drone corridors with hale»studio",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The applications of safe drone mobility are vast and diverse, such as pipeline inspection and large-scale crop monitoring. However, this innovation comes with safety and security risks. Drones have caused 91 disruptions to German air traffic in 2021 alone - in part, because not all no-fly zones are formally defined. \n\nTo solve this, the mFUND fAIRport project was started in 2020. In this project, wetransform, Deutsche Flugsicherung and Fraunhofer work together to create a comprehensive high-precision geodatabase for no-fly zones. This is achieved by merging existing datasets and with new information created through re-use of INSPIRE data, through crowdsourcing and through AI-based object detection from orthoimages.\n\nThe re-use of INSPIRE protected sites allows us to create cross-border data sets with no-fly zones. For the necessary transformation and the merging of the individual data sets, we used the open source ETL tool hale»studio. We encode the merged data sets in a wide range of formats, such as ED-269 JSON, GML and GeoPackage, and deliver them via various standardised APIs.\n\nIn this talk, we will introduce the use case, and the created data sets. We will also explain hale»studio’s declarative mapping and model transformation workflow, to show how it can improve the quality and usefulness of data to help solve problems at scale.",
        "description": ""
      },
      {
        "title": "Municipal surveys with QGIS & Input",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "In 2022 we've prepared an infrastructure based on QGIS & Input for Municipal surveys - Helping cities and regional councils in Israel to collect geospatial data with Open Source tools. \n\nEspecially focusing on common surveys which would save resources for each participating entity such as road signs, city infrastructure, number of routine oversight acitivty (littering, fixes , upcoming works, traffic issues and gardening & cleaning) along longer term planning like zoning or mapping residents complaints over time.\n\nOur goal is to allow medium to low socioeconomic status municipalities to get the opportunities of \"richer\" municipalities, who spend much of their budget on proprietary GIS software. In some cases, this opens the door for the first GIS software in for such municipalities.\n\nThis talk would cover both the technical aspects and the project management aspects of our efforts, as \"converting\" a market, traditionally familiar with only proprietary software, to Open Source, has it challenges, especially when it is the public sector.",
        "description": ""
      },
      {
        "title": "Stable Open Source Software in production; the case of MapProxy",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "MapProxy is a tile server for geospatial data that is capable to cache, accelerate and transform data from existing map services and serve them for various clients. \nMapProxy is a tile cache, but also offers many more features like full support for WMS, re-projection of tiled map services and much more. MapProxy is Open Source (Apache Software License 2.0), is easy to install and to configure and runs on various OS. \nThus, for many of our customers MapProxy is an essential part of their geodata-infrastructure (GDI), but the simple possibility to ask on a mailing list is often not enough, when they want to bring Mapproxy into production. Guidelines of IT departments often require a service contract, looking at proprietary software such a contract often is a binding part of the user agreement. But what about Open Source Software?\nWe as a service provider offer professional support and also service contracts for MapProxy (and other OSS software) and thus, we help customers to bring Open Source into production by filling the gap of missing warranty. In the talk we would like to discuss the various business models that we developed in the past, but we also want to show why MapProxy is an important part of their GDI for many customers.\n\n[1] https://mapproxy.org/",
        "description": ""
      },
      {
        "title": "LOOSE: Combining loosely coupled components into coherent architecture",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The continuously increasing amount of long-term and of historic data in EO facilities in the form of online datasets and archives makes it necessary to address technologies for the longterm management of these data sets, including their consolidation, preservation, and continuation across multiple missions. The management of long EO data time series of continuing or historic missions, with more than 20 years of data available already today, requires technical solutions and technologies which differ considerably from the ones exploited by existing systems. \n\nThe ESA project LOOSE (Technologies for the Management of LOng EO Data Time Series) enables investigating, testing and implementing new technologies to support long time series processing. \n\nFor specific tasks (such as ingestion, discovery, access, processing, analysis of EO data) a multitude of completely different mature open source components is usually available. LOOSE aims at combining functionally similar solutions from different heritages into one comprehensive framework. LOOSE even supports parallelism in a way that multiple solutions for the identical task are available and the application developer is invited to chose between these different components during implementation (e. g. \"GeoServer\" versus \"EOXServer\"). \n\nIn addition, LOOSE partners extended well-known existing components with new capabilities (=interfaces) to support efficient ingestion, discovery, exploitation optimized access, processing and optimized analysis of EO data timeseries. For example, GeoServer was extended with the capability to handle STAC metadata. \n\nOverall outcome of the project is a \"blueprint architecture concept\" which focuses on the interfaces between components and takes innovative concepts such as Bulk data retrieval from dedicated archives, OGC's Data Analysis Processing API and Data Cubes offering Discrete Global Grid Systems into consideration (see enclosed viewgraph).\n\nThe LOOSE system architecture is inspired by the EO Exploitation Platform Common Architecture (EOEPCA) and focuses on the technological evolution of selected services that enable the end-to-end workflow from retrieving long-term archived EO products to the extraction of high-level information based on processed value-added datasets. Architecture and interoperability are evaluated within LOOSE by using different implementations of these services (e.g. EOxServer and GeoServer) and deploying the whole system on two different infrastructures (DLR/LRZ and Mundi/OTC). The complete LOOSE infrastructure is built on Kubernetes and is therefore well transferrable between different cloud providers.\n\nThe validity of the LOOSE blueprint architecture is demonstrated in three different real-world application pilots.\nThese applications are covering totally different thematic areas: \n- Agricultural monitoring (based on Sentinel-1 and -2 data), \n- monitoring urbanization globally (also based on Sentinel-1 and -2) and \n- supporting fishery in the Black Sea (multi sensor approach, including in situ-data).\n\nLOOSE partners are DLR (Oberpfaffenhofen), EOX (Vienna), Terrasigna (Bucharest) and Mundialis (Bonn).",
        "description": ""
      },
      {
        "title": "The Masterportal project – Highly customizeable feature-rich WebGIS",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "This talk will give an overview of organizational and technical characteristics of the Open Source project [Masterportal](https://masterportal.org). We will also give an outlook about our activities towards getting Masterportal becoming an OSGeo project in the future.\n\nDriven by the succeeding trend of Open Governmental Data and the resulting needs for easy-to-use data platforms, the Masterportal (published under MIT license) sucscessfully developed into a powerful, customizeable Open Source solution for the visualization of geospatial data within the past years.\n\nIn order to build up a user and development community, the Masterportal Implementation Partnership was founded in 2018. This organization actually includes 35 public administration partners at the federal, state, and local levels in German-speaking countries (as of 02/2022). The most important committees of the Implementation Partnership are:\n\n* Strategic Committee (steers and controls the strategic direction of the master portal)\n* Technical Committee (supports the Strategic Committee in technical issues)\n* Product maintenance (technical further development, release management, etc.)\n* Maintainergroup (supports the product maintenance in technical further development, processing of PullRequests etc.)\n* Product Management (coordinates organizational matters, public relations, event planning, etc.)\n\nIn addition to regularly committee meetings, various workshops are organized, for example on the initial setup of the software or on special technical topics such as the integration of secure geodata services.\n\nBeside the partners from the public administration side, there are various companies, that offer support and maintenance  and contribute to the further development of Masterportal.\n\nFrom a technical point of view, the code is mainly based on OpenLayers, Vue.js and Bootstrap, while a wide range of modern ES6 libraries are used within the development. There are accurate and helpful development guide lines and code convention to ensure a reasonable code quality.\n\nWithout any further programming skills, a beginner may easily setup and configure a feature-rich modern WebGIS portal  including also an individual design. The configuration is done in a application context file, which is defined in JSON-format. Besides well-known features like printing, common map interactions, layer management, routing, search or drawing, Masterportal also supports all common OGC-Standards including WMS-Time and SensorThings API and the integration of metadata records.\n\nAlso, Masterportal serves as an underlying software basis for more individual projects  - therefore the addon concept allows maintainers to add further individual functionality, like special tools or individual feature info themes.\n\nThe talk will introduce the project and show some examples from real world clients, build upon Masterportal.\n\nTwitter Link: [twitter.com/masterportalorg](https://twitter.com/masterportalorg)  \nCode, Documentation:  [bitbucket.org/geowerkstatt-hamburg/masterportal/](https://bitbucket.org/geowerkstatt-hamburg/masterportal/)",
        "description": ""
      },
      {
        "title": "Developing a pilot method to measure biodiversity related financial risk at financial portfolio level using Earth Observation and other spatially explicit data",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Humanity is facing a number of challenges, among which climate change and biodiversity loss range in top positions. Investors are exposed to those risks through their investee companies given their dependence on biodiversity and ecosystem services. Therefore, just recently the financial community has started to pay attention to the economic and financial consequences of biodiversity loss. However, a key challenge is that a conceptual framework for measuring and understanding biodiversity-related financial risks (physical and transition risks) is less advanced than for climate-related financial risks. Thus the topic of biodiversity and corresponding risks is gaining momentum and methodologies how to assess them are being developed. In this context and due to the mounting pressure of biodiversity loss numerous frameworks like the EU Taxonomy for sustainable activities or the Task Force on Nature related Financial Disclosures (TNFD) have recently been created that try to curb these effects at scale. The intention of the international community is to channel investments to business ventures that do not harm biodiversity or ideally even regenerate it while investors are generally interested in minimizing their risks. To this end a consortium of financial specialists, biodiversity experts and remote sensing data analysts is developing a methodology and prototypical platform, initiated by WWF Switzerland, that quantifies the physical risks, i. e. risks associated with degrading ecosystem services (at the site level subject to data availability). Part of the innovation is to quantify ecosystem service dynamics at asset locations, i. e. the locations where companies actually produce their goods. The production sites and related activities are analyzed with regard to their dependency on five ecosystem services, namely (i) surface water, (ii) flood and storm protection, (iii) pollination, (iv) fiber & other materials and (v) climate regulation to quantify the physical risk at these locations by means of an index. This presentation focuses on the physical risk module, where EO data products represent a valuable source of information. The module builds on fundamental work done in the ENCORE project, in which the relationships between drivers of environmental change, natural capital assets, ecosystem services and production processes have been described exhaustively for the entire economy. Additionally, established databases containing information on the status of natural capital assets, e.g. the WRI Aqueduct 3.0 dataset on the spatial distribution of water-related risks, are used as a baseline against which other data can be compared. Up-to-date EO data products are then used to assess drivers of environmental change to investigate dynamics and trends, and to identify potential threats to the natural capital assets and the provisioning of corresponding economy-relevant ecosystem services. Here, we focus on drivers responsible for biodiversity loss, for example by using annual global land cover data from the Copernicus Global Land Service to assess habitat modification due to land cover change.",
        "description": ""
      },
      {
        "title": "UN Vector Tile Toolkit development and its application",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Vector tile has been one of the key topics in the web mapping. Thanks to the pioneers who have contributed a lot to open source development in the field of vector tile, we can now develop our geospatial information service with open source vector tile technique. UN Vector Tile Toolkit, or UNVT, was established in 2018 under the UN Open GIS Initiative, and has provided various tools to facilitate the production and use of vector tiles. UNVT grows with the global partnership among United Nations and various contributors, and now we have various experience in (1) producing, (2) styling, (3) hosting, (4) optimizing, and (5) consuming vector tiles.\n\nIn out talk, through introducing our experiences and lessons from selected projects, we will share how we can create vector tiles and their map application using open-source tools including UNVT. We will introduce our experiences such as vector tile production and update of the whole globe for UN purpose, development and use of our style tool named unvt/charites, development of an interface with Esri's geoportal, UNVT application in a sigle board PC (Raspberry Pi), development of storytelling map with UNVT, efforts of 3D visualization.\n\nResources\n- UNVT Story telling workshop\n    - Slide https://speakerdeck.com/hfu/unvt-storytelling \n    - Recording https://www.youtube.com/watch?v=CVajhAUDLMs\n- UNVT styling tool - Charites\n    - URL https://github.com/unvt/charites\n    - An example of usecase - [charites use for editing esri based style](https://qiita.com/T-ubu/items/6e31a6bc5a458b91d4cd)\n- UNVT selected resources\n    - [unvt/equinox](https://github.com/unvt/equinox) - use of UNVT in raspberry pi \n    - [unvt/nanban](https://github.com/unvt/nanban) - use of UNVT in Docker for windows user",
        "description": ""
      },
      {
        "title": "The GeoStyler Project - Status Report",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "After 2019 and 2021 we also want to give a status report of the GeoStyler [1] project on this years FOSS4G. GeoStyler is an OSGeo community project [2] and received a lot of new features and changes in the past months.\n\nGeoStyler  provides a set of parser libraries that allow the conversion between different styling formats. On top of the core functionality GeoStyler provides an user interface library that helps to integrate GeoStyler into your own web application. Using these components, GeoStyler can be used for example to create a WYSIWYG style editor. The project also maintains a GeoServer-plugin [3], which includes styling UI-components into GeoServer. \nTwo more tools from the GeoStyler universe should be mentioned: A commandline interface (CLI) and a REST interface. The CLI provides a tool for server-side style conversion for an arbitrary number of style files – completely automated. The REST interface can be used to create web services which do the conversion between formats. With these tools, it is possible to convert a huge amount of QGIS styles to SLD, or Mapfile or any other supported file based styling format and vice versa.\nGeoStyler is based on a plugin concept, so the UI works with any of the supported parsers and can thereby be used for projects that use SLD, OpenLayers, QML, etc. Currently, GeoStyler supports the styling formats OGC SLD, OpenLayers Styles, Mapfiles, QML, Mapbox and also – for assistence when styling by attributes – the geodata formats GeoJSON, OGC WFS and Shapefile. Common Query Language (CQL) for filtering is understood as well as filter encoding (OGC FE).\nThere are a number of new features such as a card layout, enhanced support of expressions, filter UI enhancements as well as various documentation updates and translations planned for this year and we expect all or most to be realized when FOSS4G 2022 takes place.\nWith this talk, we want to present the current project status and show how GeoStyler evolved since the last talk. In order to show a real-life example, we will present the results of a project, where the task was to convert UMN Mapserver based styles to QGIS using GeoStyler. \n\n[1] https://geostyler.org/\n[2] https://www.osgeo.org/projects/geostyler/ \n[3] https://docs.geoserver.org/latest/en/user/community/geostyler/index.html",
        "description": ""
      },
      {
        "title": "Making free climate and weather data more usable - status, ideas and products resulting from the research project \"FAIR\"",
        "type": "Lightning talk",
        "track": "Open Data",
        "abstract": "In this talk we'd like to present the results of a research project, that was funded by the Federal Ministry of Transport and Digital Infrastructure (https://www.fair-opendata.de/) in Germany. The goal of the project was to simplify the exchange of information and data between the German Meteorological Service (DWD) and economic and public, as well as private actors. With this goals, we think we can add a great contribution and maybe also an impetus for other countries to address the provision of meteorological data. \n\nClimate and weather data play an important role for e.g. identifying measures against climate change and optimising industries. However, a correct understanding and handling of such data is often difficult for many potential users without a meteorological background. Moreover, to process and analyse this data, users often need specialised software solutions and an infrastructure that is able to handle large amounts of data; another hurdle to be able to put the data to value.\n\nA range of services has been developed to improve the discoverability, processing, visualisation and delivery of meteorological data. In addition to weather APIs and app developments, the data products of the german meteorological service (DWD) are offered via various portals, but also through a REST API. An upload area also enables easy data provision from third parties towards the public and the meteorological service.\n\nIn the presentation, we will present the base-ideas and the implemented products that, from our point of view, increase the usability of the freely-available meteorological data.",
        "description": ""
      },
      {
        "title": "Free and Open source GIS architecture for low cost inventory mapping of urban water supply network",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "In the planning and expansion of water supply schemes, there needs to be detailed mapping and documentation of existing pipeline network and their assets. However this is usually not the case, especially where the construction of these pipelines predates advances in mapping, geoinformation and database where they in prior existed as drawing and engineering plans. In order to migrate to a fully documented inventory, digitalisation and management of a water supply network database to estimate demand and supplies to plan expansion and population growth, there needs to be an inventory of existing scheme. Historically, mapping has been done with expensive mapping and survey equipment that can pose a challenge for small organisation’s budget, making it difficult to have a complete mapping inventory of its network. \nThis article presents a geographical information system–based free and opensource software architecture for the mapping and inventory of urban water supply network. This architecture is especially useful where budget is tight and decision relating to meeting the water and sanitation-related Sustainable Development goals needs to be made. The architecture consists of data management, data collection, data analysis and project host environment tools and software. \nPostGresSQL with PostGIS was used for design and management of water supply network GIS database, basing the creation and design of features and attributes on prior knowledge of what exists on water supply networks. Features created are transmission and distribution pipelines, hydrants, valves, chambers, junctions, leaks, encroachments, pumps, pump stations, reservoirs, bulk flowmeter, treatment stations with attributes across that include diameter, pipeline material, operational status, condition, encroachment, photo; sizes, capacity, models, manufacturer etc. The PostGIS database was connected to a QGIS project environment where custom forms to were designed to capture attributes created in PostGIS. The QGIS project was linked to an android based mobile app data collection software called Qfield, hosting custom forms designed in QGIS to capture the content of the water supply features, location and attributes. Using the form on Qfield, the water supply network is mapped and attributes captured and once data capture has been carried out using Qfield software, data from field capture is synchronised to QGIS project and following edits to the data captured, it is updated to the PostGresSQL PostGIS database. QGIS software acting as the project host environment also functions as the software for mapping, visualising and analysis of data hosted and managed \nThe architecture presented is an opportunity for any organisation seeking a free and open source GIS option in capturing and documenting and managing their water supply network data. As one of the weaknesses, is that data captured using Qfield has the inherent horizontal and vertical accuracy acquired from android devices which is less accuracy than that from a survey equipment. However, Qfield has the option of connecting to GNSS equipment by blue tooth, inheriting the sub cm horizontal and vertical accuracy it offers and thus improving locational and elevation information and still offering a higher accuracy free open source option.",
        "description": ""
      },
      {
        "title": "Impact of Rainfall variability on Vegetation Phenology cycle in Niger Delta Region Using QGIS Graphical Modeler",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "For decades, the open source community has made an enormous contribution to remote sensing in terms of tools and solutions, leading to support and inspiration for research.Open source software packages capable of processing digital images have increased the availability of remote sensing datasets. QGIS is an open source software package involving experts and users from around the world. QGIS is a useful tool for spatial visualization and analysis, making it a viable alternative to other costly software packages. It has became an effective tool for processing   and analysing the climate  data, based on the potentiality and flexibility  of graphical modeler imbedded plugin. Therefore, the aim of this work is to assess impact of Rainfall variability on Vegetation Phenology cycle  in Niger Delta from 2010 to 2019 using Open source QGIS graphical Modeler. The rainfall data was collected from Tropical Rainfall Measuring Mission (TRMM).The Moderate Resolution Imaging Spectroradiometer (MODIS) data MOD11A1 of 2010-2020 were download for two years intervals from the United States National Aeronautics and Space Administration (NASA) website. The QGIS Graphical  modeler  was built using arithmetical expression and raster calculation geoprosessing tool to convert the modis data to Normalized Difference Vegetation Index(NDVI) and Rainfall variability from TRMM . The mean Rainfall  and NDVI were extracted using the administrative boundary shapefile of the study area. The results were presented in figures, maps and graphs, and they show that Rainfall Variation have occurred with considerable impacts on the vegetation phenology cycle in Nigeria between 2010 and 2020. Rainfall data and NDVI indicate a high correlation with vegetation with the implication that as rainfall increased, the vegetation also increased and inversely. In view of the above findings and observations, the occurrence of rainfall variability and its grave impact in Nigeria has been empirically determined. Therefore, while the use of remote sensing and GIS technique in continuous monitoring of climatic indices and vegetation phenology cycles are highly recommended, The open source software should also be highly encouraged and freely made available for scientific research.",
        "description": ""
      },
      {
        "title": "Trash Study Research",
        "type": "Lightning talk",
        "track": "Open Data",
        "abstract": "My name is Nicera a slum dweller living in Kibra and founder Community Mappers. At times walking in slums can be irritating for you have to step on all kinds of garbage, bare with the smelly  stench coming out from the heaps of garbage that are scattered allover along the thin footpaths in the slum. when it rains some of this trash is swept straight to the river, some to broken and open sewer. houses along the river banks are swept away and this causes flooding in our slums which has led to lose of many lives in our slums, they say rains are blessings but for us its a curse. and for this reason my organization Community Mappers saw a need to conduct a trash study research in the slums and take it to the next level by negotiating with County Governments and local stakeholders of tangible solutions which will lead to no deaths and no floods in our slums. In this study we bring out a community based approach of solving its problems through research this was brought out through qualitative and quantitative approach in this communities. We are the communities and through data and proper research we can solve some of our problems.",
        "description": ""
      },
      {
        "title": "Public Transportation GTFS Editor",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "For sustainable public transport (PT) operations and planning, the most important part of the decision-making cycle is the accurate and up-to-date collection and sharing of static public transport data. One of the biggest problems experienced is that the data for comprehensive analyzes are not accurate and powerful, and data formats change according to PT agencies. To solve these problems, PT data formats and visualization methods have been investigated.\n\nGTFS (General Transit Feed Specification) is the most commonly used format for specifying PT schedules,line,stop and route. In other words, GTFS is a data specification that allows PT agencies to publish their transit data in a format that can be consumed by a wide variety of software applications. Today, the GTFS data format is used by thousands of PT providers.\n\nOur company has developed an open-source GIS-based GTFS editor using its knowledge in data analysis, big data, and data visualization in the field of PT. Within the scope of this study, firstly, open PT data (line, stop, schedule, etc.) were obtained from different municipalities and visualized using open source GIS applications such as QGIS, SAGA GIS, JOSM, GeoDa, and data quality evaluation was made. State of art in the field of ITS has been researched and open source GTFS editors have been studied.\n\nOpen source map servers such as GeoServer were used for sharing, editing and organizing stop & line maps produced in GIS applications. For geospatial data analysis, stop points were verified by using geopandas and shapely libraries. PostgreSQL database was used to store geo-based stop & line data and the PostGIS extension was used. PostGIS adds spatial capabilities to PostgreSQL so it can store, query, and manipulate spatial data. On the server side scripting, GeoAlchemy(an extension of SQLAlchemy) was used for working with spatial databases and geospatial queries. Turf was used for any spatial operations. It is a geospatial engine, and it includes spatial operations and helper functions. - MapboxGL: WebGL-powered library was used for interactive vector maps on the web application. To render more than 1k of stops & lines with high performance, WebGL powered geospatial visualization framework DeckGL was used. NebulaGL provides geospatial drawing and editing tools for lines. It was added for creating & editing routes from the map. Osm-Nominatim is a geocoding library. It allows users to find a stop address from a location.\n\nAs a result of the studies, the open source GTFS editor was developed, which will create a geographical base for PT planning and operational methods. The developed GIS-based editor provides fast and more effective PT management by enabling the visualization, creation and management of lines and stops of PT agencies with user-friendly interfaces.",
        "description": ""
      },
      {
        "title": "As a scientist, how to dive into JavaScript for data visualization? An example with a particle stream to illustrate the movement of glaciers.",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The web is an effective way to communicate complex spatial scientific data and make them more accessible to more people. As an example, web maps with particle streams are used to communicate efficiently wind or ocean current data. As glaciers are flowing just like wind or water, we used this « particle » approach to develop a map of Antarctica showing glacier movement. But, as scientists, JavaScript is not our day today dev language. We present our strategy to map moving ice particles over the web using scientific data. Open codes and tools helped us a lot to move from scientific to web worlds. The prototype was implemented in the observableHQ collaborative development environment, reducing the learning curve and facilitating transfer from similar applications. The notebook environment is well suited to work in an iterative process and obtain a readable code to make it reproducible by other scientists. Even for the non-common Antarctica coordinate reference system, the D3.js library was the only one used to map data. Finally, the animated map was transferred to a web application for scientific communication purposes.",
        "description": ""
      },
      {
        "title": "GSoC with OSGeo",
        "type": "Full session",
        "track": "Education",
        "abstract": "Over the years, OSGeo's Google Summer of Code initiative has transformed into an initiative full of contributions towards geospatial software development. In the last 16 years, many OSGeo projects comprising incubating projects, community projects, and guest projects have progressed attributed to the contributions of student developers. Some of these students continued to participate as contributors for the projects and went on to take mentoring and organizing responsibilities. This is a true sense of FOSS4G in terms of individual and collective growth of the student developers and the OSGeo community. In this talk, the OSGeo GSoC Admins team would try to appreciate the efforts of all the mentors and students involved till now and present the state of the GSoC 2022. The Admins would also present possibilities for new projects to be part of the GSoC with OSGeo as an umbrella organization.",
        "description": ""
      },
      {
        "title": "State of GeoServer",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "GeoServer is a web service for publishing your geospatial data using industry standards for vector, raster and mapping, as well as to process data, either in batch or on the fly. \nGeoServer powers a number of open source projects like GeoNode and geOrchestra and it is widely used throughout the world by organizations to manage, disseminate and analyze data at scale.\n\nThis presentation provides an update on our community as well as reviews of the new and noteworthy features for the latest releases. In particular, we will showcase new features landed in 2.20 and 2.21, as well as a preview of what we have in store for 2.22 (to be released in September 2022).\n\nAttend this talk for a cheerful update on what is happening with this popular OSGeo project, whether you are an expert user, a developer, or simply curious what GeoServer can do for you.",
        "description": ""
      },
      {
        "title": "GeoServer Feature Frenzy",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "GeoServer is a web service for publishing your geospatial data using industry standards for vector, raster and mapping. It powers a number of open source projects like GeoNode and geOrchestra and it is widely used throughout the world by organizations to manage and disseminate data at scale.\n\nWhat can you do with GeoServer? This visual guide introduces some of the best features of GeoServer, to help you publish geospatial data and make it look great! \n\nGeoServer has grown into an amazing, capable and diverse program - attend this presentation for:\n\n- A whirl-wind tour of GeoServer and everything it can do today;\n\n- A visual guide to some of the best features of GeoServer;\n\n- Our favourite tricks we are proud of!\n\nNew to GeoServer - attend this talk and prioritize what you want to look into first. Expert users - attend this talk and see what tricks and optimizations you have been missing out on.",
        "description": ""
      },
      {
        "title": "Demystifing OGC APIs with GeoServer: introduction and status of implementation",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The OGC APIs are a fresh take at doing geo-spatial APIs, based on WEB API concepts and modern formats, including:\n\nSmall core with basic functionality, extra functionality provided by extensions\nOpenAPI/RESTful based\nJSON first, while still allowing to provide data in other formats\nNo mandate to publish schemas for data\nImproved support for data tiles (e.g., vector tiles)\nSpecialized APIs in addition to general ones (e.g., DAPA vs OGC API - Processes)\nFull blown services, building blocks, and ease of extensibility\n\nThis presentation will provide an introduction to various OGC APIs and extensions, such as Features, Styles, Maps and Tiles, DAPA, STAC and CQL2 filtering. \nWhile some have reached a final release, most are in draft: we will discuss their trajectory towards official status, as well as how good the GeoServer implementation is tracking them, and show examples based on the GeoServer HTML representation of the various resources.",
        "description": ""
      },
      {
        "title": "We are Open! OGC and OSGeo Collaboration",
        "type": "Keynote",
        "track": "Use cases & applications",
        "abstract": "The Open Geospatial Consortium (OGC) and the Open Source Geospatial Foundation (OSGeo) have a long and natural tradition of collaborating. In 2022, the Memorandum of Understanding between both organizations was updated - to pay tribute to ongoing and future activities.\n\nIn the initial MoU (2008), OGC and OSGeo agreed to work closely to coordinate with each other’s memberships regarding new standards developments and standards changes that may be required as a result of open source programs. Another important aspect of the relationship is to keep each other well informed of the respective activities and directions. Both aspects have proven to be of great importance. One goal was and is to coordinate activities in such a way as to maximize the achievement of both organizations’ mission and goals.\nThat includes to identify open source technologies that can be used as reference implementations for and validate compliance tests developed for OGC adopted standards.\nSince the first MOU, there has been an increase in OGC on developer focus and engagement of software communities and activities.  Increased collaboration has also occured by way of the OGC API code sprints. In addition, key opportunities for cross pollination have evolved given shared missions (FAIR data) and the viewpoint that FOSS4G software is beneficial for all software.\n\nThe development of the OGC API suite of standards is an excellent example on how the MoU works in practical terms. The OGC APIs are a family of Web APIs that have been created as extensible specifications designed as modular building blocks that enable access to spatial data that can be used in data APIs. These revolutionary APIs make location information more accessible than ever before through the use of RESTful principles, and the OpenAPI specification for describing interfaces. OGC APIs have been tested in close collaboration with the global developer and end user communities through hackathons, sprints, and workshops to provide a modern solution to tomorrow’s location sharing issues. For example, the 2021 Joint Code Sprint organized by OGC, OSGeo and the Apache Software Foundation (ASF) included open source implementations of OGC APIs - and became a standing sprint activity that was repeated in 2022.\n\nThis presentation provides a deeper dive into the new Memorandum of Understanding and how both open standards and  free and open source software can benefit from one another.",
        "description": ""
      },
      {
        "title": "State of GDAL",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "We will give a status report on the GDAL software, focusing on recent developments and achievements in the 3.4 and 3.5 GDAL versions released during the last year, but also on the general health of the project. In particular, we will present new drivers such as the one handing Zarr datasets (format for the storage of chunked, compressed, N-dimensional arrays) or the Spatio-Temporal Asset Catalog Items driver to create virtual mosaics from STAC items, and potential future additions such as a new JPEG-2000 based driver using the Grok library, a driver for the SAP Hana database or driver for columnar storage format  such as Apache Parquet and Arrow. The topic of coordinate epochs in geospatial datasets and how we’ve addressed it in various formats (GeoTIFF, GeoPackage, FlatGeobuf) will also be mentioned. As well as other improvements such as the JPEG-XL codec for the GeoTIFF format, or support for 64-bit integer data types in rasters. We will present the new CMake build system, the roadmap for its implementation, and its advantages for users and developers.",
        "description": ""
      },
      {
        "title": "20 Years of QGIS [community]",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "QGIS turned twenty this year. The first lines of code were written in mid-February of 2002 and the first time the code compiled and ran, it could do one thing:\nConnect to a PostGIS database and draw a vector layer.\n\nQuoting Gary Sherman - \"The mythical man of QGIS that no one has ever met\":\nThis was the humble beginning of one of the most popular open-source GIS applications. GRASS GIS is of course the granddaddy of open source GIS, but the 20th birthday of QGIS is a testament to its longevity and commitment of all those who have made it what it is today.\n\nIn this talk I'll share a walkthrough of the most game-changing features and events that shaped QGIS and its community in the past 20 years making it one of the top ten most important C++ open-source projects [1] and an overall amazing project to represent :)\n\nHappy Birthday QGIS!\n\n[1] https://www.phoronix.com/scan.php?page=news_item&px=OpenSSF-Criticality-Score",
        "description": ""
      },
      {
        "title": "QGIS Feature Frenzy - What's New in the Last Year?",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "QGIS releases three new versions per year and each spring a new long-term release (LTR) is designated. Each version comes with a long list of new features. This rapid development pace can be difficult to keep up with, and many new features go unnoticed. This presentation will give a visual overview of some of the best new features released over the last calendar year. This will be a mixture of important/popular features along with those which are easily overlooked or missed. Each highlighted feature will not simply be described, but will be demonstrated with real data. The version number for each feature will also be provided. This will let you know which new features are included in the LTR. If you want to learn about the current capabilities of QGIS this talk is for you! Potential topics include: Annotation layers * GUI enhancements * New Expressions * Point cloud support * Print layout enhancements * New renderers and symbology improvements * Mesh data algorithms * 3D * Editing",
        "description": ""
      },
      {
        "title": "Easily publish your QGIS projects on the web with QWC2",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "QWC2 (QGIS Web Client 2) is the official web application of QGIS, that allows you to publish your projects with the same rendering, thanks to QGIS Server. The environment is composed of a modern responsive front-end written in JavaScript on top of ReactJS and OpenLayers, and several server-side Python/Flask micro-services to enhance the basic functionalities of QWC2 and QGIS Server. \n\nQWC2 is modular and extensible, and provides both an off-the-shelf web application and a development framework: you can start simple and easy with the demo application, and then customize your application at will, based on your needs and development capabilities.\n\nThis talk aims at introducing this application and to show how easy it is to publish your own QGIS projects on the web. An overview of the QWC2 architecture will also be given. It will also be an opportunity to discover the last new features that have been developed in the past year and ideas for future improvements.",
        "description": ""
      },
      {
        "title": "G3W-SUITE: an OS framework dedicated to the publication and management of QGIS projects as WebGis services",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "G3W-SUITE is a modular, client-server application (based on QGIS-Server) for managing and publishing interactive QGIS cartographic projects of various kinds in a totally independent, simple and fast way.\nThe suite is made up of two main components: G3W-ADMIN (developed through Python, using Django ) as the web administration interface and G3W-CLIENT as the cartographic client., developed using a modular approach and is based on a “reactive programming” paradigm using Vue.Js, Javascript framework and OpenLayer3.\nThis components communicate through a series of API REST.\nThe application is compatible with QGIS 3.22 LTR and it is based on strong integration with the QGIS API.\nIt is released on GitHub with Mozilla Public Licence 2.0\nMany graphic/functional aspects of the WebGis publication derive directly from QGIS projects as, first of all, the general and OGC services capabilities.\nThe suite automatically inherits aspects related to the project (themes, 1: N relations, simple and atlas print layout, filter on legend based on map content, layer display order and activation status ...) and related to individual layers (activation scale, interrogability, published attribute fields, join attributes, attribute form, editing widgets ...) .\nOf particular interest is the strong integration with the QGIS DataPlotly plugin.\nQGIS projects can be published as WebGis services via direct upload (no plugins needed) on the Administration component.\nThe granular system of permissions and the subdivision into roles of users (individuals or groups) allows the management of services to be delegated to second and third level administrative users.\nIt is also possible to define consultation permissions on individual WebGis services and editing permissions on individual layers with different editing powers per user.\nFinally, it is possible to define geographic and alphanumeric constraints (both in consultation and in editing) differentiated by individuals or groups of users. Alphanumeric constraints can be based on SQL language or QGIS expressions.\nIt is also possible to define for each layer aspects relating to the preparation of predefined searches, caches and downloads in various formats.\nA particularly advanced function is related to online editing and to the possibility of easily creating web cartographic management systems by defining the various aspects at the level of the QGIS project.\nThis function (operating directly on the data through the QGIS API) allows multi-user editing thanks to a feature-lock system.\nEditing works both at the geometric level (with intra- and inter-layer snap), and at the attributes level (editing form and the widgets included) also connected by joins or 1:N relationships.\nThe call will allow to illustrate the innovations of the current and future versions.\nThese include the implementation of editing functions, user-based filters linked to the visibility of layers and attributes, the possibility of using QGIS projects based on embedded base projects and the integration of the vectorial and raster Temporal Controller of QGIS.\nOnline geographic analysis possible thanks to the integration of Processing algorithms through dedicated APIs.",
        "description": ""
      },
      {
        "title": "Better productivity for QGIS plugin developers",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Developing a plugin for QGIS is both as simple as one of the countless tutorials and as complicated as a software engineering job facing with the dynamism of the project (maintenance requirements), the size of the APIs and constraints that need to be taken into account (Windows, etc.).\n\nAt Oslandia, we create and maintain many plugins for our clients, which leads us to streamline their development... and especially their maintenance! Historically, many extensions were created using the amazing Plugin Builder and the underlying tool (pb_tool) but it no longer fit our needs.\n\nWe present here our [QGIS Plugin Templater](https://oslandia.gitlab.io/qgis/template-qgis-plugin/), based on [Cookiecutter](https://cookiecutter.readthedocs.io/) and the related work on developer tools (tests, documentation, code structure, formatting, linter...). We will also mention the other tools we are using or following closely (the 3Liz toolbelt, the other template from Gispo Coding...).\n\nYet Another Plugin Generator? Probably but we think it's worth it!",
        "description": ""
      },
      {
        "title": "Closing Session",
        "type": "Full session",
        "track": "Unknown",
        "abstract": "Closing session with Sol Katz 2022 announcement, FOSS4G 2023 presentation an much more",
        "description": ""
      },
      {
        "title": "OSGeo AGM",
        "type": "Side event",
        "track": "Unknown",
        "abstract": "The OSGeo Annual General Meeting",
        "description": ""
      },
      {
        "title": "Are you lost? Get pgRouting to find your way",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "pgRouting not only allows to get routes for any kind of transportation, from 0 wheels to 18 wheeler. \nA road is closed? Calculate how can traffic be diverted.\nDon't get a result? Check if your graph is connected.\nNeed to deliver to several clients? Traveling Salesperson problem helps to determine the route.\nFind out what is planned for version 4.0 that is a work in progress.\nLearn about the spin off for vehicle routing problems.\npgRouting extends the PostGIS / PostgreSQL geospatial database to provide geospatial routing functionality.\nAdvantages of the database routing approach are:\n* Data and attributes can be modified by many clients, like QGIS through JDBC, ODBC, or directly using Pl/pgSQL. The clients can either be PCs or mobile devices.\n* Data changes can be reflected instantaneously through the routing engine. \n* The “cost” parameter can be dynamically calculated through SQL and its value can come from multiple fields or tables.",
        "description": ""
      },
      {
        "title": "pgRouting optimization: from technical to functional",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Due to its SQL-based engine, the OpenSource Software pgRouting is the most flexible routing engine available.\nA common misconception is that pgRouting is the least performant routing engine.\n\nSo how to keep both performance and flexibility with pgRouting ?\n\nMany factors should be taken into account.\nTo begin with, the use cases.\nWhat level of precision do you need ?\nWill you be computing short or long routes ?\nWill there be many routes computed at the same time ?\n\nSimplification is the most common way to deal with performance issues.\nHowever, when accuracy is at the core of decision making, a minimum level of precision must be kept.\nReducing the number of rows the routing engine will have to process is the number one tip to enhance performances.\nBut there are many other technical and functional optimizations that can make pgRouting run much faster.\n\nWe will look at some choices we had to make in various projects.\nHow the data is the first key to optimization.\nBut also how to help the routing engine make the best of it.\nWhich algorithms are best used for which use cases, and how fine tuning the database can help too.",
        "description": ""
      },
      {
        "title": "How to Mapillary - Getting started with Street-level Mapping",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Mapillary is the platform that makes street-level images and map data available to scale and automate mapping. There are many tools available within Mapillary’s ecosystem, as well as many real world use cases where Mapillary can have an impact. In this talk, we will give an overview of the state of the Mapillary platform in 2022. This will include a look at compatible camera devices, upload methods, data and imagery management, download methods, integrations, and stories about users who apply Mapillary to solve a challenge. \n\nYou should walk away from this talk knowing how you want to use Mapillary to improve maps important to you, and what tools you need to get started.\n\nIf you are interested in improving OpenStreetMap, contributing to open data, capturing imagery in your community, or leveraging Mapillary street-level imagery and GIS data into your professional work, this talk is for you. No coding or technical experience is necessary, and the tools and features available can be adapted to any skill level. Join us!",
        "description": ""
      },
      {
        "title": "GeoServer-cloud: Cloud Native GeoServer in production environments",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "In this presentation, we show how GeoServer-Cloud has matured into a production ready, cloud-native micro-services application. It has already been successfully deployed in production for three major organizations in their respective kubernetes environments.\n\nGeoServer-Cloud is a spring-boot/spring-cloud based micro services application built on top of GeoServer. The main goal of this implementation is to have an effective and easy way to scale the different services horizontally, splitting GeoServer geospatial services and API offerings into individually deployable components.\n\nAll the services communicate with each other via a messaging queue. There’s no wait-time between configuration changes and their reflection across all services in the cluster, nor the need to reload the applications.\n\nThe last year has not only been spent hardening the code, but also a lot of emphasis has been put on the deployment procedures. In this presentation we will explain how to deploy GeoServer-Cloud in a kubernetes environment. We will showcase the official helm chart that can be used to install it everywhere.\n\nGeoServer-Cloud allows per-service auto scaling and server resource dimensioning, hence optimizing each service based on its performance characteristics. We will discuss how to achieve good load balancing based on service metrics.",
        "description": ""
      },
      {
        "title": "Processing and publishing big data with GeoServer and Azure in the cloud",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The amount of data we have to process and publish keeps growing every day, fortunately, the infrastructure, technologies, and methodologies to handle such streams of data keep improving and maturing. GeoServer is a web service for publishing your geospatial data using industry standards for vector, raster, and mapping. It powers a number of open source projects like GeoNode and geOrchestra and it is widely used throughout the world by organizations to manage and disseminate data at scale. We integrated GeoServer with some well-known big data technologies like Kafka and Databricks, and deployed the systems in Azure cloud, to handle use cases that required near-realtime displaying of the latest received data on a map as well background batch processing of historical data. \n\nThis presentation will describe the architecture put in place, and the challenges that GeoSolutions had to overcome to publish big data through GeoServer OGC services (WMS, WFS, and WPS), finding the correct balance that maximized ingestion performance and visualization performance. We had to integrate with a streaming processing platform that took care of most of the processing and storing of the data in an Azure data lake that allows GeoServer to efficiently query for the latest available features, respecting all the authorization policies that were put in place.  A few custom GeoServer extensions were implemented to handle the authorization complexity, the advanced styling needs, and big data integration needs.",
        "description": ""
      },
      {
        "title": "Deploying and operating GeoServer: a DevOps perspective",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Cloud computing is revolutionizing the way companies develop, deploy and operate software and GeoSpatial software is no exception. With benefits of cloud based deployments range from cost savings to simplified management, flexibility, lower downtime and scalability of dynamic environments it is easy to understand why more and more companies are migrating their on premise systems to the cloud but cloud based setups have their own set of hurdles and challenges.\nThe migration of the series itself can be challenging. Monitoring, debugging and scaling of applications are very much different than what you are used to.\n\nIn this presentation we will share with you the lessons we have learned at GeoSolutions to tackle these problems and share some common patterns for the migration of on premise GeoServer clusters to the cloud. We'll share with you tips on:\n- best practices to migrate your existing GeoServer cluster to the cloud\n- insights on your geoserver cluster using centralized logging and Monitor plugin\n- avoid common bottlenecks to best set up a distributed scalable GeoServer cluster \n- work containers and container orchestrators like Kubernetes",
        "description": ""
      },
      {
        "title": "Vector tiles cartography tips and tricks",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Vector tiles are changing the way we create maps. Client-side rendering offers endless possibilities to the cartographer and has introduced new map design tools and techniques. Let’s explore an innovative approach to modern cartography based on simplicity and a comprehensive vector tiles schema. \n\nAfter a short introduction or useful reminder to vector tiles, take a tour of their graphic capabilities through a series of original map design compositions. A variety of cartographic examples will be illustrated during this talk, with a particular focus on map display and performance. Rendering issues and technical limitations will also be put in perspective with pragmatic solutions or design alternatives.\n\nGet an overview of best practices for vector tiles cartography and learn about simple open-source recipes, towards advanced combinations of fills, patterns, fonts, and symbols. Selected layer parameters and style expressions will be discussed in a visual way and explained with basic syntax that you can take away.",
        "description": ""
      },
      {
        "title": "Creating vector tiles with osm2pgsql",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "For over a decade now osm2pgsql has been the standard tool for importing OpenStreetMap data into a PostgreSQL/PostGIS database for rendering of raster tiles and many other use cases. Thanks to several improvements in the last years centered around a flexible configuration language and new geoprocessing capabilities, osm2pgsql is now also a great base for creating a vector tile toolchain. It can easily handle imports of a small country in a few minutes as well as scale to a planet-sized database with minutely updates from OSM.\n\nThe talk will introduce some of the new features that allow you to customize the database table layout and contents. It will outline the few steps needed to create your very own custom vector tiles based on OpenStreetMap data. We'll see how you can use the configuration language to clean up OSM data on import and prepare it for fast access with a vector tile server like T-Rex.",
        "description": ""
      },
      {
        "title": "3D Tiles Next",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "\"3D Tiles Next\" is a major update of the \"3D Tiles\" OGC Community Standard 1.0. 3D Tiles are designed by Cesium GS, Inc. for streaming massive heterogeneous 3D geospatial datasets. 3D Tiles Next is a set of extensions in the following areas:\n- direct use of glTF models\n- using glTF for point clouds and glTF extensions for texture compression additional 3D tiles functionality\n- semantic metadata stored per tileset, feature, vertex and more\n- implicit spatial indexes (quadtree, octtree, S2 subdivision)\n\nThis presentation gives an overview of the current \"3D Tiles\" format and shows new features in the \"3D Tiles Next\" specification. It also covers other existing 3D OGC (community) standards like CityJSON or «Indexed 3D Scene (I3S)».\n\nImportant further topics are :\n- overview of viewers for 3D Tiles on the web and in native and mobile applications using game engines\n- data processing tools for producing data in these formats\n- building a community for creating 3D tiles for GIS and OSM data",
        "description": ""
      },
      {
        "title": "The GreenUr project: creating an application in QGIS to manage the impacts of urban green spaces on human health",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Globally, the population living in urban areas is increasing with a strong impact on land use patterns, particularly on the availability and use of green spaces. The impact of green spaces is beneficial to health, for example, by reducing mortality or improving mental health. These effects are also related to different ecosystem services provided by green spaces, such as regulating temperature, modifying air pollution and noise levels, and offering more opportunities for physical activity.\n\nGreenUr is a plugin for QGIS that aims at putting together knowledge and information on the impacts of green space on health. It is developed as a prototype representing a work in progress coordinated by the World Health Organization (WHO) to provide an educational tool to introduce the relation between green spaces, health, and well-being and raise awareness of the importance of green spaces in cities globally. The tool can also be used as ‘quickscan’ for urban spatial planners that would like to orientate on possible effects of current and new green space design. The plugin has been tested with different experts and locations, and it will be downloadable via the QGIS Plugin manager from the project website.\n\nThe GreenUr tool allows the users to estimate the impacts of green spaces on health in a given population. The main questions addressed by the current version of the GreenUr prototype are the following:\n\n- How much green space is available for the population of a specific city?\n- Which are the pathways through which green spaces relate to health?\n- Where within a city are health-related benefits of green spaces the largest?\n- Which are hypothetically different land-use scenarios for green spaces?\n- What would be the magnitude of the change in health impacts if future green space would be changed in cities?\n\nAll calculations performed by GreenUr are based on methodologies established by social, environmental, and epidemiological studies identified by WHO. The computational backend used is GRASS GIS and other processing methods available in QGIS. The plugin is running any common operating system and offers a demo database.",
        "description": ""
      },
      {
        "title": "swissgeol.ch - Geology in 3D: new features",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Geological data usually suffer from very low visibility because they are specialized data that are only accessible to a few people and can only be visualized and processed using special software. The swissgeol.ch portal, newly launched by swisstopo (Swiss Federal Office of Topography), aims to change this by making the data accessible on the Internet in a low-threshold and simple way using 3D visualization based and promoted with open-source technology and code.\nswissgeol.ch is a web application for the visualization and analysis of geological sub-surface data.  It has been publicly available at https://viewer.swissgeol.ch since 2020, and the open source code is can be downloaded at https://github.com/swissgeol/ngm.\nIn addition to the geo-portal of the Swiss Confederation (https://map.geo.admin.ch) which focusses on 2D spatial data, swissgeol.ch extents its functionality to 3D data above, on and below the surface. For this, it relies on 3D visualization on the web, which is based on CesiumJS and offers numerous expert tools.\nCesiumJS is the most widespread open source 3D globe library and is used worldwide in many different applications. It not only visualizes large-scale global data, but also very detailed data at the local scale, such as buildings in the 3D view of map.geo.admin.ch.\nWith the development of swissgeol.ch, an underground navigation option was developed in CesiumJS for the first time, which allows the visualization of 3D objects below the terrain. In addition to navigating underground, it is also possible to see through the earth's surface using transparency settings, as well as to slice the 3D-scene vertically.\nWith the use of 3D tiles and precise terrain (2m precision), the data is delivered in an optimized format for the web. At the same time, the download of original data of entire layers or individual objects in the layer is offered.\nAfter the positive echo of last year’s talk, we are going to present this year selected features and data in greater detail and introduce recent developments: A new user experience and a specific user space for projects, as well as new data and formats, e.g. using the new voxel Cesium Next Tiles specs.",
        "description": ""
      },
      {
        "title": "Use of remote sensing and GIS processing for mapping Chestnut stands decline in Piemonte Region",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Chestnut stands in Piemonte are presently suffering a severe decline due to the concurrence of climatic and silvicultural factors. A project funded by Piemonte region involving the Regional Chestnut Centre and IPLA S.p.A. started in 2018 with the aim of defining technical guidelines for proper interventions in declining stands. The present contribution deals with the activities of spatial monitoring of declining areas through satellite images interpretation and GIS analysis making use of QGIS and Grass tools. Methodological approach was based on the selection of Sentinel 2A e 2B images taken at the beginning and at the end of the summer season in 2017, 2018 e 2019 on a test area. Those images were then processed calculating some indexes with raster functions implemented in QGIS. NDWI Normalized Difference Water Index (B8-B12/B8+B12) resulted the more sensible to the presence of declining stands \nAccurate mapping of areas suffering different degree of damages on the whole Region was then carried out starting from a preliminary analysis of experimental parcels surveyed on the field.",
        "description": ""
      },
      {
        "title": "A tool for mapping fire burn severity and extent in watersheds for flood risk assessment",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "As climate change progresses, we are experiencing an increase in the frequency and severity of extreme weather events in many parts of the world. Climate models predict the frequency and severity of these weather events to continue to increase in the future as surface air temperatures rise.\n \nIn 2021, the Canadian province of British Columbia (BC) experienced one of the most severe fire seasons on record which destroyed communities and ecosystems across the province. In the same year, an “atmospheric river” precipitation event led to widespread flooding causing severe damage to roads and communities across BC. There is a correlation between severe wildfires and increased runoff following precipitation events in some regions.\n \nThere is a need for better prediction, monitoring, and management of fire and flood events to mitigate the damages caused by post-wildfire flooding. Remote sensing data and analysis techniques play a key role in monitoring climate-related natural disasters and helping understand and mitigate risks to communities, ecosystems, and infrastructure in areas that may be exposed to flooding. Free remote sensing datasets along with free and open source software can greatly reduce the costs and increase availability of this monitoring capability, increasing stakeholder access to geospatial intelligence.\n\nThis talk presents a tool developed at Sparkgeo for automated mapping of burn severity and extent within watersheds of interest. The tool uses multi-source public remote sensing data in a cloud-based workflow, taking advantage of recent open source initiatives including the SpatioTemporal Asset Catalog (STAC). The tool can help assess flood risk from significant rainfall events and may offer essential flood mitigation and risk management knowledge. We present the tool’s deployment to map 2021 wildfires in several British Columbia watersheds.",
        "description": ""
      },
      {
        "title": "Micronutrient Action Policy Support (MAPS) - A decision support tool for investigating the scale and geographic distribution of micronutrient availability in sub-Saharan Africa",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Micronutrient deficiencies (MNDs), so-called ‘hidden-hunger’, can have serious ramifications for the health of individuals affected and the economy of the country in which they live. MNDs are a global problem but disproportionately affect populations in low-income countries. Work to alleviate these deficiencies aligns with the UN’s Sustainable Development Goals (SDG), especially SDG2 – access to adequate safe and nutritious food. Data which can support the understanding of the scale and location of these deficiencies can be fragmented in their availability and accessibility, creating a barrier to their use in planning interventions by stakeholders in the very nations where the impacts of MNDs are most severe.   \n\nThe Micronutrient Action Policy Support (MAPS) tool is a web-hosted open access platform providing a unique enabling environment for the wider agriculture-nutrition community and beyond which allows users to view and explore MND risks at various spatial and temporal scales. The tool can provide users with dietary micronutrient supply estimates of all nations in sub Saharan Africa using national-scale and subnational-scale data. Preprocessing steps to clean these data in R language are made available through the open github repository, so that any user can replicate the data used in the tool.  \n\nPriorities for the data and functionality have been co-designed with key users from project proposal stage. Stakeholder feedback is used in continued iteration as richer content, supporting material, and functionality is planned, developed and released. \n\nThe platform is built on open-source technologies utilising Postgres and PostGIS to store, combine and interrogate a range of heterogeneous datasets to calculate micronutrient supply estimates, node.js for data APIs and web map services using Geoserver.  Further data processing is conducted using R, with the front-end interface utilising Angular, leaflet.js and chart.js.  Metadata is managed and served via Geonetwork.  The code for the platform, as well as data processing scripts, methods and processes are all open source at https://github.com/micronutrientsupport. \n\nThis talk will provide an overview of the platform along with the datasets and open-source technologies that underpin its functionality, and the UX approaches taken to ensure that this tool meets the currently unmet needs of priority users.  \n\n(https://micronutrient.support)",
        "description": ""
      },
      {
        "title": "ARCOS Platform for Monitoring of Arctic Region",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The objective of ARCOS is to design and implement an early-warning system providing continuous monitoring of the Arctic Region. Designed to generate actionable products in the security domain by processing and fusing multi-sensor data, the system integrates available information from space, non-space sources and products available from multiple Copernicus services.\nARCOS generates information at three different levels of scale and user interaction:\n1) Level 1: Automatic Early-warning System. Integration of space and non-space data sources for the triggering of alarms on the region when certain conditions happens. Automatic early-warnings are generated in case anomalous behaviours are detected. For this wide-area monitoring, automatic extraction of analytics and AI techniques are applied.\n2) Level 2: User-Driven Alert System, where space and non-space data is processed on specific locations provided by the user. The alarms can be configured based contextual information based on the user input.\n3) Level 3: Geospatial Intelligence Products. Following early-warnings generated in Level 1 or 2, geospatial intelligence products requiring human intervention are provided upon user request.\nThe system is developed using GeoNode (https://geonode.org/) as a Core component and integrates OpenEO services (https://openeo.org/) for the generation of innovative contents from open datasets like Copernicus Sentinels data, Copernicus Services data, AIS data and Social media.",
        "description": ""
      },
      {
        "title": "From photographic survey to street-level imagery integration in an OpenSource webgis: complete workflow",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "For several years Gter has been involved in the development and maintenance of the webGIS related to the management of the road network of the Province of Piacenza. Recently, the client requested the integration of the images, deriving from the photographic survey of about 520 Km of routes, in the existing webGIS (an instance of Lizmap Web Client which public version can be found here https://catastostrade.provincia.pc.it/lizmap/lizmap/www/index.php/view/map/?repository=progettipubblici&project=catasto_strade_pub).\nIn this case, the Public Administration needed the photographic survey in order to update a set of old images sparsely distributed along the network and to have a customized tool similar to services like Google Street View. Therefore, an integration of the Mapillary viewer in Lizmap Web Client has been proposed and developed; hence the survey was performed with a camera that uses front and back optics to have 360-degree photos.\nThe workflow consisted of four main tasks. The first step involved the photographic survey of the road network using a GoProFusion360 mounted on a car that took photos of the surrounding environments. The next step consisted in the processing of the images, stitching the front and the back photos in order to obtain a 360-degree panoramic image. This step has been automated through the development of a Python script together with the use of the available software of the camera from the command line. About 50000 photos were uploaded on the Mapillary platform. Images have been integrated into the Lizmap Web Client webGIS through the Mapillary viewer and utilities. The integration was achieved by developing a new feature  for Lizmap Web Client based on Mapillary JS, an Open Source library provided by Mapillary that helps developers interact with the Mapillary API.\nThe final result is a tool that makes the Public Administration able to navigate and reach the photos uploaded on Mapillary directly from a window in the webGIS.",
        "description": ""
      },
      {
        "title": "Developing a topographic data production system based on open source",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The National Land Survey (NLS) of Finland decided in the fall of 2020 to develop a national topographic data production system based on open source technologies and especially on QGIS client. Since then, many significant steps have been taken for us to be able to implement the MVP of the application for the mappers of the NLS at the start of 2024. Later on, we are planning to replace our digital elevation model production system with similar technologies which would enable us to replace the current mapping system by 2026. In this presentation I will talk about our system's architecture, the tools we are developing and furthermore, some insights that we have learned during this project.\n\nThe application's architecture is based heavily on Postgres, where we run the main database from which the national topographic maps are made. The operators can access the main database via Job management-plugin and modify the database objects on the client (QGIS). These modifications (inserts, updates, deletes) are saved in the operators' work database, from which they can register these changes to the main database.\n\nOur first challenge was to design how more than 150 operators of the NLS are able to work using the same main database. We decided to use the client-web service architecture. The benefit of that is that we can use QGIS as such as it is and we can build all of the functions required for job management into the backend application. The job management plugin communicates with the web service and the conflicts between the different work databases are handled by a separate tool. With the tool, an operator can solve conflicts that are created by another operator editing the same objects.  \n\nCurrently, we are integrating stereo compilation with QGIS, which will enable the operators to measure objects from 3D aerial stereoscopic photos. The next steps are to develop comprehensive tools for quality assurance and to improve basic QGIS tools for selecting, editing and digitizing objects. The problem is that we have over a hundred different layers, therefore the default way to choose the layer is not sufficient for the operator. We also want to develop tools for real time quality checking so that the mapper would immediately know if a quality error occurs, making the general workflow smoother.\n\nAlthough some of the components are custom-made, our purpose is to publish those components that we recognize to be useful for other QGIS users. In addition, we have seen the value of multiple premade plugins to which we are planning to contribute as well. As a whole, NLS is currently looking for new ways to be a part of the open-source community.",
        "description": ""
      },
      {
        "title": "Supporting precision farming with GeoServer:  past experiences and way forward",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The amount of data available from drones, earth observation as well as machinery itself (i.e. telemetry data) plus the advent of cloud infrastructure has given a huge impulse to innovating the way we used to support farmers and farming in general, democratizing access to data and capabilities like never before through precision (or digital) farming solutions.\n\nPrecision farming (or digital farming) has therefore become one of main use cases for GeoServer deployments over the past years and at GeoSolutions we have worked with many clients, from NGOs to large private companies (like Bayer), from startups to organizations like DLR in helping them to support their client to make sense of data and information through GeoServer and other geospatial open source technologies at scale, in the cloud.\n\nThis presentation will condense 10 years of GeoSolutions in ingesting, managing and disseminate data at scale in the cloud for the precision farming industry covering items like:\n- Proper optimizations and organization of raster data\n- Proper optimizations and organization of vector data\n- Modeling data for performance & scalability in GeoServer and PostGIS\n- Deployment guidelines for performance and scaling GeoServer\n- Styling to create NDVI and other visualizations on the fly\n\nAt the end of the presentation the attendees will be able to design and plan properly a GeoServer deployment to serve precision farming data at scale.",
        "description": ""
      },
      {
        "title": "How to deal with a massive geographic database when surrounded by datascientists ?",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The Scientific and Technical Center for Building (CSTB) built the first French database of buildings and houses to address climate change challenge, helping knowledge and decision making for massive retrofit.  \nThe pipeline factory intersects massive datasets (21 Millions buildings, >400 descriptors) and keeps adding new predictions and external datasets all the time. It allows to run analyses and predictions for all the climate change related indicators, such as housing price and energetic performance relation, heat wave impact, solar potential, etc.. \nWhile the first versions where a direct image of the classical datascientist’s approach -ie a massive dataframe driven by massive yaml config files and cryptic meta-templated scripts– ease of use and access performance soon became a limiting factor.  This is a major concern since this dataset will be one long term foundation of derived information systems. \nBetween brute force approach based on scaling resources up, and the old fashioned « data diet » normalization and optimization process, the truth is not easy to find.  \nAbusing from cartoonish humor, this talk will try to explore the benefits of normalizing back hugely redundant geographic datasets and making public interfaces (public SQL model, API’s, vector tiles, OGC API’s) so that both end users can analyze efficiently this dataset, and the data manager team can rely on more stability using those good old’ database constraints.",
        "description": ""
      },
      {
        "title": "SensorThings API in practice: the AIR-BREAK project in Ferrara",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "In the city of Ferrara (Italy) Dedagroup Public Services and other partners are involved in AIR-BREAK project (https://airbreakferrara.net/) to implement a set of geo-ICT tools for supporting an improved identification and monitoring of urban air quality.\nDifferent datasets from heterogeneous sources have been already interconnected and integrated in the Spatial Data Infrastructure of the Municipality of Ferrara, based on (geo)standard protocols for data interchange sourced by:\n•\t173 authoritative AQ monitoring stations from 3 regional environmental agencies, ARPAE Emilia-Romagna (52), ARPA Veneto (33) and ARPA Lombardia (88), for their own whole regional areas;\n•\t2 private AQ monitoring stations managed by private companies located in Ferrara;\n•\t14 new AQ monitoring stations installed by Lab Service Analytica (project partner) in the territory of Ferrara \nFor integrating and sharing dynamic hourly data about air quality and other themes, we adopted the OGC Sensor Things APIs (STA) as the reference standard protocol [1].\nSTA is based on the OGC/ISO 19156:2011 [2] and provides an open and unified framework to interconnect IoT sensing devices, data, and applications over the Web. It is an open standard addressing the syntactic interoperability and semantic interoperability of the Internet of Things. It complements the existing IoT networking protocols such CoAP, MQTT, HTTP, 6LowPAN. While the above-mentioned IoT networking protocols are addressing the ability for different IoT systems to exchange information, STA is addressing the ability for different IoT systems to use and understand the exchanged information. \nIn AIR-BREAK project, FROST solution (FRaunhofer Opensource SensorThings-Server) [3] has been deployed in the GIS server farm of the Municipality of Ferrara to complement Geoserver and other technologies already providing services for viewing/accessing data based on OGC standards.\nIndeed, among the final objectives of the project, the implementation of a standard-based Air Quality Data Infrastructure focuses on:\n1.\tcreating a bi-lateral and cooperative communication systems between authorities and citizens about air quality and its perception;\n2.\tdefining and implementing a multi-stakeholder Data Infrastructure on Air Quality to integrate existing/heterogeneous/dynamic data from both authoritative sources and crowdsourced by citizens (Rete di Monitoraggio Ambientale Partecipativo [4]);\n3.\tproviding such dynamic air quality data to local authorities involved monitoring (i.e. Municipality of Ferrara, ARPAE, Regione Emilia-Romagna) and to citizens, through standard APIs (STA) and with open licenses through the upcoming open data portal of Ferrara (June 2022);\n4.\ttesting and validating innovative solutions for air quality monitoring using in-situ IoT sensors and satellite remote sensing (e.g. Copernicus)\n\nReferences:\n[1] https://en.wikipedia.org/wiki/SensorThings_API\n[2] https://www.iso.org/standard/32574.html \n[3] https://github.com/FraunhoferIOSB/FROST-Server \n[4] https://rmap.cc/",
        "description": ""
      },
      {
        "title": "OGC API Standards: Past, Present, and Towards an Exciting Future",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Over the past several decades a significant number of geospatial datasets have been published on the Web. Many of those datasets were published through implementations of classic OGC Web Service standards. As time has gone past, the architecture of web applications has evolved, propelled by new Web and Internet standards. This evolution of web application architecture has led to a revolution in how geospatial datasets are published on the Web. To ensure that the revolution in geospatial data publication has interoperability at its core, the Open Geospatial Consortium (OGC) has developed a series of Web Application Programming Interface (API) standards. \n\nThe OGC API suite of standards is a family of specifications that have been designed as modular building blocks that spatially enable Web APIs that offer access to spatial data and implementations of geospatial algorithms. These revolutionary APIs make location information more accessible than ever before through the use of the OpenAPI specification for describing interfaces. The use of the OpenAPI specification means that implementations of OGC API Standards can describe themselves to levels of detail previously unachievable through the classic OGC Web Service standards. Such an ability to self-describe is significant because it has enabled software developers from a variety of disciplines to implement OGC API Standards to address the needs of their communities.\n\nThis presentation will provide an overview of the background, current status, and future plans for the development of OGC API Standards. The presentation will describe plans for the development of resources that improve the ability of developers to implement OGC API Standards. The presentation will also present a selection of case studies of open source software that has been implemented or enhanced during OGC Innovation activities such as testbeds, hackathons, and sprints (including the 2022 Joint Code Sprint organised by OGC, OSGeo and the Apache Software Foundation (ASF)).",
        "description": ""
      },
      {
        "title": "State of Lizmap - Past / Present / Futur",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Lizmap is an opensource server application to publish QGIS project on the web without any coding skills needed.\nIt's using QGIS Server in the backend so users have the same rendering between their QGIS Desktop and the web version of their project.\n\nQGIS Server and Lizmap are reading QGIS project to publish layers with their legend, forms, print layout, layer relationships... Some additional Lizmap configuration can be added to have dataviz capabilities, decide or not to publish the attribute table or to configure the feature filter form. No coding skills are required, all the configuration is done using QGIS Desktop user interface.\nThe QGIS project is adapted for web browsers and have a responsive UI. Lizmap include some Access Control List at different levels such as project, layer or even features.\n\nThe goal of this presentation is to show the state of this opensource project hosted on GitHub and to explain the roadmap.",
        "description": ""
      },
      {
        "title": "State of Bridge for QGIS",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "GeoCat Bridge for QGIS is a Python plugin that enables users to publish map layers as OGC data services (WM(T)S/WFS/WCS) to GeoServer or MapServer. It can also publish layer metadata to the GeoNetwork spatial catalog (CSW), linking service to metadata and vice versa, so that users can easily bind to a service from a catalog search result or find the relevant metadata for an exposed dataset.\nBridge can also export metadata, symbology and geodata to local files, so you can modify them and/or upload them manually.\n\nSince its first official release at the FOSS4G in Bucharest (2019), GeoCat has been gradually improving the plugin. One of the most requested and anticipated changes to Bridge for 2022 relates to GeoServer workspace publication. The next upcoming major release involves some major UX changes, which will allow more control over a workspace. For example, users can soon add (or overwrite) single layers to an existing workspace, whereas in older versions all workspace data was removed prior to publication. We would like to take the opportunity to discuss the upcoming release, highlighting this and other new features and improvements.",
        "description": ""
      },
      {
        "title": "QGIS & PostGIS : Tips & Tricks",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "QGIS and PostGIS are now renown as one of the best combination to setup a GIS application.\n\nThe support of PostGreSQL and PostGIS in QGIS have grown very mature and offers great features to deal with your database stored geographic data. It allows to create powerful business application easily without any advanced programming language skills, only plain SQL and a well configured QGIS.\n\nThis presentation will showcase some of the interesting features you should be aware of in order to build the perfect GIS user application. I'll explain how to:\n- Properly configure relations between layers/tables,\n- Enable transactions (and whether or not you should do it),\n- Communicate from PostGreSQL to QGIS using notifications,\n- Deal with triggers using data dependencies,\n- Store your QGIS project and style information in a PostGIS database,\n- Output your processing result directly to your database,\n- Manage your database directly from the browser.\n\nThese will be some of the key features that will be demonstrated along with a given use case.",
        "description": ""
      },
      {
        "title": "ETF testing framework: past, present and future",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "ETF is an open source testing framework for validating data and APIs in Spatial Data Infrastructures (SDIs). It is used by software solutions and data providers to validate the conformity of geospatial data sets, metadata and APIs.\n\nFor example, ETF is the underlying framework used by the INSPIRE Reference Validator to validate INSPIRE metadata, datasets and services against the requirements of the INSPIRE Technical Guidelines. ETF is also used extensively in Germany by the Surveying Authorities of the Laender to validate their datasets. This includes Real Estate Cadastral data, Topographic data, Control Points, 3D Building Models, House Coordinates and Building Polygons. In the test environments of the German Laender, a comprehensive series of attributive, relational, geometric, and topological tests are performed on the data, in addition to interacting with APIs and checking for errors in the interface contracts. Other European Union (EU) Member States are also reusing ETF to allow their data providers to test resources against national requirements. Finally, some software tools such as GeoNetwork open source include validation based on the ETF API in their workflow.\n\nGoals in designing the ETF software were to create test reports that are user-friendly and self-explanatory as well as to be able to validate large amounts of data, which can be several hundred GB in size. In order to cover different validation tasks and present them in a unified report, the architecture is modular and different Test Engines can be used. Currently the following Test Engines are supported: SoapUI for testing web services, BaseX database for testing XML data, Team Engine to validate WFS and OGC Web APIs using the OGC CITE tests, NeoTL Engine for testing WFS, OGC Web APIs and datasets. \n\nAs a horizontal and reusable tool, which could be extended to satisfy the needs of different communities and domains, ETF is currently considered as a component of the so-called Common Services Platform under the new Digital Europe Programme of the European Commission. Within this context, activities are planned in 2022 to: i) include the ETF in the OSGeo Live v.15, and ii) submit the ETF as an OSGeo Community Project.\n\nIn this talk, we will introduce the ETF testing framework, the deployment scenarios and address the current and future technical developments.",
        "description": ""
      },
      {
        "title": "Re3gistry: Your interoperable open source tool for managing and sharing reference codes",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The Re3gistry is an open source software for creating, managing and sharing reference codes in a consistent way. Released under the European Union Public License (EUPL) v.1.2 (https://github.com/ec-jrc/re3gistry), it is a key component ensuring interoperability in data infrastructures.\nThe Re3gistry supports organizations in managing and updating “reference codes” through unique identifiers. Reference codes can be used for example to define sets of permissible values for a data field or to provide a reference or context for the data being exchanged. Examples are enumerations, controlled vocabularies, taxonomies, thesauri or simply ‘lists of things’. The Re3gistry provides a means to assign identifiers to such items and their labels, definitions and descriptions in different languages. It provides a user-friendly interface where labels and descriptions for reference codes can be easily browsed by humans and retrieved by machines, including the possibility of downloading them in different formats and exploiting the information using a REST API.\nThe European Commission’s Joint Research Centre (JRC) started the development of the Re3gistry in 2014 to satisfy the interoperability requirements set by the INSPIRE Directive. In 2020, considering the high reusability of the tool beyond INSPIRE, the JRC released the code as open source. So far, the development of the Re3gistry has been supported by the European Commission under the interoperability actions ARE3NA and ELISE and, more recently, by the National Land Survey of Finland.\nIn 2021, a second version of the Re3gistry software was released. This version v2.0, complemented by subsequent minor releases, introduced several new features such as a management interface to add and modify the status of items, and the capability to trace the full lifecycle of items following the workflow defined by ISO 19135 standard. The current stable release is v.2.3.0 released in March 2022. The INSPIRE registry (https://inspire.ec.europa.eu/registry) is currently the most popular implementation of the Re3gistry software. It is the central point of access to (currently) more than 7000 reference codes, available in 23 languages and several formats (HTML, ISO 19135 XML, JSON, RDF/XML, Re3gistry XML, CSV), grouped into different centrally managed INSPIRE registers. The content of these registers is based on the INSPIRE Directive, Implementing Rules and Technical Guidelines (as illustration, used to reference INSPIRE themes, code lists and application schemas). \nThe Re3gistry is currently used by many organizations across European countries (Austria, Finland, France, Italy, Slovakia, Spain, The Netherlands and North Macedonia), different EU-funded projects and private organizations even outside Europe. Since 2021, the Re3gistry v.2.x is included in the OSGeo Live to facilitate discovery and use by the open source geospatial community. In 2022 the software will be also submitted as an OSGeo Community Project. \nSumming up, the Re3gistry is more relevant than ever to ensure semantical and organisational interoperability across any kind of systems, including Spatial Data Infrastructures (SDIs).",
        "description": ""
      },
      {
        "title": "A high performing data retrieval system for large and frequently updated geospatial datasets",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "ECMWF is a research institute and a 24/7 operational service, producing global numerical weather predictions and other data for a broad community of users. To achieve this, the centre operates one of the largest supercomputer facilities and data archives within the meteorological community. ECMWF also operates several services for the EU Copernicus programme to provide data for Climate Change, Atmospheric monitoring and Emergency services.\n\nAs part of ECMWF's Open data initiative, more and more meteorological data and web services are freely available to a wider community. ECMWF's web services include an interactive web application to explore and visualize its forecast data, a Web Map Service (WMS) server and many graphical products including geospatial weather diagrams so called Ensemble (ENS) meteograms and vertical profiles.\n\nENS meteograms and vertical profile diagrams are among the ECMWF's most popular web products and presents ECMWF's multi-dimensional real-time ensemble forecast data for a given position globally. They are freely available through various ECMWF web services,  and integrated on ECMWF's GIS based interactive web application. Datasets powering the dynamically generated diagrams are formed from a rolling archive of 10 days data, updated twice a day and each update consists of data around half a Terabyte. An upcoming update on ECMWF's forecasting system will increase the data size by a factor of 3-4 times in the near future.  In addition to ECMWF's forecast data, similar services are requested as part of various Copernicus projects producing different datasets.\n\nThis talk presents migrating legacy data structure used for ENS meteogram datasets to a more flexible, extensible, and high performing one fit to be used by GIS systems by using Free Open Source Software (FOSS). The new data structure uses Python ecosystem. The data preparation workflow as well as the challenges and the solutions that are taken when dealing large and frequently updated geospatial datasets are presented. Talk will also include early experiments and experiences to offer these datasets as part of OGC's Environmental Data Retrieval (EDR) API.",
        "description": ""
      },
      {
        "title": "Helping to Land a Mars Rover using FOSS4G Tools",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Since landing on Mars in February of 2021, the Mars 2020 Perseverance Rover has been exploring Jezero crater to investigate an ancient delta looking for the evidence of past microbial life and to better understand the geologic history of the region.  In support of Terrain Relative Navigation (TRN), which enables the Mars 2020 spacecraft while landing to autonomously avoid hazards (e.g., rock fields, crater rims), the USGS Astrogeology Science Center generated two precision mosaics: 1) the Lander Vision System (LVS) map generated from three Context Camera (CTX) orthorectified images that was used onboard the spacecraft and was the “truth” dataset that TRN used to orient itself relative to the surface during Entry, Decent, and Landing; and 2) a High Resolution Imaging Science Experiment (HiRISE) orthomosaic which was used as the basemap onto which surface hazards were mapped. The hazard map was also onboard the spacecraft and used by TRN to help identify the final, hazard-free landing location. \n\nThis talk will present the workflow used by the USGS Astrogeology Science Center to generate these critical data products including the use of FOSS4G tools like GDAL. Other open-source packages used will also be shared.",
        "description": ""
      },
      {
        "title": "State of GeoRasterLayer: A LeafletJS Plugin for Visualizing GeoTIFFs (and soon other rasters)",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "GeoRasterLayer is a LeafletJS Plugin for visualizing GeoTIFFs.  This presentation will show live demos of new features and discuss the roadmap for the next couple of years.\n\n### Features\n- Support for nearly all projections, thanks to [proj4-fully-loaded](https://github.com/danieljdufour/proj4-fully-loaded) and [epsg.io](https://epsg.io/)\n- Super faster rendering thanks to a simple nearest neighbor interpolation\n- Use of web workers means seamless integration that doesn't block main thread\n- Loads large geotiffs greater than a hundred megabytes\n- Supports custom rendering including custom colors, directional arrows, and context drawing\n- Doesn't depend on WebGL\n\n### Videos\n- [Edge Compute: Cool Stuff You Can Do With COGs in the Browser](https://www.youtube.com/watch?v=XTkNhGpfmB8&t=4190s)\n- [2019 - Algorithm Walk-through: How to Visualize a Large GeoTIFF on Your Web Map](https://www.youtube.com/watch?v=K47JvCL99w0)\n\n### Examples\n- Loading the georaster-layer-for-leaflet library along with GeoBlaze via a script tag. You can view the source code [here](https://github.com/GeoTIFF/georaster-layer-for-leaflet-example/blob/master/examples/load-via-script-tag-with-geoblaze.html) and the live demo [here](https://geotiff.github.io/georaster-layer-for-leaflet-example/examples/load-via-script-tag-with-geoblaze.html).\n- Combining two Cloud Optimized GeoTIFFs together to create an NDVI map. You can view the source code [here](https://github.com/GeoTIFF/georaster-layer-for-leaflet-example/blob/master/examples/ndvi.html) and the live demo [here](https://geotiff.github.io/georaster-layer-for-leaflet-example/examples/ndvi.html).\n- Identifying Wildfires from a Landsat 8 Scene. You can view the source code [here](https://github.com/GeoTIFF/georaster-layer-for-leaflet-example/blob/master/examples/identifying-wildfires-with-landsat.html) and the live demo [here](https://geotiff.github.io/georaster-layer-for-leaflet-example/examples/identifying-wildfires-with-landsat.html).\n- Visualizing Population COG. You can view the source code [here](https://github.com/GeoTIFF/georaster-layer-for-leaflet-example/blob/master/examples/population.html) and the live demo [here](https://geotiff.github.io/georaster-layer-for-leaflet-example/examples/population.html).\n- Display a COG that represents only one band of a Landsat scene. You can view the source code [here](https://github.com/GeoTIFF/georaster-layer-for-leaflet-example/blob/master/examples/cog-with-only-one-band.html) and the live demo [here](https://geotiff.github.io/georaster-layer-for-leaflet-example/examples/cog-with-only-one-band.html).\n- Display a COG with YCbCr Photometric Interpretation.  You can view the source code [here](https://github.com/GeoTIFF/georaster-layer-for-leaflet-example/blob/master/examples/ycbcr.html) and the live demo [here](https://geotiff.github.io/georaster-layer-for-leaflet-example/examples/ycbcr.html).",
        "description": ""
      },
      {
        "title": "geotiff.js - efficient GeoTIFF exploration in the browser and server",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "geotiff.js is a reusable library to abstract remote (Geo)TIFF files. With it, both rich visualization frontends or statistical or data access services can be implemented, as it is possible to inspect the geospatial metadata and the full spectrum raster values of the original data, instead of only 8-bit RGB(A).\n\nDue to its file abstractions, it is possible to only read the relevant portions of a file, thus greatly reducing bandwidth and response times. This effect can be further increased when reading Cloud Optimized GeoTIFF (COG) files.\n\nThe library tries to be as feature complete as possible in terms of file layout, raster cell values, RGB transformation, image data compression and metadata.\n\nThis talk will detail the features of geotiff.js, as well as its most recent additions. Additionally, it will shed light on the greater ecosystem of geospatial libraries and applications where geotiff.js is embedded or building the foundation of.",
        "description": ""
      },
      {
        "title": "Building a common building's(!) open dataset using FOSS4G, open data and open governement.",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Climate change is here. heating, construction, cooling is estimated to contribute to 30% of the C02 emissions for France. And yet, we don't really have a database of those buildings. We have footprints by the French National Geographic institute, tax raising datasets on cadastral parcels, many derived datasets for energy consumption, performance certificates, but all of them are far away from a usable and centralized reference dataset. \n\nThe national adress geolocation (BAN) project unlocked the key pivot database between all them. The Scientific and Technical Center for Building (CSTB) a public industrial and commercial company, decided to dedicated efforts to build a permanent reference dataset, and push it as an open database. \n\nThe full stack is using open source technologies (Pandas / GeoPandas, to PostGIS, Apache Spark, MLflow, QGIS, MapLibre ...), and with massive datasets (21 Millions buildings, >400 descriptors). It allows to run analyses and predictions for all the climate change related indicators, such as housing price and energetic performance relation, heat wave impact, solar potential, etc.. \n\nAs the first versions are now published, the next challenges are : \n- make the data easier to reuse\n- Push toward a official common identifier of each building, housing and parcels, through the BatID project and Etalab open government initiatives\n- Enrich the dataset with new statistics and predictions twice a year\n- Consolidate its economic rationales to make this viable on the long run\n\nThis talk will also show cool dataviz and geoviz stuff for geonerds audience :)",
        "description": ""
      },
      {
        "title": "Use of open source tools to estimate global GHG emissions.",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Hey everyone, my name is Saheel Ahmed. I work as a senior data scientist at Blue Sky Analytics. We are a climate tech startup primarily focused on creating environmental datasets for better monitoring and climate risk assessment for various stakeholders across the globe. To achieve this, we are leveraging the potential of geospatial analytics by creating a catalogue of comprehensive and accurate climate data to drive sustainable decision-making. And all this is only possible because of the open-source tools & knowledge made publicly by the good folks organising the event.\n\nGreenhouse gas (GHG) emissions from biomass burning (which includes the combustion of forests, savannas, and croplands) play an important role in regional air quality, global climate change, and human health. In the year 2021, all the continents except Antarctica witnessed major wildfires. These enormous blazes some the size of a small country aren’t just destroying native forests and vulnerable animal species. They’re also releasing billions of tons of greenhouse gases into the atmosphere, potentially accelerating global warming and leading to even more fires. Accurate assessment of biomass burning emissions is paramount to understanding and modelling global climate change.\n\nBy combining open-source tools with geospatial data, we present a global dataset that estimates the total GHG emissions due to biomass burning globally. We achieved this by linking satellite-based fire observations, aerosol optical depth (AOD), and vegetation type (based on land cover classification) to directly estimate how much carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O) were emitted from each fire. We conducted further analysis of estimated emissions by comparing our estimates with existing datasets from NASA's global fire emissions database and ESA's Copernicus global fire assimilation system. Overall, our estimates agree well against both of these sources with an R2 score of 0.91, 0.71, and MAE score of 9, 14 MtCO2e/yr against GFEDv4.1s and GFASv1.2 respectively across 245 nations between 2015-2020. The dataset includes country-level estimates of gross GHG emissions across different vegetation types such as forest, cropland, shrubland, and grassland for the last 5 years.\n\nThe dataset is currently a work in progress as we aim to add more features such as covering other landcover types, ground truth alternatives. The dataset and its documentation are available at https://github.com/blueskyanalytics/get-started. The dataset is also our contribution to the global coalition Climate Trace (https://www.climatetrace.org/), an independent group for monitoring & publishing GHG emissions across different sectors.",
        "description": ""
      },
      {
        "title": "Building the Bloomberg for Climate Data with FOSS",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "I am the Founder and CTO of Blue Sky Analytics, a Climate-Tech Startup using satellite-derived climate intelligence to power financial decisions. We provide datasets through API spanning flood, drought, wildfire, heat risk for monitoring, measuring and mitigating climate risk which can be leveraged for various use-cases.\n\nIn 2 years, we have analyzed TBs of data, delivered 5 datasets & built platforms for data visualisation and distribution from scratch using FOSS technology. This has been a rocket-ship of a journey, chasing our mission of building a ‘Bloomberg for environmental data’.\n\nHowever, the not-so-secret sauce to achieving these milestones has been FOSS. We are often asked how we procure raw geospatial data and how much we spend on it. Thanks to the abundance of open data, our data acquisition cost has been 0. Due to the generous open data policy of amazing organisations like NASA & ESA, we have been able to build a business collecting TBs of data daily & crunching them into useful insights.\n\nThis helped us scale our vision to build a global environmental data stack for tracking climate change in real-time. Moreover, before this data can be applied to climate mitigation, it needs to be analysed. This is true for any big data and today, less than 1% of global data is analysed. \n\nGiven that satellite data is the most significant source of tracking climate variables, it became imperative to tap this source. We discovered that the path to providing environmental datasets was by building a powerful geospatial data refinery along with [SpaceTime™](https://spacetime.blueskyhq.in/)  and our [dev portal](http://developer.blueskyhq.in).\n\nThere was limited infrastructure available to support the delivery of geospatial datasets so we built it, leveraging open-source tools like Postgres, QGis, GDAL, k8s etc.\n\nWhile we have proprietary layers to our models, as a team of young developers, data scientists and designers, many self-taught, our cultural ethos stands firmly with FOSS and we plan to be a leading contributor to FOSS for climate action. The most significant step for us in that direction has been providing annual, country-wise data on biomass emissions to the Al Gore-led [Climate TRACE](https://www.climatetrace.org/) inventory that can be used by the public via CC-BY-4.0 framework.\n\nAs our business expands, we aim to open-source tools & innovations we have internally developed while building our infrastructure and have started that journey with the [Raster Playground.](http://play.blueskyhq.in)\n\nClimate change is the most pressing challenge of our times, throwing at us various questions that need to be answered. This is not possible without data. Data helps us understand the problem and quantify the risk to various assets. \n\nOpen source tools and data have made it possible for 20-year-old data scientists to access sophisticated satellite data to understand the changing planet and answer these questions. BSA is a testament to the fact that fighting climate change is not possible without FOSS.",
        "description": ""
      },
      {
        "title": "Where Is Everyone?: The Global Impact of COVID on Road Traffic",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "This presentation examines the global impact of COVID-19 on traffic across 40 cities in Europe, South East Asia, Australia, and North America. I analyzed monthly rush hour traffic from 2019 to 2021 with  GeoPandas, leafmap, and  HERE's Traffic Analytics data. In addition, I correlated traffic volume with COVID positivity, mortality,  and vaccination rates to examine how these factors influence the resumption of pre-pandemic traffic patterns.\n\nBecause of the volume of the traffic data (billions of records), desktop GIS software, including spatially enabled databases, could not reliably process it on a desktop computer. Online solutions, such as Google Colab, would have been a costly alternative given the amount of data. However, GeoPandas and Jupyter notebook on notebook computer was able to process the traffic data and enhance the spatial road data and support joining the result to the road network for visualization. The method for processing the data will be discussed in detail. In summary, open source tools give researcher unprecedented abilities to process large amounts of data.",
        "description": ""
      },
      {
        "title": "Implementing OGC APIs using Elasticsearch and pygeoapi",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The Open Geospatial Consortium API family of standards (OGC API) are being developed to make it easy for anyone to provide geospatial data to the web, and are the next generation of geospatial web API standards designed with resource-oriented architecture, RESTful principles and OpenAPI. In addition, OGC APIs are being built for cloud capability and agility.\n\npygeoapi is a Python server implementation of the OGC API suite of standards. The project emerged as part of the OGC API efforts started in 2018 and provides the capability for organizations to deploy OGC API endpoints using OpenAPI, GeoJSON, and HTML. pygeoapi is open source and released under an MIT license. pygeoapi is built on an extensible plugin framework in support of clean, adaptive data integration (called \"providers''). \nElasticsearch (ES) is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.\nThe Elasticsearch data provider for pygeoapi is one of the most complete in terms of functionalities and it also includes CQL support with the CQL-JSON dialect, which allows you to take extra advantage of the ES backend.\n\nThis presentation will provide an overview of OGC APIs, pygeoapi and Elasticsearch integration, and demonstrate usage in a real-world data dissemination environment.",
        "description": ""
      },
      {
        "title": "Norwegian National SensorHub - sharing IoT data with open standards and technology",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Sensordata (IoT) is widespread in both private and public sector. However, making use of sensordata across different sectors and applications is challenging - in particular with respect to a geospatial application across different use cases. This encompasses both enviroment/climate sensors, like water-level sensors to smart-building monitoring and water pipe sensors. An interdisciplinary team from diverse sectors is working towards building national standards, an open architecture and implementing proof-of-concepts on a national sensorhub for sharing streams and archives of sensordata in Norway. The team builds upon the very successful open data ecosystems (SDI) that exists in Norway for standardized geospatial data. The project is funded from a range of partners including municipalities, the mapping authority and the maritime ports of Norway. The working group includes open source tech expertise on sensor technology alongside user and demand expertise from the different sectors. \n\nThis talk will focus on the technological advances made from the team both on software and architecture. There will be particular focus on the open architecture and software prototyping that has been developed in the working group. Both of which will be available under an open license.",
        "description": ""
      },
      {
        "title": "OpenMapTiles 3.14 - vector tiles from OpenStreetMap & Natural Earth Data",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "OpenMapTiles is an open-source set of tools for processing OpenStreetMap data into zoomable and web-compatible vector tiles to use as high-detailed basemaps. These vector tiles are ready to use in MapLibre, Mapbox GL, Leaflet, OpenLayers, QGIS as well as in mobile applications.\nDockerized OpenMapTiles tools and OpenMapTiles schema are being continuously upgraded by the community (simplification, performance, robustness). The presentation will be demonstrating the latest changes in OpenMapTiles. The last release of OpenMapTiles greatly enhanced cartography and map styling possibilities, especially for roads such as adding new tags (expressway, access, toll). But also adding concurrent route labels or motorway junctions. Improvements were also done in the countryside by adding important tracks and paths (displaying from zoom 12), cliffs, aretes, and ridges. Another enhancement is the possibility to show mountain heights in customary units (feet in the US). OpenMapTiles is also used for generating vector tiles from government open-data secured by Swisstopo.",
        "description": ""
      },
      {
        "title": "OSM planet data to vector tiles in a few hours: OpenMapTiles & Planetiler",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Converting OpenStreetMap planet data into vector tiles has been a complex and costly process, but now, thanks to the Planetiler project, it has become possible to do on a single powerful machine in just a few hours – over two orders of magnitude speed up!\n\nOpenMapTiles is a mature customizable tile generation framework and layer specification that can be tailored to specific tile generation needs. It has existed for many years, and allowed users to generate their own layers, optimizing for size or completeness. Over the years it moved to PostGIS-based ST_AsMVT approach, and made numerous small improvements. The biggest downside of OMT was the extensive hardware requirements.\n\nRecently Mike Barry rewrote core functionality of the OMT stack as a single monolithic app, making it possible to generate entire planet data in just a few hours on a single machine. Now the OMT community is actively adapting this new approach, researching if Rust would be even better approach, and experimenting how to make the process customizable and support real-time updates.",
        "description": ""
      },
      {
        "title": "Exploring the World’s Open Data Portals - Discovery, Visualisation and Analysis of Open Data with TerriaJS",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "TerriaJS is an open-source framework for web-based geospatial catalogue explorers. \n\nIt uses Cesium and Leaflet to visualise 2D and 3D geospatial data, and it supports over 50 different Web APIs, file formats and open data portals.\n\nIt is almost entirely JavaScript in the browser, meaning it can even be deployed as a static website, making it simple and cheap to host.\n\nTerriaJS is used across the globe to create next-generation Digital Twin Platforms for open geospatial data discovery, visualisation and sharing - it is used to drive\n\n- [National Map](https://nationalmap.gov.au/) (Australian Gov)\n- [Digital Earth Australia Map](https://maps.dea.ga.gov.au/)\n- [Digital Earth Africa Map](https://maps.digitalearth.africa/)\n- [Pacific Map](https://map.pacificdata.org/)\n- [NSW Spatial Digital Twin](https://nsw.digitaltwin.terria.io/) (Australian State Gov)\n- and many others\n\nSupported formats/protocols include:\n\n- Imagery services like WMS/WMTS and ArcGis Imagery/Map Service\n- Feature/vector services like WFS, ArcGis Feature Service, Mapbox vector tiles, GeoJSON, Shapefile, KMZ, GPX, GeoRSS\n- 3D sources like Cesium 3d-tiles, GLTF and CZML\n- Tabular/sensor data: CSV, SDMX, GTFS, SOS, Socrata and Opendatasoft\n- Open Data portals: CKAN, CSW, Socrata, Opendatasoft, Magda, THREDDS, WMS/WFS Servers, ArcGis Portal and SDMX\n- Geoprocessing with WPS\n- With plans to support new and upcoming services like OGC APIs and STAC in the future.\n\nIn this talk, I will show how TerriaJS can connect to Open Data Portals to\n\n- Discover open datasets\n- Visualise datasets in 2D and 3D\n- Perform aggregation/analysis on datasets\n- Create/share maps with the world!\n\n[https://terria.io/](https://terria.io/)\n\n[https://github.com/TerriaJS/terriajs](https://github.com/TerriaJS/terriajs)",
        "description": ""
      },
      {
        "title": "Building Footprint Extraction in Vector Format Using pytorch_segmentation_models_trainer, QGIS Plugin DeepLearningTools and The Brazilian Army Geographic Service Building Dataset",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Building footprint extraction is a popular and booming research field. Annually, several research papers are published showing deep learning semantic segmentation-based methods to perform this kind of automated feature extraction. Unfortunately, many of those papers do not have open-source implementations for public usage, making it difficult for other researchers to access those implementations.\n\nHaving that in mind, we present DeepLearningTools and pytorch_segmentation_models_trainer. Both are openly available implementations of deep learning-based semantic segmentation. This way, we seek to strengthen the scientific community sharing our implementations.\n\nDeepLearningTools is a QGIS plugin that enables building and visualizing masks from vector data. Moreover, it allows the usage of inference web services published by pytorch_segmentation_models_trainer, creating a more feasible way for QGIS users to train Deep Learning Models.\n\npytorch_segmentation_models_trainer (pytorch-smt) is a Python framework built with PyTorch, PyTorch-Lightning, Hydra, segmentation_models.pytorch, rasterio, and shapely. This implementation enables using YAML files to perform segmentation mask building, model training, and inference. In particular, it ships pre-trained models for building footprint extraction and post-processing implementations to obtain clean geometries. In addition, one can deploy an inference service built using FastAPI and use it in either web-based applications or a QGIS plugin like DeepLearningTools.\n\nResNet-101 U-Net Frame Field, ResNet-101 DeepLabV3+ Frame Field, HRNet W48 OCR Frame Field, Modified PolyMapper (ModPolyMapper), and PolygonRNN are some of the models available in pytorch-smt. These models were trained using the Brazilian Army Geographic Service Building Dataset (BAGS Dataset), a newly available dataset built using aerial imagery from the Brazilian States of Rio Grande do Sul and Santa Catarina. Pytorch-smt also enables training object detection and instance segmentation tasks using concise training configuration.\n\nThis way, considering the aforementioned, this talk presents the usage overview of both technologies and some demonstrations. Using metrics like precision, recall, and F1, we assess the results achieved by the implementations developed as a product of our research, showing that they have the potential to produce vector data more efficiently than manual acquisition methods.\n\nDeepLearningTools is available at the QGIS plugin repository, while pytorch_segmentation_models_trainer is available at Python Package Manager (pip). The Brazilian Army Geographic Service develops both solutions, making their codes available at https://github.com/phborba/DeepLearningTools and https://github.com/phborba/pytorch_segmentation_models_trainer.",
        "description": ""
      },
      {
        "title": "UN Open GIS Initiative: Geopaparazzi Survey Server and SMASH for Mobile Data Collection in a UN Peacekeeping Mission (MONUSCO)",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The project “Geopaparazzi Survey Server (GSS) and SMASH for Mobile Data Collection in a UN Peacekeeping mission (MONUSCO)” aimed to operationalize the use of GSS and SMASH to support field data collection in MONUSCO. This talk will cover the endeavours of this project, introducing the background, user requirements and use cases, the implementation of the online GSS at a UN central data centre as well as the project outcomes and recommendations.  \n\nMONUSCO GIS: MONUSCO GIS Unit uses mobile devices, GPS, mini UAV, satellite imagery from commercial providers, to ensure availability of detailed topographic and up-to-date mission operational data for the various GIS end products (cartographic maps, imagery, infographics, interactive web map applications, geodatabases). Having a central server for mobile data collection will ensure constant availability of standardized, quality controlled and up-to-date operational data, which is key for the provision of quality GIS products & services and consequently data-driven decisions and actions for the Mission. From the beginning, there hasn't been a centralised infrastructure readily available to all the geographically dispersed UN Mission users in the DR Congo for field mobile data collection. This project marked the genesis of it. \n\nUN Open GIS Initiative: Established in March 2016, is to identify and develop an Open Source GIS bundle that meets the requirements of UN operations, taking full advantage of the expertise of contributing partners (Member States, international organizations, academia, NGO’s and private sector). Geospatial Information Systems (GIS) have played a substantial role in providing timely and effective geospatial information products (maps and dynamic tools) to ensure the United Nations operations are equipped with suitable information to support the UN mandates through informed planning and decision-making processes. The UN has been using proprietary GIS software for the past two decades. The rapid growth and development of open-source GIS solutions present the technological potential, operational flexibility, and financial benefits as well as ease to access for UN operational partners and host nations.",
        "description": ""
      },
      {
        "title": "Maintaining a national Aerial Image Registry with QGIS",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The National Land Survey (NLS) of Finland maintains a registry of aerial imagery in Finland containing metadata of imagery since 1932. As part of the NLS' strategy of moving towards FOSS software, a novel registry management tool is being developed as a QGIS plugin. This talk describes the process of designing and implementing the new registry management software and explores the suitability of QGIS as a platform for creating highly customised spatial data management tools. While the registry management tool is developed for QGIS, the registry is migrated from Oracle to a PostGIS database, following a redesign of the data model.\n\nIn 2020, the NLS announced it aims to build its technological environment based on open source technologies. As a result, there is an ongoing effort of re-designing and implementing various existing processes and systems using open-source technologies. One such system is the national Aerial Image Registry. The registry is managed by a group of NLS employees and metadata is used in various workflows for publishing new data products from captured images and planning new aerial imaging missions.\n\nThe current registry management software is a technically dated solution based on Visual Basic 6 and an Oracle database. Key features of the new registry management tool include the ability to query the image registry and show the search results on map, editing and archiving existing data in the registry, importing new data to the registry, creating data extracts to PDF maps and spatial formats, and validating plans for aerial imaging missions. QGIS provides an user-friendly platform that is already familiar to many GIS experts that can be easily extended with plugins that provide custom functionality and features.\n\nThe NLS has prior experience of designing and developing tailored QGIS plugins to support their unique workflows, including plugins for maintaining the topographic database of Finland and the national point cloud registry. These projects have been well received by users and developers alike, and the positive feedback has encouraged the NLS to continue developing tailored QGIS plugins for its specific needs.",
        "description": ""
      },
      {
        "title": "Mapping impacts of Covid-19 in Nairobi",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "The Covid-19 pandemic has had many impacts beyond health - economic, social, etc. The Cities Covid Mitigation and Mapping (C2M2) project, from the US Department of State's MapGive initiative, sought to map and help direct policy around these secondary impacts of Covid in several countries globally. Map Kibera and GroundTruth Initiative worked to track these impacts in Nairobi, focusing on the themes of education, water and sanitation.\n\nThis talk will present the outcomes of the project, which focused on the mapping in OSM of schools, water points, and toilet facilities in the informal settlements of Kibera and Mathare. These updates to existing OSM data help show how the pandemic affected these sectors by looking historically at changes. Additionally, individual surveys about access to water during shortages and impacts of school closings and disruptions help paint a picture of how Nairobi's lower income residents have been particularly impacted by the pandemic. There is also a strong gender component to the impacts which will be highlighted.\n\nThe project used a combination of tools, which will also be presented: Kobo Toolbox for mapping and individual survey collection, OSM for map data, and data analysis in QGIS. The Kenya team was supported by many other team members from the C2M2 project for data analysis. Additionally, participants in Africa included Bukavu in the DRC and Pemba in Mozambique; we will briefly share their map outcomes as well. The \"Africa Hub\" which included Nairobi, Pemba and Bukavu showed that across the continent, economic and social impacts of Covid-19 on vulnerable groups were particularly challenging.",
        "description": ""
      },
      {
        "title": "Datahub: the confluence of open data and geo data",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Open data movement has been very active and trendy lately. Many solutions brought fresh air in the metadata ecosystem. Nevertheless, no one really pushed forward the confluence of the open data world and the geo metadata world (often powered by ISO or INSPIRE standards). \n\nActually, many organizations still use both systems, which leads to confusion for the end users: datas are duplicated, metadatas are harvested in both directions, many websites aim to serve the same goal. Overall, this split does not help to easily find your data.\nIt can also give headache to platform administrators, developers and architects who try hard to keep all catalogs synchronized.\n\nBased on this analysis, we are convinced that an ultimate solution could take the advantages of both ecosystems. Complex ISO standards, INSPIRE rules and opendata light schemas can co-exist in the same catalog. All new great ideas like quick data visualization or dataviz widgets can be supplied for any kind of data. The datahub literally came out from the need to centralize any kind of public dataset within the same platform.\n\nThought as a backend API agnostic solution, the datahub first implementation has started based on the GeoNetwork 4 api, with an ElasticSearch backend.\nThe search is fast, accurate, multilingual and customizable. The solution has been designed from use cases: how do you want to help the end users to find, use and value their datas. It brings a new experience to old fashion INSPIRE catalogs and aims to embrace modern challenges like the community, vote, favorites, publishers, usages of the datasets, dataviz and so on.\n\nLeveraging technical challenges to merge open data and metadata, the datahub emphases on a pure, intuitive and fluent user experience.",
        "description": ""
      },
      {
        "title": "Pangeo Forge: Crowdsourcing Open Data in the Cloud",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Geospatial datacubes--large, complex, interrelated multidimensional arrays with rich metadata--arise in analysis-ready geopspatial imagery, level 3/4 satellite products, and especially in ocean / weather / climate simulations and [re]analyses, where they can reach Petabytes in size. The scientific python community has developed a powerful stack for flexible, high-performance analytics of databcubes in the cloud. Xarray provides a core data model and API for analysis of such multidimensional array data. Combined with Zarr or TileDB for efficient storage in object stores (e.g. S3) and Dask for scaling out compute, these tools allow organizations to deploy analytics and machine learning solutions for both exploratory research and production in any cloud platform. Within the geosciences, the Pangeo open science community has advanced this architecture as the “Pangeo platform” (http://pangeo.io/).\n\nHowever, there is a major barrier preventing the community from easily transitioning to this cloud-native way of working: the difficulty of bringing existing data into the cloud in analysis-ready, cloud-optimized (ARCO) format. Typical workflows for moving data to the cloud currently consist of either bulk transfers of files into object storage (with a major performance penalty on subsequent analytics) or bespoke, case-by-case conversions to cloud optimized formats such as TileDB or Zarr. The high cost of this toil is preventing the scientific community from realizing the full benefits of cloud computing. More generally, the outputs of the toil of preparing scientific data for efficient analysis are rarely shared in an open, collaborative way.\n\nTo address these challenges, we are building Pangeo Forge ( https://pangeo-forge.org/), the first open-source cloud-native ETL (extract / transform / load) platform focused on multidimensional scientific data. Pangeo Forge consists of two main elements. An open-source python package--pangeo_forge_recipes--makes it simple for users to define “recipes” for extracting many individual files, combining them along arbitrary dimensions, and depositing ARCO datasets into object storage. These recipes can be “compiled” to run on many different distributed execution engines, including Dask, Prefect, and Apache Beam. The second element of Pangeo Forge is an orchestration backend which integrates tightly with GitHub as a continuous-integration-style service.\n\nWe are using Pangeo Forge to populate a multi-petabyte-scale shared library of open-access, analysis-ready, cloud-optimized ocean, weather, and climate data spread across a global federation of public cloud storage–not a “data lake” but a “data ocean”. Inspired directly by the success of Conda Forge, we aim to leverage the enthusiasm of the open science community to turn data preparation and cleaning from a private chore into a shared, collaborative activity. By only creating ARCO datasets via version-controlled recipe feedstocks (GitHub repos), we also maintain perfect provenance tracking for all data in the library.\n\nYou will leave this talk with a clear understanding of how to access this data library, craft your own Pangeo Forge recipe, and become a contributor to our growing collection of community-sourced recipes.",
        "description": ""
      },
      {
        "title": "UN Maps: OpenStreetMap supporting Peace and serving Humanity",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "UN Maps is a program led by the United Nations Department of Operational Support in support of several peacekeeping and political missions such as UNSOS, MONUSCO, MINUSCA, MINUSMA and UNISFA.\nBy leveraging internal and crowdsourcing capabilities, UN Maps aims not only to enrich topographic and operational data in UN mission areas but also to provide peacekeeping and humanitarian actors with topographic maps, operational geo-information, search and navigation tools, and imagery and street-level base maps, leveraging OpenStreetMap, the Wikipedia of maps.\nIn order to achieve its goals, the UN Maps Initiative is building a thriving community around the collection, validation, usage, and dissemination of open geospatial data. This community is called UN Mappers.\nIt benefits from the established crowdsourcing activities, such as mapathons, training opportunities and other collaborative events involving several stakeholders as the UN staff on the field (peacekeeping and agencies, funds and programs), academia (high schools and universities in Africa, EU and US), local communities and remote volunteers.\nTogether, the UN Mappers community give substantial support not only in the production of maps and web services but also in the development of innovative applications using virtual reality and data analytics. Some of the results obtained with open data to these applications will be presented during the intervention.\nFurthermore UN Mappers are working on translating and updating OSM documentation material in all 6 UN official languages, which is distributed with open license.",
        "description": ""
      },
      {
        "title": "Overview on the Free & Open Source Software for geospatial in Togo (West Africa)",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Open source software has grown rapidly in recent years in many developed countries. Developing states especially in low income states have also seen this development. The emergence of open source software, especially in the field of geographic information, has created practitioners and communities around these digital tools and commons. However, the freedom, free and easy access is seen by many technicians and decision-makers as less reliable and less efficient than proprietary software. In order to know the perceptions and the level of adoption of free software in Togo, we conducted a small survey among students, governmental and civil society actors and non professionnal cartographers. This presentation will help to highlight the importance of free software in Togo. Thus my presentation will clarify the types of projects in which open source geospatial is used, its interest for the users, in short, to have a clear idea on the state of the open source geospatial software in Togo.",
        "description": ""
      },
      {
        "title": "OpenSource to the rescue: the future of MapLibre",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "The story and the future of the MapLibre community - the project that continues to develop various browser and native technologies for map tile visualization ever since Mapbox changed their licensing on the amazing Mapbox gl js technology that sadly became proprietary restricted to Mapbox own service.\n\nThis talk will cover existing lib capabilities, how the project grew to include native, navigation, routing, 3D, and other features. How the project was able to quickly migrate to typescript with lots of additional testing and stabilization efforts. How we became a large non-centralized collective of mapping technologies covering web, android and ios devices. How hundreds of small and large donations from developers and companies have helped with extra incentives.\n\nSome possible future projects and ideas will be presented by individual feature owners, including the possibility of uniting all library efforts using a cross platform compilation from the common Rust code (web assemblies + native libs) and additional styling features.",
        "description": ""
      },
      {
        "title": "FOSS4G and Open Data to the rescue: a European story!",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Neither the open (geo)data initiative and, needless to say, the open source for geospatial one, are the new kids on the block anymore. Crucial steps have been taken all over the world in establishing the framework of openness, collaborative development and transparency ranging from hands-on events - such as code sprints or collaborative data gathering - mapathons - to funding opportunities and legislative measures. In this context, we present a 3 years-old EU supported initiative founded on open source and open data and that has reached maturity. In our talk, we present the potential support that such initiatives have in the wider framework of environmental monitoring and reporting across Europe - Geo-harmonizer. \n\nGeo-harmonizer stands for EU-wide automated mapping system for harmonization of Open Data based on FOSS4G and Machine Learning. The project unfolded between 2019 and 2022 and was co-financed under Grant Agreement Connecting Europe Facility (CEF) Telecom project 2018-EU-IA-0095 by the European Union.",
        "description": ""
      },
      {
        "title": "Open Tech Collective: sharing HOT's journey",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "At  the beginning  of  2022, HOT_tech started a collaboration with Kathmandu Living Labs on the Tasking Manager. This followed our ambition to facilitate an open  collaborative process in building and improving our open source technology by forming a  ‘collective’. The idea of a  ‘collective’ is to bring people with shared purpose together on shared ground to achieve a shared goal. A lot to be shared! In this context the shared goal is the development of the  Tasking Manager.  \n \nOur  vision is for creating with, for, and by the community for making the product more impact-driven and user friendly.  The Tasking Manager has proven itself over the years to be not just a software but  a platform to bring the different individuals, communities, organizations who share a common goal towards not only humanitarian effort and crisis response but identifying local resources and needs through mapping and data. So whether you are a mapper, a validator, a designer or an open source developer with an interest in the Tasking Manager, you can join the collective. \n \nDuring this talk we will share our journey in building the collective. You will hear an open and honest reflection on what worked, what didn’t work, what we have learned and what we hope to do going forward. We want this talk at FOSS4G  to open a conversation with other open source and geospatial communities on best ways for designing, creating and implementing open, diverse and inclusive spaces for thriving and healthy collectives and communities.",
        "description": ""
      },
      {
        "title": "Classroom GIS in South Africa",
        "type": "Lightning talk",
        "track": "Education",
        "abstract": "The introduction of technology in the education sphere has brought about improvement regarding the quality of education young and old individuals receive. A country’s development plays a huge factor in the quality of services its people receive, therefore not every country will receive the same quality of services.\nClassroom GIS has changed how Geography as an academic subject is taught. It has sparked interest in the practical component of the subject and gives more understanding to the strong relationship between theory and map work. This leads to the concept of spatial thinking and how it has allowed geography educators around the world, some without basic GIS education, to see the importance of including more GIS concepts in the high school geography curriculum.\nSeveral GIS software packages are available that educators can use to teach their students. But taking into account the availability of resources when focusing on the African continent, it is probable that free software and hardware plays a key role in the development of GIS concepts being included in the geography curriculum. It is affordable, and learning resources are readily available, in terms of tutorials, documentation and more.\n“This inclination towards GIS textbook lecturing has largely jeopardized the quality of GIS education”. - (Fleischmann and van der Westhuizen, 2020 found in The Journal of Geography Education in Africa)\nAdvocacy to include GIS practices and strategies in geography education across Africa have been documented, but has not received the necessary exposure it needs from its governments. The majority of GIS teaching has been textbook-based, making the introduction of GIS technology and education a frightening phase that educators may not want to engage in.\nTo overcome the fears behind understanding and grasping basic GIS concepts in the classroom, interactive GIS tutorials may help to remove these fears and make the adoption of GIS simple, especially within countries where service delivery (education services) is poor. \nQGIS for example is an open-source GIS software that has been around for 20 years, it has shown tremendous improvements and upgrades throughout its 20 years. Its user-friendly capabilities have improved, making it ideal to introduce more geography educators and learners to the software. It has tutorials and material that is suited for individuals from different walks of the profession. \nThe tutorials are interactive and allow first-time users to readily engage with the material. For both learner and educator to understand the material and not get overwhelmed. The key factors to understand are; GIS can be implemented into the African geography school curriculum, open-source software is key to overcoming limitations such as lack of resources and geography educators are willing to take on GIS with sufficient training. More urgent research is needed on reliable and sustainable methods and practices of teaching GIS in a secondary school classroom.",
        "description": ""
      },
      {
        "title": "The best practices of Open Source GIS base applications on Türkiye General Directorate of Hihgways",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Traffic Safety Application:\n\nIn the data preprocessing phase, data quality assessment and dirty data were determined on the accident data by using desktop GIS applications such as QGIS,SAGAGIS,JOSM,GeoDa. OpenSource GIS applications and programming languages such as R language and python were used in data cleaning, exploratory analysis and data processing. Statistics for accident data have been extracted. OpenSource map server such as GeoServer is used for sharing, editing and organizing accident maps and base maps produced in GIS applications.\nPython software language was used in the server side of the project. For geospatial data analysis, accident points were verified by using geopandas and shapely libraries. PostgreSQL database was used to store geo-based accident data and the PostGIS extension was used. PostGIS adds spatial capabilities to PostgreSQL so it can store, query, and manipulate spatial data. On the server side scripting, GeoAlchemy(an extension of SQLAlchemy) is used for working with spatial databases and geospatial queries.\nFor the client side, Turf was used for any spatial operations. It is a geospatial engine, and it includes spatial operations and helper functions. MapboxGL-WebGL-powered library is used for interactive vector maps on the web application. To render more than 100k of accidents with high performance, WebGL powered geospatial visualization framework DeckGL was used. NebulaGL provides geospatial drawing and editing tools for lines, polygons etc. It was added for selecting analysis regions from the map. Osm-Nominatim is a geocoding library. It allows users to find accident locations from an address.\n\nOpenSource GIS Tools:\n\n●\tGIS software for data visualization, processing and analysis:\n\nQGIS, GRASSGIS, JOSM, SAGAGIS, OrbisGIS\n\n●\tGIS Servers: GeoServer, MapServer, Mapnik,\tMapGuide, QGIS Server\n\n●\tBackend(Python) :Geopandas, Shapely, Postgis\n\n●\tClient(Javascript) :Turf, DeckGL, osm-nominatim\n\nAsset Management Application:\n\nWith the “Image-Based Information Management System” Application, it is work to make an information management system and Digital 3D road inventory map for the 91,126km road network under the responsibility of the General Directorate of Highways. With this project, it is possible ensured that images (approximately 21 million) of the highway are taken and the collection, digitization, storage and presentation of the road information of the objects base and the realization of asset management, maintenance services, planning, project design, measurement and evaluation processes determined by the administration through the image.\n\nOpenSource GIS Tools:\n\n●\tGIS software for data visualization, processing and analysis: QGIS\n\n●\tGIS Servers: GeoServer\n\n●\tBackend :Postgis, Turf, @swimlane/ngx-charts\n\n●\tClient :OpenLayers\n\nResults:\n\n• Data in semi HD map quality was produced.\n\n• The entire road network can be monitored and analyzed with panoramic images.\n\n• The vertical and horizontal profile of the entire highways network has been created.\n\n• 2.995.845 point inventories and 991.820km line inventories in 42  inventory were produced.\n\n• Detection of inventory deficiencies in the field and maintenance processes can be followed.\n\n• Road inventory data needed can be shared with other public institutions.",
        "description": ""
      },
      {
        "title": "The Africa Knowledge Platform",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The Africa Knowledge Platform is a one-stop-shop for the European Commission (EC)’s scientific knowledge on Africa. Developed by the Joint Research Centre, this open, visual, and interactive platform brings together a wealth of geospatial scientific information on Africa’s social, economic, territorial and environmental development. The Africa Knowledge Platform is developed using open-source geospatial technologies and the whole platform is open to the public, so that its potential value extends to academia and to stakeholders from the public, private and non-profit sectors in both Europe and Africa. The platform brings together information across 62 topics within 10 broad themes: natural resources, sustainable growth and jobs, food and agriculture, climate change, human demography, health, security, economy, energy and digital transformation. This covers all 17 of the United Nations Sustainable Development Goals. We will be taking a tour and in-depth look at the workflow used to develop and publish the platform and the technologies behind it.",
        "description": ""
      },
      {
        "title": "The Growth of OSM Communities in Tanzania Through Community Microgrants",
        "type": "Lightning talk",
        "track": "Transition to FOSS4G",
        "abstract": "The growth of OpenStreetMap communities (OSM) in Tanzania is taking shape as most organizations, institutions, and communities in general, are recognizing the importance of using and contributing to OpenStreetMap data. To support the growth of OSM communities in Tanzania, OpenMap Development Tanzania with her partner the Humanitarian OpenStreetMap Team (HOT) awarded microgrants to seven OSM communities in Tanzania - See the supported communities here. \n\nThe grants provided are supporting these communities to leverage the use of OSM and mapping to help solve different community challenges by facilitating training/workshops, purchasing tools and equipment, supporting staff, and other logistics. Most communities work in peripheral regions with a minimal understanding and use of open data and mapping technologies like OpenStreetMap, OpenDataKit QGIS, etc.\n\nThe first phase of project implementation ended with great successes and lessons learned from these communities. The general success of the microgrants so far include the following: \n\n1. Transforming communities from using traditional data collection to digital open source tools such as ODK and Kobocollect has greatly improved data management and analysis. For instance, Agri Thamani Foundation and LAVISEHA are among microgrant recipients who are new to OpenStreetMap and other open mapping technologies for generating open data; however, they now use different tools, i.e. Kobotoolbox, OpenDataKit, OSM, and Tasking Manager, to collect data for their interventions in nutrition and gender-based violence.\n\n2. Connecting OSM communities in Tanzania and encouraging collaboration in various tasks and opportunities. The grant provided an opportunity for local OSM communities in Tanzania to work together; a good example is Hope for Girls and Women in Tanzania, giving training to LAVISEHA on how communities can use open source tools like ODK to report the cases of GBV for rescue.\n\n3. Creating awareness about OSM and other communities through participating and presenting in conferences and events such as the State of the Map Africa, Community webinars, etc.\n\n4. Over 16000 building footprints were mapped and 380 km of roads were uploaded to OpenStreetMap by grantees\n\n4. Supporting the growth of youth mappers chapters - Three grant recipients are youth mappers from three different universities who are using the grants to solidify the chapter and get exposure to projects while also applying for other funding to expand their projects\n\nAlthough the microgrant is expected to end in June 2022, OMDTZ is committed to supporting these communities through training and different engagements to ensure they achieve their goal of using open data for decision-making. Together we can add more people to a map by supporting all communities in mapping.",
        "description": ""
      },
      {
        "title": "The story of OSGeo in Oceania",
        "type": "Lightning talk",
        "track": "Transition to FOSS4G",
        "abstract": "OSGeo has existed in Oceania in various forms for quite a while now. Some of the major contributors to projects such as QGIS are based in Oceania and open geo events have been organised in the region for many years. It is only in more recent times however that we have started to support these efforts through the creation of a local chapter. When a group of us came together to organise Oceania’s first regional FOSS4G SotM conference in Melbourne, 2018, it became clear structure was needed to sustain the momentum we had created.\n\nStructure was established by forming an entity and completing all the tasks that go along with that. This included creating a constitution, financial policies, forming a board of directors, establishing a membership policy, consulting with the community, working out what the entity’s primary purpose is and so much more. We’ve made plenty of mistakes along the way, but we’ve also learned a lot. There are many successes too, such as the establishment of a Microgrant program to support initiatives across the region, the continuation of annual regional conferences, funding travel so that people without the financial means to do so could attend conferences, and the welcoming of new members from far and wide.\n\nThis talk is an insight into the journey of OSGeo Oceania. It is not meant to be a how to guide or a pat on the back, but rather a chance to facilitate discussion among the FOSS4G community so that we can find ways to support the use and understanding of open geospatial software in our respective regions.",
        "description": ""
      },
      {
        "title": "What is new in Giswater 3.5",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Giswater (www.giswater.org) is a open source software aimed at being a corporate tool in water utilities with which to manage network assets in an excellent way and at the same time have the assets ready for hydraulic simulation, a feature that today Today it is known how to have a digital clone of network assets.\n\nTechnologically, it uses a set of Open Source technologies such as EPANET, SWMM, PostgreSQL, PostGIS or QGIS, all of them mature and proven, which give it a very powerful base for growth and consolidation.\n\nIts 'database centric' architecture gives it enormous potential with which maintenance operations (network outages) can be managed in an integrated way, longitudinal profiles can be made, events inventoried, among others.\n\nIt has a data model with dual-face architecture, which allows full integration of inventory and hydraulic model data, both for drinking water networks (https://github.com/Giswater/giswater_dbmodel/wiki/epanet- dual-dbmodel) and for urban drainage and sanitation networks (https://github.com/Giswater/giswater_dbmodel/wiki/swmm-dual-dbmodel) , giving full flexibility to the modeler to work with hydraulic capacities without any impact on inventory data for each asset item.\n\nThe EPA file export module has certain \"on the fly\" transformations to make the two different geometries (remember the dual-face) of the inventory elements compatible for both EPANET (https://github.com/Giswater/giswater_dbmodel/ wiki/epanet-on-the-fly-transformations) how to for SWMM (https://github.com/Giswater/giswater_dbmodel/wiki/swmm-on-the-fly-transformations).\n\nIt allows you to work with different scenarios to create different modeling conditions in order to check the worst case scenario or check how the network will respond in future scenarios. For Water Supply networks it is possible to work with demand scenarios (https://github.com/Giswater/giswater_dbmodel/wiki/epa-demand-scenarios) and for Urban Drainage projects it is possible to work with DWF scenarios (https: //github.com/Giswater/giswater_dbmodel/wiki/epa-dwf-scenarios) and hydrological scenarios (https://github.com/Giswater/giswater_dbmodel/wiki/epa-hydrology-scenarios)\n\nAdditionally, it also allows working with alternatives to plan new elements of the network without interfering with the elements of the asset inventory. By creating an alternative (https://github.com/Giswater/giswater_dbmodel/wiki/masterplan-capabilities) you can modify the physical reality of the network without affecting the real assets at all.\n\nAnother especially interesting feature is that it allows collaborative work. Large hydraulic engineering projects have been worked on to date in a sequential or fractional way, but not collaboratively. Thanks to Giswater it is now possible to work on projects in a real collaborative way, given the inherent multi-user characteristics of the Databases on which the project pivots.\n\nThe project was born seven years ago and in its version 3.5 it incorporates interesting novelties, among which the following stand out: Complete refactor of the python code, new hydraulic model capabilities with the management of multi-scenarios or the improvement of the usability of numerous tools such as dynamic zoning or the info among others.",
        "description": ""
      },
      {
        "title": "Rural water supply mapping by using FOSS4G application in Rwanda",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "[Water and Sanitation Corporation (WASAC)](https://wasac.rw/) started mapping rural water supply system in Rwanda since 2018. WASAC conducted the data collection by using QGIS, QField and PostGIS all over the country of Rwanda, and now all GIS data is available as open data and visualized in this [website](https://rural.water-gis.com/) by using Mapbox Vector Tiles. WASAC is trying to achieve universal access to water in SDGs Goal 6 by keeping updating and utilizing GIS data.\n\nWe are developing GIS system as open source, and all of source code was developed in Github through WASAC organization repositories [here](https://github.com/WASAC) under the collaboration with [The United Nations Vector Tile Toolkit](https://github.com/unvt/) team. Our approach uses quite low-cost technologies which are more sustainable in low and middle-income countries. All our data is also available in [OpenAFRICA](https://africaopendata.org/organization/water-and-sanitation-corporation-ltd-wasac).\n\nOur achievements of the project were presented in previous global conference of [FOSS4G 2019 Bucharest](https://media.ccc.de/v/bucharest-30-case-study-of-data-collection-data-sharing-for-rural-water-supply-management-in-rwanda) and [FOSS4G 2021 Buenos Aires](https://www.youtube.com/watch?v=1Y2HbWkapDA). In FOSS4G 2022, we would like to update our current situation of the GIS system to the community.",
        "description": ""
      },
      {
        "title": "The Freshwater Biodiversity Information System (FBIS) – mobilising data for monitoring freshwater ecosystems",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "Access to long-term biodiversity datasets is vital for monitoring, managing, and protecting freshwater ecosystems. Detecting critical ecosystem changes, such as losing unique biodiversity and ecosystem services, is dependent on access to data. A wealth of biodiversity data exists for river ecosystems in South Africa, but an operational information system to access these data is currently not available. To address this knowledge gap, the Freshwater Biodiversity Information System (FBIS) has been developed. FBIS is a platform for hosting, visualizing, and sharing freshwater biodiversity information for South African rivers. The project seeks to mobilize and import to the system baseline biodiversity data, identify strategic long-term monitoring sites, and train key organizations on how to use the information system. Using map-based visualizations, user-friendly dashboards and rapid data extraction capabilities, the system will improve knowledge of freshwater biodiversity and long-term river health trends, thereby supporting better-informed river management decisions and conservation planning projects.",
        "description": ""
      },
      {
        "title": "Hardening a GeoNode Project – Some considerations about container security and optimization",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The GeoNode, according to the project's website, is a platform for managing and publishing geospatial data. It brings together mature and stable open source software projects into a consistent, easy-to-use interface, allowing non-specialist users to share data and create interactive maps. In Brazil there is a growing use of GeoNode, observed mainly in governmental institutions and universities. One of the main ways of installing and configuring GeoNode is the so-called Geonode Project. It consists of a custom Django Project template, which contains, in addition to the main project files, a set of Dockerfiles of GeoNode components, such as GeoServer, Nginx (reverse proxy) and PostGIS. From a detailed analysis of the components of the GeoNode Project created, it was found that the original dockerfiles contain a series of security holes and also unnecessary packages for the execution of the stack, not recommended for production environments. A Dockerfile that follows best practices eliminates the need to run privileged containers (as root), the use of unnecessary packages, leaked credentials, like mail passwords or database DSNs, or anything that could be used for an attack. Removing known risks in advance will reduce security management work and service overhead. The objective of this talk corresponds to discuss the possible security holes found in the Geonode Project and, with the application of best practices in Dockerfiles, to make it leaner and safer for production environments. For demonstration purposes, there will have a project to be used as an example and will be hosted at https://github.com/geonode-br/hardening-geonode-docker.",
        "description": ""
      },
      {
        "title": "MapFish Print, the classic printing component, a project update",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "MapFish Print is a mature Java-based open source software (BSD-2 license) for printing maps. Opposed to frontend solutions such as inkmap (https://github.com/camptocamp/inkmap), MapFishPrint runs server side and is integrated in several open source GIS frameworks like GeoMapFish ( for creating geoportal applications) or geOrchestra (spatial data infrastructure).\nThe classic approach to deploy MapFish Print is using a WAR-file in a Servlet Container (for example Tomcat), while it can also be integrated into cloud environments with prebuilt Docker images. Alternatively, MapFish Print’s core printing library can also be integrated into other projects programmatically.\nMapFish Print supports the common data formats and standards (WMS, WFS, WMTS, GeoJSON, etc.) and provides access to rich cartographic features like rotations, grids, north-arrow or legends and multi-page printing. The layout is defined by a JasperReports template and a YAML configuration file.  The template allows users  to define the layout, include elements for maps, legends, grids and alphanumeric tables. Clients request a concrete print-out with a JSON-Request, providing along information like bounding-box, map layers and other data. The final report will be rendered by MapFish Print either as PDF or as a raster image and returned to the client.\nWe will present a summary of existing features as well as new (e.g. tiled WMS with buffered tiles for rendering large areas without label conflicts) and planned features of the MapFish Print open source project.",
        "description": ""
      },
      {
        "title": "Contributing to Geospatial Cloud Native Solutions",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Over the last years, new architecture arose with the aim of easing the deployment and the usage of geospatial data and software in the cloud. Large accounts are starting to leverage the power of Kubernetes in public clouds such as AWS or Azure associated with traditional OSGEO software such as GeoNetwork and GeoServer.\nIn this talk we'll present our experience and results of working with large institutions in the public sector (civil defence, judiciary) or in the private sector (insurances, telecoms). We'll demonstrate how working the agile way with open-source software and high-level contributors allow to tackle successfully even the most ambitious challenge.\nAs a result of these efforts (multiple 100-man days contributed to the communities), we could participate significantly in developing GeoServer cloud as well as GeoNetwork microservice. Both projects aim to solve the cloud native challenge and are well underway of succeeding.\nAfter such an initial effort, we encourage every party using open-source software to participate in the maintenance and contribute to open-source development. Only with a real open-source engagement can we as a community achieve producing sustainable best of breed open-source software. Finally, we recommend customers to make sure their service providers have a positive impact in the communities.",
        "description": ""
      },
      {
        "title": "Implementation of the Chinese Postman Problem in the Valhalla Routing Engine",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "The Routing Engine Valhalla has been extended with a solution of the Chinese Postman Problem (CPP). This means that the most efficient route to travel all roads and paths in the area can now be calculated within a defined polygon.\nThe CPP is a well-known problem from graph theory, where the goal is to visit every edge in a graph while minimizing the distance traveled. In theory, a graph can be either directed, undirected, or mixed.\n In this implementation, the CPP has been implemented for directed graphs, as this corresponds to the representation of graphs in Valhalla and the data structure of OpenStreetMap (OSM). The latter forms the data basis for the calculation of the CPP route.\nThe CPP is solved using the following set of algorithms: the Floyd-Warshall algorithm, the Hungarian method, and the Hierholzer method. After successfully implementing the theoretical code base of the CPP, the main challenge was to make the route calculation executable using real-world road networks (OSM).\nA key problem with the implementation of the theoretical CPP is that in real-world graphs, not every edge is always reachable by all other edges. Therefore, various extensions had to be made to allow the computation of a CPP route using OSM data. For example, within a larger area, rarely all road segments are accessible exclusively via the roads located in the area. It is often necessary to leave the area to access these otherwise inaccessible parts of the road network.\nEventually, we were able to create a working prototype of the CPP in Valhalla. In addition to the function of freely selecting the area to be traveled, restricted zones, so called no-go areas,  can also be defined. After selecting the vehicle type (car, bicycle, pedestrian, etc.), the CPP route can be calculated, which also includes turn-by-turn navigation.",
        "description": ""
      },
      {
        "title": "A Graph-Based Road Conflation Method Preserving Connectivity",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Connectivity of roads in a map is essential for many use cases including navigation. We present a graph-based solution to the road conflation problem which takes into account the connectivity of the road network. First, we generate a road network graph in both sources based on bifurcation points. Second, we carry out node and edge matching between the graphs where we follow shortest distance as a matching criterion. This is followed by the merging stage where graph edges with matching end nodes get conflated. Newly added roads are connected with the graph based on node and edge matching. We carry out experiments on conflating open source footway datasets from multiple cities with the OSM. The resulting conflated map contains up to 16x map feature improvements per city with geometrically accurate and smooth results around road junctions. Future work involves using different graph matching criteria to improve on the conflated output.",
        "description": ""
      },
      {
        "title": "How much “15-minutes” is your city? Using open data to measure walking proximity",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The challenges posed to the current urban mobility model by pollution-related and urbanisation issues have resulted in significantly increasing the importance of urban resilience. Mobility management, pandemics’ spreading, equal access to services and climate crisis are just some of the crucial issues that falls within the definition of urban resilience.\nOne very promising solution aiming to solve many of these issues has been presented in 2016 by Professor Carlos Moreno under the name of “15-minutes city”. The paradigm is based on the idea that every citizen should be able to reach the essential services (supermarkets, shops, parks, etc) walking not more than 15 minutes from their home. The model is being tested in some metropolitan cities around the world (e.g. Paris).\nHowever, reorganizing the city so that it presents a 15-minutes structure is not an easy task. It requires large resources and a careful planning based on data, to make sure that the project undertaken will actually have a positive effect on the urban mobility and no asset is wasted on useless projects.\nThe Business Innovation team of Dedagroup Public Services used Open Street Map data to develop an index to detect the local level of proximity within the city, showing both the areas that already conform to the 15 minutes model and the ones that do not, where taking action would improve the quality of life of the citizens living there.\nThe presentation will be focused on this proximity index, describing the assumptions behind its definition, such as the choice of city services to be considered essential, the nature of the road network used to compute walking distances and the area tiling chosen for the task.\nThe index will be then showcased on the city of Florence, together with an analysis of the city from a proximity point of view and a what if scenario: how would the index change if the municipality (and other relevant stakeholders) decided to make interventions on low proximity areas?\nThe case of Ferrara will be also presented to show that the proximity index can be the basis for further analyses: coupling the index with resident population count can help to spot the areas that are both under-served and highly populated, that are the ones where more people would benefit from improvements.",
        "description": ""
      },
      {
        "title": "Introducing Wayfarer - a Python Routing Library",
        "type": "Lightning talk",
        "track": "State of software",
        "abstract": "Compass Informatics [1] is pleased to announce the open sourcing of its routing library Wayfarer [2][3]. \n\nWayfarer is a pure Python library that allows spatial features to be loaded into a NetworkX [4] network format. Once in this format the data can be manipulated and analysed using the huge range of graph algorithms in NetworkX. \n\nThe Wayfarer library provides a number of helper functions for example to calculate routes, split edges, find ends of paths, and retrieve features by keys.\n\nThe talk will outline the use cases for the library, and when it may be suitable to use rather than alternatives such as pgRouting. Case studies will be presented including Wayfarer’s use in Ireland's Pavement Management System to help designate works and surveys on the road network. Wayfarer is also currently used for an Environmental Protection Agency project to create fully connected river networks in Ireland. \n\n[1] https://compass.ie/\n[2] https://pypi.org/project/wayfarer/\n[3] https://github.com/compassinformatics/wayfarer\n[4] https://networkx.org/",
        "description": ""
      },
      {
        "title": "Buses moving on the map - use case of building web mapping portal",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "This presentation will be a real story about the process of building webmapping portals with usefull public transport information and the quality of air.\nTwo portals will be shown one from Warsaw and one from Cracow (https://gdziejestautobus.pl/mapa/, https://www.mapakrakow.pl/).\nThis will be an use case of using opensource software and open data for building the web mapping portal. The challenges will be presented by constructing layers with live positions of public transport vehicles and the state of air quality in Poland.\nThe technical details will be presented along with the logistic and an business aspects. Following points will be covered: used development software, user experience challenges, design of project, project organization, effort, cost, legal issues.\nThere will be shown the sources of data from open public API services in Poland. The one is the open API with vehicles location data of public transport office in Warsaw. The second is the data coming from public office responsible for environment protection and monitoring.\nBoth presented portals shows the live position of public transport vehicles in the capital of Poland. The portal for Cracow will also show the live state of air pollution in the city. The pollution data come from sensors located in Poland collection the quality of air.\nBoth portals are the examles of: how to connect opensource Web-GIS tools and open public data to build an interesting web mapping site showing usefull data in the convenient spatial way.",
        "description": ""
      },
      {
        "title": "Mapping Historical \"Street View\" Images of New York City: Visualizing Geotagged Archival Photos",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Street-level photographs of New York City from the early 1900s show how people used to live, from their clothes and vehicles to their stores and advertisements. Several open source projects have mapped archival “street view” images of New York, relying on various collections of photos with locations. These interactives, primarily built with Mapbox GL JS, are instructive when visualizing a newly-digitized archive, in this case a set of over 60,000 photos from the construction of the NYC subway between 1900 and 1950 with approximate coordinates.\n\n1. [“Street View, Then & Now: New York City's Fifth Avenue”](http://publicdomain.nypl.org/fifth-avenue) compares 1911 wide-angle photographs from the New York Public Library to 2015 Google Street View imagery. A mini-map shows each photo’s location and field of view, and a visitor to the site can “go south”, “go north”, or “cross the street” using the arrow keys. The project came out of the [NYC Space/Time Directory](http://spacetime.nypl.org), an initiative to communicate the history of the city using historical maps, geodata, and open source tools. Code: https://github.com/nypl-publicdomain/fifth-avenue.\n\n2. [“1940s.NYC”](https://1940s.nyc) places digitized photos of most buildings in the five boroughs of New York City, collected from 1939 to 1941 by the Tax Department with help from the Works Progress Administration, on a map. Zooming in loads georeferenced scans of historical maps, and clicking on a marker opens a panel displaying the historical photos. [“80s.NYC”](http://80s.nyc) remixed the site, using more recent images from the Department of Finance. Code: https://github.com/jboolean/1940s.nyc, https://github.com/bdon/80s.nyc.\n\n3. [“A Stroll Down Flatbush Avenue circa 1914”](https://stroll-down-flatbush.chriswhong.com) strings together 65 photographs, captured approximately every 50 feet, from the [New-York Historical Society’s “Subway construction photograph collection, 1900-1950.”](https://digitalcollections.nyhistory.org/islandora/object/nyhs%3Asubway) Geometries for the photos, which are often set at nearby intersections, were manually modified, and a mini-map showing those points is navigable with the up and down keys. Code: https://github.com/chriswhong/stroll-down-flatbush.\n\nThe entire subway construction photograph collection contains nearly 100 times as many photos as shown along Flatbush Avenue with associated latitudes and longitudes across New York City. What are the best practices for mapping these geotagged archival photos, with imprecise and duplicate locations, as well as rich text metadata like titles, topics, dates, and descriptions?",
        "description": ""
      },
      {
        "title": "Open Source geospatial applications for energy and environment integration",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "The use of GIS to support energy planning is now widespread and well consolidated, as evidenced by the numerous studies available in the international literature. Many companies and governmental institutions have transferred their data and results into open source web platforms or tools for public access.\n\nWithin the broad topic of the interaction between renewable energy and environment, over the last years RSE S.p.A. has faced the necessity to develop and maintain WebGIS and online platforms related to various aspects of the energy system, in order to characterize the territory and its possible influences on renewable energy sources integration in the energy system, thus supporting the decision-making process towards energy transition.\n\nOne of the most significant products is the Integrated Atlas for the National Energy System and Renewable Sources, a WebGIS platform which represents on a national scale significant variables of the energy sector (resources, demand, installed plants, territorial constraints) under a system view, with the principal aim of supporting energy planning. From a technical point of view the Integrated Atlas is developed on TerriaMap, a catalogue-based web geospatial visualisation platform developed by the Australian research centre CSIRO. TerriaMap uses the JavaScript library TerriaJS together with other open source libraries as React JS, Leaflet and Cesium, for 3D visualization.\n\nBesides standard WebGIS functionalities, the Integrated Atlas provides the access to TOTEM (Territory Overview Tool for Energy Models), an advanced open source tool for the energetical characterization of the territory, essential for supporting multi-energy modelling. Starting from spatial and energy data, the TOTEM tool estimates electrical and heat demand, wind and solar resource and other significant energy variables on hourly and provincial scale. Concerning technical details, the tool and its web interface are developed in Python and use libraries such as Pandas, Flask ad Bokeh. The tool is opensource and it will be release under MIT License, however only a portion of the input data are currently publicly available due to data providers’ restrictions.\n\nThe need to harmonize data and analysis about the relations between energy and environment and provide an access point to the developed tools has inspired the creation of the Energy and Environment Geoportal, based on the Mapstore Open Source web platform developed by GeoSolutions. This platform allows viewing and querying geospatial published data, integrating different remote resources into interactive and immediate representations such as maps, dashboard or geostories.\n\nAs an example, detailed results of above cited multi-energy models, which receive spatialized energy data as inputs, have been synthesized in a geostory, an immersive narration which explains how a multi-energy analysis based on detailed territorial characterization of a region can support the evaluation of the best alternatives for its energy development. The geostory is accessible from the Energy and Environment Geoportal.\n\nIt must be specified that these products are still under development and subject to continuous updates of both data and technology. More details about contents and tools are left to the final presentation.",
        "description": ""
      },
      {
        "title": "Developing an UI for historical orthophotos timeseries data",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "In 2020 National Land Survey of Finland had scanned and digitized over 100.000 historical orthophotos from 1931 to 2020. This unique dataset had been open for a couple of years via a WMS-T API service but was not well known to the public at the time, and not truly available to less technically oriented users. Hence, there was a need to get them findable and easily accessible with a web browser. \n\nInstead of building a completely new service, the images were to be published with the Open Source based national geoportal of Finland - Paikkatietoikkuna. The geoportal is built with Oskari Map Application Platform which was enhanced for this use case to support timeseries data. The historical orthophotos are scattered both in time and in geography. To improve end-user experience and make discovering data easier an OGC API Features service was used for metadata.  \n\nThe final result was received with quite a bit of enthusiasm: after publishing the data in Paikkatietoikkuna geoportal in June 2021, the visitor numbers soared to new records. Nearly all of the feedback has been positive, and now it is possible for anybody to benefit from this extremely valuable and fascinating historical data. \n\nThe code is fully open source and can be easily used in any Oskari instance – all you need is the data, which, obviously, isn’t the easiest part of this all.",
        "description": ""
      },
      {
        "title": "European (Inspire) Data Tour",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "This talk provides concrete tips on how to improve your open data accessibility and discovery.  We use real world analysis of what Europe has today, rather than specifications, guidelines, or theory.\n\nWe recently investigated the linkage between Metadata (CSW Dataset and Service Metadata records) and actual downloadable/viewable data (WFS, WMS, WMTS, and Atom).  We also looked at other linkages between the documents (for example, metadata document links, \"operatesOn\" links,  Inspire \"ExtendedCapabilities\", and other MetadataURL links).  \n\nFollowing links isn't as simple as just taking the given URL and resolving it - we will look at \"fixing\" the URL as well as setting request headers.  We will also investigate comparing two different metadata documents (from different URLs) to see if they are \"the same\" even if they aren't really equivalent.\n\nIf you are responsible for an INSPIRE catalogue or web service, attend this talk to learn what works (and does not work) based on real world analysis rather than theory. Or just attend to be sure you did not show up in the examples.",
        "description": ""
      },
      {
        "title": "Using Terraform to manage HOTOSM's infrastructure as code",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "Humanitarian Openstreetmap Team (HOT) administers several free-software applications with varied deployment architectures on multiple cloud platforms. As an organization that values openness and transparency, we actively seek out open source tools that help us enact our principles of open participation and collaboration. In that vein, we chose Terraform as the tool for managing infrastructure at HOT.\n\nUsing HOT's experience managing OSM Galaxy infrastructure using Terraform, this talk describes our use of Terraform to manage infrastructure at scale in order to improve DevOps processes with  infrastructure reproducibility, security, cost and change management. \n\nWe will present these advantages in the context of our own team's experiences and the challenges we faced trying to build a scaling technology stack and compare Terraform with popular Infrastructure as Code (IaC) alternatives.\n\nThe talk will use OSM Galaxy API (galaxy.hotosm.org) as a case study to describe the process of porting infrastructure to Terraform in order to manage infrastructure continuously at enterprise scale - which is particularly relevant for non-profits and organizations that develop compute-intensive technology.",
        "description": ""
      },
      {
        "title": "Curated Major Map Features Library",
        "type": "Lightning talk",
        "track": "Open Data",
        "abstract": "A team of experienced mappers and language experts at Meta has reviewed a dataset of major map features from OpenStreetMap(OSM), and used the curated results on one of their validation processes to check for quality issues of the Daylight and OSM maps. The dataset of the curated results is considered as a reference library of major map features with their key information. During the validation process, this library is used to compare against Daylight and OSM data to look for suspicious changes on features included in the library.  With such reference library, the Meta Basemap Team is able to keep a stable quality on major map features in an efficient manner. To ensure the data in this library is up-to-date and comprehensive, systematic approaches to continuously improve the library are also developed.  In our talk, we will share more details about this curated library and processes, and how we maintain the freshness of the library.",
        "description": ""
      },
      {
        "title": "spatialEpisim: an open-source R Shiny app for tracking COVID-19 in low- and middle-income (LMIC) countries",
        "type": "Lightning talk",
        "track": "Use cases & applications",
        "abstract": "It is essential to understand what future epidemic trends will be, as well as the effectiveness and potential impact of public health intervention measures. The goal of this research is to provide insight that would support public health officials towards informed, data-driven decision making. We present spatialEpisim, an R Shiny app (https://github.com/ashokkrish/spatialEpisim) that integrates mathematical modelling and open-source tools for tracking the spatial spread of COVID-19 in low- and middle-income (LMIC) countries.\n\nWe present spatial compartmental models of epidemiology (ex: SEIR, SEIRD, SVEIRD) to capture the transmission dynamics of the spread of COVID-19. Our interactive app can be used to output and visualize how COVID-19 spreads across a large geographical area. The rate of spread of the disease is influenced by changing the model parameters and human mobility patterns.\n\nFirst, we run the spatial simulations under the worst-case scenario, in which there are no major public health interventions. Next, we account for mitigation efforts including strict mask wearing and social distancing mandates, targeted lockdowns, and widespread vaccine rollout to vaccinate priority groups.\n\nAs a test case Nigeria is selected and the projected number of newly infected and death cases are estimated and presented. Projections for disease prevalence with and without mitigation efforts are presented via time-series graphs for the epidemic compartments.\n\nPredicting the transmission dynamics of COVID-19 is challenging and comes with a lot of uncertainty. In this research we seek primarily to clarify mathematical ideas, rather than to offer definitive medical answers. Our analyses may shed light more broadly on how COVID-19 spreads in a large geographical area with places where no empirical data is recorded or observed.",
        "description": ""
      },
      {
        "title": "Development of a disaster information collection system using smartphones and cloud computing",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "When natural disasters such as floods and earthquakes occur, much work is required for recovery and reconstruction. Depending on the scale of the disaster, it is often not enough for the government and local governments to do the work alone. Disaster relief through volunteer activities by many individuals and organizations is also necessary.\n\nIn this kind of work with diverse participants, it is very important to collect and analyze information, plan the work, and divide the work among the various members.\n\nIt would be very beneficial to have computer technology available to survey disaster situations and collect, consolidate, and share data\n\nIt would be useful to be able to use the smartphones that many people own for information gathering and sharing mechanisms.\n\nJapan's Coordinating Organization for Volunteer Organizations in Disasters\n(Japan Voluntary Organizations Active in Disaster) and its component organization, the IT Disaster Assistance and Response Team, developed a smartphone-based disaster information collection system in 2019 that has been used to collect and use information at many flood sites.\n\nThis system transmits disaster information collected by smartphones and consolidates it into a Google Spread Sheet.\n\nAn API has also been created to distribute information collected from Google Spread Sheet in GeoJSON format.Using this API, it is also possible to use disaster information on web maps and QGIS.\n\nIn this presentation, I will describe the status of the creation and use of this system.",
        "description": ""
      },
      {
        "title": "If a tree falls in the forest, does the river notice?: Creating and Visualizing Equivalent Clearcut Area using Satellite Imagery, Python and Crossfilters",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "Forest disturbance can have a significant impact on the hydrologic regime and health of watersheds, aquatic habitats, and their overall ecological functions. Although these impacts can vary as a result of physical and hydroclimatic conditions in watersheds, over the past decades a simple metric known as equivalent clearcut area (ECA) has emerged to quantify the cumulative disturbance at any point in time in a watershed, accounting for the temporal dynamics, including recovery, of historical disturbance. \nECA is widely used as an indicator to quantify forest disturbances as it not only covers all disturbance types but also considers the subsequent recovery of these disturbed areas through space and time. An ECA coefficient of 100% means there is no hydrological recovery due to planting trees, on the other hand, an ECA of 0% means 100% of the disturbed area has reached its maximum potential in recovery. \nThe prime objective of this project was to generate an annual time series of ECA for every watershed within the area covered by the Nadina Natural Resource District in British Columbia, Canada. The disturbance types identified in this project were forest fire, timber harvest, pest infestation, and permanent infrastructure development. The time and location of these disturbances were integrated with the BC Freshwater Atlas, in order to provide quantitative estimates of ECA for the watershed associated with stream reaches in the study area. \nDue to the cumulative nature of ECA over space and time, all types of forest disturbances have to be combined and recovery factors must be applied to account for vegetation and hydrological recovery over time. Previous field research was used to find the relationship between the tree stand height and the age following logging. \nIn order to apply the methodology, open source data and tools were used. We used two publicly available global satellite image derived raster products of forest change for our initial disturbance data source. We updated the forest change raster products using GDAL with local public geospatial datasets to assign change types for each year from 1985-2020, creating a unified multiband raster of change. The multiband raster was processed within Python, using GDAL and NumPy libraries, and recovery factors were applied to each pixel dynamically based on the year and type of change. The result of this processing was a collection of time series for watersheds broken down by year and type of change present. \nThe ECA results for the study area were loaded into PostGreSQL for delivery to a web application that allows a user to compare a location’s ECA with the forest disturbance input. The application relies on Mapbox GL to interact with a web map and the spatial data display of the forest disturbance locations. D3 and Crossfilter JS libraries were used to interact with the disturbance data, allowing a user to filter histograms based on a variety of attributes, which are reflected in the histograms and web map in real time. ECA time series data is displayed in interactive histograms and line charts.",
        "description": ""
      },
      {
        "title": "The Potent Mix of Computer Vision, Graph Theory, Satellite Imagery, Vehicles, and Roads",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "We discuss the \"Diet Hadrade\" codebases, which provides an open-source, lightweight mechanism for leveraging remote sensing imagery and machine learning techniques to aid in humanitarian assistance and disaster response (HADR) in austere environments. \n\nIn a disaster scenario (be it an earthquake or an invasion) where communications are unreliable, overhead imagery often provides the first glimpse into what is happening on the ground. The rapid identification of both vehicles and road networks directly from overhead imagery allows a host of problems to be tackled, such as congestion mitigation, optimized logistics, evacuation routing, etc.  Such challenges often arise in the aftermath of natural disasters, but are also present in crises like the current invasion of Ukraine where roads are choked with civilians fleeing the fighting.\n\nAutomobiles provide an attactive proxy for human popuplation due to their mobile nature and the necessity of population movement in many disaster scenarios.  In this project, we deploy the YOLTv5 computer vision object detection codebase to rapidly identify and geolocate vehicles over large areas. Vehicle detections yield significantly greater utility when combined with road network data. We use the CRESI computer vision framework to extract up-to-date road networks with travel time estimates, thus permitting optimized routing. The CRESI codebase is able to extract roads using only overhead imagery, so flooded areas or obstructed roadways will sever the CRESI road graph; this is crucial for post-disaster scenarios where existing road maps may be out of date and the route suggested by cloud navigation services may be impassable or hazardous.\n\nDiet Hadrade provides a number of graph theory analytics that combine the CRESI road graph with YOLTv5 locations of vehicles. We combine the car detections with the road network to infer how congested certain areas are. Congestion information is important for everyday life, but also crucially important in disaster response scenarios when roads may become impassable due to both natural phenomena as well as traffic.  \n\nWe leverage the detailed road graph and vehicle location information to illustrate a number of scenarios, such as: bulk evacutation, optimal aid disbursement locations, critical intersections, and detection and automated avoidance of dangerous locales.  These capabilities are presented in an interactive dashboard that computes optimal routes on the fly based on user inputs.",
        "description": ""
      },
      {
        "title": "YouTube as Remote Sensing",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "No elevation data exists of Molokini Crater, an island off the coast of Maui that gets 300,000 visitors annually. To make my own, I created a photogrammetric model using a ten year old YouTube video fed into an open source photogrammetry package. I exported the resulting mesh as a point cloud and filtered it using opensource LiDAR program CloudCompare and exported it as a raster. I then georeferenced the raster and scaled the model vertically with QGIS. The National Geodetic Survey reports that my model, down sampled to 1m resolution and made with OSS, is the most comprehensive survey of Molokini Crater in existence, and it was done in less than a day with a zero dollar budget. Photogrammetry is how Google Maps produces it's 3D function and it's an old technique but a proliferation of tourist and drone videos could open up new avenues for creating maps of small extents. This process is easily automated and it's only a matter of time before photogrammetry software is capable of integrating images from different times of day which massively increases the amount of usable media for modeling. This could be the beginning of internet-archived archaeology where environments captured as video can be reconstructed and viewed in a new way.",
        "description": ""
      },
      {
        "title": "rassta: Raster-based Spatial Stratification Algorithms",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Spatial stratification of landscapes allows for the development of efficient sampling surveys, the inclusion of domain knowledge in data-driven modeling frameworks, and the production of information relating the spatial variability of response phenomena to that of landscape processes. This work presents the rassta package as a collection of algorithms for spatial stratification developed in the R environment. The core ideas implemented in the rassta package include the multi-scale, hierarchical landscape stratification based on spatial intersection, the application of non-parametric distribution estimators to define the typical landscape configuration of stratification units, and the use of spatially explicit landscape correspondence metrics for non-probability sampling and predictive modeling. The theoretical background of rassta is presented through references to several studies which have benefited from landscape stratification routines. The functionality of rassta is presented through code examples which are complemented with the geographic visualization of their outputs. Moreover, domain-specific applications are presented to demonstrate the applicability of rassta for the spatial modeling of diverse environmental phenomena.",
        "description": ""
      },
      {
        "title": "Geo Engine: Exploratory data analysis with spatio-temporal workflow processing",
        "type": "General session talk",
        "track": "State of software",
        "abstract": "Geo Engine is a cloud-ready geospatial analysis platform that provides intuitive and low-threshold access to geospatial data, its processing, interfaces, and visualization. Users can access the engine in a browser-based user interface as well as with Jupyter notebooks in Python. An important element is the homogenized \"Datacube\"-like view of heterogeneous data, which allows research groups and companies easy access and low-threshold analyses. At the same time, it is a framework for the creation and operation of geodata portals.\n\nThe development is based on research results from the field of spatio-temporal data processing from the database systems group at the University of Marburg, Germany. It is currently used in scientific projects focused on environmental and biodiversity monitoring, where it provides native time series processing, the combination of raster and vector data, and a user interface that enables linked views between maps, tables, and plots. In addition, it is used for the provision of customized apps, for example for web-based remote-sensing learning and project portals.\n\nThe presentation gives an overview of the system and its features. The processing backend will be discussed, which allows tile-based (for raster data) or chunk-based (for vector data) processing, taking into account the time semantics of the data. In addition, we show a use case demonstration where we exemplify the seamless transition from Geo Engine’s UI to Python notebooks and also the step back. Finally, we give an overview of future development goals.",
        "description": ""
      },
      {
        "title": "SagtaMapDownloader - A guide on how QGIS and QGIS Server is being used to assist teachers and students in learning Geography in the classroom",
        "type": "General session talk",
        "track": "Education",
        "abstract": "The SagtaMapdownloader is an online tool that allows teachers to teach GIS concepts that are part of the curriculum in South Africa.  This tool bridges the gap for students and teachers in their journey to become GIS specialists.  This session will walk users in exploring how QGIS and QGIS Server are used to give an in-depth understanding of all map work that students are tested for during national exams. The map downloader allows geography students to explore and interact with different types of maps namely hybrid, topographic and orthophoto maps.  Other tools that are available for exploration include the ability to print maps with magnetic declination for each map series, the ability to create profiles using elevation data available on the system. The session will walk users in the architecture behind the map downloader and how it will be improved in future iteration as the needs of teacher/students changes",
        "description": ""
      },
      {
        "title": "WebAssembly4G: Where we are, and where we're heading",
        "type": "General session talk",
        "track": "Use cases & applications",
        "abstract": "WebAssembly's adoption is gaining traction and still, its potential is not yet fully utilized, especially for the processing and visualization of geo data in and outside of browsers. In this session I will give a technical introduction to WebAssembly. I will show its current state and adaptation in FOSS4G projects and will talk about the ongoing advancements of the technology and possible future scenarios.\n\nThis will also be a hands-on session, where after showing how to get up and running, I will share my experience, tips and tricks collected while porting the latest versions of GEOS, PROJ, GDAL, SpatiaLite and osgEarth to the web platform. \n\nThe composition of existing OSGeo/FOSS C/C++ libraries in a portable and sandboxed form also brings many advantages outside of browsers. The talk will close with some demos about how WebAssembly enables us to build for the web, as well as for any other platform.",
        "description": ""
      },
      {
        "title": "Cloud Native Geospatial: Lessons Learnt Building Digital Earth Africa",
        "type": "General session talk",
        "track": "Open Data",
        "abstract": "Digital Earth Africa (DE Africa) is an operational platform with a mission to produce decision-ready products and to harness and increase the capacity of Earth observation users across the African continent. DE Africa’s mission is supported by a platform that involves delivery of data and services hosted in the public cloud. More than three Petabytes of Earth observation (EO) data covering the African continent are routinely updated and made available for free.\n \nThis talk will explore how cloud native geospatial technologies, such as the Cloud Optimised GeoTIFF data storage format and Spatio-Temporal Asset Catalog metadata standard, enable us to more easily organise, share and analyse these petabytes of data. We’ll discuss how we work with the global EO community to develop standards that enable federation and interoperability. And we’ll demonstrate how DE Africa has been able to to build capacity across Africa, from enabling individuals to run scientific analyses through to assisting national space agencies set up their own platform and supporting industry to deliver innovative products on top of our service.\n\nWe will conclude with covering key lessons learnt in building the DE Africa platform, and look to the future, as we transition to African-led operations. The authors would like to acknowledge the support of the Digital Earth Australia team, international partners and many individuals who have helped the platform become realised.",
        "description": ""
      },
      {
        "title": "Community sprint",
        "type": "Full day",
        "track": "State of software",
        "abstract": "Also this year, after the online conference, there will be the OSGeo community sprint.\n\nParticipation is free of charge and anyone involved or interested in getting involved in OSGeo community projects is welcome.\n\nThis year we will try to offer the format an hour with the developer, where we ask seasoned community members to donate time for welcoming new members and introduce them to different projects or guide them through the setup of the development environment or translation tools and get started on the Foss4G journey.\n\nYou can choose to contribute to one or more projects. The sky is the limit. There’s always plenty to do – and it’s not all about programming. Translation, documentation, feedback, discussions, and testing are very important for projects! Conference registration is not a prerequisite for participation in the code sprint. \n\nPlease register [here](https://wiki.osgeo.org/wiki/FOSS4G_2022/Community_sprint)",
        "description": ""
      },
      {
        "title": "Community sprint",
        "type": "Full day",
        "track": "Unknown",
        "abstract": "Also this year, after the online conference, there will be the OSGeo community sprint.\n\nParticipation is free of charge and anyone involved or interested in getting involved in OSGeo community projects is welcome.\n\nThis year we will try to offer the format an hour with the developer, where we ask seasoned community members to donate time for welcoming new members and introduce them to different projects or guide them through the setup of the development environment or translation tools and get started on the Foss4G journey.\n\nYou can choose to contribute to one or more projects. The sky is the limit. There’s always plenty to do – and it’s not all about programming. Translation, documentation, feedback, discussions, and testing are very important for projects! Conference registration is not a prerequisite for participation in the code sprint. \n\nPlease register [here](https://wiki.osgeo.org/wiki/FOSS4G_2022/Community_sprint)",
        "description": ""
      }
    ]
  },
  {
    "year": 2023,
    "conference_title": "FOSS4G 2023",
    "start_date": "2023-06-26",
    "end_date": "2023-07-02",
    "total_events": 296,
    "total_presentations": 260,
    "academic_presentations": 36,
    "non_academic_presentations": 260,
    "types": {
      "Talk": 226,
      "Lighting talk": 24,
      "Keynote": 7,
      "Side event": 2,
      "Side event long": 1
    },
    "tracks": {
      "Use cases & applications": 95,
      "State of software": 76,
      "Open Data": 33,
      "Community & Foundation": 12,
      "Open source geospatial ‘Made in Europe’": 12,
      "Unknown": 9,
      "Open Standard": 7,
      "Transition to FOSS4G": 6,
      "Education": 6,
      "AI4EO Challenges & Opportunities": 4
    },
    "presentations": [
      {
        "title": "Open Source Geospatial Tools for Humanitarian Response - HOTOSM",
        "type": "Side event",
        "track": "Community & Foundation",
        "abstract": "Open source mapping: Connect and work with the Tech & Innovation team at Humanitarian OpenStreetMapTeam (HOT)\n\nCome and meet the Tech & Innovation team at the Humanitarian OpenStreetMap Team (HOT), hear about the status of the existing and experimental open mapping tools, and explore how you can get involved!\n\nIn this side event, you will hear about the latest open source technology developments ranging from remote mapping using HOT Tasking Manager, field mapping using the Field Mapping Tasking Manager (FMTM) to uploading imagery using Open Aerial Map, exporting OSM data using the HOT Export Tool and AI-assisted mapping with one of the most recent projects we are working on - fAIr.\n\nWe want to connect with people who are working on anything from imagery to data visualization and provide a space to get hands-on and deep-dive into the various stages of the open mapping workflow. Participants may choose any workflow they would like to focus on, and in groups, deep dive into the topic of interest. We are looking for YOUR contributions. Whatever your skills/interest might be, there will be something for YOU to input and collaborate on with our team!\n\nRegistration: Anyone is welcome to drop in the session! \nThis event will be held on June 27 from 15:00. Please register here - https://docs.google.com/forms/d/e/1FAIpQLSdAYmcWCdZ2X7Oorx1pj-l698w4jBv6gHhV3O-cRtvqT9G1Tw/viewform - and tell us what you are most interested in working on during the session. We will reach out to you before the side event. \nLook forward to meeting you in person!\n\nWhatever your technical background is, we will find an area that matches your interest! Take a look at the HOTOSM Github repos: https://github.com/hotosm",
        "description": ""
      },
      {
        "title": "QGIS - Ask me anything!",
        "type": "Side event",
        "track": "Community & Foundation",
        "abstract": "QGIS Chairman Marco Bernasocchi and core developer Matthias Kuhn will be available for an hour to answer any QGIS-related questions. With the two of them, interested parties have access to over 20 years of combined expert knowledge in the development, use and organisation of QGIS and QGIS-based products. Questions about specific use cases, upcoming developments or the functioning of the QGIS project with its international contributors will be tackled.",
        "description": ""
      },
      {
        "title": "Opening session",
        "type": "Talk",
        "track": "Unknown",
        "abstract": "Opening session with institutional greetings.",
        "description": ""
      },
      {
        "title": "The Importance of Seeding - from 3 ECTS to Shaping a better world",
        "type": "Keynote",
        "track": "Unknown",
        "abstract": "In this keynote, we will explore the significance of seeding in the context of open-source software. Using QField as an example, we will explore the steps needed to turn a student's project into the leading fieldwork app that helps hundreds of thousands of people with their work and can help address many of the Sustainable Development Goals.\n\nWe will discuss the challenges faced during the initial stages of development and what steps played a crucial role in overcoming them. We will also highlight the importance of community and industry involvement and how these helped QField reach global success and over 800K downloads.\n\nThrough this keynote, attendees will gain insights into the role of seeding and commitment in developing and growing open-source software, highlighting its impact on innovation, collaboration, and sustainability.\n\nJoin us for an insightful discussion on planting seeds and the potential to drive positive change through open-source software.",
        "description": ""
      },
      {
        "title": "QGIS.org - The vision and mission for the next 20 years of QGIS awesomeness",
        "type": "Talk",
        "track": "State of software",
        "abstract": "QGIS turned 20 years old last year. The first lines of code were written in mid-February 2002 and when the programme was first compiled and run, it could do precisely one thing:\nConnect to a PostGIS database and draw a vector layer.\n\nNowadays, QGIS is the go-to GIS solution for millions of users, and to make sure that QGIS's future is as bright as its past, we did a lot of work on communication, strategy and outreach.\nIn this talk, I’ll overview all the work done, the current status and the future of QGIS and its community.",
        "description": ""
      },
      {
        "title": "State of GRASS GIS",
        "type": "Talk",
        "track": "State of software",
        "abstract": "This talk gives an overview of the current state of the GRASS GIS project for both users and developers. Latest version of GRASS includes even more tools parallelized using OpenMP to speed up massive data processing. The graphical user interface is changing as the single-window layout matured and is becoming the number one choice and a default setting. This adds to a quicker startup without a need for a welcome screen and streamlined data management. The code quality of C and C++ code improved significantly in the last year, the code compiles with strict compiler settings and we are heading towards pedantic compliance. Last but not least, this summer GRASS GIS celebrates its 40th birthday!",
        "description": ""
      },
      {
        "title": "State of GDAL (versions 3.6 and 3.7)",
        "type": "Talk",
        "track": "State of software",
        "abstract": "We will give a status report on the GDAL software, focusing on recent developments and achievements in the 3.6 and 3.7 GDAL versions released during the last year, but also on the general health of the project.\nThe discussed topics will be as various as the scope of GDAL is, covering the new single CMake build system, the full open source write vector support for the Esri FileGeodatabase format, a Arrow-based columnar oriented read API for vector layers implement in the Arrow, (Geo)Parquet, GeoPackage and FlatGeoBuf drivers, new vector layer API for table relationsihp management, new raster drivers for the JPEG-XL, KTX2, BASISU, NSIDCbin formats, multi-threaded read capabilities in the GeoTIFF driver, multiple performance improvements in the GeoPackage driver, advanced API to read raster compressed data, a new vector driver for the General Transit Feed Specification (GTFS),  support for the new Seek Optimized ZIP (SOZip) specification, etc.",
        "description": ""
      },
      {
        "title": "Demystifying Re:Earth: A Technical Examination of Nocode WebGIS Platform",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Join us for an in-depth exploration of the technological foundations of Re:Earth, Eukarya Inc.'s open-source WebGIS platform. This 30-minute session will provide a comprehensive analysis of the underlying mechanisms that empower Re:Earth's no-code, user-friendly interface. We'll dissect the core architecture, illustrate its data handling and visualization processes, and elucidate the robust framework that facilitates plug-in development. Aimed at both technology professionals and enthusiasts, this talk offers a rigorous, detailed insight into the groundbreaking engineering that positions Re:Earth at the forefront of geospatial data interaction.",
        "description": ""
      },
      {
        "title": "State of GeoNetwork",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The GeoNetwork-opensource project is a catalog application facilitating the discovery of resources within any local, regional, national or global \"Spatial Data Infrastructure\" (SDI). GeoNetwork is an established technology - recognized as an OSGeo Project and a member of the foss4g community for over a decade. \n\nThe GeoNetwork team would love to share what we have been up to in 2023! \n\nThe GeoNetwork team is excited to talk about the different projects that have contributed with the new features added to the software during the last twelve months. Our rich ecosystem of schema plugins continues to improve; with national teams pouring fixes, improvements and new features into the core application.\n\nWe will also talk about the UI revamp through the geonetwork-ui framework, and the new perspectives it could bring to your catalogs. Progress of our main branches (4.2.x), and release schedule.\n\nAttend this presentation for the latest from the GeoNetwork community and this vibrant technology platform.",
        "description": ""
      },
      {
        "title": "State of GeoServer",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GeoServer is a web service for publishing your geospatial data using industry standards for vector, raster and mapping. Choose additional extensions to process data (either in batch or on the fly) and catalog records.\n\nGeoServer is widely used by organizations throughout the world to manage, disseminate and analyze data at scale. GeoServer web services power a number of open source projects like GeoNode and geOrchestra.\n\nThis presentation provides an update on our community as well as reviews of the new and noteworthy features for the latest releases. In particular, we will showcase new features landed in 2.22 and 2.23, as well as a preview of what we have in store for 2.24 (to be released in September 2023).\n\nAttend this talk for a cheerful update on what is happening with this popular OSGeo project, whether you are an expert user, a developer, or simply curious what GeoServer can do for you.",
        "description": ""
      },
      {
        "title": "State of GeoNode",
        "type": "Talk",
        "track": "State of software",
        "abstract": "This presentation will introduce the attendees to those which are GeoNode's current capabilities and to some practical use cases of particular interest in order to also highlight the possibility of customization and integration. We will provide a summary of new features added to GeoNode in the last release together with a glimpse of what we have planned for next year and beyond, straight from the core developers.",
        "description": ""
      },
      {
        "title": "State of pgRouting",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pgRouting is evolving rapidly, many changes have been taking place. Lets catch on.\n\nThe focus of this talk will be on the **topology** functions that were created on 2013, Its been 10 years, and its their time to go:\n* Why \"I\" don't want to use them any more\n* New specialized functionality has been created that substitute the work that the topology functions are doing in a very rustic way.\n* A quick guide on how not to use the \"soon to be deprecated topology functions\"",
        "description": ""
      },
      {
        "title": "Upgrade your Postgres and PostGIS will thank you",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Every year, there's a new Postgres major release that improves on performance in certain areas and could provide new hooks for extensions like PostGIS to take advantage from them. If not planned well, upgrading your production databases can become a pain. Sooner than you think you'll be running on EOL (End-of-Life) versions because the upgrade has been postponed too many times. Don't!\n\nDid you know Postgres upgrades can be greatly automatized these days with downtimes of only a few seconds? This talk will show you how and will also present some essential features from recent Postgres and PostGIS versions to get you excited for the new upgrade.",
        "description": ""
      },
      {
        "title": "Open-Source Solutions: Expanding our Humanity with Data Stories",
        "type": "Keynote",
        "track": "Open Data",
        "abstract": "Geospatial analysis welcomes an audience to interact with complex interactions and dynamic shifts in ecosystem balance. Location intelligence collected as data layers mirror a symphony or chapters in a book. We will explore the potential risks of vulnerable cities by exploring the environment, economics, built infrastructure, and how they intersect. We build the story or music over time while exploring the tensions we create. Let’s examine the edges of eco-geomorphic frameworks and listen for a narrative.",
        "description": ""
      },
      {
        "title": "State of STAC",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The SpatioTemporal Asset Catalog (STAC) specifications are a flexible language for describing geospatial information across domains and for a variety of use cases. This talk will present the current state of the specifications, which includes the core STAC specification and the API specification built on top of OGC APIs. While the core specification has been stable for roughly two years and doesn't need a lot of updates, the API specification got numerous updates and is finally close to a stable release. This presentation digs into additions to STAC extensions and the latest community developments. We survey the updates to the open-source STAC ecosystem, which includes software written in Python, Node.js, and more. Finally, let's also look into the near future.",
        "description": ""
      },
      {
        "title": "Standardized Data Management with STAC",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "STAC is a well-known and acknowledged spatiotemporal metadata standard within the community. There are many applications with open-source data; however, there are few adoptions by premium satellite imagery providers. At UP42, we adopted STAC as the core metadata system within our applications and provided STAC API for users to manage their data easily. The ongoing adoption challenges with multiple data providers taught many takeaways that we would like to share with the community.\n\n- UP42: a short introduction \n- Data management challenges at UP42\n- Solution with STAC & cloud-native asset format\n- STAC implementation: lessons learned \n- Current state and way forward",
        "description": ""
      },
      {
        "title": "The STAC JavaScript Ecosystem + CNG Excursion",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The SpatioTemporal Asset Catalog (STAC) (and Cloud Native Geospatial ecosystem) for/in JavaScript has evolved in the last year. This talk will update you on the current state of the ecosystem and gives an outlook on what is missing. For STAC talk will cover libraries such as stac-js, stac-layer, stac-browser, stac-node-validator, and more. We'll dive into what the libraries do, how they relate to each other and give you some hints how you get started. At the end, a short excursion into the cloud-native geospatial ecosystem in JavaScript for COG, geoparquet, geozarr and other file formats will be provided as well.",
        "description": ""
      },
      {
        "title": "Earth-Search: A STAC API of Open datasets on AWS",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Earth-Search is a publicly available SpatioTemporal Asset Catalog (STAC) API providing an index for some of the public datasets available through the AWS Registry of Open Data (RODA) and has been shown to be a valuable resource for accessing the Sentinel-2 archive as Cloud-Optimized GeoTIFFs. A new version of Earth-Search is an update and enhancement of the Sentinel-2 metadata as well as new Collections of data available on AWS, including Landsat Collection 2, NAIP, and Sentinel-1.\n\nThis talk will include a summary of the STAC catalog, what STAC extensions are used and how the data is best accessed based on file formats. We will also dive into the datasets that are available through the API and will present the architecture for indexing including a discussion of data latency. We will provide resources and tutorials for how to get started with public geospatial datasets on AWS.",
        "description": ""
      },
      {
        "title": "GeoMapFish Project Status",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GeoMapFish is an open source WebGIS platform developed in close collaboration with a large user group. It targets a variety of uses in public administrations and private groups, including data publication, geomarketing and facility management. OpenLayers and an OGC architecture allow the use of different cartographic engines: MapServer, QGIS Server, GeoServer. Recently new features have been added such as vector tiles integration, from raw data to visualization. In order to get rid of the AngularJS dependency, a roadmap has been established for a migration to a web components architecture. K8S support is evolving with the implementation of the necessary tools for Azure environments. A highly integrated platform, a large number of features, fine grained security and a mature reporting engine are characteristics of the GeoMapFish solution. In this talk, we will present the key usages, the state of the migration process to web components and latest functional developments, including backend - frontend decoupling allowing to plug in multiple front-end WebGIS clients.",
        "description": ""
      },
      {
        "title": "Valhalla Routing Engine",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Valhalla proved, since its inception in 2015, to be a valuable part of the OSM software universe, occupying an important niche in the routing section. It's arguably one of the most feature-rich open-source routing engines, serving many different use cases and integrations/deployments.\n\nHowever it's a fairly complex system which is hard to comprehensibly document and new users or developers are often overwhelmed. So, I'd like to introduce its general architecture, capabilities and showcase \"new\" features (the last talk was given in 2016 on FOSS4G NA), as well as the accompanying open-source software, like various libraries, clients and docker image(s).",
        "description": ""
      },
      {
        "title": "State of GeoRasterLayer (for Leaflet)",
        "type": "Talk",
        "track": "State of software",
        "abstract": "We will discuss the state of [GeoRasterLayer](https://github.com/geotiff/georaster-layer-for-leaflet), a JavaScript library that renders GeoTIFFs directly on a LeafletJS map without a server.  This will include an introduction of new features, including the following:\n- shifting warping off the main thread to a pool of web workers\n- improved support for extent calculations by increasing vertex density of polygon representations of bounding boxes\n- high-resolution support by using [geowarp](https://github.com/danieljdufour/geowarp)\n\nWe will also look to the future and discuss the following:\n- support for raster types other than GeoTIFF/COG\n- geozarr support\n- similar integrations into other web mapping libraries\n\nAudience feedback and ideas will be most welcome!",
        "description": ""
      },
      {
        "title": "Get most out of STAC Browser",
        "type": "Talk",
        "track": "State of software",
        "abstract": "STAC Browser is a full-fledged web interface for browsing and searching static STAC catalogs and STAC APIs. It has been rewritten from scratch with a lot of new functionality. This talk will introduce STAC Browser, showcase new functionality and uncover some unexpected gems such as the broad range of customization possibilities. Lastly, the presentation will guide you through a set of best practices for your static STAC catalog or STAC API so that you get the most out of STAC Browser with regards to functionality and user experience.",
        "description": ""
      },
      {
        "title": "An overview of Cloud-Native Geospatial",
        "type": "Talk",
        "track": "State of software",
        "abstract": "“Cloud-Native Geospatial” is a new paradigm for performing efficient data access and compute the cloud in an interoperable way in order to achieve scalable and repeatable analysis of geospatial data. The last few years have seen major developments in open standards and open software that make this possible, supporting full end to end interoperable workflows on remote sensing data, starting from data discovery to publishing of derived products.\n\nThis talk will provide an overview of what Cloud-Native geospatial is and why it is important for building scalable architectures. It will cover the current state of the Spatio Temporal Asset Catalog (STAC) specifications, and the landscape of cloud-optimized file formats, for raster, vector, and point-cloud data formats (COG, GeoZarr, GeoParquet, COPC).",
        "description": ""
      },
      {
        "title": "Digitizing the french railway network - an open source endeavour",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "From detecting vegetation hazard to measuring catenary geometry, it all begins with **three trains equipped with LiDaR** mapping systems, roaming the french railway network.\n\nLet us see how **geospatial opensource softwares** enabled us to build a cost-effective and comprehensive solution, starting from basic raw data processing up to setting up a geographic information system, full of relevant data that **brings up railway maintenance to another level**.",
        "description": ""
      },
      {
        "title": "Implementation of Statistical Geoportals in Latin America and the Caribbean",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Based on the implementation of the Global Statistical and Geospatial Framework (GSGF) proposed by the UN and implemented in Latin America and the Caribbean by the Economic Commission for Latin America and the Caribbean (ECLAC), a set of specific technological components were developed, such as a geoportal, a statistical manager and an API with the possibility of consuming information from different applications. At the same time, components already existing in the community were implemented such as Kobo Toolbox, GeoNode, Airflow, MapLibre, Nominatim and Metabase for the integration of information from the collection in the territory to the publication of the data. The project was initially carried out with a group of countries: Argentina, Paraguay, Honduras, Guatemala, Dominican Republic, Costa Rica and Ecuador.",
        "description": ""
      },
      {
        "title": "ESA-NASA-OGC Open Science Persistent Demonstrator",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Open Science Persistent Demonstrator (OSPD) is a long-term inter-agency initiative aiming to enable and communicate reproducible Earth Science across global communities of users and amplify inter-agency Earth Observation mission data, tools, and infrastructures. This talk will highlight the status and roadmap of the initiative (kicked off in 2023) and will provide an outlook on the first pilot activities of the demonstrator, as well as opportunities for participation for the FOSS4G community. \nIn the scope of this activity, ESA, NASA and OGC work together on the development of a long-term Open Science framework (e.g., a permanent open science demonstrator) in which participating organisations provide data, tools, and infrastructure in a coordinated approach, building on existing investments where appropriate. \nIn the frame of this activity, the OGC supports the Open-Source and Open Science Community by developing a persistent demonstrator that makes Open Science more tangible to a bigger audience, helps in exploring new forms of communication of scientific results to stakeholders, and helps develop the necessary standards to ensure the highest levels of interoperability across participating organizations. At the same time, it makes Earth Observation results available to other disciplines and communities, creates attention beyond the Earth Observation community, and directly impacts decision makers and political agendas.\nThe goal here is to demonstrate interoperable, collaborative research that allows reuse of existing components. These other resources are either offered as part of emerging Open Science Environments or in the form of either directly accessible “cloud-native” data/functions or by means of Web APIs. To reach this goal, it is essential to empower communities of practice to share FAIR (Findable, Accessible, Interoperable, Reusable) descriptions of their resources and capabilities. To allow this system to scale, it is crucial to avoid infinite combinations of community and application specific metadata, functions, data and products.  \nOne focus is the facilitation of direct participation of the scientific community as the primary users of this framework, and of the open-source for geospatial community as essential contributors to the activity. To handle modelling complexity, OGC, NASA and ESA will define manageable processes and best practices for communities conducting geoscience research in multiple domains using heterogeneous data and tools on a distributed infrastructure. These agreements will include, but not limited to, standards, vocabularies, and ontologies for data and workflows and develop community-wide open source science mechanisms, modeling considerations and design patterns.",
        "description": ""
      },
      {
        "title": "Open Data Analytics API in GeoNetwork",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "In the OGC world, you have a catalog to look for metadata/datasets, and the OGC API Features to fetch the data, paginate, filter and so on.\nThe use cases have evolved since then and data consumers expect more complete abilities from their data catalogs. Nowadays we want to analyze, understand and reuse our datasets and providing such tools is a great way to encourage data owners to share and open their warehouse. A data API could then offer:\nFull text search on data points\nData fetching, paging, sorting and filtering\nData analytics, aggregation, computation\nData joining\nAnd those operations should perform in an optimized and scalable manner.\nIt's what GeoNetwork has offered for decades now, and GeoNetwork is taking the move to opendata to address all those use cases.\n\nYou might have heard about columnar formats, and columnar vector formats such as Arrow, Parquet… After an introduction of the context and the expectation of a well shaped data API, we’ll present different approaches and types of flow architectures\n- Warehouse formats\n  - Static files (parquet)\n  - Index\n  - Databases (PostGIS, Cytus)\n- Api models and implementation\n  - OGC API Features limitation\n  - Duck DB\n  - Pure SQL\nAnd compare the different stack in terms of efficiency depending on various use cases.\n\nThe final goal is to provide an API which serves search, analytics and dataviz purposes.",
        "description": ""
      },
      {
        "title": "Redmine Geo-Task-Tracker (GTT) Plugin",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Redmine Geo-Task-Tracker (GTT) Plugin provides geospatial support for Redmine. Redmine is a well-known OSS issue management system. GTT Plugin enables to attach geospatial information(Point, Polyline and Polygon) to each issues. It is effective in management many issues based on geospatial infromation(ex. Road and park management). This talk introduces features and some use cases.",
        "description": ""
      },
      {
        "title": "MapComponents for your React application",
        "type": "Talk",
        "track": "State of software",
        "abstract": "MapComonents is an open-source framework extending React for mapping applications. It can be used to develop browser-based applications that do not require any backend, as well as web clients that use an arbitrary number of backend services. MapComponents uses MapLibre for rendering, raster, and vector tiles. \n\nIt provides working defaults wherever possible enabling the usage with minimal parameters. At the same time, it exposes the entire MapLibre API allowing very granular control of the result where it is needed. Solutions for more complex and common requirements such as PDF export, a feature editor, layer tree, WMS loader, measure tools, or bookmarks are provided as ready-to-use and highly configurable drop-in components. Exotic requirements include the swipe tool, the magnifying glass that partially shows two synchronized MapLibre instances or components that render 3D meshes or deck.gl.\n\nLayers on the map are covered by several components and example codes in our lab repository. It can be combined with a backend for managing a more extensive data set. In addition, it also works as a progressive web app offline with most functions. Creating dashboards and complex user interfaces that combine maps and diagrams MapComponents is more straightforward than traditional approaches, given the declarative nature of React and its vast ecosystem of existing components. \n\nThe presentation will show and explain an actual example and its function. MapComponents framework is available under the MIT license and developed by WhereGroup GmbH.",
        "description": ""
      },
      {
        "title": "GeoNetwork Orientation and Demo",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "Welcome to GeoNetwork and FOSS4G! GeoNetwork is a leading open-source web catalog for keeping track of the spatial information.\n\nThis is an orientation session, so if you are new to foss4g we can help explain how everything fits together, and how the pieces of the puzzle form a whole. If you are migrating from ESRI environment this is a critical talk to attend as open source technology is often presented in isolation.\n\n\nJody is an experienced open source developer, digging how this technology works. Jonna is part of the QGIS community looking how to successfully use GeoNetwork.\n\nThis presentation shares our findings and experience with you, and touches on what makes GeoNetwork succeed:\n\n* We look at what GeoNetwork is for, the business challenge it is faced with, and the amazing technical approach taken by the technology.\n* We will demo the the publishing workflow to see what is required, and look at how harvesting can jump start your catalog contents \n* We peek under the hood at how the editor works, and discover the central super-power of GeoNetwork\n* Look at examples of how GeoNetwork has been extended by organizations to see what is possible with this technology\n\nGeoNetwork is an established technology - recognized as an OSGeo project and member of the foss4g community for over a decade. We would love to welcome you to the conference and share what this project has to offer.",
        "description": ""
      },
      {
        "title": "Enhancing Researchers' Data FAIR Experience for producing Policy-Relevant Insights through STAC Open Source Software and Specifications",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Joint Research Centre (JRC) of the European Commission is committed to providing independent, evidence-based science and knowledge that supports EU policies. To facilitate this, the JRC has developed the Big Data Analytics Platform (BDAP), a data platform that allows data scientists to easily access, analyze, view, and reuse scientific data to generate and communicate evidence-based insights and foresight. \n\nBDAP hosts spatiotemporal data at petabyte scale from various domains, including elevation, meteorological, administrative, and satellite Earth Observation data. Its architecture leverages almost entirely on Free and Open Source software and tools. The platform offers a cluster environment with both CPU and GPU machines, allowing for large-scale data processing. Additionally, users can visualize and interactively analyze their data through Jupyter Notebooks and Voilà dashboards. \n\nRecently, BDAP implemented the Spatio Temporal Asset Catalog (STAC) specification to describe its data. The catalog hosts different types of data, which share the basic STAC fields. Thanks to the STAC modularity each data type can be described with its own STAC extensions. \n\nBDAP reuses and benefits from various STAC Free and Open Source software and tools. In particular, from the STAC ecosystem it implements the STAC Browser for displaying and searching data, it provides STAC compliant APIs through STAC FAST-API backed by an elasticsearch instance, and uses PySTAC as a Python library for working with STAC metadata. This implementation helps BDAP in its FAIRification process improving users' search, access, and reuse of data.  \n\nIn this presentation, the design and implementation of the STAC compliant set of software tools will be described. Some real use cases will be presented, with an example on the creation of analysis ready data cubes from Sentinel-2 Earth Observation satellite imagery.",
        "description": ""
      },
      {
        "title": "Processing and publishing Maritime AIS data with GeoServer and Databricks in Azure",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The amount of data we have to process and publish keeps growing every day, fortunately, the infrastructure, technologies, and methodologies to handle such streams of data keep improving and maturing. GeoServer is an Open Source web service for publishing your geospatial data using industry standards for vector, raster, and mapping. It powers a number of open-source projects like GeoNode and geOrchestra and it is widely used throughout the world by organizations to manage and disseminate data at scale. We integrated GeoServer with some well-known big data technologies like Kafka and Databricks, and deployed the systems in Azure cloud, to handle use cases that required near-realtime displaying of the latest AIS received data on a map as well background batch processing of historical Maritime AIS data. \n\nThis presentation will describe the architecture put in place, and the challenges that GeoSolutions had to overcome to publish big data through GeoServer OGC services (WMS, WFS, and WPS), finding the correct balance that maximized ingestion performance and visualization performance. We had to integrate with a streaming processing platform that took care of most of the processing and storing of the data in an Azure data lake that allows GeoServer to efficiently query for the latest available features, respecting all the authorization policies that were put in place.  A few custom GeoServer extensions were implemented to handle the authorization complexity, the advanced styling needs, and big data integration needs.",
        "description": ""
      },
      {
        "title": "Dynamic Tiling: From Cloud Optimized Raster to Map tiles",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Over the recent years, Cloud Optimized Raster format have gain popularity not only because they ease access but also because the enable fast visualisation of the data. During this talk I'll go over the principles of dynamic tiling and talk about the different cloud optimized raster format. I'll also present the latest news about TiTiler.",
        "description": ""
      },
      {
        "title": "Serverless Planet-scale Geospatial with Protomaps and PMTiles",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Protomaps is a simple, self-hostable system for tiled vector datasets. In the year since last FOSS4G, we've rolled out a new compressed specification (V3), added support for tile generation tools, and open sourced key integrations with content delivery networks. This talk will give an overview of:\n\n* Why you might want to, or not want to, deploy Protomaps for your application\n* PMTiles write support in the popular Tippecanoe and Planetiler tools\n* The new open source integrations of Protomaps with AWS Lambda and Cloudflare\n* Overview of real-world deployments for users in web GIS, journalism and the public sector",
        "description": ""
      },
      {
        "title": "MapStore, a year in review",
        "type": "Talk",
        "track": "State of software",
        "abstract": "MapStore is an open source product developed for creating, saving and sharing in a simple and intuitive way maps, dashboards, charts and geostories directly online in your browser. MapStore is cross-browser and mobile ready, it allows users to: \n\n- Search and load geospatial content served using widely used protocols (WMS, WFS, WMTS, TMS, CSW) and formats (GML, Shapefile, GeoJSON, KML/KMZ etc..)\n- Manage maps (create, modify, share, delete, search), charts, dashboard and stories directly online\n- Manage users, groups and their permissions over the various resources MapStore can manage\n- Edit data online via WFS-T with advanced filtering capabilities\n- Deeply customize the look&feel to follow strict corporate guidelines\n- Manage different application contexts through an advanced wizard to have customized WebGIS MapStore viewers for different use cases (custom plugins set, map and theme)\n\nYou can use MapStore as a product to deploy simple geoportals by using the standard functionalities it provides but you can also use MapStore as a framework to develop sophisticated WebGIS portals by reusing and extending its core building blocks.\n\nMapStore is built on top of React and Redux and its core does not explicitly depend on any mapping engine but it can support both OpenLayers, Leaflet and Cesium; additional mapping engines could be also supported (for example MapLibre GL) to avoid any tight dependency on a single engine.\n\nThe presentation will give the audience an extensive overview of the MapStore  functionalities for the creation of mapping portals, covering both previous work as well work for the future releases.  Eventually, a range of MapStore case studies will be presented to demonstrate what our clients (like City of Genova, City of Florence, Halliburton, Austrocontrol and more) and partners are achieving with it.",
        "description": ""
      },
      {
        "title": "Faster, easier, more powerful map tile creation with Tippecanoe 2.0",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Development of Tippecanoe, a widely-used open-source C++ tool for creating vector map tilesets, has moved to Felt, where it is a key component of the zero-configuration data ingestion pipeline that processes Felt’s public data library layers as well as uploads from external users.\n\nVersion 2 of Tippecanoe improves its automatic choice of zoom levels, and makes visual improvements to coordinate rounding, small polygons, and the distribution of points in low zoom levels. It now runs faster and uses less memory and disk space. There are new options to generate label points for polygons, to order features by attributes, and to use Visvalingam line simplification. Tippecanoe now accepts FlatGeobuf input as well as GeoJSON and CSV, and can generate output in PMTiles as well as MBTiles.",
        "description": ""
      },
      {
        "title": "MapTiler SDK, the MapLibre experience on steroids",
        "type": "Talk",
        "track": "State of software",
        "abstract": "**MapTiler SDK** is a TypeScript layer that adds new capabilities on top of MapLibre GL, both in terms of UI and core features. It also comes with an interface to **MapTiler Cloud REST API**.\nThe features we have added on top of MapLibre are of two kinds: many convenient helpers to make the developers' life easier, and plenty of built-in defaults that are specially made to use MapTiler data without having to specify annoying URLs or {ZXY} patterns, yet keeping it 100% backward compatible with MapLibre. In addition, all our services from MapTiler Cloud API, such as geocoding, IP geolocation, coordinate transforms, or static maps generation, are now easily accessible with well-documented TypeScript functions. All this with an open-source license.\n\nIn the talk, we are going to present the library, showing practical examples and outputs. We believe, that the SDK is going to make the life of the web mapper easier not only by providing a close integration of MapTiler services but also by the new components and library itself. \n\nThe demo will feature some nice weather visualization we’ve been working on lately!",
        "description": ""
      },
      {
        "title": "TiPg: a Simple and Fast OGC Features and Tiles API for PostGIS.",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Following the work we did with TiTiler (a python module which is designed to create Raster services), we decided to develop the same kind of project but for Vector. Using Postgres/PostGIS as datasource and FastAPI/Pydantic for the web framework, TiPG is a lightweight application which user can include into their own FastAPI application and easily customized. \n\nDuring this session I'll go over the design principle of the TiPg python module and also show some of its great features.",
        "description": ""
      },
      {
        "title": "Mapillary: The path to 2 billion images",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Mapillary is an open platform for street-level imagery and map data that began in 2013. Since then around 1.8 billion images have been contributed from around the world. Imagery has been contributed from horseback in Kyrgyzstan, boats in the canals of Amsterdam, and bicycles on the streets and trails of Sydney. As Mapillary approaches 2 billion images, we’d like to summarize the latest features, acknowledge some of the amazing contributions, and hint at some of the updates that are coming.\n\nSome of the things that we have been working on include:\n\nDesktop Uploader improvements including support for videos and popular cameras.\nImprovements to Mapillary Tools, command line scripts for working with and uploading geotagged imagery and video.\nMobile app updates including multi-tasking, redesigns, multi-language support, and upload improvements.\nCamera Grant programs in the US and Europe, providing 360º cameras for people interested to map pedestrian infrastructure.\nIntegrations with Rapid Editor, an AI powered OpenStreetMap editor which we will demo in more detail at a workshop.\nUpdated Help Pages to make capturing, uploading, and using street-level imagery far easier.\n\nAfter walking through the latest Mapillary improvements, we will take a look at case studies of organizations contributing and using imagery. We’ll zoom in on an NGO, a government agency, and a commercial entity, each of which are using Mapillary in different ways.\n\nWe’ll finish our talk with an exploration of upcoming Mapillary features and projects. We encourage questions and suggestions in the Q&A and hope for a productive conversation at the end as we walk together towards 2 billion images.",
        "description": ""
      },
      {
        "title": "GeoServer Orientation and Demo",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "Welcome to GeoServer, a popular web service for publishing your geospatial data using industry standards for vector, raster and mapping.\n\nIf the previous sentence made no sense to you, or if you are new to foss4g, or even just new to GeoServer, attend this talk to get pointed in the right direction!\n\nThis presentation provides a gentle introduction to FOSS4G and we will do our best to say the quiet part out loud:\n\n* Demo: We have learned from experience, and will introduce GeoServer using a demo.\n* Usage: Concepts using both a demo, and diagrams  to connect to your data and publish as a spatial service.\n* Checklist: Preflight check-lists capturing common oversights when deploying GeoServer for the first time.\n* Value: What role GeoServer plays in your organization and what value the application provides.\n*  Community: How the project is managed, and a discussion of the upcoming activities.\nAttend this presentation to get a running start on using GeoServer in your organization!\n\nAttend this presentation to get a running start on using GeoServer in your organization!",
        "description": ""
      },
      {
        "title": "geoserverx - a new CLI and library to interact with GeoServer",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "geoserverx is a modern Python package that provides an efficient and scalable way to interact with Geoserver REST APIs. It leverages the asynchronous capabilities of Python to offer a high-performance and reliable solution for managing Geoserver data and services.\nWith geoserverx, users can easily access and modify data in Geoserver, such as uploading and deleting shapefiles, publishing layers, creating workspaces, styles, etc. . The package supports asynchronous requests along with synchronous method to the Geoserver REST API, which enables users to perform multiple tasks simultaneously, improving performance and reducing wait times.\nApart from being implemented in Python Projects, geoserverx also provides CLI support for all of it's operations. Which makes it useful for people who want to avoid Python all-together. \nIn this talk we discover for the very first time about how geoserverx work and underlying code ideology. Along with that we'll also spread some light on upcoming modules to be integrated in geoserverx",
        "description": ""
      },
      {
        "title": "GeoServer Cloud in depth",
        "type": "Talk",
        "track": "State of software",
        "abstract": "A typical GeoServer deployment involves exposing it as a front service to publish a number of layers directly to the internet, where a single instance, or even a couple, and an on-premise deployment model is enough.\n\nWithin larger companies though, more often than not GeoServer is a critical component of a more significant infrastructure, used to host tens of thousands of layers to accommodate organization requirements across various departments and workflows that involve several other systems, and complex cloud deployments.\n\nThese scenarios are where GeoServer Cloud shine, enabling devOps teams to set up clusters of GeoServer pods that are scalable, have improved resiliency, security, and resource utilization; and increased observability and integration with telemetry systems for monitoring, debugging, and tracing.\n\nIn this talk, we'll explore in depth how GeoServer Cloud achieves these goals, from technology and design choices to detailed overviews of technical improvements that were required, supported by success stories of current CampToCamp customers that got GeoServer Cloud in production.",
        "description": ""
      },
      {
        "title": "GeoServer Feature Frenzy",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GeoServer is a web service for publishing your geospatial data using industry standards for vector, raster and mapping. It powers a number of open source projects like GeoNode and geOrchestra and it is widely used throughout the world by organizations to manage and disseminate data at scale.\n\nWhat can you do with GeoServer? This visual guide introduces some of the best features of GeoServer, to help you publish geospatial data and make it look great! \n\nGeoServer has grown into an amazing, capable and diverse program - attend this presentation for:\n\n* A whirl-wind tour of GeoServer and everything it can do today.\n* A visual guide to some of the best features of GeoServer.\n* Our favorite tricks we are proud of!\n\nNew to GeoServer - attend this talk and prioritize what you want to look into first. Expert users - attend this talk and see what tricks and optimizations you have been missing out on.",
        "description": ""
      },
      {
        "title": "Monitoring Landfill sites from space",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Landfill sites are for storing waste in a secure and secluded manner but they can cause a lot of damage to the environment by generating greenhouse gases and contaminating soils by releasing heavy metals and toxins. Monitoring the area of landfill sites from space is a challenging problem because of the huge amount of unstructured data and unavailability of standard datasets and procedures. By combining open-source tools with geospatial data, we present a global dataset that monitors the changes in the landfill area. We have achieved this by developing a deep learning based segmentation model that uses multispectral satellite data and segments the landfill areas from them. In order to develop the model, we have labelled landfill sites from optical imagery from all over the world. Our current segmentation model has 31 million parameters and has achieved an accuracy of 77.6% on the test set. Currently, the dataset contains temporal data from 2021 of the major landfill sites from more than 7 countries and it is growing daily as new data is coming in. In future, we aim to enhance this dataset by adding more variables other than the area, for instance height of the landfill and will also explore other higher resolution data for validating our results further.",
        "description": ""
      },
      {
        "title": "Evaluation and assesment of open source projects",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "The National Land Survey of Finland (NLS) is a government agency that maintains finnish property register and uses various administrative information systems that handle crucial data. To develop, manage, and maintain these systems, NLS follows a Business Technology Standard model and aims to publish its own production applications as open source software and use open source applications in development when possible. \n\nDuring the development of new information systems, NLS follows an agreed and approved management model and uses only components and software that meet development guidelines. Examples of such components are QGIS and PostgreSQL. However, if NLS needs to adopt and evaluate components that are not yet included in the development guidelines, it must evaluate associated open source projects, record and process considerations, and accept them in accordance with the change management process. \n\nTo evaluate the maturity of open source projects, NLS has developed a tool that continuously evolves to reflect the needs of the organization. The tool is a checklist of criteria that can be used to assess the maturity of a project and compare it to similar products. The presentation explains the items in the tool and their significance as part of the metrics. \n\nThe tool that NLS has developed could be valuable for individuals and companies in similar positions when evaluating open source projects for their needs. The experiences gained by NLS can also help improve weak points that open source software producers may not have considered in their own projects.",
        "description": ""
      },
      {
        "title": "A Soft Transition to FOSS in a Decentral Administration",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "The public administration of the Swiss canton Aargau chose to use OSS for the publication of all open WMS, using GeoServer-Cloud and PostgreSQL. Meanwhile, the decentral offices, which gather geographical data and style this data are used to using proprietary software for this purpose. The strategy chosen was to provide a soft transition to OSS, by providing automated conversion processes based on a new FOSS project and by improving existing OSS with regards to styling conversions towards SLD.",
        "description": ""
      },
      {
        "title": "Land of 60000 zoning plans - QGIS to the rescue!",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "This project was a pilot of a larger upcoming project, where the aim is to produce a national interoperable data model for every valid zoning and city plan in Finland. The project is part of the development of the Finnish Environment Institute’s Built Environment Information System and the harmonization of national land use planning information. \n\nThe aim of this presentation is to present the overall workflow of the project and the transition from proprietary data towards an open source national database with common spatial and descriptive information. Currently the data used in municipal decision making processes in Finland consists of proprietary data that is lacking spatial information or is outdated.\n\nThe transformation of the zoning and city plans from two different data providers created a lot of topological errors and unmatched geometries. QGIS was a key tool for fixing these errors - the digitizing and geometry repair tools were used in solving these issues. \n\nThis pilot project was implemented in Southern Savonia, Finland. In the region,  zoning has been executed for approximately 80 % of the whole land area. The focus of the project was to investigate the compatibility of the base data and how to automate the processes of merging, fixing, updating and comparing the data. The data was in vector format and was provided by the National Land Survey of Finland and municipalities of Southern Savonia. \n\nThe automation processes were built with a python script and the quality control was made with manual digitization. The official documentation of the zoning and city plans were included in the borderline vector data. The final product was uploaded to a GitHub repository. The project also managed to produce a timeline for the upcoming nationwide project and the distribution between automated and manual workload in similar projects. \n\nThe methods and the results of the project could be duplicated in other countries or lead the way towards more open national or regional land use planning.",
        "description": ""
      },
      {
        "title": "Integrated modeling with k.LAB... and QGIS",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The Knowledge Laboratory, in short k.LAB, is a software stack that embraces the FAIR principles: findable, accessible, interoperable and reusable. Its objective is to support linked knowledge across the borders of the domains of single modelers and scientists. k.LAB’s fascinating novelty is the use of semantics to create a natural language to describe the models and the qualities that want to be observed.\n\nModelers can develop their models and publish them to the network. Publishing makes them findable and accessible within the network. Since everything in the network is observable, when running a model, k.LAB looks for the best knowledge unit able to resolve the particular request. Interoperability is build and reusability is a natural consequence.\n\nThe k.LAB software stack is free and open source and relies on various projects of the Osgeo community as Geoserver, Openlayers and the Hortonmachine. It has been in development for almost 2 two decades and got a particular visibility boost in 2021, when the Statistics Division of the UN Department of Economic and Social Affairs and the UN Environment Program, in collaboration with the Artificial Intelligence for Environment & Sustainability at the Basque Centre for Climate Change, launched the Artificial Intelligence powered application for rapid natural capital accounting: the ARIES for SEEA Explorer. \n\nLately a python client that allows interaction with k.LAB has been released. This opens up to new ways to observe the world from within common GIS tools as for example QGIS.\n\nAn overview of the state of the art of the project will be given.",
        "description": ""
      },
      {
        "title": "SMASH and the new survey server - state of the art",
        "type": "Talk",
        "track": "State of software",
        "abstract": "SMASH , the digital field mapping application for android and IOS that superseded the well known app geopaparazzi has been around for some years now. The last two years were a positive development storm after a quite calm year and brought many fixes as well as enhancements. Examples are better postgis and geopackage support, but also some hidden gems like geocaching. \nThe big news is on the serverside though. A new survey server has been developed in tight cooperation with a local government agency to best create effective surveying workflows and tools for survey teams. To attract a wider developer community to contribute to the project, the django framework was chosen for the server backend.\nThis presentation will give an overview of everything happened lately in the SMASH field mapping world.",
        "description": ""
      },
      {
        "title": "Openness as a strategic advantage in modern geospatial",
        "type": "Talk",
        "track": "State of software",
        "abstract": "In the last decade, 5 complementary assets have intersected, creating a series of new capabilities for our community. Modern geospatial did not exist even five years ago, and openness - the combination of open standards, open data all glued together with open source code is a key contributing factor. \n\nThis talk will present the case for openness being a competitive advantage for a modern, innovative technology company. We will discuss why we have been right all along, and why we will end up being even righter in the future. If you want a solid dose of confirmation bias, this is the talk for you!",
        "description": ""
      },
      {
        "title": "DigiAgriApp: the app to manage your agricultural field",
        "type": "Talk",
        "track": "State of software",
        "abstract": "DigiAgriApp is a client-server application to manage different kinds of data related to farming fields. It is able to store information about crops (specie, farming forms/system...), any kind of sensor data (included sensors and device hardware, weather, soils...), irrigation information (system type, openings...), field operations (pruning, mowing, treatments...), remote sensing data (taken from different devices as mobiles, drone, satellites) and production quantities.\n\nThe DigiAgriApp server is composed of a PostgreSQL/PostGIS database and a REST API service to interface with it. The server is developed using Django and the Django REST framework extension with other minor extensions are used to create the REST API. This service plays the key interface between the database and the client. We choose a nested way to create the API, of which the main element is the farm; this way the user can see only the farms related to him and from there he can look to other nested elements, first of all the farm’s fields and later other elements like sensor and remote data or other sub-fields like rows and plants. The REST API is using JavaScript Object Notation as input and output format to simplify and standardize the communication with it.\n\nTo obtain data from the sensors the server is also composed of a growing number of services to work with data providers, of which currently only a few are implemented. The Message Queue Telemetry Transport provider is a demon listening continuously to a broker (backend system to coordinate different clients) and several topics to obtain data as soon as they are provided; the second provided that is already implemented is related to remote sensing data and uses the SpatioTemporal Asset Catalogs specification to obtain the data. STAC is a common language to describe geospatial information, so it can more easily be worked with, indexed and discovered.\n\nThe client side instead is developed using Flutter, an open-source UI software development kit based on dart, a programming language designed for client development. Flutter is able to create cross-platform applications and it was chosen precisely because of its ability to realize cross platform applications.\n\nAll the code is released as Free and Open Source software with a GNU General Public License Version 3 license; it is available in the DigiAgriApp repository on GitLab and the client application will be published also in the main stores for mobile apps.",
        "description": ""
      },
      {
        "title": "Tiling big piles of raster data using open source software and MapTiler Engine",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "When publishing (raster and vector) data in the form of a web mapping application, the first step is always to prepare a cache of the data. Currently, tiled images seem to be the industry standard - and the internal format of the tiles is either PBF (for vector data) or PNG/JPEG/WebP or similar raster data formats supported by current web browsers and desktop mapping applications (e.g. QGIS). \n\nMost of the tools out there are going to store the raster tiles in a file-system structure, using directories for the Z and X tile coordinates and file names for the Y coordinate. This is limiting for practical purposes as on some filesystems you can exceed the maximum number of files easily. While for the vector data, the OpenMapTiles project seems to be well established, along with Tippecanoe and Planetiler,  for the raster data tiles, the field of tiling possibilities is wide open.\n\nThe tiling process can be very demanding on hardware resources and time-consuming. Having the possibility to parallel process the data or even use a cluster of machines for faster tiling could be crucial for some applications.\n\nIn this talk, we will give an overview of the current possibilities for tiling, focused (but not exclusively) on the raster data tiles. Gdal2tiles, QGIS tile generating tools,  mapproxy-seed, mapcache_seed, and others. Each of the tools has its place in the geospatial data provider ecosystem, and so does MapTiler-Engine. With MapTiler-Engine, users can process large amounts of geospatial data and store them in various output tile formats. It supports many input data formats and adds modifications such as output color, resolution, and more. It also supports different tile matrix sets. MapTiler-Engine has a graphical user interface for easy usage, but it also has a command line interface, so you can make it part of a larger toolchain.",
        "description": ""
      },
      {
        "title": "HOW OPEN SOURCE TOOLS ENRICHES OSM WITH COMMUNITY MAPPING IN TANGA-TANZANIA",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Open source tools have played a significant role in enriching OpenStreetMap (OSM) with community mapping in Tanga, Tanzania. These tools have enabled local communities to actively participate in mapping their own areas, which has led to a more accurate and detailed representation of the community on OSM. The use of open source tools in community mapping has also allowed for increased collaboration and sharing of data between community members, as well as with other organizations and researchers.\n\nOne such open source tool that has been used in community mapping in Tanga is QGIS. This tool has been used to create detailed maps of the community, including roads, buildings, and other infrastructure. The use of QGIS has also allowed for data analysis, which has helped community members identify areas in need of improvement and target resources more effectively.\n\nAnother open source tool that has been used in community mapping in Tanga is OpenDataKit (ODK). ODK has been used to collect data in the field, such as information on the availability of healthcare facilities and services. This data has been used to create detailed maps of the community, which has helped community members identify areas in need of improvement and target resources more effectively.\n\nThe use of open source tools in community mapping in Tanga has also led to increased collaboration and sharing of data between community members, as well as with other organizations and researchers. For example, community members have been able to share their data with organizations working on healthcare and education projects, which has helped these organizations target resources more effectively.\n\nOverall, the use of open source tools in community mapping in Tanga has been a significant factor in the success of OSM in the area. These tools have enabled local communities to actively participate in mapping their own areas, which has led to a more accurate and detailed representation of the community on OSM. The use of open source tools in community mapping has also allowed for increased collaboration and sharing of data between community members, as well as with other organizations and researchers, which has helped to improve the community and target resources more effectively.",
        "description": ""
      },
      {
        "title": "IdeaMap Sudan - Building a geodata community in a data scarse context",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "IDeAMapSudan is a 2.5-year project finishing in March 2023. The project aims to develop a community-led geospatial database for mapping deprived urban areas (e.g., informal settlements) that will support the decision-making process for displacement and socio-economic reconstruction in Khartoum, Sudan. To that end, nine trainers from different governmental and non-governmental organizations were selected to be trained by a team of international experts from the Faculty ITC of the University of Twente, The Netherlands; the Universite Libre de Bruxelles, Belgium; and from the African Population and Health Research Center Kenya. These nine trainers were taught the essential competencies in using Free, and Open Source Geospatial Software to produce, compile, curate and distribute spatial data. Once the training of the nine trainers was completed, a series of community workshops were organized so that the trainers could train local community actors in tasks related to spatial data curation in close relation to their communities. The datasets produced from this process were then used to create a deprivation model and additional open data sets that can be used to help local communities and actors to take actions to mitigate several types of deprivations:\nUnplanned urbanization - e.g. small, high-density, disorganized buildings\nSocial risk - e.g. no social safety net, crime\nEnvironmental risk - e.g. flood zone, slopes\nLack of facilities - e.g. schools, health facilities\nLack of infrastructure - e.g. roads, bus service\nContamination - e.g. open sewer, trash piles\nLand use/rights - e.g. non-residential zoning\n\nThis talk will describe three significant aspects of the project: the curriculum of competencies and the software tools used to teach these competencies; the phases and challenges of assembling a team and infusing it with a sense of community and participation; and the importance of disseminating results and evaluate the social impact open source software and open data can have.",
        "description": ""
      },
      {
        "title": "The template for a Semantic SensorThings API with the GloSIS use case",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Motivation: \nSpatial Data Infrastructures (SDI) developed for the exchange of environmental\nhas heretofore been greatly shaped by the standards issued by the Open\nGeospatial Consortium (OGC). Based on the Simple Object Access Protocol (SOAP),\nservices like WMS, WFS, WCS, CSW became digital staples for researchers and\nadministrative bodies alike. \n\nIn 2017 the Spatial Data on the Web Working Group (SDWWG) questioned the overall\napproach of the OGC, based on the ageing SOAP technology\n[@SDWWG2017]. The main issues identified by the SDWWG can be summarised as:\n\n- Spatial resources are not identified with URIs.\n- Modern API frameworks, e.g. OpenAPI, are not being used.\n- Spatial data are still shared in silos, without links to other resources.\n- Content indexing by search engines is not facilitated.\n- Catalogue services only provide access to metadata, not the data.\n- Data difficult to understand by non-domain-experts.\n\nTo address these issues the SDWWG proposed a five point strategy inspired on the\nFive Star Scheme [@BernersLee2006]:\n\n- **Linkable**: use stable and discoverable global identifiers.\n- **Parseable**: use standardised data meta-models such as CSV, XML, RDF, or JSON.\n- **Understandable**: use well-known, well-documented, vocabularies/schemas.\n- **Linked**: link to other resources whenever possible.\n- **Usable**: label data resources with a licence.\n\nThe work of the SDWWG triggered a transformational shift at the OGC towards\nspecifications based on the OpenAPI. But while convenience of use has been the\nfocus, semantics has been largely unheeded. A Linked Data agenda has not\nbeen pursued.\n\nHowever, the OpenAPI opens the door to an informal coupling of OGC services with\nthe Semantic Web, considering the possibility of adopting JSON-LD as\nsyntax to OGC API responses. The introduction of a semantic layer to digital\nenvironmental data shared through state-of-the-art OGC APIs is becoming a\nreality, with great benefits to researchers using or sharing data.\n\nThis communication lays down a simple SDI set up to serve semantic environmental\ndata through a SensorThings API created with the `glrc` software.  A use case is\npresented with soil data services compliant with the GloSIS web ontology.\n\nSensorThings API:\n\nSensorThings API is an OGC standard specifying a unified framework to\ninterconnect Internet of Things resources over the Web [@liang2016ogc].\nSensorThings API aims to address both the semantic, as well as syntactic,\ninteroperability. It follows ReST principles [@fielding2002principled],\npromotes data encoding with JSON, the OASIS OData protocol\n[@chappell2011introducing] and URL conventions. \n\nThe SensorThings API is underpinned on a domain model aligned with the ISO/OGC\nstandard Observations & Measurements (O&M) [@Cox2011], targeted at the\ninterchange of observation data of natural phenomena. O&M puts forth the\nconcept of `Observation` has an action performed on a `Feature of Interest`\nwith the goal of measuring a certain `Property` through a specific `Procedure`.\nSensorThings API mirrors these concepts with `Observation`, `Thing`,\n`ObservedProperty` and `Sensor`. This character makes of SensorThings API a\nvehicle for the interoperability of heterogeneous sources of environmental\ndata.\n\n\n`glrc`:\n\n`grlc` (pronounced \"garlic\") is a lightweight server that translates SPARQL\nqueries into Linked Data web APIs [@merono2016grlc] compliant with the OpenAPI\nspecification. Its purpose is to enable universal access to Linked\nData sources through modern web-based mechanisms, dispensing the use of the\nSPARQL query language. While losing the flexibility and federative capacities\nof SPARQL, web APIs present developers with an approachable interface that can\nbe used for the automatic generation of source code.\n\n\nA `glrc` API is constructed from a SPARQL query to which a meta-data section is\nprepended. This section is declared with a simplified YAML syntax, within a\nSPARQL comment block, so the query remains valid SPARQL. The meta-data provide\nbasic information for the API set up and most importantly, the SPARQL end-point\non which to apply the query. The listing shows an example. \n\n```\n#+ endpoint: http://dbpedia.org/sparql\n\nPREFIX dbo: <http://dbpedia.org/ontology/>\nPREFIX dbr: <http://dbpedia.org/resource/>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n\nSELECT ?band_label { \n    ?band rdf:type dbo:Band ;\n          dbo:genre dbr:Hard_Rock ;\n          rdfs:label ?band_label .\n} ORDER BY ?band_label\n```\n\nA special SPARQL variable formulation is used to map into API parameters. By\nadding an underscore (`_`) between the question mark and the variable name,\n`glrc` is instructed to create a new API parameter. A prefix separated again\nwith an underscore informs `glrc` of the parameter type. The `?band_label`\nvariable in [Listing @lst:1] can be expanded to `?_band_label_iri` to create a\nnew API parameter of the type IRI.\n\n\nUse case: GloSIS: \n\nThe Global Soil Partnership (GSP) is a network of stakeholders in the soil\ndomain established by members of the United Nations Food and Agriculture\nOrganisation (FAO). Its broad goals are to raise awareness to the importance of\nsoils and to promote good practices in land management towards a sustainable\nagriculture. \n\nAcknowledging difficulties in exchanging harmonised soil data as an important\nobstacle to its goals, the GSP launched in 2019 an international consultancy to\nassess the state-of-the-art and propose a path towards a Global Soil Information\nSystem (GloSIS) based on a unified exchange. A domain model resulted, based\non the ISO 28258 standard for soil quality [@SchleidtReznik2020], augmented with\ncode-lists compiled from the FAO Guidelines for Soil Description [@Jahn2006].\nThis domain model was then transformed to a Web Ontology, relying on the Sensor,\nObservation, Sample, and Actuator ontology (SOSA) [@Janowicz2019], and other\nSemantic Web standards such as GeoSPARQL, QUTD and SKOS. The GloSIS web ontology\nhas been successfully demonstrated as a vehicle to exchange soil information as\nLinked Data [@GloSIS]. \n\nA prototype API for the GloSIS ontology, formulated in compliance with the\nSensorThings API specification, will be presented in this communication. It\ndemonstrates how the same set of SPARQL queries can be used to query through a\nReST API any end-point available over the internet, sharing linked soil data in\naccordance with the GloSIS ontology. Thus providing a clear step towards the\nfederated and harmonised system envisioned by the GSP.",
        "description": ""
      },
      {
        "title": "Digitizing and improving GIS for Global Health: from data collection to geospatial data management for a measles vaccination campaign in Cameroon",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In Cameroon, the planning and monitoring of a measles vaccination campaign is implemented in an open source software called Iaso built on a Python based backend combining Django and Postgres/Postgis ; the frontend is React based. Iaso aims to provide a number of core functionalities to support ongoing geospatial data management: a mobile application, a web dashboard, a mapping function to merge various data sources, a user-friendly API for data science and scripting, and a seamless bi-directional integration with DHIS2 (standard health information system in low- and middle-income countries).\n\nIaso is articulated around three essential components : a central georegistry interface, a mobile data collection tool and a micro planning interface. Those tools are integrated seamlessly with each other to provide a powerful platform to manage, update, merge and validate multiple data sources and structured information collected. Geospatial data from GPS collection to the management of multiple reference lists of organization units (Health, Administrative or School pyramid) are Iaso's foundation. Those features allow interconnecting collected data to existing hierarchical features coupled with planification and collection of survey campaigns in the field through the mobile application and the web platform.\n\nIaso exposes a full API providing various endpoints allowing data scientists to integrate data analysis pipeline through external analytic platform. As a geospatial data management platform, it provides versioning of every dataset and is designed to keep a full history of all the changes on the data of interest from the forms to the geometry or metadata of the organization units. It also features seamless integration with QGIS and other desktop applications through a templated Geopackage format.\n\nIn this presentation, the tool is explained and described from the planning of the vaccination campaign in Cameroon to the near real-time monitoring of the campaign (eg. stock and team planning management).\n\nSource : https://github.com/BLSQ/iaso",
        "description": ""
      },
      {
        "title": "Community Activation for the Kahramanmaraş Earthquake Response via OpenStreetMap",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "On February 6, 2023 a sequence of major earthquakes with magnitudes 7.8 and 7.5 have struck Southern Turkiye and Northern Syria, causing massive damage and very high number of casualties in both countries. The sequence of earthquakes were followed with hundreds of aftershocks within the month following the earthquakes, as well as triggering other major earthquakes, such as the 6.4 magnitude earthquake that had struck Antakya on February 20. Humanitarian OpenStreetMap Team (HOT), with Yer Çizenler (YÇ), HOT’s local partner within the Turkish OSM community, have activated to map the missing road and building base data with the help of regional and global OpenStreetMap communities.   \n\nMore than 7 thousand contributors from these communities, together, have contributed to the addition of more than 1.4 million buildings, 70,000 km of roads into OpenStreetMap for the use of field volunteers and organizations worldwide. \n\nIn this talk, the audience will be informed about the coordinated efforts within this mapping activation, the impact of the data created with some example use cases within the response activities. The audience will be informed about various open data sources that were used to enhance the existing OSM data, and their licensing and compatibility considerations during the mapping process. The presenters will also describe the validation, data quality assurance and monitoring methods, approaches and tools utilized for ensuring the OSM data is reliable, current and is able to meet community standards within both short and long terms.",
        "description": ""
      },
      {
        "title": "Tools for linking Wikidata and OpenStreetMap",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Editors of OpenStreetMap can use my software to search for a place or region, generating a list of candidate matches from Wikidata, which can then be checked and saved to OpenStreetMap.\n\nLinking the two projects isn't without controversy. They use different licenses which raises questions about what information from one project can be copied to the other.\n\nIn the presentation I will give details of a new version of the editing tool.\n\nI will talk about the benefits of linking, the process of finding matches, the community response - including the controversy - and how people can get involved.",
        "description": ""
      },
      {
        "title": "A Contemporary Nolli Map: Using OpenStreetMap Data to Represent Urban Public Spaces",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "More than 250 years ago, Giovanni Battista Nolli, an Italian architect, engineer and cartographer, was concerned with how and where space is or is not publicly accessible. In his map 'La nuova topografia di Roma Comasco', he mapped publicly accessible interior and exterior spaces of Rome with an impressively high level of detail as a figure-ground map. Since Nolli’s time, both the character and diversity of public spaces as well as cartographic technology have changed. In my Master thesis, I aim to adapt Nolli's underlying idea for today’s circumstances on the basis of open data, and seek to develop methods for processing volunteered geographical information from OpenStreetMap (OSM) to identify, categorize, and map public spaces based on thematic and geometric information.\n\nFirst, it has to be clarified what is considered public space and what is not. Given the data available via OSM as well as in terms of feasibility, I focus on the aspect of public accessibility and exclude indoor spaces. Data processing is implemented as a Python script based on existing OSM and geospatial Python packages. The code is available as Open Source on [GitHub](https://github.com/ester-t-s/osm-public-space-mapper). The application of the framework and methods is tested in two case studies in Vienna, Austria. The result can be visualized as 'contemporary Nolli map'.\n\nIn my talk, I will give insights into the methodology and framework for data analysis I developed as part of my Master thesis",
        "description": ""
      },
      {
        "title": "eoAPI - The Earth Observation API",
        "type": "Lighting talk",
        "track": "State of software",
        "abstract": "eoAPI is an open source project which aim to create a full Earth Observation API, combining STAC metadata API (stac-fastapi), a Raster dynamic tile service (TiTiler) and a Vector Tiles service (TiPg). \n\nUsing eoAPI AWS CDK template you're almost two command lines away of setting your own Earth Observation services.",
        "description": ""
      },
      {
        "title": "MIERUNE BASE: The geospatial service for serving and sharing datasets",
        "type": "Lighting talk",
        "track": "State of software",
        "abstract": "MIERUNE is a geospatial tech company in Japan. We set FOSS4G as a foundation of us and continuously join the communities as an user, a developer or a contributor. Thesedays we have been committing our new service - MIERUNE BASE. MIERUNE BASE is focussing on easily serving and sharing datasets on a simple architecture based on serverless and FOSS4G. In this talk, we will introduce the architecture or techniques of MIERUNE BASE.",
        "description": ""
      },
      {
        "title": "Evaluation of the geometric accuracy of the base map 1 : 100 000 of Madagascar compared to the CASEF ortho image",
        "type": "Lighting talk",
        "track": "Open Data",
        "abstract": "The quality of geospatial data is generally measured by its logical consistency, completeness,\npositioning quality, semantic quality, temporal quality and genealogy [1]. In fact, concerning the\nsituation of geospatial data in Madagascar in the past, since 1992, the old orthophotos had been\nattached to the national reference system which is the international 1924 with Laborde as a\nprojection. The first old orthophotos were achieved during the environmental program in 90s. In\nother hand, the remain old orthophotos were produced with the mission as national securing land\ntenure. However, the geometric accuracy and details of all the old orthophotos are different as well\nas they do not cover the national territory. If they cover a large area for about 60 000 km2, some\nusers have noticed discrepancies of a few meters or even more than a dozen meters on certain\npoints, even though the field of application is land. In December 2019, a ministerial order was\ndeveloped to define the technical specifications of photogrammetric work in the country. In this\nspecification, according to Chapter 4, Section 14, the accuracy of the orthophoto / orthoimage is\nestimated by the planimetric root mean square deviation (emqXY) calculated from the differences\nbetween the ground coordinates and measured orthoimage coordinates of certain clearly identifiable\ntopographic features. For the orthophoto / orthoimages in urban areas, the emqXY must be better\nthan 1 m CE90 which is the circular error at the 90th percentile. For the rest of the territory other\nthan the urban area, it must be better than 3 m CE90 [2].\nTherefore, not only is it crucial to be able to measure this quality, but also to control, to improve,\nand finally to guarantee it [3]. The basic map in Madagascar is the topographic map at the scale of 1\n: 100 000. However, the average age of these maps is 60 years. Consequently, the contained\ninformation no longer meets the needs of most users. On the other hand, orthoimages produced later\nseem to be much more accurate. To evaluate the accuracy of the 1 : 100 000 topographic map, we\nfirst identified an orthophoto that could be used as a reference. Furthermore, we considered the\northobase elaborated in 2014 from the SPOT5 image and the control result of the CASEF\n(Agricultural Growth and Land Security) project orthoimage. The 2014 orthobase was produced\nwithin the framework of our cooperation with the La Reunion (France) region, while the CASEF\northoimage was developed for the purpose of land tenure security in Madagascar.\nIn order to conduct this study, we tried to answer the following series of questions : 1) what is the\nmost accurate orthoimage to serve as a reference; 2) what is the average value of the deviations of\nthe objects on the 1 : 100 000 topographic maps as well as those of these derived products (SCAN\n100 and BD 100) compared to those of the reference orthophotos. 3) Finally, is there a set of\nparameters to reposition the SCAN 100 / BD 100 on this orthoimage?\nTo achieve this study, several steps were taken including literature reviews, collection of a few\nsamples and observations of results from previous work. We also made researches on the reference\ndata from which the BD 100, the topographic maps at 1 : 100 000 and SCAN 100 will be evaluated.\nFrom this comparison, we could see that the attachment to the national reference system of the\nCASEF orthoimage is more accurate than that of the orthobase. In addition to that, coordinate\npointing of identifiable geographic objects on both datasets were made with statistical evaluation of\nthe differences. Related to tools that we are adopting, since that our budget has been limited in\nterms of software license, so that we are using open source geospatial software to make our\norganization better with QGIS during the evaluation process.\nAfter evaluating four (04) sheets on the 1 : 100 000 map of Mahajanga, Antalaha, Manjakandriana\nand Toamasina, we quantified the root mean square errors at 109.3 m, 108.6 m, 128.4 m and 51.9 m\nrespectively. The deviations are disparate, therefore there is no single set of parameters to reposition\nthe 1,100,000 topographic map. We concluded that the BD 100 should be left as it is, and that a newset of geographic databases should be developed at different scales, in particular the new version of\nthe BD 100.",
        "description": ""
      },
      {
        "title": "Catasto-Open: open-source tools for the Italian Cadastre",
        "type": "Lighting talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "Catasto-Open is an open-source set of tools for the Italian Cadastre that manages geospatial data in a user-friendly and efficient manner. The tool is designed to store, retrieve and manipulate cadastral data, including property boundaries, ownership information, and other relevant details. By leveraging GeoServer and MapStore technologies, it allows for the integration with existing GIS systems, making it a versatile and valuable resource for managing geospatial data in an OGC-compliant pipeline. The tool is accessible to a wide range of users, including government agencies, private companies, and individual property owners, also Catasto-Open can be easily customizable to meet the specific needs of different users.",
        "description": ""
      },
      {
        "title": "Get your own OpenStreetMap dataset running in a Geoserver instance in 2 steps",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Get your preferred OSM dataset (ie. country) running in a local Geoserver instance with only 2 commands and avoid any dependence on an external provider.\nSimple, fast, clean solution. Lowering the barrier to entry to geospatial technology use and development.\n\nDocker-compose setup which assembles the necessary components to implement a Geoserver instance that publishes the OpenStreetMap (OSM) layers locally on a single host/machine (Postgis is required to store the OSM layers).\nInstructions for this project are based on this repository OSM-Styles, but make a much simpler execution plan. \nThe steps and scripts are intended to run in the context of Linux, Mac and Windows environments.",
        "description": ""
      },
      {
        "title": "Leveraging on OpenStreetMap (OSM) for Improved Census Data Delivery",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "The delivery of national census programs to aid nations in coming up with better strategies for serving the population’s needs and better plans for sustainability. While on the other hand, several developing nations around the world have not been able to deliver highly accurate census data and results to aid in these efforts. This leads to the implementation of policies that are not inclusive among other limitations introduced along the way. \nBy leveraging on open data platforms such as OpenStreetMap, open-source geo applications can be built to aid developing nations in accurate and location-driven data-capturing processes. Having digital location strategies and innovation as the major component for census data collection can potentially lead to vast growth in digital economies across developing nations and unleash endless possibilities and potential innovations which are inclusive and fit for purpose. This also provides a platform and chance to have more contributions towards OSM at the national level while delivering accurate and much-needed data.",
        "description": "The talk will give highlight on a technology built in Zimbabwe and just highlighting how this idea can be leverages across the world for a better census outcomes."
      },
      {
        "title": "Offline web map server \"UNVT Portable\"",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "UNVT Portable is a package for RaspberryPi that allows users to access a map hosting server via a web browser within a local network, primarily for offline use during disasters. It is designed to aid disaster response by combining aerial drone imagery with OpenStreetMap and open data tile datasets.\n\n\"UNVT Portable\" is a map server that allows you to freely use web maps from devices such as smartphones even in an offline environment. It is mainly designed to work in an offline environment in the event of a major disaster, and various open data tiles are prepared in advance, such as drone aerial images taken after a disaster, OpenStreetMap, and satellite images released for free by JAXA（Japan Aerospace Exploration Agency）, etc. Combine sets to create the maps you need in times of disaster. We envision a use case for municipalities, etc. to understand the situation after a disaster and to respond to disasters. It is built using open source software such as Apache and MapLibre and Raspberry Pi, and is completely open source. Unlike tools such as Google Maps, which are difficult to use for secondary purposes, it is being developed as open source so that it can be released in a form that can be easily used by anyone, including local governments, international organisations and private companies.",
        "description": "\"UNVT Portable\" is a map server that allows you to freely use web maps from devices such as smartphones even in an offline environment. It is mainly designed to work in an offline environment in the event of a major disaster, and various open data tiles are prepared in advance, such as drone aerial images taken after a disaster, OpenStreetMap, and satellite images released for free by JAXA（Japan Aerospace Exploration Agency）, etc. Combine sets to create the maps you need in times of disaster. We envision a use case for municipalities, etc. to understand the situation after a disaster and to respond to disasters. It is built using open source software such as Apache and MapLibre and Raspberry Pi, and is completely open source. Unlike tools such as Google Maps, which are difficult to use for secondary purposes, it is being developed as open source so that it can be released in a form that can be easily used by anyone, including local governments, international organisations and private companies."
      },
      {
        "title": "Why is popularity the biggest enemy of WMS?",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "The Web Map Service (WMS) is the most popular standard of sharing data remotely. It is commonly used as a basemaps, a way of presenting governmental spatial data, and as a data source when creating vector datasets. Creating a WMS requires original data to be read and then rendered. This process can be slow, especially if the source data is heavy and not optimized. This is the case, for example, with Sentinel 1 global satellite data, which is a collection of daily revisions with a total volume of 250 GB per one day. Here we demonstrate an efficient way to share such a very large data set as WMS using Mapserver scaled with Kubernetes. \n\nMapserver is used as engine of our WMS, because of it speed and ease of automation. In order to optimise the performance of the service and therefore the user experience, it is recommended to store the data in the right format, with the right file structure also being aware of limitations of storage, bucket or disk read speed. GDAL provides a set of options that can be executed in a single command to overwrite the original data with new, cloud optimized. It is usually good practice to store selected zoom levels as a cache, but for time series data that is enriched daily, the cache is not overwritten as new data arrives, but is incremented. \n\nDespite its popularity and advantages, WMS as a standard of serving data has its limitations. The potentially large disk read time is multiplied by the number of users sending requests. Tests using JMeter (100 users sending 100 GetMap requests in a loop) have shown that on a relatively strong processor (32CPU), the greatly increased traffic acts as a distributed denial-of-service (DDoS) - the server stops responding. \n\nThis problem is solved using Kubernetes (K8s) which allows metric-based automatic horizontal scaling of containerised applications, in this case – Mapserver. Prometheus as a K8s cluster monitoring tool allows custom metrics to be defined e.g., number of http requests per time interval. Prometheus makes it possible to distribute the traffic between newly created pods so that all requests can be answered. \n\nThe aim of the talk is to stimulate discussion, confront the idea with experts and demonstrate good practice in creating a publicly accessible WMS, with a focus on optimising speed under heavy source data conditions, supported by a working example and statistics.",
        "description": ""
      },
      {
        "title": "Surface Runoff Processes and Design of Erosion Control Measure in Landscape and Artificial Slopes",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Surface runoff is one of the processes with direct impact on water erosion. Surface runoff has two basic components: a) sheet runoff and b) rill runoff. Observation of these phenomena at various scales and then using mathematical models to describe their observations plays a key role for soil protection. One of the models developed to compute these phenomena is SMODERP, used for example in the flexible and adaptive approach to land management and landscape planning called Model of Living Landscape project. Innovative application of the SMODERP model (https://github.com/storm-fsv-cvut/smoderp2d) named SMODERP Line is presented in this contribution. SMODERP Line is accessible through various interfaces including OGC Web Processing Service (WPS) which can be easily integrated into user-defined processing pipelines or web applications. Usage of SMODERP2D Line will be demonstrated in the QGIS environment through a new OWSLib-based QGIS WPS Client Plugin (https://github.com/OpenGeoLabs/qgis-wps-plugin).\n\nThis contribution was supported by grant RAGO - Living landscape (SFZP 085320/2022) and Using remote sensing to assess negative impacts of rainstorms (TAČR - SS01020366).",
        "description": ""
      },
      {
        "title": "Visualization of accidental chemical release simulation",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Chemical incidents, such as accidents at heavy chemical plants or large-scale toxic gas leaks, are difficult to assess accurately because of the large spatial extent of the damage and the rapidly changing scope/level/target of the damage over time. These characteristics also make it hard to conduct experiments to recreate or simulate large-scale chemical incidents in real world. In the case of large-scale chemical accidents or release, post-incident damage assessment is as important as prevention, but spatial ambiguity makes it difficult to assess the extent of damage to victims, and there is little way to identify fake victims from real ones. \n\nIn this 5 year-long study, we aim to combine the results of a chemical diffusion model and the location data of mobile service subscribers on the incident spot over time. For this, FOSS4G based 3D geospatial web service using GeoServer, Postgresql/PostGIS, Cesium, etc. will be developed to assess the level of chemical exposure of each victim and calculate the level of damage based on it. \n\nIn 2022, the first year of the study, we developed a prototype that combines the time-dependent output of the chemical diffusion model with the time-dependent location data of individuals and successfully visualized it in a Web 3D globe. In the coming year, we'll further develop this system into an integrated risk assessment platform for chemical accidents by combining chemical exposure assessment model and damage calculation model.",
        "description": ""
      },
      {
        "title": "How I discovered, tested and fixed a bug in GeoDjango",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Here comes a developer story about **contributing to GeoDjango**.\n\nAn unfortunate combination of a valid, but unconventional spatial reference on the one hand, and \"smart\" logic for a mixed-geometry dataset: Geometries supposed to be located in Austria ended up in the Near East.\n\nInvestigation showed that GeoDjango's behaviour for returning the SRID of the dataset was not according to its documentation (see [Django ticket #34302](https://code.djangoproject.com/ticket/34302)). While fixing the issue, additionally, an incorrect type cast from `None` to string was discovered.\n\nIn this talk **you will also learn**:\n1. How to set up the GeoDjango test suite with a PostGIS docker container\n2. How the Django code review process looks like",
        "description": ""
      },
      {
        "title": "A QGIS plugin for local weather sensor data",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Ground-based weather sensor networks are essential in monitoring local weather patterns and climate. Integration of such data into GIS environments is critical to supporting manifold applications including urban planning, public health studies, and weather forecasting.\nThese networks use scattered geolocalized sensors to measure multiple atmospheric variables (e.g. air temperature, wind speed, precipitations). Often, data is distributed online by network managers which can be either local/national authorities, private companies, or volunteers. Due to the diversity of data providers, both formats and access patterns of meteorological sensor data are heterogeneous and the preprocessing tasks (e.g. temporal aggregations, spatial filtering) are generally time-consuming.\nGiven the above and to increase end-users exploitation of such sensor data, we present the development of an experimental QGIS plugin facilitating access and preprocessing of openly available data from ground-based sensor networks and enabling their direct use in QGIS. The plugin is designed to implement REST APIs connections and HTTP requests to download data. A user interface allows for selecting time intervals and types of observation to be downloaded. Once data is retrieved, the plugin provides options for filtering, outliers removal, time aggregation with summary statistics as well as observation mapping into a standard GIS layer. These functionalities are only partially available in similar existing QGIS plugins. The plugin leverages FOSS Python libraries for data handling including Pandas. The Dask parallel computing library is also exploited to speed up I/O operations on raw data.\nThe current version of the plugin is developed to retrieve and process weather sensor data provided by the Environmental Protection Agency of Lombardy Region (ARPA Lombardia), Northern Italy. The data retrieval is based on the Sodapy Python library, a Python client for the Socrata Open Data API. The plugin's work-in-progress source code is available at (https://github.com/gisgeolab/ARPA_Weather_plugin) released under MIT license. The plugin is being developed within the LCZ-ODC project (agreement n. 2022-30-HH.0) funded by Italian Space Agency (ASI), which aims to identify Local Climate Zones within the Metropolitan City of Milan.\nOngoing work includes the extension of the plugin functionalities to incorporate additional data providers, starting from other Italian regional ARPAs. The goal of this project is to provide a reproducible framework to access and handle weather data into QGIS, thus extending the capability of the software to support a wider range of practitioners and applications.",
        "description": ""
      },
      {
        "title": "Organising Mapathons",
        "type": "Lighting talk",
        "track": "Open Data",
        "abstract": "OpenStreetMap is an open source data which any one can access it free. This data is contributed by the local communities or individuals voluntarily. For them to gather together, we use mapathons to bring them for the mapping. A lot of data is added during these mapathons to help vulnerable people around the Globe.",
        "description": ""
      },
      {
        "title": "Synchronising data updates with Kart version control",
        "type": "Lighting talk",
        "track": "Open Data",
        "abstract": "Do you get regular data-drops from suppliers, and struggle with viewing changes between releases and keeping everything synchronised? In this talk we'll explain how from both a consumer and a publisher point of view you can use Kart to make your life easier.\n\n—\n\nWe’re drowning in data, but the geospatial world lags badly behind in versioning tools compared to our software counterparts. Kart (https://kartproject.org) is solving this with a practical open tool for versioning datasets, enabling you to work more efficiently and collaborate better.\n\nKart allows you to quickly and easily manage history, branches, data schemas, and synchronisation for large & small datasets between different working copy formats, operating systems, and software ecosystems.\n\nModern version control unlocks efficient collaboration, both within teams and across organisations meaning everyone stays on the same page, you can review and trace changes easily: ultimately using your time more efficiently.",
        "description": ""
      },
      {
        "title": "QGIS Data Versioning with Kart",
        "type": "Lighting talk",
        "track": "State of software",
        "abstract": "Maybe you've heard of Kart, the great new geodata versioning tool from the team at Koordinates? But did you know that Kart also has a QGIS plugin so you can do real data versioning without needing to leave QGIS?\n\nIn just 5 minutes we'll demonstrate how to import data into a new Kart repository, make and review some changes, merge a branch, and push everything to a remote server. All from QGIS!\n\n—\n\nWe’re drowning in data, but the geospatial world lags badly behind in versioning tools compared to our software counterparts. Kart (https://kartproject.org) is solving this with a practical open tool for versioning datasets, enabling you to work more efficiently and collaborate better.\n\nKart allows you to quickly and easily manage history, branches, data schemas, and synchronisation for large & small datasets between different working copy formats, operating systems, and software ecosystems.\n\nModern version control unlocks efficient collaboration, both within teams and across organisations meaning everyone stays on the same page, you can review and trace changes easily: ultimately using your time more efficiently.",
        "description": ""
      },
      {
        "title": "Adding static type hints to fiona",
        "type": "Lighting talk",
        "track": "State of software",
        "abstract": "Static type hints according to [PEP 484](https://peps.python.org/pep-0484/) (and its extensions) have been a part of Python since version 3.5, which came out in 2015. [Research from 2021](https://lp.jetbrains.com/python-developers-survey-2021/#stacked-chart-863) shows that 3 out of 4 Python developers already use optional type hinting at least sometimes in their projects. Time is ripe for static type hints to enter the FOSS4G Python world!\n\nA [GitHub issue on `fiona`'s issue tracker to add static type hints to the library](https://github.com/Toblerity/Fiona/issues/1125) recently gained some traction. Currently, it is [envisioned](https://github.com/Toblerity/Fiona/issues/1125#issuecomment-1424553816) to create type stubs for `fiona` 1.9 and possibly move the type hints into core `fiona` with the future 2.0 version.\n\nThis talk will give an overview on the current status of the effort to add type hints to `fiona`. Furthermore it will briefly discuss considerations and the reasoning behind design decisions taken up until then. Contributions to the effort are very much welcome – just take part in the [discussion on GitHub](https://github.com/Toblerity/Fiona/issues/1125).",
        "description": ""
      },
      {
        "title": "3D City Model-based Aid Station Operation Visualization and Management using Cesium.js",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "How do you run an aid station in case of a disaster? Scenarios are planned for each city, but there are limitations in applying them to actual aid station operations. In our presentation, we will present a case study on the development and simulation of a aid station management tool using digital twin technology and share various visualization techniques in a 3D city model environment. \n\nThe study site is Ulju-gun, a county of about 220,000 people in southern South Korea, with two nuclear power plants operating within a few kilometers of each other. Moving people to shelters to protect them in the event of a disaster such as a radioactive leak is very essential and crucial part of disaster management. \n\nThe aid station management tool presented in this presentation leverages ground-truth 3D modeling data of the shelter buildings that will be operational during a disaster to provide facility placement and editing capabilities. This allows relief tents to be automatically placed or edited based on the scenario. It also provides the ability to monitor the overall changes that may occur at the shelter through a dashboard, including real-time victim status, food, beverage, and medical support, supply status, shelter information, and disaster situation information.\n\nThe Cesium platform is used to service the data and the Three.js library is used to handle the viewing and placement of 3D model data in glTF format. Other open source implementations include React, Turf.js, Apache ECharts, and GeoServer.\n\nWe believe that the findings mentioned in this study provide a good example of how 3D city model-based shelter operations and visualization techniques can be applied to disaster preparedness systems to support effective decision-making and resource allocation.",
        "description": ""
      },
      {
        "title": "BikeDNA: A tool for Bicycle Infrastructure Data & Network Assessment",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Access to high-quality data on existing bicycle infrastructure is a requirement for evidence-based bicycle network planning, which can support a green transition of human mobility. However, this requirement is rarely met: Data from governmental agencies or crowdsourced projects like OpenStreetMap often suffer from unknown, heterogeneous, or low quality. Currently available tools for road network data quality assessment often fail to account for network topology, spatial heterogeneity, and bicycle-specific data characteristics. \n\nTo fill these gaps, we introduce BikeDNA, an open-source tool for reproducible quality assessment tailored to bicycle infrastructure data. BikeDNA performs either a standalone analysis of one data set or a comparative analysis between OpenStreetMap and a reference data set, including feature matching. Data quality metrics are considered both globally for the entire study area and locally on grid cell, thus exposing spatial variation in data quality with a focus on network structure and connectivity. Interactive maps and HTML/PDF reports are generated to facilitate the visual exploration and communication of results. \n\nBikeDNA is based on open-source python libraries and Jupyter notebooks, requires minimal programming knowledge, and supports data quality assessments for a wide range of applications - from urban planning to OpenStreetMap data improvement or transportation network research. In this talk we will introduce how to use BikeDNA to evaluate and improve local data sets on bicycle infrastructure, examine what BikeDNA can teach us on the current state of data for active mobility, and discuss the importance of local quality assessments to support increased uptake of open and crowd-sourced data.",
        "description": ""
      },
      {
        "title": "Pedaling towards Progress: An Analysis of Capital Bikeshare Trips in Washington D.C. using Open-Source Geospatial Tools",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In this presentation, we showcase a unique approach to analyzing Capital Bikeshare trips in Washington D.C. using Open-Source Geospatial (FOSS4G) tools and technologies. Our project involved loading trip data into a PostGIS database, utilizing the Valhalla routing engine and OpenStreetMap data to find the optimal routes between each pair of stations, and then constructing a topogeometry table to represent these routes. Using this topogeometry table, we are able to estimate the number of Capital Bikeshare trips that occur on each road in Washington D.C.\n\nThe use of FOSS4G tools and technologies allowed us to perform this analysis in a cost-effective and efficient manner, while also providing high-quality results. The results of our analysis have important implications for urban planning and mobility research, as they can be used to understand the patterns and impacts of bike-share usage in cities.\n\nOur presentation will provide an overview of the methodology used in our project, as well as a discussion of the results and their implications. We will also share our experiences using FOSS4G tools and technologies and provide insights on how these tools can be used in similar projects. This presentation is of interest to geospatial professionals, urban planners, and anyone interested in using FOSS4G tools for data analysis and mobility research.",
        "description": ""
      },
      {
        "title": "State of Oskari",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Oskari is used world wide to provide web based map applications that are built on top of existing spatial data infrastructures. Oskari offers building blocks for creating and customizing your own geoportals and allows embedding maps to other sites that can be controlled with a simple API. In addition to showing data from spatial services, Oskari offers hooks for things like using your own search backend and fetching/presenting statistical data.\n\nThis presentation will go through the improvements to existing functionalities and new features introduced in Oskari during the last year including:\n\n- Theme support\n- UI rewrite progress\n- Cloud compatibility improvements\n\nYou can try some of the functionalities Oskari offers out-of-the-box on our sample application: https://demo.oskari.org.\n\nLink: https://oskari.org",
        "description": ""
      },
      {
        "title": "Suomi.fi-maps - national service implementation with Oskari platform",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Suomi.fi-maps offers to the public administration and government agencies a centralized service for utilizing maps and location data. In the Suomi.fi-maps service, a user may compile their own map views from the map layers available in the service, as well as from their own objects and materials provided by service interfaces of their own organization. \nOskari platform is used to implement the Suomi.fi-maps system. Suomi.fi-maps is used to enable all the Finnish residents to use maps and the location data to find about the services they are interested in. \nIn addition to other open data the open materials of the National Land Survey may also be used: various terrain and background maps, property boundaries and aerial photographs. User may connect their own interfaces to the Suomi.fi-maps service or add their own objects to the map to be published.\nThis presentation describes with examples, how the Oskari platform and its features are used used to implement the Suomi.fi-maps service and lessons learned.",
        "description": ""
      },
      {
        "title": "Oskari Embedded Maps and integrations with RPC API",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Oskari (https://www.oskari.org, https://github.com/oskariorg) provides a super-easy-to-use tool for creating mobile friendly maps that can be embedded onto websites or used as is. When embedding the maps on existing websites one can utilise the RPC API to further leverage the capabilities of Oskari. The API allows for integrating with existing services and external data sources so that the end result will be a seamless spatially enabled service running on any modern web browser.\n\nWhile creating maps with Oskari requires no expertise in programming, utilising the RPC API requires basic knowledge of JavaScript. This talk will present the possibilities of Oskari RPC API among with some examples of live services created using it.",
        "description": ""
      },
      {
        "title": "Cartographic generalization with Open Source",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Generalization is a crucial topic in the map production process, describing the derivation of a map of a smaller scale from another one. It combines maintaining essential features and removing less important ones to offer a readable map. Often, this complex topic is reduced to a selection of attributes, creating label geometries, and simplifying line and area geometries.\n\nThe presentation shares the knowledge of the cartographer's toolkit by introducing the whole set of available generalization operators and showing less-known approaches for creating better maps. The entire collection of operators consists of simplification, smoothing, aggregation, amalgamation, collapse, merging, refinement, exaggeration, enhancement, and displacement, which can be implemented by algorithms.\n\nThe goal is to go behind the standards of creating centroids for labelling and using a Douglas-Peucker Algorithm for line simplification. A showcase of polygon simplification and creating label geometries are shown, demonstrating how to implement the operators using PostGIS with OpenStreetMap data. Several existing and working solutions for simplifying geometries and labels are presented to showcase possibilities.",
        "description": ""
      },
      {
        "title": "Cartographic design for vector tiles: Best practices and open-source recipes for beautiful maps",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Vector tiles are changing the way we create maps. Client-side rendering offers endless possibilities to the cartographer and has introduced new map design tools and techniques. Let’s explore an innovative approach to modern cartography based on simplicity and a comprehensive vector tiles schema.\n\nTake a tour of vector tiles cartography basics and learn about the latest trends through a number of examples illustrated with the MapTiler maps. Get an overview of best practices and learn about simple open-source recipes, towards advanced combinations of fills, patterns, fonts, and symbols. Selected layer parameters and style expressions will be discussed in a visual way and explained with basic syntax that you can take away.",
        "description": ""
      },
      {
        "title": "“Let’s put it on the map!”  a manifestation about : Cartographic interaction, a two-way dialogue between a user and a map mediated by a computing device.",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "These days we have an incredible amount of (open-source) geo-spatial data, remote sensing data and insights, plus the tools to share them with the world! But when building a web map application or dashboard we often end up with too cluttered visualizations, confusing jargon, scary technology or struggle in communicating with the geo-data illiterate. GIS technology can be hard to understand. \n\nHow do we design and build a map application showing a huge amount of geo-data accompanied by the elaborate functionality to discover it? \nAs GIS experts we think from a technological perspective, adding more and more buttons, layers, panels, pop-ups, legends, draw tools, scale-bars. But these GIS terms makes an application confusing, scary and technically hard to understand for the user.. \nOn the other hand, UX and IX designers think about usability, smooth experiences and helping users to easily navigate, see, use and interpret an application. But they lack the understanding of specific map related design requirements and map related interactivity. Here, the map is taken for granted and is often not well designed..\n\nI often find myself mediating between the GIS and cartographic professionals, web-developers, UX and IX designers and data-designers. I believe there is still a lot we can improve with each other! \nSo let’s bridge the gap and join the conversation about Interactive Cartography! In this talk I will give some clear useful examples. What is Interactive Cartography and what can we learn in this?  Be amazed with some simple examples which can quickly improve your web map application!",
        "description": ""
      },
      {
        "title": "Investigating war crimes, animal trafficking, and more with open source geospatial data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "At Bellingcat, a non-profit investigative organization in the Netherlands, we research war crimes, find tiger smugglers, monitor environmental degradation and track extremist hate. To do this, we use \"open sources\", including public databases, social media posts, and a wide range of geospatial data and tools. The use of these new online sources has dramatically changed investigative journalism and humanitarian accountability research in the past five years, and there remains tremendous potential for further development, especially in the geospatial realm.\n\nIn this talk, Bellingcat data scientist Logan Williams will present case studies from our research to illustrate how invaluable open source geospatial tools and data are for \"open source\" investigative research. Some of the most useful tools for investigators are designed for very different purposes, from academic meterology to outdoor recreation. Additionally, some of Bellingcat's own FOSS geospatial tools, based on Open Street Map and Copernicus satellite data, will be showcased. Finally, the talk will discuss opportunities for deepening the connections between the open source geospatial community and the open source investigation community.",
        "description": ""
      },
      {
        "title": "Creating Global Edge-Matched Subnational Boundaries",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "FieldMaps.io is a personal initiative originally created to develop offline interactive reference maps for humanitarian actors. However, in short time, it transitioned to helping develop common operational datasets that form the foundation for humanitarian response planning. Over the past 2 years, enormous effort has gone into releasing a high-resolution composite dataset able to be updated daily from multiple sources. This talk will cover 3 aspects of the project.\n\nAlgorithm\n\nEdge-matching resolves gaps and overlaps between hundreds of separate national data sources, requiring an algorithm that can perform at global scale. The resulting methodology uses something akin to a euclidean allocation raster applied to vector space, free of the compromises other approaches like generalization and snapping make. If you've ever been challenged by topology or data cleaning, you might find some insights into solving your own problems with the ideas contained here.\n\nPipeline\n\nThe edge-matching algorithm involves multiple complex and computationally intensive steps. Although Geopandas and GDAL usually come to mind when building multi-step geoprocessing scripts, PostGIS ended up being the fastest and best scaling tool for transforming gigabytes of vector data. I'll challenge your assumptions of how it can be used to create pipelines on both desktops and in the cloud, and make a case for why you should include it in your next project.\n\nSources\n\nA composite dataset is only as good as the foundations it builds upon, and great care was taken in selecting which sources were used in this project. For international boundaries, I'll go into detail about how I used only public domain sources to create an ISO 3166 compliant dataset. At the subnational level, I'll highlight two projects that each curate updated administrative boundaries: one by the United Nations, another by an academic institution.\n\nWhether you're a remote sensing specialist in search of the best topologically valid boundaries to run zonal statistics with, a Python developer frustrated by your pipelines constantly running into memory limits, or just want to run this tool on your own boundaries, I hope you come away from this talk with a valuable concept you can apply to your own work.\n\nData: https://fieldmaps.io/data\n\nTool: https://github.com/fieldmaps/edge-extender",
        "description": ""
      },
      {
        "title": "An Investigation into Updating the Building Stock Data for Municipalities in Baden-Württemberg, Germany",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "In the submitted paper, the topicality of the building stock in municipalities in Baden-Württemberg, part of the Federal Republic of Germany, is examined. Three municipalities were selected and included in the study according to the spatial type concept of the Federal Office for Building and Regional Planning (BBSR 2023): rural town 2,000-5,000 inhabitants, small town 5,000-20,000 inhabitants, medium-sized town, 20,000-100,000 inhabitants. The analysis concept is explained and the quantitative and qualitative results of the project, which is currently in its final phase, are presented. The aim is to use these results to derive and communicate recommendations for action for the municipalities, but also for the public surveying administration, in order to contribute to timely and effective action by municipal decision-makers and citizens through faster provision of geospatial data.",
        "description": ""
      },
      {
        "title": "Open resources and open standards for multi source marine twinning",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "Spatial data interoperability has been on the spot among the Open Geospatial Consortium members for almost 30 years, but the current moment is notable for several reasons. An enormous amount of data is growing exponentially due to the novel sensors that bring observations from previously inaccessible areas in such resolution. We can observe and explore the global ocean with modern computational resources and AI models. Federated Data Spaces initiatives emerge with the paradigm of multi-source data integration harmoniously supporting heterogeneous models.\nSpeakers will present recent advancements in the data mesh methods based on two environments endorsing open source implementations used for the integrations.\nFirst is the Federated Marine SDI (FMSDI) Pilot, which focuses on advancing the implementation of open data standards, architecture, and prototypes for use with the creation, management, integration, dissemination, and onward use of marine and terrestrial data services for the Arctic. Use cases developed in the recent phase of the FMSDI pilot further demonstrated the capabilities and use of OGC, IHO and other community standards in response to a grounding event and the evacuation of a cruise ship or research vessel in the Arctic. \nThe approach collated with Iliad - Digital Twin of the Ocean and its interoperability patterns model. Based on the specific requirements for data transfer, access and computation, it looks to generalise core architectural patterns with standard implementations. These patterns address the core issues of data publishing, aggregation and extensive analyses close to the data. Together, they enable a viable overall digital twin ecosystem. Data mesh of observations with data lakes and assembly are essential building blocks that allow the flow and synchronisation of data between different data owners. A open, common information model, defined on the domain-specific and well-known generic ontologies, Analysis Ready Data, and Essential Variables concepts, allows for the traceability of provenance and various expressions. It is a critical prerequisite to achieving data interoperability and explainable AI. Application packaging of processing chains allows for seamless compute-to-data, remote computation, or even mobile control when data is too big to flow. The computation is executed in a controlled environment, and the results harmonised for further use or available as decision-ready information. \n\tPresenters will describe these patterns and illustrate them with OGC and partners' open implementations (like OGC-NA, EDR, geoXACML, HubOcean sync API) from the projects.",
        "description": ""
      },
      {
        "title": "OGC API standards for geospatial data cubes",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "A presentation and demonstration of data cube functionality implemented based on OGC API Standards and draft Candidate Standards.\n\nIncluding:\n\n- OGC API - Tiles,\n- OGC API - Maps,\n- OGC API - Coverages,\n- OGC API - Discrete Global Grid Systems,\n- OGC API - Processes - Part 1: Core, and Part 3: Workflows and Chaining (\"Nested Processes\", \"Collection Input\", \"Collection Output\"),\n- OGC Common Query Language (CQL2)\n\nwith a focus on providing efficient access to analysis-ready sentinel-2 data and additional processing close to the data, in the context of wildfire risk assessment.",
        "description": ""
      },
      {
        "title": "Geo-Spatial meets Linked Data: open source solutions for semantic spatial data exchange",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The Ontology discipline made its way into the Computer Science domain in the\n1990s, filling a gap in the architecture aspect of a still infant engineering\ndomain. Its most visible impact happened around the industry consortium Object\nManagement Group (OMG), leading first to the Unified Modelling Language (UML)\nand later to the Model Driven Architecture (MDA). MDA became the base\ninfrastructure of data architectures and exchange mechanisms specified by\ninstitutions such as the Open Geo-spatial Consortium (OGC) or the European\nCommission (through the INPIRE directive).\n\nHowever, a parallel path has been treaded by the World Wide Web Consortium\n(W3C). First with the specification of the Resource Description Framework (RDF),\na new paradigm for data encoding leveraged on the WWW, and later with the Web\nOntology Language (OWL), a pragmatic approach to ontology encoding, building on\nRDF. This infrastructure developed by the W3C became known as the Semantic Web,\nand also as Linked Data, for the innovative paradigm through which it connects\ndisparate data sources and data domains.\n\nThe OGC would eventually approach the semantic web, specifying GeoSPARQL in\n2013, an ontology and query language for linked geo-spatial data. However,\ntechnologies supporting this new standard were slow in materialising.\n\nMore recently, the specification by the OGC of a new set of data standards based\non the OpenAPI technology set out a clear path for the convergence of\ngeo-spatial data with the Semantic Web. New software is emerging, opening\nan entirely new world to geo-spatial data provision, a clear step forwards in\npractically, usability and semantics.\n\nThis address starts by reviewing the core concepts of the Semantic Web and\nthen reviews state-of-the-art software for the management, publication\nand exploration of linked geo-spatial data. This addressed is targeted at SDI\nprofessionals and data scientists wishing to upgrade the semantics of the data\nthey create and use.",
        "description": ""
      },
      {
        "title": "Geo enabling your APIs with the location building blocks",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "The need to integrate geospatial data into products and services has resulted in a proliferation of Free and Open Source web APIs which often do not adopt any standards, thus requiring more development time and a lack of interoperability between solutions. For instance a bounding box has been written in multiple ways, depending on whether developers use the coordinates of the four corners, only upper left and lower right, latitude or longitude first, or some other variation.\n\nThe good news is that the Open Geospatial Consortium, a neutral, consensus-based organization, has been developing open standards for geospatial information. These standards are [developed as building blocks](https://blocks.ogc.org/), which means they could be easily incorporated into existing applications in order to enable a piece of geospatial functionality. The [location building blocks](https://blocks.ogc.org/register.html) are freely available to anyone to download and use.\n\nIn this presentation, we describe the conceptual model for the existing building blocks, which uses semantic annotations to define the different components. We also describe a practical example of how a building block could be integrated into an application and provide some resources for developers who want to build applications with the location building blocks.",
        "description": ""
      },
      {
        "title": "Demystifing OGC APIs with GeoServer: introduction and status of implementation",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "The OGC APIs are a fresh take at doing geo-spatial APIs, based on WEB API concepts and modern formats, including:\n\n* Small core with basic functionality, extra functionality provided by extensions\n* OpenAPI/RESTful based\n* JSON first, while still allowing to provide data in other formats\n* No mandate to publish schemas for data\n* Improved support for data tiles (e.g., vector tiles)\n* Specialized APIs in addition to general ones (e.g., DAPA vs OGC API - Processes)\n* Full blown services, building blocks, and ease of extensibility\n\nThis presentation will provide an introduction to various OGC APIs and extensions, such as Features, Styles, Maps and Tiles, STAC and CQL2 filtering. \nSome have reached a final release, some are in draft: we will discuss their trajectory towards official status, as well as how good the GeoServer implementation is tracking them, and show examples based on the GeoServer HTML representation of the various resources.",
        "description": ""
      },
      {
        "title": "Standardizing Satellite Tasking for Consumers",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "One decade ago, we saw the launch of the first earth observation cubesats by Planet Labs. In the years since we have seen hundreds of satellites launched, and dozens of startup companies launching taskable satellites. While this has led to incredible opportunities to leverage multiple sensors and sensor modalities, the massive increase of data has also created challenges in data management, discovery, and usage. The community driven SpatioTemporal Asset Catalog (STAC) specification was an important step forward in exposing data to users in a standard way that enables cloud-native workflows and has been successful across government and industry.\n\nThe process of actually tasking satellites, however, is still very much non-standard; each data provider exposes a unique API, if at all. Some data aggregators have created a single tasking API that proxies and translates to multiple data provider APIs, but this is still non-standard, and proprietary. \n\nElement 84 has been leading an effort to create a community standard API around how users order future data and how providers respond to those requests. Working with government groups, commercial satellite operators, and data integrators, we have hosted working sprints to develop a specification and open-source tooling demonstrating the power of a tasking API specification.\n\nThis talk will cover the current status of the community tasking API specification, future plans, and a demonstration of how to use the API to order data.",
        "description": ""
      },
      {
        "title": "How to secure pygeoapi and streamline protected OGC APIs",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "Securing a modern API in an effective way is critical to prevent unauthorized access and ensure the privacy and integrity of data. In general, there are three common mechanisms that can be used for API security: API keys, OAuth2/OpenID Connect, and JSON Web Tokens (JWT). Each of these mechanisms provides a different level of security and flexibility, depending on the requirements of the API. Modern OGC APIs are agnostic and rely completely on the adoption of OpenAPI security schemes so the implementers can use the mechanism that perfectly fits with their requirements. \nfastgeoapi is a new open-source tool designed to be an authentication and authorization layer on top of a vanilla pygeoapi that offers out-of-the-box a secured infrastructure easily pluggable and configurable through the a standard OpenID Connect protocol. \nThis talk aims to describe the recipe to configure and protect a vanilla pygeoapi with Keycloak and Open Policy Agent in order to publish secured OGC APIs in a standard manner.",
        "description": ""
      },
      {
        "title": "Overview of draft OGC Styles & Symbology \"SymCore\" 2.0 models & encodings",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "An overview of the Core Models and Encodings for Styling and Symbology - Part 1: Core (\"SymCore\") 2.0 draft candidate Standard.\n\nIn comparison to the current OGC Symbology Conceptual Model: Core Part (\"SymCore\") version 1.0, the new draft candidate Standard aims to better reflect its classification as an OGC Implementation Standard by including the requirements classes needed to enable the implementation of interoperable encodings, renderers (e.g., OGC API - Maps / OGC API - Tiles) and systems parsing and/or generating style definitions (e.g., OGC API - Styles, visual style editors, style transcoders).\n\nIt does so by featuring:\n\n- A modular logical and conceptual model for styling capabilities,\n- A minimal Core requirements class including clear extension mechanisms, through the definition of abstract Selectors, Symbolizers, and Expressions,\n- a basic Vector Styling requirements class,\n- a basic Coverage Styling requirements class,\n- requirements classes providing additional styling functionality,\n- a JSON encoding of the conceptual and logical model facilitating machine readability,\n- a CSS-inspired encoding of the conceptual and logical model facilating hand-editing.\n\nThe latest version of the draft is available in HTML (https://opengeospatial.github.io/ogcna-auto-review/18-067r4.html) or PDF (https://opengeospatial.github.io/ogcna-auto-review/18-067r4.pdf).\n\nThe official GitHub repository is at: https://github.com/opengeospatial/styles-and-symbology",
        "description": ""
      },
      {
        "title": "SOZip: using directly (geospatial) large compressed files in a ZIP archive!",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "SOZip ([Seek-Optimized ZIP](https://sozip.org)) is a new open specification on top of the ZIP archive format to compress one or several files organized and annotated such that a SOZip-aware reader can perform very fast random access (seek) within a compressed file.\nSOZip makes it possible to access large compressed files directly from a .zip file without prior decompression. It is not a new file format, but a profile of the existing ZIP format, done in a fully backward compatible way. ZIP readers that are non-SOZip aware can read a SOZip-enabled file normally and ignore the extended features that support efficient seek capability.\nWe will present how SOZip works under the hood and discuss about SOZip implementations, in particular in [GDAL](https://gdal.org), which make it possible for its downstream users, in particular [QGIS](https://qgis.org), to read seamlessly and efficiently large compressed files in [GeoPackage](http://www.geopackage.org/), [FlatGeoBuf](https://flatgeobuf.org/), or shapefile formats.",
        "description": ""
      },
      {
        "title": "Using Nix to build development environments as you always wanted",
        "type": "Talk",
        "track": "State of software",
        "abstract": "This talk is going to reveal the secret of building and running development or\nuser environments as you always wanted. Each of your projects can run in\nisolated, fully self contained environment, using the latest, or really old, or\nheavily customized geospatial packages regardless of Linux distro or Mac version you\nuse. You can have as many environments as you want, and the environment will change as you\nchange between your projects, branches or commits.\n\nNo, we are not going to run containers, Flatpaks of Snaps for that. We are going\nto enjoy the most advanced package manager [Nix](https://nixos.org/), the\nlargest collection of software in the world called [Nix packages\n(nixpkgs)](https://github.com/NixOS/nixpkgs), unique tooling they provide and\n[Geonix](https://github.com/imincik/geonix) [Devenv](https://devenv.sh/) projects built on top of that.",
        "description": ""
      },
      {
        "title": "Packaging Geospatial Software for Debian and Ubuntu Linux",
        "type": "Talk",
        "track": "State of software",
        "abstract": "This presentation will delve into the intricacies of packaging geospatial software for Debian Linux and its derivatives, including Ubuntu, OSGeoLive, and others.\n\nIt will begin by contrasting the differences between packaging for an operating system and application-level package managers. The presentation will then provide an introduction to the Debian GIS Team and their established practices for packaging, including resources for finding information. The focus will then shift to the crucial steps involved in preparing the software for distribution, such as creating metadata and dependencies, building the package, testing its functionality, and ultimately making it available to end-users for easy installation and use.",
        "description": ""
      },
      {
        "title": "Adding Quality Assurance to open source projects: experiences from GeoTools, GeoWebCache an GeoServer",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Working in large open source projects, with several people contributing to the code, can be challenging, especially trying to keep everyone on the same page, and generating code that has enough similarities to allow shared maintenance.\n\nThe advent of platforms like GitHub also made it easier for one time contributors to donate small and large bits of code to the platform, generating in the process a fair amout of “review stress” in the project maintainers.\n\nThe presentation covers how pull request checks, formatting and static analysis tools have been used to streamline basic checks in the code:\n\n* Testing the code on a variety of operating systems, Java versions and integrations with data sources before the code can be contributed to the project\n* Enforcing common formatting\n* Adding basic checks with CheckStyle\n* Locating obvious errors, leftover code, basic optimization issues using the Java compiler linting, ErrorProne, PMD and SpotBugs\n* Improving readability of the code as well as enforcing best practices and common approaches with the same tools.\n* Effects on the dynamics of code reviews\n\nThe presentation will cover all those aspects, with examples from the author’s experience with the GeoTools, GeoWebCache and GeoServer projects.",
        "description": ""
      },
      {
        "title": "Algorithm Talk: How to Re-project a Raster at Warp Speed",
        "type": "Talk",
        "track": "State of software",
        "abstract": "We will discuss the algorithms inside [geowarp](https://github.com/danieljdufour/geowarp), a high-performance and very low-level JavaScript library for reprojection, resampling and cropping of data from GeoTIFFs and other rasters.  This talk will be at the abstract algorithmic level and is suitable for everyone.  Here are some of the various algorithms that we will discuss:\n- [proj-turbo](https://github.com/danieljdufour/proj-turbo): fit an unknown reprojection function to a simple affine transformation \n- [fast-min](https://github.com/danieljdufour/fast-min)/[fast-max](https://github.com/danieljdufour/fast-max): calculating the range of your raster data leveraging the theoretical limits of the data types\n- near-vectorize: automatically determining the optimal resampling algorithm based on relative pixel size\n- [dufour-peyton-intersection](https://github.com/GeoTIFF/dufour-peyton-intersection): calculate the pixels of an arbitrary raster inside an arbitrary polygon\n- various resampling techniques including nearest, bilinear, vectorization, and box-based statistical methods",
        "description": ""
      },
      {
        "title": "Elephant in the room",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "There are no Free (as in Beer) and Open Source Cloud Datastores. Let's have an opinionated look at some of the better alternatives to store and modify, private and public data for spatial applications.\n\nHaving build FOSS cloud interfaces 4 Geo since forever I decided to look at the current state of data stores.\n\nWe have pretty much figured out how to do serverless in the cloud. Data at rest though is a completely different beast. The going gets tough the closer you work to the metal. There is an overwhelming multitude of formats, models and standards to chose from. Should we consider relational, document, and/or [column orientated] data files?\n\nWith too many to discuss we put the spotlight on some exciting new players such as bit.io and geoparquet.\n\nA recent Panorama (BBC) report asked; Is the cloud damaging the planet? Is it?\n\nIs there anything we can do? We want to share some best practices in regards to building data store interfaces as well as running these services at scale, and in production.",
        "description": ""
      },
      {
        "title": "Collaborative mapping without internet connectivity",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This talk is about a prototype that enables collaborative mapping without the need of any internet connectivity, only a local network is required. It runs fully in the browser, hence is cross-platform, it basically runs on any smartphone. The users form a peer-to-peer network in order to exchange their data.\n\nIt can be used in situations where there either is no internet infrastructure, it's spotty or it was destroyed. In the disaster response case, only a local network, without any server infrastructure, would be needed.\n\nIn the talk you'll learn about content-addressing, WebRTC and peer-to-peer networks and of course experience a live demonstration of the prototype.\n\nThe tech-stack is [Svelte] for the application, [OpenLayers] for displaying the map, [IPFS] for the storage, [libp2p] for the networking. The project is licensed under the Apache/MIT licenses.\n\n[Sevelte]: https://svelte.dev/\n[OpenLayers]: https://openlayers.org/\n[IPFS]: https://ipfs.tech/\n[libp2p]: https://libp2p.io/",
        "description": ""
      },
      {
        "title": "Measuring Water Level Changes in Reservoir using Jason-3 Altimetry Mission",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "This talk will describe the usage of Jason-3 Altimeter data, which records the topographic height of the surface of the earth every ~10 days, to help measure the changes in water level of reservoirs across the globe. The use of NASA Common Metadata Repository (CMR) API to download and subset is described along with navigating the maze of various Jason-3 Level-2 Products depending on the use-case. \nThis talk introduces to this open dataset and various other altimetry missions, to allow for multi-mission monitoring of reservoirs of the world. It further uses Free and Open Source Software (CMR Specification, Xarray) to pre-process the data for use.",
        "description": ""
      },
      {
        "title": "Development of maplibre applications in sveltekit",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Recently, [sveltekit](https://kit.svelte.dev/) is becoming a more popular framework for developing web application. It has been released as v1.0.0 last December. However, there are still not many use cases of developing maplibre applications in sveltekit compared to other frameworks like react. The author is involved in developing maplibre application with sveltekit in United Nations Development Programme ([geohub](https://github.com/UNDP-Data/geohub)), and also developing sveltekit based Web-GIS applications for water asset management at Eastern African countries ([watergis](https://github.com/watergis)). Hence, several useful maplibre boilerplate and components were developed in sveltekit during those projects' work. [watergis/sveltekit-maplibre-boilerplate](https://github.com/watergis/sveltekit-maplibre-boilerplate) is a template which can start developing maplibre application in sveltekit with minimuum source code.  Furthermore, [watergis/svelte-maplibre-components](https://github.com/watergis/svelte-maplibre-components) consists of various useful maplibre components to add more functionality easily to your web application (all components are documented [here](https://svelte-maplibre.water-gis.com/)). For instance, this component library provides you features of exporting maps, adding legends, styling maps, sharing maps, measuring distance and integrating with Valhalla api, etc. In this talk, these maplibre boilerplate and components will be briefly introduced.",
        "description": ""
      },
      {
        "title": "OpenDataCube Fast Deploy using Docker (Fast Cubing)",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Geospatial information from satellites is increasingly being used by decision-makers and scientists alike. However, there are two fundamental issues with this kind of data and related handling technologies. Firstly, data processing typically requires long time and a-priori expert knowledge compared to traditional data sources. Second, integrating satellite data into processing pipelines can be expensive in terms of software and application development efforts. The OpenDataCube (ODC) was created to help users solve these issues. Although ODC offers an alternative to being used as a data management application, its deployment is typically challenging for inexperienced users. Therefore, the primary purpose of this work is to provide potential ODC users with a ready-to-use, portable instance of this software.\nThe software is produced and published in a Docker container. In comparison to the traditional installation and configuration of the ODC, the tool proposed here provides an environment where the ODC database is already set up. It helps to avoid occasional conflicts that are common in SQL and Python installations. Even though other ODC implementations are available as a Docker container, the proposed solution has some advantages. Specifically, Python geospatial libraries are integrated in the container to support data manipulation. While available ODC instances are designed to process satellite images only (mainly Sentinel and Landsat data), the tool contains scripts to automatically adapt and ingest non-satellite data (e.g. raw ground-sensor network data, land cover/soil maps, etc.) by creating also metadata files when they are missing. The proposed solution makes available processing pipelines to re-grid, georeference and import datasets into the ODC. Both scripts and pipelines can be used through Jupyter notebook interfaces, which allow users also to perform exploratory analyses on the ingested data.\nThe source code is available at (https://github.com/gisgeolab/LCZ-ODC) and is released under a MIT license. The software is being developed within the LCZ-ODC project (agreement n. 2022-30-HH.0) funded by the Italian Space Agency (ASI) and aimed to identify Local Climate Zones within the Metropolitan City of Milan. Given the nature of the datacube development, this tool promotes Open Geospatial Consortium (OGC) compliant data sharing. Ongoing work focuses on the development and integration of additional pre-processing scripts with the aim of supporting the ingestion of additional types of data as well as providing new ready-to-use embedded processing functionalities.",
        "description": ""
      },
      {
        "title": "Application of FOSS4G for improving the environmental impact assessment process - a noise case",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Because environmental impact assessment(EIA) process is a combination of detailed fields that require a lot of expertise (e.g., noise, air pollution, odor, water pollution, ecological environment, living environment, etc.), despite its long history, the process is still complex and slow, and it is not easy to break away from the document/drawing-centered work process. Since the nature of the environment involves many geographic/spatial context, if it can be assisted with a spatio-temporal system, it can be expected to show very high efficiency compared to the current process.\n\nTo verify the feasibility of such a system, we adopted a FOSS4G-based approach and developed a pilot system in this study. Specifically, we used GeoServer and Postgresql/PostGIS for handling and providing data spatially, and Cesium for 3D geospatial based visualization. We focused on the design and implementation of APIs to assemble the sub-processes of EIA, as well as the visualization and UI of the pilot system. \n\nThis system demonstrates how the noise propagate during and after the construction in an interactive way. We expect the system will increase the non-expert stakeholder's understanding of noise propagation visually. \n\nThrough this presentation, we will discuss our findings implemented in a EIA process centered on the noise, from the first step of applying for approval from the civil/construction operator to the last step of deriving the final evaluation opinion by the noise expert in charge, and provide clues to the future of Digital EIA. \n\nIn the future, we believe that the expansion to other EIA media and the smooth implementation of current legal and administrative tasks will make it a system that can be used in the field.",
        "description": ""
      },
      {
        "title": "Creating a Peaceful and Profitable Society: FOSS4G and New Employment Opportunities",
        "type": "Keynote",
        "track": "Community & Foundation",
        "abstract": "We will explore how Re:Earth as a digital public good could support a \"Peaceful Profitable Society\" and create new employment opportunities.\nRe:Earth is an open source platform built around a geographic information system that digitally represents geospace and enables analysis and visualization of cities and regions. The use of such digital public goods offers opportunities to develop new ways of working and improve their own lives, especially for the socially vulnerable.\n\nIn particular, we will explore the potential for vulnerable populations, such as refugees and single mothers, to use Re:Earth to pave the way for self-empowerment. We will also delve into how digital public goods such as Re:Earth can impact society as a whole, especially how they can be a tool for the vulnerable to improve their own lives and contribute to the realization of a \"society where peace is profitable\".\n\nThis speech will provide insight into how such digital public goods can impact individual lives and society as a whole, and how they can help shape a \"society where peace is profitable\".",
        "description": ""
      },
      {
        "title": "Geochicas: From SOTM to FOSS4G, a Geospatial journey",
        "type": "Keynote",
        "track": "Community & Foundation",
        "abstract": "Geochicas is a initiative born in State of the Map Sao Paolo and adopted by FOSS4G communities over the past years. We would like to share with you what had happened in the last couple of years and what we foresee in the future of the initiative. How Geochicas is part of a larger ecosystem of siblings organizations working towards having a more balanced presence of women and minority groups in the Geospatial communities.",
        "description": ""
      },
      {
        "title": "QGIS Feature Frenzy - both for the Long-term release (3.28) and the Latest release (3.32)",
        "type": "Talk",
        "track": "State of software",
        "abstract": "QGIS releases three new versions per year and each spring a new long-term release (LTR) is designated. Each version comes with a long list of new features. This rapid development pace can be difficult to keep up with, and many new features go unnoticed. This presentation will give a visual overview of some of the most important new features released over the last calendar year. \n\nIn March of 2023 a new Long-term release was published (3.28), and shortly before FOSS4G, the latest stable version of QGIS (3.32) will be released. I will start by comparing the new LTR (3.28) to the previous (3.22). Here I will also summarize by category the new features found in the latest LTR (GUI, processing, symbology, data providers etc.).\n\nI will then turn my attention to the important new features found in the latest releases (3.30 & 3.32). Each highlighted feature will not simply be described but will be demonstrated with real data. The version number for each feature will also be provided. If you want to learn about the current capabilities of QGIS, this talk is for you!  \n\nPotential topics include: Annotation layers * GUI enhancements * New Expressions * Point cloud support * Print layout enhancements * New renderers and symbology improvements * Mesh support * 3D * Editing",
        "description": ""
      },
      {
        "title": "QGIS 3D, point cloud and elevation data",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Since we have introduced QGIS 3D in 2017, it has gone through major improvements. In addition to new features, several new data formats have been also integrated to QGIS.\n\nThis presentation will cover the latest improvements made as result of the recent crowdfunding efforts to introduce point cloud processing, enhance 3D maps for elevation data.",
        "description": ""
      },
      {
        "title": "Mergin Maps: A Year of Progress",
        "type": "Talk",
        "track": "State of software",
        "abstract": "**Mergin Maps** has become a popular open-source GIS platform for collecting, managing and sharing geospatial data. In the past year, we have introduced several new features and improvements to the platform. Our goal is to provide a flexible and powerful GIS solution that is accessible to users of all levels, from seasoned professionals to those just getting started. In this talk, we will highlight the latest developments and demonstrate how they can benefit users in various fields.\n\nOne of the significant updates is the **introduction of workspaces**, which allows users to organize their projects, data, and users in a hierarchical structure. This new feature streamlines the management of multiple projects and simplifies the process of adding and removing users.\n\nAnother update is the implementation of **tracking**, which enables users to collect and visualize location data. This feature is particularly useful for tracking vehicles, equipment, and personnel in the field, and can be customized to include various attributes.\n\nFinally, we will discuss the Mergin Maps roadmap for the future, including plans for new features, enhanced integrations and community-driven development. We believe these changes will make Mergin Maps more accessible and user-friendly for everyone, regardless of their level of experience. \n\n**Whether you are a seasoned GIS professional or new to the world of geospatial data, this talk will provide valuable insights into the latest developments in Mergin Maps and its potential for your work.**",
        "description": ""
      },
      {
        "title": "QField news - stakeout, measurements, printing and many more",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The mobile application QField is based on QGIS and allows fieldwork to be carried out efficiently based on QGIS projects, offline or online. Developments in recent months have added additional functions to the application that are useful for fieldwork. Examples are used to present the most important new features. Discover the most recent features like 3D-layers handling, printing of reports and atlases, elevation profiling of terrain and layers, multi-column support in feature form, azimuth values in the measuring tool, locked screen mode, the QR-code reader, stakeout functionalities, the official release of the iOS version and many more.",
        "description": ""
      },
      {
        "title": "Traffic Analysis with QGIS and GTFS: GTFS-GO",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GTFS is stands for General Transit Feed Specification, which is developed by Google and used for describing schedules of public transpotation. A bunch of dataset is distributed in the world and GTFS includes geospatial information - stops and routes. To utilize such intresting data, we have developed GTFS-GO - QGIS plugin to process GTFS. You can translate GTFS to GIS data and visualize them by GTFS-GO. The plugin can be used for analyzing public transportaion by aggregating traffic frequencies on each stop or route. In this talk, you can see how GTFS is visualized or analyzed by using GTFS-GO on QGIS.",
        "description": ""
      },
      {
        "title": "QFieldCloud - seamless fieldwork for QGIS",
        "type": "Talk",
        "track": "State of software",
        "abstract": "QFieldCloud enables the synchronisation and consolidation of field data collected by teams using QField. From small individual projects to large data collection campaigns, the platform allows you to manage the collaboration of multiple people on the same project, assign different roles and rights to different users, work online and offline, and keep track of changes made. In 2022, QFieldCloud was testable as a beta version. Already during the beta phase, over 40,000 registered users synchronised their projects via the platform. Beginning of 2023, the official version was released. A brief overview of how QFieldCloud works and how the platform is built is given.",
        "description": ""
      },
      {
        "title": "Felt Maps for Sharing and Collaboration",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Introducing Felt, a new map sharing and collaboration product.\n\nWe connect closely with the current ecosystem of open source mapping tools and make it easier to work together with colleagues inside and outside mapping. In this talk, we will show:\n\n- How current users of programs like QGIS bring Felt into their workflows\n- Where Felt lets them expand into new areas like community feedback\n- How we’ve used and expanded core OSS libraries like MapLibre, GDAL, Pelias, and Tippecanoe\n- Why we’re pushing forward emerging formats and standards like PMTiles\n\nSession attendees will gain an important new tool for their stack, a product made for extending the reach of existing open source mapping tools and improving collaborative map-making beyond analysis.",
        "description": ""
      },
      {
        "title": "OSGeoLive project report",
        "type": "Talk",
        "track": "State of software",
        "abstract": "OSGeoLive is a self-contained bootable DVD, USB thumb drive or Virtual Machine based on Lubuntu, that allows you to try a wide variety of open source geospatial software without installing anything. It is composed entirely of free software, allowing it to be freely distributed, duplicated and passed around. It provides pre-configured applications for a range of geospatial use cases, including storage, publishing, viewing, analysis and manipulation of data. It also contains sample datasets and documentation. OSGeoLive is an OSGeo project used in several workshops at FOSS4Gs around\nthe world.\n\nThe OSGeoLive project has consistently and sustainably been attracting contributions from ~ 50 projects for over a decade. Why has it been successful? What has attracted hundreds of diverse people to contribute to this project? How are technology changes affecting OSGeoLive, and by extension, the greater OSGeo ecosystem? Where is OSGeoLive heading and what are the challenges and opportunities for the future? How is the project steering committee operating? In this presentation we will cover current roadmap, opportunities and challenges, and why people are using OSGeoLive.\n- Project page https://live.osgeo.org\n- Link to the presentation https://live.osgeo.org/en/presentation.html",
        "description": ""
      },
      {
        "title": "OSGeo and OGC MoU: one year later!",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "In January 2022, OSGeo and  [OGC](https://www.ogc.org/) signed a new and [updated version of the Memorandum of Understanding (MoU)](https://www.ogc.org/pressroom/pressreleases/4709) that aims to maximize the achievement of the mission and goals of the two organizations: promoting the use of Open Standards and Open source software within the geospatial developer community. Identifying open source technologies that could be used as Reference Implementations for OGC Standards and validating OGC compliance tests are examples of activities that can take place within the scope of the agreement.\n\nMore than one year after the agreement was signed and almost one year after it was introduced to the OSGeo community in a keynote at FOSS4G 2022, this presentation will summarize all activities accomplished and future plans, including the establishment of the OSGeo Standards Committee within OSGeo and the organisation of the [3rd joint code sprint](https://developer.ogc.org/sprints/20/), in Switzerland, together with the Apache Software Foundation.\n\nThe presentation will also reiterate the benefits of the new agreement, which allows OSGeo charter members to represent the priorities of OSGeo in the development of OGC Standards and supporting documents and services.",
        "description": ""
      },
      {
        "title": "Türkiye and Syria Earthquakes Mapping Response",
        "type": "Keynote",
        "track": "Open Data",
        "abstract": "Powerful earthquakes hit southern Turkey and Syria on 6 February 2023. These earthquakes in Turkey and Syria caused thousands of casualties and destroyed cities. Geospatial infrastructure is critical to respond to these earthquakes during rescue operations, humanitarian effort as well as planning recovery activities.\n\nYercizenler coordinated mapping activation with the collaboration of Humanitarian OpenStreeMap team to improve open geodata infrastructure in the earthquake affected region and supporting humanitarian response in the scope of mapping. \n\nTürkiye Earthquakes Mapping Response aims to complete open map data infrastructure before and after the event in affected areas. This response is structured with following workstreams; Remote Mapping, Post-disaster Field Data Collection, Global Community Activation and Geo-data Integration.\n\nIn this talk; we will talk about how open data and community activation helped save lives after earthquakes, what challenges we faced and what we have learnt during the Türkiye Earthquakes mapping Response effort.",
        "description": ""
      },
      {
        "title": "Making of a community - beyond the recipe",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "In our allocated 15 minutes, we would like to take you on a trip following the winding roads of building a community, the Romanian geospatial community: geo-spatial.org. We want to share our story, beyond our geodata and knowledge portal, to the very core of the values and principles that have guided us through difficult times and made our overcame challenges even brighter. \nIn our more than a decade of existence, we’ve organised over 25 national FOSS workshop, a regional FOSS4G in 2013 and a global FOSS4G in 2019, we’ve initiated collaborative geo-related projects and managed to infuse the geospatial component in various non-spatial organisations, such as the ones in education or investigative journalism.",
        "description": ""
      },
      {
        "title": "The power of collective intelligence: HOT’s approach to open tech and innovation",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "Are you interested in open geospatial tech for humanitarian purposes?  Have you ever wondered who the people behind the geospatial technologies are? The collective brains? In this talk, we will tap into the power of the tech collective at Humanitarian OpenStreetMap Team, share our experience, excite you about joining the collective and get some hands-on input from YOU!\n\n\nMeet two members of the Humanitarian OpenStreetMap Team (HOT) - Petya & Synne. We are a global team that operates with four regional Open Mapping Hubs: https://www.hotosm.org/hubs/. In developing and improving open geospatial tech for humanitarian purposes, our vision is to creatively meet the needs of the communities through collective, community-centered efforts.  Our mission? To amplify community-led innovation for impact through diversity, creativity & passion!\n\n\nSome of the stories we will share will be about our experiences and lessons learnt on collective projects and products (https://github.com/hotosm/) ranging from the HOT Tasking Manager collective , collaborating with Kathmandu Living Labs (KLL) in Nepal, to development of a Field Mapping Tasking Manager (FMTM). We will also share some of the boldest regional activities, including OpenStreetMap (OSM) Hackfest in Asia Pacific and the Ideas Lab in Eastern and Southern Africa.\n\n\nYou will also find out how YOU can get involved by contributing to open geospatial tech. Expect a short participatory exercise [the collective brains/ power of collective intelligence]  during this session!",
        "description": ""
      },
      {
        "title": "How to join OSGeo (for projects)",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "Welcome to the Open Source Geospatial Foundation, proud hosts of FOSS4G, and advocate for free and open source geospatial software everywhere. This is a call out to open source software developers; please join OSGeo and help us help you!\n\nJoin OSGeo today:\n\n* Even just listing your project on the osgeo.org website is a great first step. Help us promote your technology so users can discover and enjoy your software.\n* The OSGeo “community program” gives project teams a chance to join the foundation with an emphasis on supporting innovation and new projects. The foundation provides some direct support, assistance along with endorsement and recognition from our board.\n* For established projects please join our “incubation program” to be recognized for excellence and as a full OSGeo committee.\n\nUnlike other foundations OSGeo does not require that you give up or transfer any Intellectual Property; we simply ask that you be spatial, open-source, and open to participation.\n\nThis presentation gives clear instructions on how to join OSGeo, and representatives from recent successful projects will be on hand to answer your questions.",
        "description": ""
      },
      {
        "title": "Scaling GeoServer in the cloud: clustering state of the art",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GeoServer deployments in the cloud and kubernetes are becoming the norm, while the amount of data published is also growing, both in terms of layers and size of data. As a result, the need for scaling up is becoming more and more common.\n\nThis presentation covers GeoServer clustering approaches, comparing the available options and their suitability to different environments. We will cover:\n* Managing the GeoServer configuration, stable configuration with planned upgrades versus dynamic runtime changes.\n* Deployment options (monolithic, separate tiling, microservice oriented)\n* Dynamic configuration clustering with JMS, external database storage, and distributed memory.\n\nAttend this presentation to get an update on GeoServer cloud and clustering options, and pick the option that is the best match for your specific use case.",
        "description": ""
      },
      {
        "title": "Comparison of GeoServer configuration deployment options",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The goal of this presentation is to give an overview of the different options available for deploying a GeoServer configuration to different environments. In addition to the common data_dir folder deployment option, we will explore the possibilities offered by existing extensions and by the REST API, including different client libraries around it. We will also discuss the advantages that can be brought by Terraform for this use case.",
        "description": ""
      },
      {
        "title": "EOReader - Remote-sensing opensource python library for optical and SAR sensors",
        "type": "Talk",
        "track": "State of software",
        "abstract": "**EOReader** is a remote-sensing opensource python library reading optical\nand SAR constellations, loading and stacking bands, clouds, DEM and spectral indices in a sensor-agnostic way.\n\n| **Optical**                                                                                                                                                                                                                                                                                                   | **SAR**                                                                                                                                                                          |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `Sentinel-2` and `Sentinel-2 Theia`<br>`Sentinel-3 OLCI` and `SLSTR`<br>`Landsat` 1 to 9<br>`Harmonized Landsat-Sentinel`<br>`PlanetScope`, `SkySat` and `RapidEye`<br>`Pleiades` and `Pleiades-Neo`<br>`SPOT-6/7`<br>`SPOT-4/5`<br>`Vision-1`<br>`Maxar`<br>`SuperView-1`<br>`GEOSAT-2` | `Sentinel-1`<br>`COSMO-Skymed`<br>`TerraSAR-X`, `TanDEM-X` and `PAZ SAR`<br>`RADARSAT-2` and `RADARSAT-Constellation`<br>`ICEYE`<br>`SAOCOM`<br>`Capella` |\n\nIt also implements **sensor-agnostic** features, such as `load` and `stack` many bands:\n- satellite bands (optical or SAR)\n- spectral indices\n- clouds\n- DEM\n\n## Context\n\nAs one of the [Copernicus Emergency Management Service](https://emergency.copernicus.eu/) Rapid Mapping and Risk and Recovery Mapping operators, \n[SERTIT](https://sertit.unistra.fr/) needs to deliver geoinformation (such as flood or fire delineation, landslides mapping, etc.) based on multiple EO constellations.\n\nIn rapid mapping, it is important to have access to various sensor types, resolutions, and satellites. Indeed, SAR sensors are able to detect through clouds and during nighttime while optical sensors benefit from of multi spectral bands to better analyze and classify the crisis information.\n\nThis is why SERTIT decided to decouple the sensor handling from the extraction algorithms: the latter should be able to ingest semantic bands without worrying about how to load the specific sensor band or in what unit it is.  \nThe assumption was made that all the spectral bands from optical sensors could be [mapped bands]( https://eoreader.readthedocs.io/en/latest/optical_band_mapping.html) between each other, in addition to the natural mapping between SAR bands.\n\n## Examples\n- [Why EOReader?](https://eoreader.readthedocs.io/en/latest/notebooks/why_eoreader.html)\n- [Basic tutorial](https://eoreader.readthedocs.io/en/latest/notebooks/base.html)\n- [Optical data](https://eoreader.readthedocs.io/en/latest/notebooks/optical.html)\n- [SAR data](https://eoreader.readthedocs.io/en/latest/notebooks/SAR.html)\n- [VHR data](https://eoreader.readthedocs.io/en/latest/notebooks/VHR.html)\n- [Water detection on multiple products](https://eoreader.readthedocs.io/en/latest/notebooks/water_detection.html)\n- [STAC](https://eoreader.readthedocs.io/en/latest/notebooks/stac.html)",
        "description": "[EOReader](https://github.com/sertit/eoreader) is a remote-sensing opensource python library reading optical and SAR sensors, loading and stacking bands, clouds, DEM and index in a sensor-agnostic way.\n\nThe main goal of EOReader is to simplify the access to a numerous offer of remote sensing data, providing easy to understand and sensor-agnostic functions to read, load and stack multiple bands and indices (and even DEM or cloud bands).\n\nFor example, one important feature of EOReader is the mapping of optical bands in order to access them sensor-agnostically (i.e. with RED band, you can access the band number 4 of Sentinel-2, 8 of SENTINEL-3 OLCI, 1 of Pleiades, see [here](https://eoreader.readthedocs.io/en/latest/optical_band_mapping.html) for more information)\n\nWe wanted also to eliminate tricky steps such as orthorectification or geocoding for SAR and Sentinel-3 data. By automating it, we allow more users to develop applications with remote-sensing data.\n\nLast but not least, we want to provide an opensource library using cutting-edge technology, this is why we are using xarrays and support dask.\n\nThe Github repo can be found [here](https://github.com/sertit/eoreader). The API documentation can be found [here](https://eoreader.readthedocs.io/en/latest/)."
      },
      {
        "title": "Geological Service of Kosovo - Legal Infrastructure, Responsibilities & Technical - Analytical Research Capacities in Geology",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The presentation will be focused on the elaboration of the relevant legal basis of the Geological Service of Kosovo. In addition, the description of the main responsibilities will be made, as well as the elaboration of technical analytical capacities which enable the development of research in the field of geology.",
        "description": ""
      },
      {
        "title": "Introducing Terra Draw: A JavaScript Library To Draw On Any Web Map",
        "type": "Talk",
        "track": "State of software",
        "abstract": "If you have ever had the experience of having to write code to draw on web maps, you'll know how painful the process can be - especially when situations get more complex.\n\nTerra Draw is an open source JavaScript library that provides a new way to add drawing functionality to a host of web mapping libraries, including Leaflet, OpenLayers, Google Maps, MapboxGL JS and MapLibreGL JS.\n\nThe library provides a selection of built in modes that 'just work' across different mapping libraries. These features include elementary drawing tools like point, line and polygon, as well as supporting more advanced concepts like snapping, rotation and scaling. \n\nTerra Draw is also designed to be extendable so that you can write your own custom modes and adapters (thin wrappers for each mapping library). The architecture of the library means that any mode work can work with any adapter and vice versa creating a strong multiplier affect as new modes and adapters are written. This decoupling has the added benefit that drawing libraries can be swapped out without breaking your app!\n\nThe talk will examine the history of the library, how to get started, and also an opportunity to hear more about the future of Terra Draw.",
        "description": ""
      },
      {
        "title": "vis.gl, the powerful framework suite behind deck.gl and kepler.gl",
        "type": "Talk",
        "track": "State of software",
        "abstract": "[Slides](https://docs.google.com/presentation/d/1RgI-t9U5dlTmmo2Yq--m8vICxFSUn_YIdxUrudBXNZ4/edit?usp=sharing)\n\n[vis.gl](https://vis.gl) is a suite of composable, interoperable open source geospatial visualization frameworks (GPU powered) centered around deck.gl. During the last 4 years vis.gl has played an essential role in the development of geospatial applications during the last 4 years. \n \nWith close to 100K daily downloads from npm, it’s widely used today in many areas and industries: from academics teams, to enterprise companies like Uber, Foursquare, CARTO, Google or Amazon.\n\nThe open governance of vis.gl has guaranteed the evolution and maintenance of the framework, the project [joined](https://openjsf.org/blog/2022/06/07/openjs-world-2022-openjs-foundation-welcomes-urban-computing-foundation-vis-gl-and-kepler-gl/) the OpenJS foundation in 2022 with the main goal of re-enforcing the open evolution of the project.\n\nDuring this talk we’ll do a quick and high level introduction of the most important [frameworks](https://vis.gl/frameworks) that belong to this suite (deck.gl, kepler.gl, loaders.gl, etc.), we’ll do an update of the most important features and milestones achieved in the last year, and we’ll share the strategy and direction for the next year.",
        "description": ""
      },
      {
        "title": "Gisquick: Let’s share (Q)GIS much quicker",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Gisquick (https://gisquick.org/) is an open-source platform for publishing GIS projects on the web. A GIS project is defined by a QGIS project file including data sources (files, databases, even virtual layers) and symbology defined in the QGIS desktop application using the styling tool.\n\nWith the help of the Gisquick plugin for QGIS, it is possible to upload the data to the Gisquick server and host the map. \n\nGisquick is a fully featured hosting platform, where the project administrator can fine-tune web publishing attributes, set predefined scales, bounds, or visibility. Also group permissions on the project level as well as layer level (query, edit, export) may be defined. Vector data - geometry and attributes - can be edited directly on the web. \n\nInterface between the frontend and backend is based on open standards (OGC WMS and WFS). The mapping application has standard components from the GIS point of view: decent layer switcher, attribute table, zoomable map, printing tool (based on QGIS templates), and customizable feature-detail form.\n\nAll this can be tested on our demo platform https://demo.gisquick.org/ - but you can also make your own deployment via Docker images. Gisquick is open-source software published under the GNU GPL.\n\nIn the presentation, we are going to present various features of Gisquick and show practical examples and discuss technologies used for its development.",
        "description": ""
      },
      {
        "title": "G3W-SUITE and QGIS integration: state of the art, latest developments and future prospects",
        "type": "Talk",
        "track": "State of software",
        "abstract": "G3W-SUITE is a modular, client-server application (based on QGIS-Server) for managing and publishing interactive QGIS cartographic projects of various kinds in a totally independent, simple and fast way.\n\nAccessing administration, consultation of projects, editing functions and use of different modules are based on a hierarchic system of user profiling, open to editing and modulation.\n\nThe suite is made up of two main components: G3W-ADMIN (based on Django and Python) as the web administration interface and G3W-CLIENT (based on OpenLayer and Vue) as the cartographic client that communicate through a series of API REST.\n\nThe application, released on GitHub with Mozilla Public Licence 2.0, is compatible with QGIS LTR versions and it is based on strong integration with the QGIS API.\n\nThis presentation will provide a brief history of the application and insights into key project developments over the past year, including:\n * new editing functions and greater integration with QGIS tools and widgets in order to simplify the preparation of web cartographic management systems\n * QGIS embedded project management\n * WMS-T and MESH data management and integration of TimeSeries functions\n * on/off management for the individual symbology categories as in QGIS\n * integration of the QGIS Processing API to allow the integration of QGIS analysis modules and perform online geographic analysis\n * structured management for log consultation on three levels: G3W-SUITE, QGIS-SERVER and DJANGO\n\nThe talk, accompanied by examples of application of the features, is dedicated to both developers and users of various levels who want to manage their cartographic infrastructure based on QGIS",
        "description": ""
      },
      {
        "title": "State of PDAL",
        "type": "Talk",
        "track": "State of software",
        "abstract": "PDAL is Point Data Abstraction Library. It is a C/C++ open source library and applications for translating and processing point cloud data. It is not limited to LiDAR data, although the focus and impetus for many of the tools in the library have their origins in LiDAR. PDAL allows you to compose operations on point clouds into pipelines of stages. These pipelines can be written in a declarative JSON syntax or constructed using the available API. This talk will focus on the current state of the PDAL Pointcloud processing library and related projects such as COPC and Entwine, for pointcloud processing. Coverage of the most common filters, readers and writers along with some general introduction on the library, coverage of processing models, language bindings and command line based batch processing. First part will be covering new features for current users. Some discussion of installation method including Docker, binaries from package repositories, and Conda packaging. For more info see https://pdal.io",
        "description": ""
      },
      {
        "title": "Lidar data processing, management and visualisation in a browser using Pointview",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Pointview is a product developed by IT34 for working with Lidar and Photogrammetry data. It gives the user the possibility to upload,  process, visualize and work with data. \n\nLidar data formats such as LAS, LAZ, E57 can be uploaded, processed and visualized in the browser.\n\nPhotogrammetry: Images from drones or video from phones can be uploaded, processed into a 3d point cloud and visualized in a browser. \n\nIn addition, data can be captured using our SmartSurvey app that captures video which is used for building a 3d pointcloud, together with an ortophoto and dem. The data is later available for visualization in Pointview or in QGIS though a WFS service.\n\n\nMoreover, the system offers a complete management system where the user can create projects for organizing the data, can share the data with other users and manage the access.\n\nThe system uses various data processing workflows for data processing based on open source components such as:\nPostgresSql + Postgis for storing the data and for geometry based analysis.\nOpenLayers for visualizing the images and ground control points results as rasters\nGeoserver for publishing data as WMS/WFS, \nQGis for visualizing data,\nPDAL for lidar data processing, \nGDAL for raster data processing, \nCloudCompare for lidar data processing, \nPotree for Data Visualization",
        "description": ""
      },
      {
        "title": "Expanding Geospatial Data Access: Lessons from Radiant MLHub and the Shift to Source Cooperative",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Radiant Earth is building a new data sharing utility called Source Cooperative that aims to make it trivially easy for data providers to publish data on the Internet. Source Cooperative is the next generation of Radiant MLHub which Radiant Earth built to share Earth observation training datasets. In this talk, we will share lessons learned about sharing data from working with NASA, Planet, Sinergise, AWS, Microsoft, and others. We will also share how we’re applying those lessons to create Source Cooperative.",
        "description": ""
      },
      {
        "title": "Let's defense my country using FOSS4G!",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This talk is about the current state of MilMap and its ongoing development. MilMap is a military geo-portal system widely and successfully used in every sectors of Korean military. The system is now undergoing major change from geo-portal to military digital twin system. \n\nMilMap is developed on top of numerous open source projects such as PostGIS, GeoServer, GeoWebCache, Cesium, OpenLayers, mago3D, OpenGXT. The system provides several functionalities like POI search, geospatial data search, layer control, satellite image search and download, spatial terrain analysis, coordinates reading, and map notes, to the military officers through the intranet. Although the system provides geospatial analytics functions through OGC WPS(Web Processing Service), the current system is basically a web based 3D GIS for data viewing and printing. Thanks to MilMap, military officers can now access the huge amount of geospatial data(maps, imagery, 3D, POI, and others) in their browser without installing additional software. \n\nMilMap is now undergoing major development to be a more customized, automated, and analytical system. The future MilMap will support user data uploading for intelligence sharing, more bespoke battle field analysis and others. In the long run, MilMap is expected to be a cloud based military digital twin system for geospatial intelligence sharing and battle field analysis & simulation.",
        "description": ""
      },
      {
        "title": "Opensidewalkmap: A Project And Open Source Framework For An Web-Based Urban Pedestrian Network Inventory Using Openstreetmap Data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The interest on urban pedestrian networks is growing, with impacts centered at UN SDGs numbered 3, 11, 10 and 13: the improvement of accessibility helps in reducing inequalities and the fostering of non-motorized locomotion improves well-being and sustainability in urban scenarios. The idea behind OpenSidewalkMap is to leverage the multi-purpose OpenStreetMap data for the pedestrian network data. The structure of the project is decentralized, with localities deployed as nodes on a world web-map. At each node there’s a modular structure within a webpage, containing apps that have a different role, in order to create what is intended to be a full-fledged inventory, whose functionality can be expanded as new modules can be added. Currently there are four modules: “Webmap” containing an interactive cartographic representation of the data; “Optimized Routing” that uses the data to create an optimized routing, currently only for a wheelchair profile based on an empiric equation;  “Dashboard” featuring statistical charts to look at the bigger picture of the data, mainly focused on value percentages, thus giving attribute completeness, also giving a look at the data aging and number of revisions; “Data quality tool” looking at most common possible errors on data, giving direct link to editors, being at this point focused on finding invalid values, with geometrical and topological error detection planned to be included; there are 4 planned modules: “data watching” to monitor changes on data, to track and combat possible vandalism against data since OSM data is universally editable; “Tiles” giving raster and maybe vector tiles; “API” giving features on request; “Surveying And Validation” to list projects in different platforms/editors to expand and validate available data. This way the inventory will include continuously the full cycle of data: creation and collection; storage, maintenance and management;  application and analysis. The project is aimed to have zero-maintenance costs, as long as everything is hosted using current freely available Microsoft github infrastructure, with all code and data being maintained inside github repositories, webpages deployed with github pages, updated using github actions. In case of shutdown of any of these services, the software can still be deployed in another server infrastructure with a similar workflow. There is lots of room for improvement, with only the node for the city of Curitiba being available as of february 2023. The homepage of the code is available at:  https://kauevestena.github.io/opensidewalkmap/ .",
        "description": ""
      },
      {
        "title": "What's new and coming up in OpenLayers",
        "type": "Talk",
        "track": "State of software",
        "abstract": "OpenLayers is a powerful web-mapping library, and it has been around for quite a while. Far from being stuck in a past state where it offered most features anyone could expect, the community of contributors and maintainers are continuously pushing it forward, rethinking orientations and taking in new trends. Be it cloud-native formats, emerging standards or drastic performance improvements, more and more innovations are becoming parts of OpenLayers feature set.\n\nThis talk will give you an overview of the past few years of development, and show in how many incredibly useful ways OpenLayers can be used nowadays. We will also discover the exciting developments that are shaping up for the future, and how all this is being made possible.",
        "description": ""
      },
      {
        "title": "State of the OL-Cesium library",
        "type": "Talk",
        "track": "State of software",
        "abstract": "[OL-Cesium](https://github.com/openlayers/ol-cesium/) is a popular Open source Javascript library that you can leverage to add 3D to a new or existing OpenLayers application. You code the logics in a single place and it gets applied to both OpenLayers 2D map and Cesium 3D globe. The library handles the synchronization of the view, layers, styling, for you. This behaviour is customizable.\n\nSince its creation, 9 years ago, the library has attracted a large community of users. It has evolved to follow OpenLayers, Cesium and the global javascript ecosystem.\nThis talk is about the strengths of the library, its state and the plans for the future.",
        "description": ""
      },
      {
        "title": "MapServer Features by Example",
        "type": "Talk",
        "track": "State of software",
        "abstract": "MapServer, a founding OSGeo projects, has been powering mapping systems since the mid 1990s. This talk gives an overview of the many features of MapServer that have been developed over the past 25 years, with a focus on advanced functionality that is not well-known as they deserve. \n\nFeatures will be shown using sample Mapfiles - the configuration files used by MapServer. Examples will include advanced symbology, special layer types such as graticules, charts, and contours, displaying data from S3 buckets, and more!",
        "description": ""
      },
      {
        "title": "GeoStyler - One Tool for all Styles",
        "type": "Talk",
        "track": "State of software",
        "abstract": "When it comes to styling of geodata many tools have their own solution: SLD, QGIS-Styles, OpenLayers-Styles, Leaflet, …\n\nBut what to do if you need to share the same style across different formats?\nGeoStyler brings the solution. With its standalone parsers, nearly any (layer based) style can be converted from one format to another - from SLD to OpenLayers, QGIS, Mapfile, and vice versa.\n\nOn top of this, GeoStyler offers a library of React UI elements to easily create styles in your own WebGIS.\n\nThis talk will give an overview of possible use cases for GeoStyler, its latest developments such as the new layout and the support for expressions, as well as past and upcoming community events.",
        "description": ""
      },
      {
        "title": "Connecting SMODERP with Living Landscape - QGIS Plugin",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Model of Living Landscape (MLL) is a set of empirical based tools for land management and landscape planning. It recognizes the complexity of the interactions between humans and the natural environment, and it aims to create a sustainable and resilient landscape that supports the well-being of both people and nature. One of the core MLL components is a process-based model for rainfall-runoff and erosion computation called SMODERP. The model operates on the principle of cell-by-cell mass balance, calculated at each time step. SMODERP (https://github.com/storm-fsv-cvut/smoderp2d) is open-source software implemented in Python language to ensure compatibility with most GIS software solutions. The current implementation supports Esri ArcGIS, GRASS GIS and QGIS. In this contribution, a new QGIS SMODERP plugin linking the hydrologic model outputs to MLL will be presented. The plugin performs the input data preparation on the background using GRASS GIS data provider, computation is done by SMODERP Python package, and results visualised with predefined map symbology in QGIS map canvas. \n\nThis contribution was supported by grant RAGO - Living landscape (SFZP 085320/2022) and Using remote sensing to assess negative impacts of rainstorms (TAČR - SS01020366).",
        "description": ""
      },
      {
        "title": "Visualizing Geospatial Data with Apache Superset",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Apache Superset is one of the most used no-code platforms for business intelligence. It allows for the exploration and visualization of data, from simple line charts to highly detailed geospatial charts, without the need for programming skills. These charts can be published on interactive dashboards to provide users with meaningful and up-to-date information. Currently, a plug-in for visualizing cartodiagrams is in development which is based on the OSGeo projects OpenLayers and GeoStyler. This plug-in gives users the ability to use any visualization of Superset within a geospatial context, so that e.g. simple pie charts or even complex location based timeseries can be displayed on a map. Thereby, Superset becomes a powerful tool for visualizing geospatial data.\n\nThis talk gives a brief overview of Superset and possible use cases while focussing on geospatial data.",
        "description": ""
      },
      {
        "title": "Migration strategies: Or how to get rid of a deprecated framework",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Deprecation of a used framework is a common risk for software projects. Migrations are very time-consuming and costly, without showcasing any new functional features. This can make them an unpopular task, that tends to be postponed until there is no other choice, be it for a customer or the community of an open source project.\n\nDuring the last decade for instance, AngularJS has been one of the most popular web frameworks around. This was not any different in FOSS4G projects, where it had been adopted in geoportals and other frontend components. With the end of the decade, active development of AngularJS came to an end and since summer 2021 no more security updates are provided. This has become a major challenge for many web ecosystems - including FOSS4G ones - where AngularJS is still very present, but will have to be replaced in the long run.\n\nThis talk will present various open source projects and how they differently approach this challenge. It will reflect on lessons learned so far and aspires to provide inspiration for other projects in a similar situation.\n\nGeomapfish is a WebGIS framework that allows to build geoportals. It is a community driven project. Its frontend is based on the ngeo javascript library, which has been built on top of AngularJS and OpenLayers. Due to its wide functionality, the project’s goal is to prevent a one shot migration. It has been decided for a continuous migration based on (Lit Element) web components, that allow to integrate migrated functionalities step by step.\n\nGeoportal.lu is the national geoportal of Luxembourg. It is based on the Geomapfish framework, but has a very customized frontend. The requirement here is similar. Instead of migrating all at once, the different parts should be continuously integrated. After following the Geomapfish migration strategy based on web components at first, the project is finally migrated to another javascript framework (vue), without giving up on the continuous migration.\n\nGeonetwork is a well-known FOSS4G catalog application. On top of its powerful backend, sits a frontend that is also based on AngularJS. Once again, its functionality is so vast, that a complete rewrite would be enormous. Thus came up the idea of geonetwork-ui: A new project that could live alongside Geonetwork without the goal to become isofunctional, but to complement it. A project providing libraries specialized in proposing user interfaces by leveraging Geonetwork’s backend capabilities.",
        "description": ""
      },
      {
        "title": "Styling Natural Earth with GeoServer and GeoCSS",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Natural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth one can build a variety of visually pleasing, well-crafted maps with cartography or GIS software.\n\nGeoServer GeoCSS is a CSS inspired language allowing you to build maps without consuming fingertips in the process, while providing all the same abilities as SLD.\n\nIn this presentation we’ll show how we have built a world political map and a world geographic map based on Natural Earth, using CSS, and shared the results on GitHub. We’ll share with you how simple, compact styles can be used to prepare a multiscale map, including:\n* Leveraging CSS cascading.\n* Building styles that respond to scales in ways that go beyond simple scale dependencies.\n* Various types of labeling tricks (conflict resolution and label priority, controlling label density, label placement, typography, labels in various scripts, label shields and more).\n* Quickly controlling colors with LessCSS inspired functions.\n* Building symbology using GeoServer large set of well known marks.\n\nJoin this presentation for a relaxing introduction to simple and informative maps.",
        "description": ""
      },
      {
        "title": "GeoServer used in fun and interesting ways",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "GeoServer is the start of so many great open source success stories. \n\nThis talk introduces the core GeoServer application and explores the ecosystem that has developed around this beloved OSGeo application. Our presentation draws on the GeoServer ecosystem for use-cases and examples of how the application has been used successfully by a wide range of organizations.\n\nEach use-case highlights a capability of GeoServer providing an overview of the technology drawn from practical examples.\n\n* Andrea Amie is on hand to share success stories highlighting GeoServer use in managing vulnerable ecosystems, agriculture information management, and marine data management.\n* Jody Garnett will look at how GeoServer technology powers cloud services\n* Gabriel will look at am amazing remixes for Cloud Native GeoServer\n* GeoServer technology powering the OSGeo community, including GeoNode, geOrchestra\n* A showcase of examples collected from our user list\n\nAttend this talk to learn what GeoServer is good for out-of-the-box, and for inspiration on what is possible using GeoServer and the FOSS4G community.",
        "description": ""
      },
      {
        "title": "Open source geospatial software in support of the common European Green Deal data space",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "Published in 2020, the European strategy for data sets the vision for Europe to become a leader in a data-driven society by establishing so-called common European data spaces in all strategic societal sectors. Data spaces are envisioned as sovereign, trustworthy and interoperable data sharing environments where data can fairly flow within and across actors, in full respect of European Union (EU) values to the benefit of European economy and society. The development of data spaces is accompanied by a set of horizontal legislative measures, including, among others, an Implementing Act on high-value datasets under the Open Data Directive that lays down a list of datasets (many of which being geospatial) that EU Member States public sector organisations are required to make available for free, under open access licenses, in machine-readable formats and via Application Programming Interfaces (APIs). \nThe talk will describe the activities around open source geospatial software and open geospatial data that the European Commission’s Joint Research Centre (JRC) has performed to support the development of the common European Green Deal data space, focused on environmental data sharing and instrumental to address climate changes and environmental challenges in line with the top priority of Von der Leyen’s Commission 2019-2024. \nA key enabler to bring public data into this data space is the infrastructure setup for the EU INSPIRE Directive, which is technically coordinated, maintained and operated by the JRC. The INSPIRE Directive itself, together with the Directive on public access to environmental information, are currently subject of an impact assessment that might lead to a revision of the legal framework (GreenData4All initiative). This is accompanied by an overall modernisation of the technical infrastructure, increasingly based on open source software both at the Commission side (GeoNetwork for the INSPIRE Geoportal, ETF for the INSPIRE Reference Validator and Re3gistry for the INSPIRE Registry) and at the Member States side, where FOSS4G tools are the primary choice for both serving and consuming data. Thanks to a number of INSPIRE Good Practices promoted by the community, new standards and approaches for data encoding and sharing (e.g. based on OGC APIs) are bringing additional value to the INSPIRE stack. The same set of approaches ensures the full alignment and complementarity between INSPIRE and the Implementing Act on high-value datasets, thus positioning open source geospatial software as a true enabler for the Green Deal data space.",
        "description": ""
      },
      {
        "title": "EGMS: Validating 10.000 million open geospatial ground motion timeseries at EU scale",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The European Ground Motion Service (EGMS) is part of the Copernicus Land Monitoring Service (CLMS) lead by the EEA (European Environment Agency). EGMS is based on the full resolution InSAR processing (20x5m) of the European Space Agency (ESA) Sentinel-1 (S1). This massive geospatial timeseries dataset is composed by ~10.000 million timeseries distributed over 31 European countries. The baseline covers 2015-2020 and updates are being published on a yearly basis. It is publicly accessible at https://egms.land.copernicus.eu/ with a 3D viewer and download service. \n\nThis open dataset consists of three product levels (Basic, Calibrated and Ortho). The Basic and Calibrated are offered at full resolution 20x5m (Line of Sight) whereas the Ortho product offers horizontal (East-West) and vertical (Up-Down) anchored to the reference geodetic model resampled at 100x100m.\n\nSixense is coordinating a consortium responsible for the independent validation of this continental scale geospatial dataset. The validation goal is to assess that the EGMS products are consistent with user requirements and product specifications, covering the expected range of applications. To evaluate the fitness of the EGMS ground motion data service seven reproducible validation activities (VA) have been developed gathering validation data from different sources across 12 European countries:\n\n• VA1 – Point density check performed by Sixense. \n\n• VA2 – Comparison with other ground motion services carried out by NGI (Norwegian Geotechnical Institute). \n\n• VA3 – Comparison with inventories of phenomena/events performed by BRGM (French Geological Survey). \n\n• VA4 – Consistency check with ancillary geo-information carried out by NGI. \n\n• VA5 – Comparison with GNSS data performed by TNO (Dutch Geological Survey). \n\n• VA6 – Comparison with insitu monitoring data performed by GBA (Austrian Geological Survey). \n\n• VA7 – Evaluation XYZ and displacements with Corner Reflectors performed by TNO. \n\nThe validation environment developed and maintained by Terrasigna includes all the necessary elements to perform all the validation tasks from data collection and description to execution of the different methodologies. The objective of this portable Kubernetes/Terraform cloud-based system is to guarantee reproducibility of all the validation activities:\n\n• A MinIO web-based validation data upload tool where scientists can upload their validation data and EGMS subsets.\n\n• A validation data catalogue based on GeoNode (based on OGC CSW) where all validation sites data is properly described and georeferenced to ensure reproducibility.\n\n• JupyterHub notebook environment where scientists can develop their validation scripts (Python/R). These notebooks produce graphs and figures to be included in the yearly validation reports.",
        "description": ""
      },
      {
        "title": "Increasing the uptake of Earth Observation services and products through European efforts",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "In this talk we introduce a European initiative with global effects that aims to support the uptake of Earth Observation (EO) data products and services by increasing European capability to generate timely, accurate, disaggregated, people-centred, accessible and user-friendly environmental information based on EO data. The initiative - Open Earth Monitor Cyberinfrastructure - is following a well defined workflow:\n(1) Identify gaps and needs analysis : finding out what are the bottlenecks of data platforms together with stakeholders;\n(2) Use open source EO computing engine : integrating EO with in-situ data to obtain improved geospatial data services and products; \n(3) Build better data portals: harmonise, bridge and improve existing open source platforms;\nMake data platforms FAIR: improve accessibility of data with open source licences and capacity building; \n(4) Serve concrete goals: all Open Earth Monitor activities are centred around pre-defined use cases with various stakeholders. \n\nWe do not plan to reinvent the wheel, therefore all our efforts will focus on improving existing open source solutions and other initiatives, such as: OpenEO.org, Geopedia.world, GlobalEarthMonitor.eu, EarthSystemDataLab.net, OpenLandMap.org, EcoDataCube.eu., LifeWatch.eu, XCUB and EuroDataCube.com. Our developments will materialise in a series of monitoring tools at European as well as global level in various fields: forestry, natural hazards, biodiversity, crop monitoring etc. \n\nIn the context of Open Earth Monitor, Cyberinfrastructure is defined as the coordinated aggregate of software, hardware, human expertise and other technologies required to support current and future discoveries in science and engineering, enabling relevant integration of often disparate resources to provide an useful and usable framework for research, discovery and decision-making characterised by broad access and \"end-to-end\" coordination. \n\nOpen Earth Monitor Cyberinfrastructure has received funding from the European Union's Horizon Europe research and innovation programme under grant agreement No. 101059548.  (HORIZON-CL6-2021-GOVERNANCE-01).",
        "description": ""
      },
      {
        "title": "EuroGEOSS Prototype Development",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "Europe is a world leader in Earth Observation (EO) and climate change studies. An outstanding example is Copernicus, the most ambitious EO programme worldwide, which in addition to being an independent system is also a strong component of the Group on Earth Observation (GEO), an intergovernmental partnership aiming to improve the availability, access and use of open EO to support policy and decision making in a wide range of sectors.\nSince 2005, the Global Earth Observation System of System (GEOSS) has been a key initiative by GEO to integrate platforms and connect existing infrastructures using common standards for sharing and using digital resources. Europe is delivering a regional contribution to GEO, named EuroGEO, by covering the last mile of the EO value chain. However, this regional node lacks the effective interoperability needed to implement a European ecosystem to fully support the policy cycle. \nTo fill this gap, the development of a sustainable EuroGEOSS ecosystem connecting many European assets including data, sensor networks, analytical methods and models, computing infrastructures, products and services that support European objectives (i.e. a EuroGEOSS ecosystem), is of a vital importance in the evolution of the initiative. \nThe purpose of this talk is to present the rationale and the development status of a EuroGEOSS prototype, that the European Commission’s Joint Research Centre is conceptualizing.\nStarting with the analysis of use cases with the highest European policy priority, five of them were identified as the prominent ones to be replicated. Along with the replication of use cases, a monitoring framework of issues and gaps identified in the life cycle will be populated meanwhile. \nThe EuroGEOSS prototype architecture will implement the following patterns: a) Portal and Single Sign On; b) Meta catalogue of the services (data, models, infrastructures, etc.); c) High flexibility and modularity level; d) Adoption of the Machine Learning operation (MLOps) methodology. \nThe EuroGEOSS ecosystem is not conceived as another platform. It will rather be a virtual platform leveraging on: a) open sources and open interoperability standards (normative and de facto); b) interconnection of novel technologies; c) inclusion of relevant European communities such as those around EuroGEO and INSPIRE; d) Scalable interoperable infrastructures: CREODIAS, OpenEO, etc.\nThe development of a EuroGEOSS prototype will last until the end of 2024, documenting the status of gaps, challenges in the available data and infrastructure, as well as assisting a future scenario and business model and a possible operationalization.",
        "description": ""
      },
      {
        "title": "Open Source for Geospatial Software Resources Platform for Geospatial Data Exploitation – OSS4gEO: community led Open Innovation at ESA and beyond",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Our talk presents an initiative that works to develop an open, interactive, user intuitive platform for a constantly updated, comprehensive and detailed overview of the dynamic environment of the open source digital infrastructure for geospatial data storage, processing and visualisation systems. OSS4gEO is designed as a repository that functions as an extended metadata catalogue, curated by the community and a tool for metrics computation, visualisation, ecosystem statistical analysis and reporting.\n\nThe initial development of the Open Source for Geospatial Software Resources platform builds on previous extensive work started in 2016 that has materialised into a pioneering overview of open source solutions for geospatial, voluntarily updated by the team. Starting in 2023, OSS4gEO has become a part of a wider ESA EO Open Innovation initiative to actively support and contribute to the EO and geospatial open source community and it is intended as a seed action to better understand, represent and harvest the geospatial open source ecosystem. \n\nThere are 3 main objectives that OSS4gEO aims to achieves: \n(1) It aims to offer an informed and as complete as possible overview of the open source for geospatial and EO ecosystem, together with various capabilities of filtering and visualisations, within the platform as well as technical solutions to programmatically access and extract data from the database (APIs) to use in any purpose, including commercial;\n(2) It aims to provide guidance through the complexity of the geospatial ecosystem so that one can choose the best solutions, while understanding their sustainability, technical and legal interoperability and all the dependencies levels;\n(3) It aims to serve as a community building, a promoting and maintaining platform for new and innovative open source solutions for EO and geospatial, developed within various projects, research centres, small or large companies, universities or through individual initiatives. \n\nOur talk will outline the OSS4gEO initiative as a community-led, bottom-up initiative, highlight current and future developments and co-development activities and introduce the wider ESA EO Open Innovation context.",
        "description": ""
      },
      {
        "title": "MobiDataLab - Building Bridges on the way for FAIR mobility data sharing",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "MobiDataLab is the EU-funded lab for prototyping new mobility data sharing solutions.\nOur aim is to foster data sharing in the transport sector, providing mobility organising\nauthorities with recommendations on how to improve the value of their data,\ncontributing to the development of open tools in the cloud, and organising hackathons\naiming to find innovative solutions to concrete mobility problems.\n\nStarted in 2021, the project investigated mobility data and services and did grown an\nopen knowledge base about mobility data as one of the four main pillars of the project.\nWith the realization of tools and the combination of data and services in the Transport Cloud,\nwhich is the second pillar of the project, a representative set of technical\n\"mobility data sharing enablers\" has been grown.\n\nIn the second half of the project, these assets are being provided to the public.\nThe Virtual and  Living Labs will host environments for mobility data stakeholders\nto explore the state of the art for data, services and their interaction to solve\nmobility data challenges. All aligned with the FAIR statement - making data and services\nfindable, accessible, interoperable and reusable.\n\nThe challenges are mainly based on a broad set of use-cases, defined by the core project group,\nthe reference group and external stakeholders. These challenges are the core of the Livind and\nVirtual Labs, where participants building solutions for the given challenges and exploring new\nopportunities with the shared mobility data and services.\n\nWith the feedback of the labs, our partners, the reference group and external stakeholders\n- mobility data providers from public and private sector, municipalities,\ngovernmental institutions, start-up communities and stakeholders from research and industry,\nthe project will make challenges transparent and remove barriers for data sharing.\n\nSince the project started in February 2021, we will present our achievements \nprovide an outlook on the last mile of the project, where we are bringing the\ntools on the road.\n\nFurther information on the project is available via https://mobidatalab.eu and https://github.com/mobidatalab .\n\n\nMobiDataLab is funded by the EU under the H2020 Research and Innovation Programme (grant agreement No 101006879).",
        "description": ""
      },
      {
        "title": "Runtime environment for the validation of the Copernicus Ground Motion Service",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Copernicus Ground Motion Service (EGMS) is a European Union (EU) initiative under the Copernicus program, which aims to provide near-real-time information about ground deformation caused by natural or man-made hazards. The service uses a variety of data sources, including satellite radar imagery, to monitor and analyze ground motion in areas prone to landslides, sinkholes, earthquakes, and other hazards. Given the sensitive nature of the service, EGMS product validation is a key activity in assuring the user community (especially the decision makers) of the quality of the ground motion and deformation information provided.\n\n\nThe main goals of the EGMS validation system are as follows: to provide a reproducible environment on top of modern cloud infrastructures (with a particular focus on the European geo clouds), to enable the development of scientific tools that validate EGMS characteristics, to facilitate the reproducibility of the validation tasks, and to account for key performance indicators (which will allow shareholders to monitor the quality of the primary EGMS product).\n\nTo achieve the first goal of providing a reproducible environment, we have focused on providing Terraform modules that facilitate the deployment of our software stack on any supported cloud platform. The software stack is built on top of the Kubernetes container orchestration system, which runs on top of a managed cloud environment. Kubernetes provides uniform services regardless of the underlying cloud platform.\n\nFor the goals of developing the validation tools and the execution of those tools we decided on using an unified approach based on the JupyterHub solution. JupyterHub is used for providing an unified development environment based on R and Python EO software tools (based on modified Pangeo Docker images). Also Jupyter is used for executing the validation tools outside of JupyterHub by leveraging an internal python service that uses papermill to execute the notebook and then “nbconvert” to generate a html webpage containing the required visualizations and documentation in human readable form.\n\nThe validation system is complemented by an bespoke web dashboard aimed for providing reports and information related to the status of the various key performance indicators.\n\nOverall the whole validation system was developed by solely using FOSS4G components: GeoPandas, RasterIO, GeoNode, GeoServer and JupyterHub.",
        "description": ""
      },
      {
        "title": "Low-cost AirQuality stations + open standard (OGC SensorThings) + open data (CC-BY) + open source (FROST + QGIS plugin for sensors)",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "This is the story of 2 twin projects (namely AIR-BREAK and USAGE) undertaken by Deda Next on dynamic sensor-based data, from self-built air quality stations to the implementation of OGC standard compliant client solution.\nIn the first half of 2022, within AIR-BREAK project (https://www.uia-initiative.eu/en/uia-cities/ferrara), we involved 10 local high schools to self-build 40 low-cost stations (ca. 200€ each, with off-the-shelf sensors and electronic equipment) for measuring air quality (PM10, PM2.5, CO2) and climate (temperature, humidity). After completing the assembling, in late 2022 stations were installed at high schools, private households, private companies and local associations. Measurements are collected every 20 seconds and pushed to RMAP server (Rete Monitoraggio Ambientale Partecipativo = Partecipatory Environmental Monitoring Network - https://rmap.cc/).\nHourly average values are then ingested with Apache NiFi into OGC’s SensorThings API (aka STA) compliant server of the Municipality of Ferrara (https://iot.comune.fe.it/FROST-Server/v1.1/) based on the open source FROST solution by Fraunhofer Institute (https://github.com/FraunhoferIOSB/FROST-Server).\nSTA provides an open, geospatial-enabled and unified way to interconnect  Internet of Things (IoT) devices, data and applications over the Web (https://www.ogc.org/standard/sensorthings/). STA is an open standard, it builds on web protocols and on OGC’s SWE standards and has an easy-to-use REST-like interface, providing a uniform way to expose the full potential of the IoT (https://github.com/opengeospatial/sensorthings/).\nIn second half of 2022, within USAGE project (https://www.usage-project.eu/), we released the v1 of a QGIS plugin for STA protocol.\nThe plugin enables QGIS to access dynamic data from heterogeneous domains and different sensor/IoT platforms, using the same standard data model and API. Among others, dynamic data collected by the Municipality of Ferrara will be CC-BY licensed and made accessible from municipal open data portal (https://dati.comune.fe.it/).\nDuring the talk, a live demo will be showcased, accessing public endpoints exposing measurements (timeseries) about air quality (from EEA), water (BRGM), bicycle counters, traffic sensors, etc.",
        "description": ""
      },
      {
        "title": "OGC API feature services built with Hakunapi",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "National Land Survey of Finland (NLS) has built multiple feature services based on the OGC API Features standard since 2019. These services provide cadastral and topographic data, buildings, geographic names, and addresses both as open and contract-based APIs.\n\nThe engine behind these services is Hakunapi – a high performance server implementation to easily build “off-the-shelf” Simple Features and customized Complex Features services with geospatial data backed by a PostGIS database. Currently the OGC API Features (Part 1, 2 and 3) standard is supported. The codebase is based on Java, and it utilizes also other geospatial libraries such as JTS Topology Suite and GeoTools.\n\nHakunapi is now Free Open-Source Software available at GitHub with the version 1.0 released in May 2023. On the last few years NLS has internally used the library for services providing both Simple Features (like traditional topographic database) and Complex Features (cadastral registry and geographic names with some hierarchical feature structures too).\n\nThis talk presents key features and benefits of using Hakunapi for implementing feature services based on the OGC API Features standard. Also experiences and best practices by NLS on developing these services and our roadmap towards modern OGC API services is discussed.\n\nDemo: https://beta-paikkatieto.maanmittauslaitos.fi/inspire-addresses/features/v1/ \n\nCode: https://github.com/nlsfi/hakunapi",
        "description": ""
      },
      {
        "title": "Smart Maps for the UN and All - keeping web maps open",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "Do you want to broaden your horizons by learning about geospatial support for the United Nations operations? Or are you interested in developing highly efficient and portable geospatial apps which make use of PMTiles, COPC, COG, Raspberry Pi, and a cool Web3 technology named IPFS (Inter-planetary File System)? We are doing both in the Domain Working Group 7 (DWG 7) on Smart Maps of the UN Open GIS Initiative.\n\nIn this participatory and voluntary DWG established in Firenze in August 2023, participants bring in their objectives and combine efforts within the Partnership for Technology in Peacekeeping to bring greater involvement to peacekeeping through innovative approaches and technologies that have the potential to empower UN global operations. In addition to our core objective to support the use of UN Vector Tile Toolkit in the UN Global Service Centre, DWG 7 is supporting domestic and campus-level service operations, and supporting 3D geospatial data such as point clouds and 3D city models. We are combining efforts to define and implement the concept of Smart Maps.\n\nWe are happy to share with you our new effort named Model UN Development and Operations (MUNDO) that simulates geospatial support for the United Nations operations by making use of existing open geospatial data and our Smart Maps technologies. MUNDO project is not only useful for demonstrating the technology for the UN staff, but also useful for learning about the situation and the UN’s effort. We are also happy to share with you our new concept of WebMaps3, which introduces Web3 technology for web maps. By combining IPFS and cloud optimized formats like PMTiles, COPC, and COG, we were successful in hosting a vector tiles service from a newly released nation-wide cadastre dataset on a Raspberry Pi, within 10 days after the release, by producing a 14GB PMTiles file.",
        "description": ""
      },
      {
        "title": "Offline web map server \"UNVT Portable\"",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "UNVT Portable is a package for RaspberryPi that allows users to access a map hosting server via a web browser within a local network, primarily for offline use during disasters. It is designed to aid disaster response by combining aerial drone imagery with OpenStreetMap and open data tile datasets.",
        "description": "\"UNVT Portable\" is a map server that allows you to freely use web maps from devices such as smartphones even in an offline environment. It is mainly designed to work in an offline environment in the event of a major disaster, and various open data tiles are prepared in advance, such as drone aerial images taken after a disaster, OpenStreetMap, and satellite images released for free by JAXA（Japan Aerospace Exploration Agency）, etc. Combine sets to create the maps you need in times of disaster. We envision a use case for municipalities, etc. to understand the situation after a disaster and to respond to disasters. It is built using open source software such as Apache and MapLibre and Raspberry Pi, and is completely open source. Unlike tools such as Google Maps, which are difficult to use for secondary purposes, it is being developed as open source so that it can be released in a form that can be easily used by anyone, including local governments, international organisations and private companies."
      },
      {
        "title": "UNDP's one stop shop for cloud based geospatial data visualisation and analytical tool",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "United Nations Development Programme (UNDP) is a United Nations agency tasked with helping countries eliminate poverty and achieve sustainable economic growth and human development. \n\nRecent advances in technology and information management have resulted in large quantities of data being available to support improved data driven decision making across the organization. In this context, UNDP has developed a corporate data strategy to accelerate its transformation into a data-driven organisation. Geo-spatial data is included in this strategy and plays an important role in the organization. However, the large scale adoption and integration of geo-spatial data was obstructed in the past by issues related to data accessibility (silos located in various country offices), interoperability as well as sub-optimal hard and soft infrastructure or  know-how.\nAll this issues have been addressed recently, when UNDP SGD integration started developing  a geospatial hub - GeoHub - to provide geospatial data visualisation and analytical tools to UNDP staff and policymakers.\n\n[UNDP GeoHub](https://geohub.data.undp.org/) is a repository of a wide array of data sets of the most recent time span available at your fingertips! It is a centralized ecosystem of geospatial data and services to support development policymakers. It allows users to search and visualise datasets, compute dynamic statistics and download the data. In addition, GeoHub provides a feature to share their maps with the community easily. With our repository, you can also upload to share your valuable data to share with the community! It connects geospatial knowledge and know-how across the organization to enhance evidence-based decision-making with relevant data-led insights.\n\nGeohub ecosystem consists of [sveltekit](https://kit.svelte.dev/) & [maplibre](https://maplibre.org/) based frontend web applications and various FOSS4G software in the backend side. [PostgreSQL/PostGIS](https://postgis.net/), [titiler](https://developmentseed.org/titiler/), [pg_tileserv](https://github.com/CrunchyData/pg_tileserv) and [martin](https://martin.maplibre.org/) are deployed in Azure Kubernetes (AKS) to provide advanced visualisation and analysis for users. All source code is published in [Github](https://github.com/UNDP-Data/geohub) with an open-source license.",
        "description": ""
      },
      {
        "title": "Disaster Mapping Prioritization in OSM",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "One of the primary motivations for the Open Mapping Hub Asia-Pacific to increase the quantity and quality of OpenStreetMap (OSM) data in the region is the region's high exposure to multiple types of hazards.\n\nApart from assisting response efforts following a disaster event by providing access to critical geospatial information, the hub aims to ensure that OSM data is already available in high-risk areas, even before a disaster occurs, to be used in critical anticipatory action such as developing early warning systems and mitigation plans. It is critical to have a systematic method for determining the OSM mapping requirements in these disaster hotspots.\n\nAlthough some tools separately assess the Completeness of OSM Data and the Disaster Risk Level of a location, a new tool that combines these assessments is required to highlight the areas that should be prioritized for mapping in OSM.\n\nThe Open Mapping Hub Asia-Pacific created a data-driven method for determining which areas in OSM disaster mapping should be prioritized. The resulting method is deployed as a QGIS plug-in and distributed to OSM communities for offline assessments to identify disaster-prone areas that have not yet been mapped in OSM.",
        "description": ""
      },
      {
        "title": "CesiumJS and OpenLayers for a metropolitan cooperation web platform based on the digital twin of Rennes Métropole.",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In a context of digital transition and the increasing availability of urban data, Rennes Métropole wishes to better equip its decisions and public policies on the basis of data and cooperation.\n\nUltimately, the goal is to :\n- Promote cooperation and the contribution of the actors of the territory, in particular the citizens\n- \"Enlighten\" public decisions and policies, in particular the democratic, ecological and energy transition projects carried out by Rennes Métropole.\n\nIssues of transparency, public service efficiency and cost control are also sought.\n\nThe metropolitan cooperation platform that is currently developed will consist of one or more tools based on the digital twin intended to equip public decisions and policies on the basis of data and cooperation.\n\nThe platform is developed partly on VC Map which is an Open-Source JavaScript framework and API for building dynamic and interactive maps on the web. It can display 2D data, oblique imagery and massive 3D data including terrain data, vector data, mesh models, and point clouds making it easy for users to explore and interact with the data in an integrated and high-performance map application.  VC Map is built upon open, proven, and reliable GIS and web technologies such as OpenLayers and Cesium for the visualization of 2D and 3D geo-data.\n\nA particular effort was made on the design in order to offer users, mainly citizens of Rennes Metropole, a pleasant user experience that allows an exploration of the development projects of the metropole in 2D and 3D.\n\nWe will present the cooperation platform through three use cases of interest for Rennes Metropole : \n\nSolar Cadaster : Simulation of the photovoltaic production potential of the roofs and comparison with the energy consumption of the residents, the costs and the capacity of the network.\n\nLinear transport systems : Mediation (including visualization) and consultation with citizens and communities for the implementation of a linear transport infrastructure\n\nExposure to electromagnetic waves : Visualization of exposure levels to electromagnetic waves (simulations and real and real measured values) as well as objects (radioelectric relays and sensors) on the territory of the City of Rennes.",
        "description": ""
      },
      {
        "title": "Geoconnex.us: a standards based framework to discover water data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Web has an increasing number of web applications being developed to freely provide their information and is a hub for open data publishing. For this to happen as a self-sustained ecosystem, data must be findable, accessible, interoperable, and reusable to both humans and machines across the wider web. This session delves into Web Best Practices for publishing data using open source and standards-based solutions.\n\nThe geoconnex.us project is about providing technical infrastructure and guidance to create an open, community-contribution model for a knowledge graph linking hydrologic features in the United States as an implementation of Internet of Water principles. This knowledge graph can be leveraged to create a wide array of information products to answer innumerable water-related questions. \n\nImplementation has two parts: persistently identified real world objects and organizational monitoring locations that collect data about them. Both must be published to the Web using persistent URIs and communicated with common linked data semantics in order for a knowledge graph to be constructed. \n\nThe Internet of Water Coalition supports the first part with a Permanent Identifier Service and reference hydrologic reference features (e.g. watersheds, monitoring locations, dams, bridges, etc.) within the US.\n\nIn support of the second part, geoconnex.us takes advantage of pygeoapi using the OGC API - Features standard to publish structured metadata resources about individual hydrologic objects and the data about them. pygeoapi supports extending this standard by incorporating domain-specific structured data into the HTML format at the feature level, and allowing for external HTTP URI identification. In addition, pygeoapi’s flexible plugin architecture enables for custom integration and processes. This means that individual features from various sources can have structured, standardized metadata harvested by search engines and assembled into a useful knowledge graph.\n\nThis spatial feature-based linked data architecture enables data interoperability between independent  organizations who hold information about the same real world thing without centralizing data infrastructure - answering important questions like, “Who is collecting water data about my local stream and its tributaries?” or “What data do we have about water upstream and downstream of East Palestine, Pennsylvania?”",
        "description": ""
      },
      {
        "title": "G3W-SUITE as a tool for the preparation of web cartographic management systems",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Lazio Region Authority (Italy) has been using for several years a system based on the G3W-SUITE and QGIS application which has allowed it, not only to publish public web services, but to prepare web cartographic management systems dedicated to internal staff for the management of territorial aspects of own competence:\n * management of damages caused by wildlife and related reimbursement procedures\n * environmental impact assessment practices\n * wolf genetics\n * signaling the presence of wild boar in urban areas\n * nests and strandings of sea turtles\n * road accidents with wildlife\n\nThe close integration between the suite and QGIS has allowed to create web cartographic management systems characterized by:\n * numerous geometry editing features\n * customization of the structure of the editing and attribute consultation forms\n * simplification of attributes compilation thanks to the ability to inherit from QGIS: editing widgets, \n * mandatory and uniqueness constraints, default values, conditional forms and drill down cascade based on expressions\n * possibility of defining geographical constraints in visualization and editing in order to divide the \nterritory based on areas of competence associated with individual users\n * possibility to differentiate the information content accessible on the basis of different users and roles\n * descriptive analysis of the data through integration with the graphs created with the DataPlotly plugin\n\nThanks to the contribution and funding from the Lazio Region dedicated to the development and integration with the QGIS functions related to data editing, G3W-SUITE is configured as a valid tool for the preparation of advanced geographic data management systems on the web.\n\nAs an example, we report a series of use cases:\n * Environmental Protection Agency of the Piemonte Region: post-event damage and usability census, management and cartographic representation of post-earthquake inspection requests\n * Gran Paradiso National Park: park route signage management\n * Piemonte Region: preparation of Civil Protection Plans\n * Environmental Protection Agency of the Lombardy Region: Hydrological Information System",
        "description": ""
      },
      {
        "title": "Routing Machine, state and side-effects",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "The routing machine is about the route track a user can take from one point to the other with directions after reaching each point. For paid services such as Google maps, this already exists, and Google has applied a centralized model of usage. In this talk, we will talk about the type of libraries and already existing implementations that are almost deprecated but we can keep alive, since for the open source community, the ability to customize and change, they are essential. There are no active Open Source or community versions of the routing machine for maps. We need to change that. We can do that by improving a couple of things that already exist.  Having more wrappers for different types of implementations, say Vue, or React, and finally Svelte. The routes should be updated and the selection of the type of route, car, bike, or walking should reflect the data received from maps. And define a safer business model. Open Source is more active and strong than paid and centralized services. We need to make sure that what we are offering and implementing as services to our clients can reflect a similar dedication the first have.",
        "description": ""
      },
      {
        "title": "View Server - EO Data Visualization in a Cloud Native Way",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "The View Server (VS) is MIT licensed, Docker based, cloud-native, scalable software stack providing external services for searching, viewing, and downloading Earth Observation (EO) data. Services implementations are following OGC Web services standards STAC, OpenSearch, WMS, WMTS, WCS.\n\nHaving EOxServer and MapCache as core components, enables EO Data publication in a modular and configurable way. The process starts with data harvesting, preprocessing and metadata ingestion and ends with serving pre-cached and on demand rendered images through an attached Web client based on OpenLayers and EOxC libraries or on individual service endpoints.\n\nEOxServer allows dynamic generation of visual images from multi-spectral data. In this way, specific bands or channels of the original images can be selected as the grey or red, green, and blue output colour channels. It also supports flexible rendering based on previously extracted image statistics, pansharpening on the fly, filtering the long time periods of products intersecting with the query in CQL syntax utilizing metadata parameters and more.\n\nVS provides both S3, OpenStack Swift, HTTP and local files support when considering data storage and can be deployed in Docker Swarm environment via docker-compose templates or in Kubernetes environment as a set of Helm charts.\nThe software stack was and is used by EOX in a quite a number of operational deployments for ESA, like the VirES projects, Copernicus Space Component Data Access system (CSCDA), or more recently Earth Observation Exploitation Platform Common Architecture.\n\nLinks:\nhttps://eox.at/2021/09/eoxserver-1-0/\nhttps://eoxserver.org/\nhttps://github.com/EOxServer/eoxserver/\nhttps://gitlab.eox.at/vs/vs",
        "description": ""
      },
      {
        "title": "Aircraft trajectory analysis using PostGIS",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "PostGIS supports geometries with a Z dimension and geometries with M (measure) values, but there are not a lot of examples of *both* of these being used together. One use case is the analysis of airplane tracks which requires both - that is to say every vertex has an altitude and a timestamp. \n\nThis talk will show how live positional data transmitted from aircraft can be accessed in a PostGIS database. I will then show how a sequence of these positions can be represented effectively as LINESTRINGZM geometries which can be analyzed as trajectories using native PostGIS functions.\n\nWith spatial SQL, we can do things such as determine anomalous changes in an aircraft's velocity or altitude and find the exact point in time at which two aircraft came closest to one another. The focus on the talk will be showing how future work on large datasets of ADS-B data can be done using PostGIS and other open-source geospatial tools.\n\nI will cover how to use Python and PostgreSQL's PL/Python language extension to import the data and QGIS to render the data, but the analysis will be be done in SQL.",
        "description": ""
      },
      {
        "title": "Lessons from Successful Enterprise GIS Implementations with QGIS and PostGIS",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In this talk, I'll share some practical tips and tricks for managing an enterprise GIS workflow with QGIS and PostGIS. I'll showcase some real-world examples to highlight the benefits of using a centralized spatial database to manage GIS data, and I'll walk through the steps to set up a QGIS project for creating, updating, and deleting data directly from QGIS. \n\nMy goal is to help organizations that are planning to set up a PostGIS-powered QGIS workflow and are looking for innovative ways to maximise the benefits of the joint powerhouse of QGIS and PostGIS.\n\nAs we dive deeper, I'll explore some of the key technical aspects of using QGIS and PostGIS for enterprise GIS. I'll share some tips for configuring and integrating the tools, and showcase how to set up an easily accessible end-user workflow for creating and editing data in QGIS using QGIS forms.\n\nThroughout the talk, I'll also share some stories from different projects to illustrate how these tips and tricks have been successfully applied in practice. I will do my best to ensure that you’ll leave the talk with an understanding of the benefits of using QGIS and PostGIS as part of their Enterprise GIS workflows.\n\nWhether you're a GIS professional, team leader / project manager or anyone seeking to optimize their GIS data management, this talk will provide valuable insights and practical advice for optimizing your GIS data management. Join me as we explore the power of open source tools for enterprise GIS!",
        "description": ""
      },
      {
        "title": "More correct maps/data with Postgis Topology rather then Simple Feature ?",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In Norway we now get more up-to-date maps for land resource map (AR5), because the domain experts on agriculture in the municipalities in Norway have got access to a easy to use client. This system includes a simple web browser client and a database built on Postgis Topology. \n\nIn this talk we will focus on, what is it with Postgis Topology that makes it easier to build user friendly and secure tools for updating of land resource maps like AR5. We will also say a couple of words about advantages related to traceability and data security, when using Postgis Topology.\n\nIn another project, where we do a lot ST_Intersection and ST_Diff on many big Simple Feature layers that covers all of Norway, we have been struggling with Topology exceptions, wrong results  and performance for years. Last two years we also tested JTS OverlayNG, but we still had problems. This year we are switching to Postgis Topology and tests so far are very promising. We also take a glance on this project here in this talk. \n\nA Postgis Topology database modell has normalised the data related to borders and surfaces as opposed to Simple Feature where this is not the case. Simple Feature database modell may be compared to not using foreign keys between students and classes in a database model, but just using a standard spreadsheet model where each student name are duplicated in each class they attend.\n\nURL’s that relate this talk\n\nhttps://gitlab.com/nibioopensource/pgtopo_update_gui\nhttps://gitlab.com/nibioopensource/pgtopo_update_rest\nhttps://gitlab.com/nibioopensource/pgtopo_update_sql\nhttps://gitlab.com/nibioopensource/resolve-overlap-and-gap",
        "description": ""
      },
      {
        "title": "Leveraging the Power of Uber H3 Indexing Library in Postgres for Geospatial Data Processing",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Uber H3 library is a powerful geospatial indexing system that offers a versatile and efficient way to index and query geospatial data. It provides a hierarchical indexing scheme that allows for fast and accurate calculations of geospatial distances, as well as easy partitioning of data into regions. In this proposal, we suggest using the Uber H3 indexing library in Postgres for geospatial data processing.\n\nPostgres is an open-source relational database management system that provides robust support for geospatial data processing through the PostGIS extension. PostGIS enables the storage, indexing, and querying of geospatial data in Postgres, and it offers a range of geospatial functions to manipulate and analyze geospatial data.\n\nHowever, the performance of PostGIS can be limited when dealing with large datasets or complex queries. This is where the Uber H3 library can be of great use. By integrating Uber H3 indexing with Postgres, we can improve the performance of PostGIS, especially for operations that involve partitioning of data and distance calculations.\n\nWe propose to demonstrate the use of Uber H3 indexing library in Postgres for geospatial data processing through a series of examples and benchmarks. The proposed presentation will showcase the benefits of using Uber H3 indexing for geospatial data processing in Postgres, such as improved query performance and better partitioning of data. We will also discuss the potential use cases and applications of this integration, such as location-based services, transportation, and urban planning.\n\nThe proposed presentation will be of interest to developers, data scientists, and geospatial analysts who work with geospatial data in Postgres. It will provide a practical guide to integrating Uber H3 indexing with Postgres, and offer insights into the performance gains and applications of this integration.",
        "description": ""
      },
      {
        "title": "GeoNode at work: how do I do this, how do I do that?",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "GeoSolutions has been involved in a number of projects, ranging from local administrations to global institutions, involving GeoNode deployments, customizations and enhancements. A gallery of projects and use cases will showcase the versatility and effectiveness of GeoNode, both as a standalone application and as a service component, for building secured geodata catalogs and web mapping services, dashboards and geostories. In particular the recent advancements in data ingestion and harvesting workflows will be presented, along with the many ways to expose its secured services to third party clients. Examples of GeoNode’s builtin capabilities for extending and customizing its frontend application will be showcased.",
        "description": ""
      },
      {
        "title": "Time series raster data in PostgreSQL with the TimescaleDB and postgis_raster",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Raster data is a type of digital image data that is stored and processed as a grid of cells, each of which represents a specific area or location in the image. This grid is known as a raster or pixel grid, and each cell contains a value that represents a characteristic of the corresponding area or location in the image, such as color, elevation, temperature, or other attributes. Depending upon the resolution of the data these raster file sizes can vary from a few MBs to few GBs. Hence reading data from a large set of raster dataset which has time dimension associated with it is challenging.\n\nPostgreSQL can be used to store time series raster datasets, which are raster datasets that have a time dimension associated with them. This can be useful for storing and analyzing raster data that changes over time, such as satellite images, climate data, or land cover change data.\n\nTo store time series raster datasets in PostgreSQL, we will use the postgis_raster extension, which provides support for storing and manipulating raster data in the database, and the TimescaleDB extension to add time series functionality to PostgreSQL, allowing us to store and query raster data with a time dimension.\n\nUsing the TimeScaleDb extension we will partition the raster table by converting it to hypertable which is what TimescaleDB uses to optimally store and process time series data. This can help us to optimize query time.  \nFor aggregated values from raster data over time and space, we will use the Continuous aggregate feature of TimescaleDB which is a form of materialized view to pre-compute and store raster data over time.\nMoreover, TimescaleDB allows compression of data which can be very helpful in cases where the data is huge which is usually the case with raster datasets in postgres saving us space in the Database and optimizing some queries. \n\nThe proposed presentation will be of interest to developers, data scientists, and geospatial analysts who work with Raster datasets. It will provide a practical guide to querying the raster datasets in PostgreSQL with TimescaleDB and postgis_raster extension.",
        "description": ""
      },
      {
        "title": "#30DayMapChallenge with Open tools",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "#30DayMapChallenge is a daily map making challenge which is held since 2019 every year in november on social network. This challenge has become year after year popular for the mapmakers community, and more than 8000 maps have been posted in 2022 session.\nLast year was my first participation, it was a great opportunity to try to make unusual maps, complete sleeping projects, and be updated with geospatial technologies. \nIn this talk will be presented how this challenge has been completed and especially which open tools has been used to make the 30 maps.",
        "description": ""
      },
      {
        "title": "A review of Mapillary Traffic Sign Data Quality and OpenStreetMap Coverage",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Traffic signs are a key feature for navigating and managing traffic safely, affecting all of us on a daily basis. However, traffic sign datasets  are lacking on open government data portals as well as OpenStreetMap (OSM).\n\nMapillary’s computer vision capabilities can extract more than 1,500 classes of traffic signs  globally from street-level imagery. Generated traffic signs are available on iD Editor, Rapid and JOSM Mapillary plugin to enrich OpenStreetMap data. \n\nOur team wanted to know how the accuracy of traffic signs detected by Mapillary compared with the reality on the ground (the ground truth). To answer this question we collected more than thousands ground truth data in San Francisco and used this information to produce the recall, precision, and positional accuracy of our machined generated traffic sign data. This provided some interesting insights in OpenStreetMap and the level of completeness and gaps of that dataset. \n\nIn this talk, we will cover Mapillary’s traffic sign extraction capabilities, Mapillary generated traffic sign data against ground truth data and OSM’s traffic sign coverage in San Francisco’s downtown. We will be also addressing how data quality can be improved using various data collection techniques and the role of post-processing  with Structure from Motion and control points annotations.",
        "description": ""
      },
      {
        "title": "Introduction to decentralized geospatial digital twins: merging all LiDAR datasets in the world",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "How do you create a near-real-time source of 3D geospatial data from around the world?\n\nThe French Institute of Cartography and start-up Extra are collaborating to develop a decentralized protocol for this purpose. The Circum protocol will merge LiDAR datasets from various providers, sell this data source to consumers, and redistribute the value back to the original providers.\n\nCircum uses blockchain technology and 3D surface reconstruction algorithms to carry out its mission. Learn about the protocol’s key mechanisms with the team at this conference.",
        "description": ""
      },
      {
        "title": "Implementing Digital Twin City in MapLibre with the integration of different information sources",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Use case for the implementation of a platform that supports data that contributes to the publication and management of Digital Twins, based on the use of MapLibre as a web viewer and at the same time consuming information from different geospatial sources, including Mesh, Raster, DEM; and near real time data sources such as OneBusWay or OpenTripPlanner based on GTFS formats, for the comparison and analysis of information.",
        "description": ""
      },
      {
        "title": "Open Source Basded 3D City Model Visualization - A LH Urban Digital Twin Case",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The LH Urban Digital Twin Platform is a comprehensive solution for new town planning and development that utilizes open source digital twin technology. The platform combines real-world data with spatial information context to offer a three-dimensional sharing/collaboration integration support system. \n\nDevelopers will appreciate the platform's flexibility and scalability, which are based on a microservice architecture that connects multiple modules independently and loosely. The platform utilizes open standards WMS, WFS, WCS, WPS OGC Web Service standard features through GeoServer and GeoWebCache, a tile cache server that accelerates map delivery built into GeoServer. Additionally, the platform supports visualization of data in various formats using mago3D, F4DConverter, and Smart Tiling. \n\nThe platform offers a range of services, including automatic apartment building placement, construction site safety management, 3D urban landscape simulation, environmental planning simulation, and underground facility visualization simulation. The platform also features real-time monitoring and visualization of IoT-based data, which is of particular interest to developers interested in smart city development. \n\nFirstly, the presentation will show how open source based digital twin visualize the complex 3D city models in a web browser. Secondly it will showcase the platform's features and data, including actual system's functions and service UI/UX through a video. Attendees will gain insights into how the platform can be used to support rational decision making during complex urban planning, design, development, and operation stages. \n\nThis presentation is of interest to developers working in the field of urban planning, design, and development, as well as those interested in open source digital twin technology. \n\nLH Corp is one of the largest public companies in Korea providing land and housing for public purpose. They are owned and controlled by the Korean government. They’ve played a large role in new town development and housing welfare.",
        "description": ""
      },
      {
        "title": "AR: Why open map data is critical to the future of computing",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Use cases should drive product development, not the other way around. Maps and the products we use to consume them have the biggest impact on the world when these principles are adhered to. How many government portals have you visited where a carefully curated map is presented that hardly anyone sees let alone uses? Presenting the data to the user in an intuitive way that helps them make a decision or take action is essential.\n\nLarge paper maps of the 1700s were well suited to a captain’s desk as their ships traversed the oceans. Road atlases of the 20th century helped to spur family adventures and weekend getaways as highway networks were constructed around the world. The small computers in our pockets today allow us to see when the next train will arrive and which one will get us home sooner. These examples took the technology of the day and used it to make products with significant impact on society. The mobile internet in particular changed mapping in one of the most notable ways since humans started abstracting 3D space on 2D surfaces.\n\nWe’re on the cusp of another great shift in the way maps are used with many exciting use cases awaiting discovery. The technology powering this potential is Augmented Reality (AR). This talk will explore some of the use cases that AR is supporting and where it might be useful in future. We’ll look at how AR can be accessed and how the medium of access affects its utility. With these use cases in mind, we’ll assess how open tools and map data enable AR. Some of the data and tools we’ll look at include:\nGeometries of pedestrian ways\nAssociated attributes: Incline, safety, lighting, access, surface type, accessibility features\nBuilding entrances\n3D building data\nVPS for localisation\nRouting algorithms\n\nThe talk will conclude with a summary of Meta’s approach to map building and how open source geospatial technology powers the maps we build for today and the years ahead.",
        "description": ""
      },
      {
        "title": "Selection of noise measurement points based on road network using PyQGIS",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "This is a plug-in created using pyQGIS, and an example of using it as basic data for decision-making on noise measurement station selection policy will be presented.\nAs data for use in decision-making by public institutions, we introduce cases in which basic public data are utilized and processed to ultimately be used as core data for decision-making.\nIt will be time to talk about how text-based data held and provided by public institutions is being used for their spatial expression and policy making, and why the opening of public data will play a more important role in the future.",
        "description": ""
      },
      {
        "title": "OpenStreetMap Seasonal Differential in Citizen Science Volunteered Response Mapping of Flood Disaster Vulnerable Communities in Nigeria",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The ever-increasing threat from disaster is an urgent call for a proactive discourse on pragmatic elimination and reduction of the challenges and stresses caused by disasters. This study, therefore, leverages on the research gap as it applies to the application of crowdsourced rapid response mapping in a developing country of Nigeria, where, critical geospatial data is grossly unavailable to respond to vulnerable resilient communities. The study deployed two research techniques namely: participatory crowdsourced mapping and gamification. The HOT tasking manager data analytics was used to analyze the level of participation and contribution of volunteer mappers over time while QGIS was used to produce maps unveiling building footprints generated in OSM, before and after Mapathon. The study delineated 8 LGAs for a mapping task of 2015 grids and 639 grids for Mapathon battle season-1 and 2 respectively.Season-1 was the months of flood(Rainy Season) while Season-2 was the flood receding months (dry season)  Results  unveiled analysis of flood response mapping season-1, had a total of 571,659 edits comprising 481,912 buildings and 22,244km of roads contributed by initial 7,601 participants, but completed by 1,644 volunteers, mapping 4,946 grids within a timeline of 38months at the rate of three hours 38minutes per task. 70% of volunteer mappers engaged were beginner mappers Maps showing before and after Mapathon in OSM were produced for ONELGA, Numan Sarbon Birnin and Ilorin West LGAs respectively. However, analysis of flood response mapping season-2, unveiled a total of 357,168 edits comprising 325,023 buildings and 7,438km of roads were contributed by the initial 4,006 participants, but completed by 801 volunteer mappers using 2,238 grids within a timeline of 14months at the rate of two hours 33minutes per task. Maps showing before and after in OSM were produced for Afikpo North, Warri South, Logo and Jamare LGA respectively. The Study contributed to a measurable target of SDGs 1 to 7, 11, 13, 15 and 17. The study generated massive critical open geospatial data needed for effective disaster response and SDGs, and paving way for effective geoinformation e-governance in Nigeria. Lastly, the study promotes the relevance of citizen-generated data for national geospatial data infrastructure development and participatory crowdsourced mapping using OpenStreetMap at local levels. The study has also bridged a critical scientific research gap and inquiry in OSM GIScience.",
        "description": ""
      },
      {
        "title": "Gleo Feature Frenzy",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Gleo is a nascent javascript WebGL mapping library. It aims to find a niche alongside Leaflet, OpenLayers, MapLibre and Deck.gl.\n\nThis library was presented at FOSS4G 2022, with an emphasis on its architectural foundations: geometry/reprojection/antimeridian handling, and object-oriented abstractions for WebGL data structures.\n\nThis session provides a tour of the features developed during the last year. These include, among others:\n- Work done as part of the OSGeo-OGC codesprints (OGC API clients, experimental symbols)\n- Animated symbols (render loop)\n- Symbol class decorators (ability to add more functionality to a cartographic symbol class during runtime)\n- Flexibility of scalar field manipulation (symbols that render as a magnitude instead of a colour, then the field renders as e.g. a heatmap)\n\nThese functionalities are a fresh approach to cartographic rendering and will provide a glimpse of the potential of Object-Oriented WebGL manipulation for cartographic rendering.",
        "description": ""
      },
      {
        "title": "How to get points of interest from OSM",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "This talk is exactly what it says on the tin: I want to extract restaurants or shops or train stations from OpenStreetMap. Or every POI there is. How do I do that and why extraction is so damn hard? This talk is not exactly a one-two-click instruction: we will see how data gets into OSM and why it is not easy to get it out.",
        "description": ""
      },
      {
        "title": "Locality-Sensitive Hashing with the Hilbert Curve for fast reverse geocoding",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "3geonames.org is a free api for fast reverse geocoding, using a new technique of locality-preserving hashing of 2d/3d spatial points to 1d integers via a combination of Hilbert curve and bit interlacing. This talk expands on the use-case and the performance/accuracy advantages of this technique. (The talk slides will be available at: https://3geonames.org/prizren.html )",
        "description": "https://api.3geonames.org/\nFast Reverse Geocoder Lite - A simple but fast reverse geocoding API up to city/neighborhood granularity level.\n\nConvert a geographic coordinate (latitude and longitude) into city, neighborhood name, wikipedia, wikidata, geocode, elevation, timezone, country, region and OSM tags.\n\nGenerate a truly random location anywhere on Earth; or in a particular country. Get locality results in either XML or JSON.\n\nThis reverse geocoding API is ideal for applications which process location data (for example mobile GPS data) and need to correlate this quickly with real place names. \nIt takes as input a Latitude, Longitude pair, and outputs either XML (default) or JSON text with the name of the nearest city/neighborhood and other regional information based on both Geonames.org and OpenStreetMap.org regional/city/neighborhood polygons.\n\n\nSend a reverse geocoding request as per API spec: <url>/latitude,longitude\ne.g., \n\t\thttps://api.3geonames.org/51.4647,0.0079 (XML output - Default)\nor\n\t\thttps://api.3geonames.org/51.4647,0.0079.json (JSON output)\n\nOther API Functions\n\nPick a random location at or near land: http://api.3geonames.org/randomland\n\nPick a random location anywhere: http://api.3geonames.org/random - (Note: Since the earth is mostly Ocean, most returned results will have no placename reverse geocoding responses, only latitude,longitude, geocode, geonumber and threegeonames)\n\nPick a random location in a give country (eg., Germany - DE): http://api.3geonames.org/randomland.DE (XML Default)- http://api.3geonames.org/randomland.DE.json (JSON)"
      },
      {
        "title": "Modernising Tasking Manager Infrastructure",
        "type": "Talk",
        "track": "Unknown",
        "abstract": "Talk for the HOTOSM Tasking Manager.",
        "description": ""
      },
      {
        "title": "Improving QGIS plugin developer experience",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "At the National Land Survey of Finland (NLS) we are developing multiple QGIS plugins, and we needed a way to share the common code and break the components to smaller independent plugins while still providing a good developer experience. \n\n \n\nOne of the main issues when sharing library code between different QGIS plugins is the runtime environment uncertainty. Since Python import machinery is not easily configurable to support multiple versions of dependencies (like nested node_modules in nodejs-world), the runtime is limited by default to a single version of a library, and later access to the same module is cached. This limits the version available to all plugins in a single QGIS session to the code that is first run, which makes sharing code difficult, especially when breaking API changes are necessary to the dependency library code. \n\n \n\nAt NLS we developed tooling to work around these limitations, which improves the developer experience and allows sharing of common QGIS plugin code easily via standard Python libraries. Tool provides a streamlined developer workflow and necessities like typing and IDE helpers, and a way to package a plugin that depends on other standard Python libraries. \n\n \n\nDevelopment environment for a QGIS plugin can be initialized simply by using a virtual environment, installing the dependencies and launching QGIS with the plugin and its dependencies fully setup. This works with bootstrap code passed on the command line, which will provide QGIS access to the virtual environment, setups the plugin from the environment with access to any library dependencies. Tool also provides a debugger session and could also provide for example hot reload signals for the plugin when code is changed. This provides a quicker and easier feedback cycle for the developer and simplifies the workflows when developing QGIS plugins. \n\n \n\nRuntime dependencies are reorganized at build-time to be imported for a sub-package of the plugin, so only the exact packaged version of a dependency is used at runtime. This works by rewriting external library dependency import statements in the source code. Tool also generates the metadata.txt file in a way that is compatible with standard Python packaging tools, for example setuptools. This allows easily sharing the same code both as Python library and as a QGIS plugin.",
        "description": ""
      },
      {
        "title": "Developing a real-time quality checker for the operators on QGIS",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The National Land Survey (NLS) of Finland decided in the fall of 2020 to develop a national topographic data production system based on open source technologies and especially on QGIS client. Since then, many significant steps have been taken to implement the MVP of the application for the operators of the NLS at the start of 2025. \n\nThe latest significant expansion of our product has been the development of a comprehensive and user-friendly way to handle data quality management for the operators. Our aim was to develop it in a way that changes for the quality rules could be easily made and maintained and that would be as informative as possible. The basic idea behind quality management is clear: our customers want high-quality data, and we want the operators to have clear and easy-to-understand checks for their workflow that do not limit their productivity. For this, we have developed a tool, simply named Quality management tool. \n\nThe reason we couldn't use the basic QGIS tools was that they were not easily modifiable or extensive enough for our use cases and quality demands for our data. We have been able to use some of them, such as geometry checks, but for the most part, the quality tools had to be manually selected and configured, which would take the operator's time. \n\nThe key concept of quality management is that the operator gets real time feedback about the quality, so the errors can be fixed as part of the basic workflow and there is no need for separate phases for quality control. Additionally, we would not limit the user from saving their work to their local database, regardless of the errors they may have, so that the workflow would not be interrupted. \n\nAt this moment we have published the graphical user interface for visualizing the quality check results (can be found here: https://github.com/nlsfi/quality-result-gui) but on this talk I would show how it can work on a larger scale. For this purpose, I would present the tool with use case videos. I would also like to talk about the architecture of the tool and how we are going to develop the tool even further. I hope that some listeners can apply this tool for their workflows and benefit from this example.",
        "description": ""
      },
      {
        "title": "Open data of digital twin city models in CityGML format and their import into OpenStreetMap: Project PLATEAU2OSM",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "In recent years, 3D city models have gained popularity for supporting urban planning, citizen engagement, and research. As technology and infrastructure have improved, many cities and countries now use 3D models to address urban issues, encourage participation, and inform decision-making.\n\nThe Japanese government, including the Ministry of Land, Infrastructure, Transport and Tourism's Project PLATEAU, have promoted open 3D city models and 3D point cloud data. Over 100 cities are currently developing and releasing open digital twin data in CityGML format as of February 2023. Binyu et al. published the results of these efforts, which are also highlighted in the 3D City Index benchmarking report. The report shows that seven out of 40 cities (18%) compared were Japanese cities.\n\nThis report discusses the current state of open digital twin data in Japan, which is compatible with the open database license ODbL. The data can be imported into popular tools such as OpenStreetMap, and converters have been developed for this purpose. Since 2022, import work has been conducted on an experimental basis in collaboration with national and international communities. Sharing the results and challenges of this work is expected to promote the use of 3D city model data globally.",
        "description": ""
      },
      {
        "title": "Project PLATEAU ～The initiative of Digital Twin in Japan～",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Project PLATEAU is an initiative led by the Ministry of Land, Infrastructure, Transport and Tourism of Japan (MLIT), to develop and utilize 3D city models compliant with CityGML standards. MLIT aims to establish rules of creation of 3D city models as part of general  operations in each local government, and also to release them as open data to promote utilization for urban planning and business creation.",
        "description": ""
      },
      {
        "title": "Supercharging deck.gl layers with extensions",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "deck.gl is a popular open source data visualization library that uses the power of WebGL to render huge amounts of data performantly in the browser. A collection of versatile layers allows the user to create many different types of visualizations, with excellent support for geospatial data in particular.\n\nThe core layers can be extended by the means of deck.gl extensions to create interactive experiences which are not possible in other data visualization frameworks.\n\nThis [talk](https://docs.google.com/presentation/d/1YMfMeB38mhhYXNJ5OH55r5OsieZrbugR91BcEouSm2E/edit?usp=sharing) will give an overview of deck.gl, including some of the core layers and will then focus on three of the latest extensions:\n\n  - The CollisionFilterExtension avoids collisions between features on screen. This can be used to selectively show large cities in preference to small ones on a map when they would otherwise overlap or laying out labels.\n  - The MaskExtension implements realtime masking of data by an arbitrary spatial boundary. An example use case is clipping a set of roads and places of interest to the boundary of a city.\n  - The TerrainExtension offsets the 3D component of features by referencing a separate 3D layer. For example, a set of pins on a map can be placed at the correct height relative to a 3D terrain layer.",
        "description": ""
      },
      {
        "title": "Graph-based geo-intelligence",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "We developed a free graph-based geo-intelligence engine that serves fast, scalable, and reliable data analysis. The engine's value lies in its flexibility and applicability to any relational dataset, as well as its integration of open-source technologies and libraries. We chose to build our geo-intelligence engine on a graph infrastructure to enable faster, index-free queries and better support for interconnected data.\n\nTo showcase the capabilities of our engine, we have developed a geo-financial software that provides users with a powerful tool for analyzing financial scores of companies based on geo-location. Businesses can quickly and easily analyze data to gain valuable insights into competitors, potential partnerships, and market trends. Our software presents the results of the analysis in a user-friendly and visually appealing format, making it accessible even to non-technical users.\n\nOur geo-financial analysis software is based on user-specified location and range. The user interacts with an Angular frontend, which incorporates the Leaflet library for map interaction and an OpenStreetMap basemap. The backend is based on Golang, which handles authentication and message queueing interaction with a Python analysis tool. The data retrieved for Python processing comes from a Neo4j graph database, which is accessed through Cypher queries and networking algorithms. All of the software components are located in separate containers, promoting flexible and independent scalability achieved with Docker Compose and orchestrated by Kubernetes.\n\nIn this presentation, we will discuss our graph-based geo-intelligence engine, which is the backbone of our application. We will showcase the geo-financial analysis application itself, providing a demo and demonstrating how it can be used for business geo-intelligence analysis. Throughout the presentation, we will continuously discuss the open-source technologies that are at the core of our work and focus on the value that each of them has brought to our achievements.",
        "description": ""
      },
      {
        "title": "Road condition assessment and inspection using deep learning",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Road Surface Inspector is a system developed by IT34 with the purpose of speeding up the process of road damage registration by using deep learning. The time consuming process of inspection and registration of road damage is reduced significantly by using our Road Scanner Inspector app that can be placed in the windshield of any vehicle. The app records a video and gps coordinates, which are later processed in order to find different types of damage - potholes, cracks, damaged markings using deep learning.\n\nThe system can also detect other types of assets such as traffic signs, traffic lights, manholes and others that can be used in fx digitalization tasks.\n\nThe results of the image analysis are presented on a webgis portal as heatmaps presenting the condition of the road in the areas that were inspected using the app. The heatmaps are further used by the decision makers in order to prioritize the road maintenance work. \n\nWhile using the app, Gps logs are built in realtime based on the positions sent by the phone while driving. These are further used for street inspection documentation.\n\nOpen source components.\nPostgres + Postgis for storing the data and for geometry based analysis\nPyTorch and Yolo7 for deep learning\nOpenLayers for visualizing the images/detection results as rasters in webgis\nGeoserver for publishing data as WMS/WFS\nQGis as an external visualization tool for the data",
        "description": ""
      },
      {
        "title": "A Synesthete's Atlas: Performing Cartography in Real Time",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Since April 2022 I've been manipulating projected digital maps in collaboration with improvising musicians, dancers, and spoken word artists across Europe and North America. Constraining my project to use only web mapping technologies, \"A Synesthete's Atlas\" is a curious mutation of expanded cinema, applying strategies from experimental film & animation, color theory, the Light and Space movement, and concrete poetry to geography.\n\nI'll present Carto-OSC, an assemblage of open source libraries, data, and protocols, plus 1000+ lines of JavaScript that integrates it all into a touch-surface interface. I'll discuss my motivations and use of the OSC protocol to control the manipulations, offer aesthetic observations, and present video excerpts of previous performances.",
        "description": ""
      },
      {
        "title": "Introducing GeoAI Technology to Undergraduates in Public Two-Year Community Colleges",
        "type": "Talk",
        "track": "Education",
        "abstract": "GIS instructors at an American technical college have created a five-course certificate in GeoAI.  The first cohort of undergraduate students has completed the degree requirements two years later. This presentation will discuss the formation for the degree, the courses, and the resulting graduates.  The presentation will discuss the learning outcomes for the degree and individual AI and machine learning GIS courses.",
        "description": ""
      },
      {
        "title": "Training the future with FOSS4G",
        "type": "Talk",
        "track": "Education",
        "abstract": "The use of free open source software is catching on and (at least) in Finland governmental institutions are making the big switch to open source software from other solutions. This opens up the need and possibility for training. \n\nTraining needs may differ from no previous training or knowledge to advanced GIS professionals so customising the training and exercises are important. Some might need to start with basic GIS and spatial information in general and continue to hands-on learning and multiple different exercises to help them learn the use of different tools and workflows in QGIS. \nFor more advanced users, training and helping with different programs for example GeoServer and QField or deepening the knowledge of different workflows such as visualisation or Python in QGIS are more in order.\n\nFOSS4G has also been catching on and spreading in schools and universities. These new professionals that have used FOSS4G from the very beginning of their studies can be more efficient and skillful using these different programs. They may also demand more from the software and think of new ways to modify and perfect their workflows and produce new innovations. They can be a new and very important resource for developing different areas of FOSS4G.\n\nTraining new and more experienced professionals in FOSS4G is a very important step for implementing new tools and workflows into different industries and businesses. Training also works both ways, through discussion and hands-on exercises some new and interesting needs may emerge and those could be possible to develop further into new tools or plugins. \n\nThe more institutions, businesses and other users are interested in switching to FOSS4G, the more new opportunities and needs for different tools and working methods arise. This in turn helps to develop the software further.",
        "description": ""
      },
      {
        "title": "MOOC EOODS - Massive Open Online Course for earth observation and open data science: a course to educate the next generation of EO researchers in data cubes, cloud platforms and open science",
        "type": "Talk",
        "track": "Education",
        "abstract": "The **Massive Open Online Course - Earth Observation Open Data Science (MOOC EOODS)** teaches the concepts of data cubes, cloud platforms, and open science in the context of Earth Observation (EO). \n\nIt aims at Earth Science students, researchers, and Data Scientists who want to increase their technical capabilities onto the newest standards in EO cloud computing. The course is designed as a MOOC that explains the concepts of cloud native EO and open science by applying them to a typical EO workflow from data discovery, data processing up to sharing the results in an open and FAIR way. \n\nThe [EO College platform](https://eo-college.org/welcome) hosts the course and hands-on exercises are carried out directly on European EO cloud platforms, such as [Euro Data Cube](https://eurodatacube.com/) or [openEO Platform](https://openeo.cloud/), using open science tools like the [Open Science Data Catalogue](https://opensciencedata.esa.int/) and [STAC](https://stacspec.org/en) to embed the relevance of the learned concepts into real-world applications. The MOOC is an open learning experience relying on a mixture of animated lecture content and hands-on exercises created together with community renowned experts. \n\nAfter finishing the course, the participants will understand the concepts of cloud native EO, be capable of independently using cloud platforms to approach EO related research questions and be confident in how to share research by adhering to the concepts of open science.\n\nThe MOOC is valuable for the EO community and open science as there is currently no learning resource available where the concepts of cloud native computing and open science in EO are taught jointly to bridge the gap towards the recent cloud native advancements in EO. The course is open to everybody, thus serving as teaching material for a wide range of purposes including universities and industry, maximizing the outreach to potential participants. \n\nOur talk will give an overview of the MOOC at the current status. Furthermore, we encourage review, feedback on its content and discussion.",
        "description": ""
      },
      {
        "title": "Introduction to Coordinate Systems",
        "type": "Talk",
        "track": "Education",
        "abstract": "Introduction to basic but important concepts about Coordinate Reference Systems (what is doable in 20 min ;)\n\n   - Geographic Coordinate (Reference) Systems\n   - Different Datums/Ellipsoids\n   - Projections (Mercator, UTM, LCC, ...)\n   - EPSG catalog\n   - WKT (well known text) description\n   - Reference to PROJ.org library\n\nThe purpose is to explain basic concepts to have a good basis to understand later more complex problems. The presentation will have a lot of links to go deeper into any area of interest.",
        "description": ""
      },
      {
        "title": "DGGSs and you!",
        "type": "Talk",
        "track": "Education",
        "abstract": "Discrete Global Grid Systems (DGGS) are gaining popularity as a new method of geospatial data representation. This presentation will provide an overview of the concept of DGGS and its advantages over traditional geospatial data representation methods. \n\nWe will explore the similarities and differences between these different DGGS frameworks, including their cell shapes, grid resolutions, and ability to handle different types of geospatial data. We will also discuss the benefits of using DGGS in geospatial data applications, such as remote sensing, climate modeling, and environmental monitoring.\n\nOverall, this presentation will provide a comprehensive overview of the concept of DGGS and its potential applications in geospatial data analysis and visualization. Attendees will gain a deeper understanding of the advantages and challenges associated with different DGGS frameworks and will gain insights into the ongoing research efforts in this field.",
        "description": ""
      },
      {
        "title": "Many Data Sources, One Web Map: Data cleaning and optimization with FOSS",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Long Island Zoning Atlas is an interactive web map that displays zoning data, public services, and demographic data for municipalities all across Long Island excluding New York City. The app focuses on statistics that help affordable housing advocates plan housing projects. This year we rebuilt the Long Island Zoning Atlas using our new FOSS stack. The project presented a problem very common to GIS projects: transforming data from many different sources, in this cases towns. We were given the data in many different formats and needed to transform it all into clean, usable data which is organized to our needs and renders quickly and efficiently on the web.",
        "description": ""
      },
      {
        "title": "Look, how we build geospatial CMS without using GeoServer and EAV!",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Using generic or standard content management system (CMS) like Wordpress or Strapi for managing geospatial data isn't an optimal solution. Since object geometry isn't just one of many data fields, it requires special handling for setting the data (e.g., on the map), storing data, transforming data for various needs (geometry output format, CRS etc.) and using them for spatial analysis.\n\nWhen talking about a geospatial CMS, one would think that using GeoServer should be a must. How else would you vizualize a non-trivial amount of data on the map, right? Although Geoserver might be a good answer, that's not the only one. We, at our company, have developed our custom geospatial CMS using the OpenLayers mapping library on the frontend and PostgreSQL (with PostGIS, of course) on the backend, using PHP Laravel and GeoJSON as middle man between the data store and the frontend.\n\nCMS platforms frequently have one specific feature. Different objects may have various attributes. Using the EAV (entity-attribute-value) model is one of the methods that is frequently utilized, although this choice usually comes with a number of issues, such as querying and storing the data. We used the possibility to swap out the EAV model for a straightforward json field in our CMS.\n\nThis talk will present what choices we had to make to build solution in such way and what some of our challenges were.",
        "description": ""
      },
      {
        "title": "Self-hosted CMS maps for everyone",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Privacy aware Content Managment System (CMS) operators don't let their viewers accept cookies from an external map provider. But creating a map used to require specialized GIS knowledge and hosting a map server is not everyone's cup of tea.\n\nThis talk explains how non-experts can serve a map based on OpenStreetMap vector tiles from a CMS. A MapLibre GL JS based Wordpress plugin displaying a self-hosted PMTiles dataset is shown as an example.",
        "description": ""
      },
      {
        "title": "River Runner: navigating and indexing hydrologic data with open standards and data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Hydro Network-Linked Data Index (NLDI) is a system that can index data to a hydrographic network  and offers a RESTful web service to discover indexed information upstream and downstream of arbitrary points along the stream network. This allows users to search for and retrieve geospatial representations of stream flowlines, catchments, and relevant water monitoring locations contributed by the water data community - without downloading the national dataset or establishing links themselves.\n\nThis is done by data providers publishing open information about the locations of their data within the context of the U.S. stream network. Data linked to the NLDI includes various federal, state and local water infrastructure features and water quantity and quality monitoring locations. The NLDI is being developed as an open source project and welcomes contributions to both its code and indexed data, with the main implementation currently being maintained by the U.S. Geological Survey. \n\nThe community of practice surrounding the NLDI extends to R and python developers working on clients that allow scientists to quickly retrieve data relevant for specific hydrologic analyses. As the NLDI community grows, a similar concept could be applied at a global scale, facilitating the development of downstream tools and applications. \n\nWhile the NLDI is limited to the US, global work would be possible by leveraging global stream network datasets such as MERIT-Hydro. A proof-of-concept global River Runner allowing discovery of the flowpath downstream of arbitrary points anywhere on Earth has already been implemented using MERIT-Hydro and OGC-API Processes in pygeoapi. This session includes demonstrations of the NLDI and the global River Runner.",
        "description": ""
      },
      {
        "title": "Transit Access to Essential Services in the face of Climate Change",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Climate change’s impact on public transportation tends to focus on improving transit infrastructure to reduce stoppages. While this is important, it does not take into account the effect it has on communities, often already underserved, that rely on the transit system. As part of The Opportunity Project’s Building Climate Change Resilience Through Public Transit sprint, our team at Data Clinic set out to develop an open source, user-friendly, and scalable tool to communicate intersectional risks faced by transit infrastructure and community access at the local level. This solution was inspired by both the event, and user research with key stakeholders in transit agencies, academia, and community organizations.\nIn this presentation, we will demonstrate TREC: Transit Resiliency for Essential Commuting, and expose key decisions that resulted in a geospatial solution designed for wide audiences, and geographic and data scalability. TREC’s transit stop-level insights can become crucial tools for transit planners and community organizations to prioritize and advocate for infrastructure improvements that take community effects into account.\nFocused initially on two locations- one small (Hampton Roads, Virginia) and one large (New York City) transit system, each station is treated as a destination providing access to essential services during localized climate change events. In this MVP, we employ flooding as our climate scenario, the event most cited as recurring and disruptive by our stakeholders. \nUsing OpenStreetMap to calculate walksheds around each station obtained from GTFS data, we categorize importance in accessing essential services such as hospitals and jobs around a transit stop. Layered onto this, we bin current flood risk for each station using the prevalence of buildings with moderate- to extreme high-risk of flooding according to open data, and provide polygons representing projected flood risk in 2050.\nWhile we built the TREC UI to maximize accessibility of this contextualized data to multiple stakeholders, we also seek to optimize usability of the repo to allow tech-mature transit planners to adopt the tool internally and incorporate their proprietary fine-grained data. Further, we are committed to expanding the functionality of TREC according to user feedback. \nThe threat of climate change disrupting daily life on a recurring basis, beyond large-scale disasters, continues to grow. With the help of this tool, we hope to democratize relevant data, inspire the open publication of localized geospatial data related to climate change, and enable human-centered decisionmaking through a multidimensional lens.",
        "description": ""
      },
      {
        "title": "Editable topologies in pgRouting",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "pgRouting, a PostGIS extension containing algorithms and tools for working with graph data, has become a highly flexible member of the FOSS routing engine family. In this talk, I want to demonstrate just how flexible it can be by showing how routable networks (called 'topologies' in pgRouting) can be made editable.\n\nI will take the audience from theoretical conception of editable topologies (how can edits, insertions and deletions be handled in PostGIS?) through its implementation. Finally, I will end with a demonstration of a fully editable topology in a web mapping application based on a real world example using OpenStreetMap data.",
        "description": ""
      },
      {
        "title": "Mapping Japan cultural heritages with OpenSource based architecture",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Japan fascinates the world with its rich culture, materialized with a full of cultural sites in its territory as example. To protect it, the Law for the protection of cultural properties established a “cultural heritage” designation system, where designated places should be preserved. \nWith the collaboration of the Nara National Research Institute for Cultural Properties, Japan cultural heritages has been mapped as a WebGIS tool where more than 100,000 places can be visualized. \nIn this talk will be presented tool functionalities and technically its OpenSource based architecture.",
        "description": ""
      },
      {
        "title": "Development of tool for validity of decision support algorithm for environment impact assessment (EIA) Based on open source",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The open source based environmental impact assessment(EIA) decision support verification tool(verification tool) is a web-based tool for verifying the EIA algorithm based on the EIA review decision support algorithm using data for each Environmental Impact.\nThis verification tool was developed using open source projects such as PostGIS, GeoServer, and Openlayers. However, the flowchart library used a commercial software called GoJS.\nThis verification tool is intended to verify the adequacy of the implementation of the EIA algorithm developed by experts in each Environmental Impact.\nIt is possible to support comprehensive decision-making, including opinion gathering, by operating the review decision-making algorithm based on data by Environmental Impact and environmental impact analysis results.\nThe spatial analysis required to verify the algorithm was developed using OpenGXT of the OGC WPS service. It includes a way to visualize the result processed through this spatial analysis function.\n\nThis paper is based on the findings of the research project “Development of integrated decision support model for environmental impact assessment project,”(2023-003(R)) which was conducted by the Korea Environment Institute (KEI) and funded by research and development project (Project No. 2020002990003) of the Environmental Industry & Technology Institute (KEITI) and the Ministry of Environment (MOE).",
        "description": ""
      },
      {
        "title": "How UNICEF is leveraging open-source geospatial solutions to drive better results for children",
        "type": "Keynote",
        "track": "Use cases & applications",
        "abstract": "“Whether it is to know where children are, what access they have to facilities (education, health, transportation), what environment they live in (water, air), where risks exist (hazards, diseases), where events happen or where services and resources are available; most of the operational data used by UNICEF is geospatial” (UNICEF Geospatial Roadmap, 2019). At UNICEF we realize that we need to leverage geospatial information to enhance decision-making and optimize resource allocation and drive effective interventions. Geo-enabling UNICEF’s data, systems and processes aims at transforming data into easily accessible, readily available, and actionable geospatial information that can address key questions, such as: “How many children have been affected by a flood?”, “Where children have limited access to schools and limited access to health services?”. This information is critical to support decision-making to ultimately drive better results for children.\nUNICEF has recently adopted a hybrid corporate geospatial architecture, which aims at bringing together the advantages of both commercial and open-source GIS world. This presentation aims at discussing how UNICEF is leveraging modern open-source geospatial solutions to address some of the key data-management challenges.\nSpecifically, two open-source geospatial projects developed by UNICEF will be showcased and discussed: GeoRepo and GeoSight. GeoRepo is a web-based system that will help us store, manage and share a commonly agreed, versioned, official set of administrative boundaries and other core geospatial datasets. It will help us ensure that geospatial data is used consistently across all internal systems and will also strengthen our interoperability with external systems. GeoSight, on the other hand, is a web geospatial data platform developed by UNICEF to bridge the gap between web mapping systems and the Business Intelligence / data analytical platforms. GeoSight is specifically designed to simplify the process of harmonizing data from multiple data sources. It also allows users to easily create online maps for visualizing multiple indicators at subnational levels (e.g. at the province or district level). Both platforms are built using Django and React and use modern open-source geospatial standards and libraries, such as MapLibre and vector tiles.",
        "description": ""
      },
      {
        "title": "Why FOSS4G Needs a Global Open Data Platform",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "In recent years, the software industry has witnessed a remarkable trend away from traditional standalone applications and towards online multiplayer platforms that offer users a more integrated and collaborative experience. \n\nAs this trend continues, it is becoming increasingly important for open source tools to stay competitive by providing seamless access to data and connectivity.\n\nIn this talk I will introduce mapstack, outline our mission to bring all of the world’s open location data together in one place, and share my thoughts on how such an unprecedented open resource will benefit the wider FOSS4G ecosystem.",
        "description": ""
      },
      {
        "title": "Open Data for Geospatial: Opportunities and Challenges",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Open data and geospatial technology have the potential to revolutionize decision-making processes across a variety of sectors, including urban planning, disaster response, environmental management, and more. However, the use of open data in the geospatial domain poses its own set of challenges, including data quality, reliability, and standardization concerns. Managing, maintaining, and updating large datasets can also be resource-intensive, posing a challenge for organizations and communities that rely on open data.\n\nThis talk will explore the opportunities and challenges of using open data in the context of geospatial technology. I will begin by discussing the potential benefits of open data, including increased transparency, improved collaboration, and the ability to make more informed decisions. I will then delve into the key challenges of using open data in geospatial contexts, including issues related to data quality and reliability, standardization, and the sheer volume of data. We will explore strategies for managing and maintaining large datasets, such as crowdsourcing and automated data processing, and discuss best practices for ensuring data quality and reliability.\n\nThis talk is relevant to anyone interested in the intersection of open data and geospatial technology, including data scientists, GIS professionals, policymakers, and community leaders. Attendees will come away with a deeper understanding of the opportunities and challenges of using open data in geospatial contexts and gain practical insights on how to leverage this data to drive social and economic impact. By the end of the talk, attendees will be equipped with the knowledge and tools they need to make the most of open data in the geospatial domain.",
        "description": ""
      },
      {
        "title": "Data Governance with Open Metadata Integrating OGC - CSW Services",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The Open Metadata platform allows the integration of data and metadata for the management of governance within an organization to integrate different sources, control its publication, its access, standardize the processing and even to be able to analyze the lineage. What we are going to share is the adaptation of one of the data sources to the OGC - CSW service to be able to consume the cataloged metadata transparently in the system.",
        "description": ""
      },
      {
        "title": "Rethink geo/open metadata edition in GeoNetwork",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "This presentation is the follow up of the datahub paradigm presented last year: The confluence of geo data and open data. This time we will look at the metadata edition and maintenance aspect.\n\nWriting metadata to describe a dataset is an essential part of managing a catalog. Each record in a catalog has been written, or at the very least enriched, by actual humans. GeoNetwork is a very widely used open-source metadata catalog; as such, it offers powerful tools in this regard: custom edition forms, batch editing, templates, custom XSL processing, advanced edition in XML, etc.\nDespite all these features, authoring metadata is often felt as a difficult process, involving complex actions, convoluted validity rules and an intricate knowledge of metadata schemas like ISO19139.\n\nOur vision for this new metadata editor can be summed up in three phrases:\n- Make metadata accessible to everyone\n- Forget about metadata schemas\n- Build your own editor\n\nThis editor is made to feed content into your Datahub. Whether you want to describe open data, geo data or anything else, the editor will make it simple for you! Come and discover the concepts behind the scenes.",
        "description": ""
      },
      {
        "title": "Orfeo ToolBox : roadmap to a more modular and pythonic OTB",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Orfeo ToolBox is now a mature software with more than 100 applications dedicated to remote sensing and data extraction.  \nIt is used both in academic works, in operational processing chains.\nOTB now needs to be more modular (\"core\", \"machine learning\", \"SAR\", \"feature extraction\") and also easier to use through Python. \nWe will present the recent developments and our roadmap.",
        "description": ""
      },
      {
        "title": "Kart: Practical Data Versioning for rasters, vectors, tables, and point clouds",
        "type": "Talk",
        "track": "State of software",
        "abstract": "We’re drowning in data, but the geospatial world lags badly behind in versioning tools compared to our software counterparts. Kart (https://kartproject.org) is solving this with a practical open tool for versioning datasets, enabling you to work more efficiently and collaborate better.\n\nWe will introduce you to Kart and demonstrate some of the key features, including our QGIS plugin. And we'll highlight what’s coming next on our roadmap.\n\nSince 2022 we have added support for Raster and Point Cloud datasets, and we'll be showing how we build on Kart's versioning and spatial filtering techniques to efficiently navigate, access, and use large and small datasets. For rasters and point-cloud datasets, we'll show how you can get the benefits of Kart without having to duplicate data that is already hosted in S3 in a useful format.\n\nKart allows you to quickly and easily manage history, branches, data schemas, and synchronisation for large & small datasets between different working copy formats, operating systems, and software ecosystems.\n\nModern version control unlocks efficient collaboration, both within teams and across organisations meaning everyone stays on the same page, you can review and trace changes easily: ultimately using your time more efficiently.",
        "description": ""
      },
      {
        "title": "Spatial Analysis with the CARTO Analytics Toolbox",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The CARTO Analytics Toolbox (AT) is a collection of spatial functions that add spatial capabilities to Data Warehouses. At the moment, BigQuery, Snowflake, Redshift and PostgreSQL versions are available.\n\nThis talk will show some of the main functions of the AT, and discuss some examples of spatial data analysis performed in different DWs. Special emphasis will be put on the functionality related to spatial indexes, particularly H3 and Quadbin.\n\nThe Analytic Toolbox functions are also the building blocks for other tools both from CARTO and outside of CARTO, which will be briefly introduced as well.",
        "description": ""
      },
      {
        "title": "Revolutionizing Solar Potential Assessments in Kosovo Using Drones and Machine Learning",
        "type": "Keynote",
        "track": "Unknown",
        "abstract": "Imagine a future where entire communities can harness the power of the sun to fuel their homes and businesses, reducing their dependence on traditional energy sources and helping to build a more sustainable world. At FOSS4G, I am excited to share with you a groundbreaking project that is making this vision a reality in Kosovo, using the latest geospatial technology.\n\nThrough the USAID funded Kosovo Energy Security of Supply (KESS) activity, DT global is working to promote sustainable energy solutions in Kosovo. A partnership between DT Global and DevGlobal, are leveraging the power of drones, GIS software, and open-source machine learning models to revolutionize the way we evaluate the solar potential of individual structures. By accurately delineating the boundaries of rooftops using drone imagery, we can then apply cutting-edge photogrammetry analytics to determine the optimal placement of solar panels.\n\nBut we're not stopping there. By training the Ramp* open buildings model to successfully identify and delineate rooftops in Kosovo, using data obtained from the Kosovo Cadastral Agency's 2023 high-resolution aerial survey campaign, we are laying the groundwork for a national-level approach to mapping building footprints that can be utilized for a range of applications beyond evaluating rooftop solar potential.\n\n*Ramp is an open-source machine learning model and toolset for extracting building footprints from high-resolution satellite imagery at scale.",
        "description": ""
      },
      {
        "title": "Closing session",
        "type": "Talk",
        "track": "Unknown",
        "abstract": "FOSS4G 2023 conference closing session.\nAnnouncement of the 2023 Sol Katz Award recipient.",
        "description": ""
      },
      {
        "title": "OSGeo AGM",
        "type": "Talk",
        "track": "Unknown",
        "abstract": "OSGeo Foundation Annual General Meeting",
        "description": ""
      },
      {
        "title": "What in-game maps can teach us",
        "type": "Talk",
        "track": "Unknown",
        "abstract": "Let's look away from familiar continents and comfortable symbolics. When you are making an entirely new world, how do you map it? When any choice can be made from scratch, why game makers sometimes use common carthographic paradigms, or circumvent them? And given we are at a GIS conference, what can we learn from imaginary maps, that can improve our real-world work? Let's connect a Nintendo Switch to a projector and dive into games!",
        "description": ""
      },
      {
        "title": "pycsw project status 2023",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pycsw is an OGC CSW server implementation written in Python and is an official OSGeo Project. pycsw implements clause 10 HTTP protocol binding - Catalogue Services for the Web, CSW of the OpenGIS Catalogue Service Implementation Specification, version 3.0.0 and 2.0.2. pycsw allows for the publishing and discovery of geospatial metadata, providing a standards-based metadata and catalogue component of spatial data infrastructures. The project is certified OGC Compliant, and is an OGC Reference Implementation.\n\nThe project currently powers numerous high profile catalogues such as IOOS, NGDS, NOAA, US Department of State, US Department of Interior, geodata.gov.gr, Met Norway and WMO WOUDC. This session starts with a status report of the project, followed by an open question answer session to give a chance to users to interact with members of the pycsw project team. This session will cover how the project PSC operates, the current project roadmap, and recent enhancements focused on ESA's EOEPCA, Open Science Data Catalogue and OGC API - Records.",
        "description": ""
      },
      {
        "title": "pygeoapi project status 2023",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pygeoapi is an OGC API Reference Implementation. Implemented in Python, pygeoapi supports numerous OGC APIs via a core agnostic API, different web frameworks (Flask, Starlette, Django) and a fully integrated OpenAPI capability. Lightweight, easy to deploy and cloud-ready, pygeoapi's architecture facilitates publishing datasets and processes from multiple sources. The project also provides an extensible plugin framework, enabling developers to implement custom data adapters, filters and processes to meet their specific requirements and workflows. pygeoapi also supports the STAC specification in support of static data publishing.\n\npygeoapi has a significant install base around the world, with numerous projects in academia, government and industry deployments. The project is also an OGC API Reference Implementation, lowering the barrier to publishing geospatial data for all users.\n\nThis presentation will provide an update on the current status, latest developments in the project, including new core features and plugins. In addition, the presentation will highlight key projects using pygeoapi for geospatial data discovery, access and visualization.",
        "description": ""
      },
      {
        "title": "geOrchestra - project status",
        "type": "Talk",
        "track": "State of software",
        "abstract": "geOrchestra is a complete spatial data infrastructure (SDI) and combines a number of widely used open source components. These include GeoNetwork as a metadata catalogue, GeoServer, GeoWebCache, GeoFence, and Jasig CAS. During this talk we will present the project and its latest developments.\n\ngeOrchestra is an open source, modular, interoperable and secure spatial data infrastructure designed by people for people.\n\nThe technical architecture is based on modularity and interoperability. The extensive use of the Spring Framework allows the integration of additional components. Compliance with OGC standards is central, because only then can the various components and any external IDS work together.\n\ngeOrchestra is supported by an underlying server infrastructure, which can be configured in an automated way if necessary. We support deployment on Kubernetes as well as Ansible. geOrchestra has proven to be an innovative IDS in a highly orchestrated environment. Its modular architecture allows it to deploy individual components as microservices. Individual components such as GeoServer-cloud or GeoNetwork Microservices can therefore be scaled as needed.\n\nNevertheless, an SDI must be user-friendly and adopt a user-centric approach. This is the latest development that the geOrchestra community has started to follow. New modules such as the Datafeeder simplifies the data registry and the Datahub portal makes it very easy for a user to find the right dataset.\n\nCurrent developments related to geOrchestra include a rewrite of the GeoNetwork metadata catalogue to provide a complete new user interface for editing metadata.",
        "description": ""
      },
      {
        "title": "Interfacing QGIS processing algorithms from R",
        "type": "Talk",
        "track": "State of software",
        "abstract": "R is well-known for its unsurpassed provision of well documented statistical functions and packages in the default installation. Less well-known is its excellent support for spatial data through packages such as sf, terra, and stars. A thriving ecosystem of diverse and often topic-specific packages build on these foundations, making R a powerful command-line GIS (Geographic Information System) for reproducible research. However, dedicated GIS software (e.g. QGIS) offers specific processing algorithms that are either not available in R, or may achieve a higher level of performance than their equivalents in R. This presentation describes how it is now possible to combine the strengths of R and QGIS through R packages that interface processing algorithms provided by QGIS. These packages (qgisprocess, qgis) allow users to create data processing pipelines that combine R and QGIS algorithms almost seamlessly. We discuss the current state of these R packages and demonstrate the usage of their most important functions by example. Finally, we shed light on future development directions and seek feedback from the community.",
        "description": ""
      },
      {
        "title": "Easily publish your QGIS projects on the web with QWC2",
        "type": "Talk",
        "track": "State of software",
        "abstract": "QWC2 (QGIS Web Client 2) is the official web application of QGIS, that allows you to publish your projects with the same rendering, thanks to QGIS Server. The environment is composed of a modern responsive front-end written in JavaScript on top of ReactJS and OpenLayers, and several server-side Python/Flask micro-services to enhance the basic functionalities of QWC2 and QGIS Server.\n\nQWC2 is modular and extensible, and provides both an off-the-shelf web application and a development framework: you can start simple and easy with the demo application, and then customize your application at will, based on your needs and development capabilities.\n\nThis talk aims at introducing this application and to show how easy it is to publish your own QGIS projects on the web. An overview of the QWC2 architecture will also be given. It will also be an opportunity to discover the last new features that have been developed in the past year and ideas for future improvements.",
        "description": ""
      },
      {
        "title": "Investigating war crimes, animal trafficking, and more with open source geospatial data (encore)",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "At Bellingcat, a non-profit investigative organization in the Netherlands, we research war crimes, find tiger smugglers, monitor environmental degradation and track extremist hate. To do this, we use \"open sources\", including public databases, social media posts, and a wide range of geospatial data and tools. The use of these new online sources has dramatically changed investigative journalism and humanitarian accountability research in the past five years, and there remains tremendous potential for further development, especially in the geospatial realm.\n\nIn this talk, Bellingcat data scientist Logan Williams will present case studies from our research to illustrate how invaluable open source geospatial tools and data are for \"open source\" investigative research. Some of the most useful tools for investigators are designed for very different purposes, from academic meterology to outdoor recreation. Additionally, some of Bellingcat's own FOSS geospatial tools, based on Open Street Map and Copernicus satellite data, will be showcased. Finally, the talk will discuss opportunities for deepening the connections between the open source geospatial community and the open source investigation community.\n\nBy popular demand this is an encore of the talk held on 28.06 @ 15:00",
        "description": ""
      },
      {
        "title": "GeoNode UI: Deep Dive on MapStore and Django integration for GeoNode",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GeoNode is a Web Spatial Content Management System that uses the Django Python web framework. MapStore is an open source WebGIS product and highly customizable framework that has been used as the default user interface to visualize catalog, map viewer and geospatial applications in GeoNode.\n\nThis presentation provides an overview of the integration of the MapStore framework inside the GeoNode ecosystem and the main differences with the MapStore product, along with guidelines and references to resources for its customization and the development of custom functionality.",
        "description": ""
      },
      {
        "title": "MapStore real world case study: the hybrid infrastructure of the City of Genova",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Born in 2016 thanks to the funding of the National Operational Program for Metropolitan Cities (PON METRO 2014-2020) the current Spatial Data Infrastructure (SDI) of the city of Genova is a hybrid infrastructure, where open source components and technologies are merged together with proprietary ones (such as the Oracle Database) in a well designed platform with respect of all national guidelines (promoted by AgID - Agenzia per l’Italia Digitale) and international standards. \nTo support the Geoportal initiative, the city of Genova has collaborated with GeoSolutions as a company closely involved in the most important Open Source projects worldwide in the geospatial field with the aim to provide the necessary support for all the SDI stack in terms of  deployment, development but also the staff training to make it autonomous as much as possible in the maintenance of the overall system.\nThe city of Genova Geoportal as well as the wider Geospatial Infrastructure are both reachable online.\nA simple and at the same time robust WebGIS based on the Open Source MapStore software is provided with the inclusion of both advanced GSI functionalities and also most common geospatial tools like:\n\n- Geospatial data search via OCG Web Services and Nominatim\n- 2D and 3D visualization of geospatial data using a map agnostic engine supporting OpenLayers, Leaflet and Cesium for the 3D\n- Editing and Styling of geospatial layers\n- Download functions of geospatial data working on top of OGC services \n- And many more\n\nThe aim is to provide ready-to-use tools for all users (both citizens and employed analysts worked in the PA) by leveraging the maturity of the Open Source Software as well as the simplicity of integration with the pre-existing COTS software in order to maximize the reuse of the existing infrastructure and minimize the need for customizations and a possible use of commercial support even for educational purposes.\nMany cross-cutting projects usually gravitate around the SDI in the Public Administration and its own Geoportal. To date, more than 300 geospatial layers are available in the Geoportal which allows them to be viewed and consulted within preconfigured MapStore maps, dashboards and geostories and/or used through geospatial services (such as WMS, WMTS, WFS, WCS and CSW) developed according to international standards (OGC - Open Geospatial Consortium) and exposed through GeoServer and GeoNetwork with also a fine grained security tier represented by GeoFence to manage authorizations on geospatial data.",
        "description": ""
      },
      {
        "title": "Securing Your Open Source Geospatial Stack with Single Sign On",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "(or what happens when GeoServer and PostGIS meet Active Directory)\n\nThis talk will present a case study of how Astun implemented a single sign on (SSO) system for a large \ncommercial client. The client stored their spatial data in a PostGIS database and provided both direct access \nto the database via QGis and from QGis via WMS using GeoServer to carry out the styling and rendering of the \ndata. Staff are divided into 4 teams and then are subdivided by end client in to small groups. Some of the \ndata in the system is restricted to just the group working on a specific problem for a specific client, other \ndata is shared with the whole team, and some is available to the whole company.\n\nThe client brief was to move their on site system to \"the cloud\", and to allow staff to connect to the data \nfrom anywhere in the world with only one user account and password for access to PostGIS and GeoServer data. \nInitially, the project planned to leverage the existing corporate Azure Active Directory system to provide the \nnecessary authentication and authorizations. However, early experiments showed that the time between \nrequesting a new group and it appearing on the server was (sometimes) longer than the lifetime of the new \ngroup. \n\nAstun provided an open source solution, using Keycloak to handle the user and administrator facing frontends, \nwith user data being stored in an OpenLDAP server. It was then possible to make use of the LDAP service to \nperform authentication and authorization of users to both PostGIS and GeoServer, making sure that data \nrestrictions applying in one were duplicated in the other. \n\nThe talk will cover details of the process and look at some of the issues that were encountered during the \nproject.",
        "description": ""
      },
      {
        "title": "Modernising Tasking Manager infrastructure using Terraform, cloud-native tools and good sense",
        "type": "Talk",
        "track": "Unknown",
        "abstract": "Learn how Humanitarian Openstreetmap Team uses modern tools like Terraform, AWS serverless, and other tools to modernise the collaborative mapping tool - Tasking Manager. The talk will focus on balancing infrastructure costs, cloud vendor lock-in, performance and DevOps processes.\n\nTasking Manager is an important collaborative mapping tool that is considered a public good. In recent times, the tool has left a lot to be desired in terms of performance and availability. The HOT Tech team set out to overhaul the architecture, and deployment processes of Tasking Manager. I discuss the soon-to-go-live improvements that touch upon Terraform, AWS Serverless, CircleCI, Observability processes, and Developer Experience. \n\nLinks: https://github.com/hotosm/tasking-manager",
        "description": ""
      },
      {
        "title": "Open source tooling for hydrodynamic simulation software development",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In this talk we give an example of how open source tooling enables companies to fast-track software development, while simultaneously benefitting the FOSS4G community. Our use case is the development of the user interface for hydrodynamic simulation software, including editing and analysis, called the 3Di Modeller Interface.\n\nTraditionally hydrodynamic simulation software companies develop their own user interfaces, usually closely resembling GIS packages, (re-)implementing features like background maps, layer management, geoprocessing tools, and styling options. In our approach we turned it around. Instead of developing our own GIS-like software, we used QGIS to leverage development. Specifically for larger governmental agencies (where a certain well-known proprietary GIS suite is often the only GIS that employees are allowed to use), we packaged our implementation in an installer, enabling modellers to use QGIS for hydrodynamic analysis within their organisations.\n\nThis approach has several advantages for users and for the FOSS4G community. For users, hydrodynamic modelling tools seamlessly integrate with the ever expanding GIS capabilities that QGIS has to offer; and users can built their own custom tooling, combining our own open libraries for hydrodynamic modelling with FOSS4G libraries like PyQGIS, Shapely, NetworkX, GDAL or QGIS.\nFor the FOSS4G community, this approach increases the user base, including users that are into developing their own plugins, it increases sustainable memberships, and creates job opportunities for FOSS4G developers.\n\nThe 3Di Modeller Interface is developed by Nelen & Schuurmans, a Dutch water and IT company, in collaboration with Lutra Consulting, a European FOSS4G company. Its development relies on several open source projects: QGIS, Shapely, GDAL, GeoAlchemy2, and NetworkX, amongst others. When we started in software development, we used open source mainly because it was free of cost. During the development, the board of directors became convinced that contributing to several open source projects (financially and/or developing) is the way forward.",
        "description": ""
      },
      {
        "title": "Tracking Climate Change in Africa with open data",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Climate Change is affecting our daily lives. Already for many years, we are interested in how this will influence agriculture and livelihoods on the African continent. In this talk we will show a tracking methodology with open data and opensource software. The main data source is satellite imagery from METEOSAT (MSG) as well as rainfall estimates by NOAA to show trends in the last 15 years. We will share links to free data and scripts and make a list of all software used in a step-by-step guide.",
        "description": ""
      },
      {
        "title": "Running OGC API - Features as Smart Contract",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "Motivation\n\nNew, evolving technologies allow to host data and program code (smart contracts) on distributed blockchains. Beside other aspects, like validating geospatial data and their transactions, this technology might also be interesting for building distributed services for the ‘classical’ spatial data infrastructures.   \n\nPrototype\n\nDuring the last months a prototype was developed to test the capabilities of smart contracts to distribute spatial data using the OGC API – Features specification and gain some experiences in its design, typical workflows, limitations etc.\nThe prototype is designed as smart contract on the ‘Internet Computer (IC)’ blockchain (see https://internetcomputer.org/). This allows to store program code and the spatial data in one container on the blockchain and execute it on demand.\nTo simplify the test, a fixed workflow is implemented. \n\n1)\tData providers upload a spatial dataset (currently glider GNSS tracks in the IGC format) on a simple webpage running within the container\n\n2)\tThe spatial data is persisted on the IC blockchain \n\n3)\tUsers access the data via OGC API – Features with their browser (html representation) or with their GIS\n\nPresentation\n\nIn the presentation, I would like to share some experiences on developing geospatial interfaces in a blockchain environment and show the current state of the prototype. Especially the coding with the programming language ‘Motoko’, the exposed interfaces, and the distribution on the blockchain with its costs will be addressed. \nI would further like to discuss use cases of the approach, e.g. a simplified data distribution for smaller data providers, and the potential extensions on this approach, like introducing a user management, adding metadata or integrating dynamic data sources. \n\nLinks\n- Entrypoint (OGC API Features): https://mtlom-hiaaa-aaaah-abtkq-cai.raw.ic0.app/\n- Github page (mainly Motoko code): https://github.com/janschu/igc_tools \n\nRelated:\n- Internetcomputer (IC): https://internetcomputer.org/\n- IGC - International Gliding Commission – GNSS Flight Recorders Spec: https://www.fai.org/sites/default/files/igc_fr_specification_2020-11-25_with_al6.pdf \n- OGC API – Features: https://ogcapi.ogc.org/features/",
        "description": ""
      },
      {
        "title": "B6, Diagonal's open source geospatial analysis engine",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "Diagonal is a steward-owned data science consultancy working with projects in the built environment. We build interactive tools to help people understand the tradeoffs inherent in their plans to evolve cities. Our tools are powered by B6 - an in-memory geospatial analysis engine we built to work with large data sets describing the built environment. We typically use it work work with OpenStreetMap and open government data. To enable others to repeat our analyses, we recently released B6 as open source. In this talk, we'll give an overview of B6, including how it's implemented, and how we use it in our commercial work.",
        "description": ""
      },
      {
        "title": "fAIr - Free and Open Source AI for Humanitarian Mapping",
        "type": "Talk",
        "track": "Unknown",
        "abstract": "The name fAIr is derived from the following terms:\n\nf: for freedom and free and open-source software\nAI: for Artificial Intelligence\nr: for resilience and our responsibility for our communities and the role we play within humanitarian mapping\n\nfAIr is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT), designed to enhance the efficiency and accuracy of mapping efforts for humanitarian purposes. By utilizing computer vision techniques and open-source AI models, fAIr detects crucial map features from satellite and UAV imagery, starting with buildings. The service fosters collaboration with local communities, enabling them to create and train their own AI models, ensuring relevance and fairness in mapping. Through a constant feedback loop, fAIr progressively improves its computer vision models, contributing to the continuous advancement of humanitarian mapping.This talks will talk about our journey and vision for using AI",
        "description": ""
      },
      {
        "title": "Mergin Maps: capture geo-data and share your QGIS projects with ease",
        "type": "Talk",
        "track": "Open source geospatial ‘Made in Europe’",
        "abstract": "We show how Mergin Maps can be used in various real-world situations to use the power of QGIS ecosystem to speed up and effectively capture data in the field and reliably collaborate with your team. We will not dive into technical details, but focus more on general understanding of what can be done nowadays in the field of professional geo-data capturing.\n\nDo you need to capture the location of plants or animals with your personal phone? Or distribute this task to a group of volunteers without need to train them? Or your company has a network of pipes or fiber cables, you use QGIS in the office for analysis and you want to use the same map as your colleagues on site? Are you fed up with using for such tasks a camera and MS Excel or even pen and paper?  This talk can show you how others solve these challenges with Mergin Maps.\n\nMergin Maps is a free and open-source platform powered by QGIS rendering engine to capture and share  geo-data with ease. It has been developed by Lutra Consulting since 2017 and it has served thousands of companies and individuals in full production for more than 2 years. It comes with Android, iOS apps that do not need any training to be used by the general public. Also a powerful server to store, version and collaborate on your QGIS projects.",
        "description": ""
      },
      {
        "title": "When vector tiles are not enough: advanced visualizations with deck.gl",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Deck.gl is a framework for visualization, animation and 3D editing of large volumes of data (up to millions of points), in the browser, with optimal performance thanks to WebGL technology and the computing power of the GPU.\n\nDeck.gl is prepared to work seamlessly with WebGL based map libraries such as MapLibre GL JS, Mapbox GL JS or Google Maps. It extends their capabilities with a large number of formats, data types and layer visualizations, such as point clouds (tessellated or not), real 3D vector data, 3D models, on-the-fly clustering, trip animations, GPU filtering, etc. The deck.gl code is not only free, but designed with extensibility in mind, making it very easily customizable.\n\nIn this presentation we will show 4 use cases developed for companies and administrations with specific needs. We chose deck.gl (over Mapbox/MapLibre alone) to provide rich interactivity and the ability to visually analyze large amounts of data.\nWe will expose the challenges we faced and how deck.gl was used:\n1. Information system for precision irrigation: in a region of 25,000 plots, we show animated time series of evapotranspiration data, vegetative vigor, or water needs during an annual cycle.\n2. Biodiversity world map: instant loading of a dataset of 200,000 points with GPU filtering, providing interactivity and refresh rates far beyond the ones offered by Mapbox or MapLibre.\n3. Precision topographic measurements on terrain surface models: visualization of point clouds, terrains, textures, contour lines and other vector cartography in 3D, multi-profiles, and in-browser 3D editing.\n4. Urban data control panel: from a dataset of 40,000 georeferenced records, we apply spatiotemporal and categorical filtering, 3D dynamic aggregation and symbolization, and computation of indicators and graphs in real time.",
        "description": ""
      },
      {
        "title": "The Survey of Vectortile techniques: Static vs Dynamic",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Vectortile ecosystem have made big changes in Web Mapping, especially in terms of Client-side map rendering. Thesedays, costs of producing and streaming tiles have been dramatically reduced by some techniques - tippecanoe, PMTiles… and so on. However we have the problem important but unsolved yet: Dynamic tiles. Techniques which are matured and widely used are for Static tiles. Static tiles are not good at streaming data frequently updated but we sometimes need to dynamically serve such data. In this talk, I’ll survey techniques for Dynamic tiles which already exist and propose the solution for this.",
        "description": ""
      },
      {
        "title": "Serving earth observation data with GeoServer: COG, STAC, OpenSearch and more...",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Never before have we had such a rich collection of satellite imagery available to both companies and the general public. Between missions such as Landsat 8 and Sentinels and the explosion of cubesats, as well as the free availability of worldwide data from the European Copernicus program and from Drones, a veritable flood of data is made available for everyday usage.\nManaging, locating and displaying such a large volume of satellite images can be challenging. Join this presentation to learn how GeoServer can help with with that job, with real world examples, including:\n\n* Indexing and locating images using The OpenSearch for EO and STAC protocols\n* Managing large volumes of satellite images, in an efficient and cost effective way, using Cloud Optimized GeoTIFFs.\n* Visualize mosaics of images, creating composite with the right set of views (filtering), in the desired stacking order (color on top, most recent on top, less cloudy on top, your choice)\n* Perform both small and large extractions of imagery using the WCS and WPS protocols\n* Generate and view time based animations of the above mosaics, in a period of interest\n* Perform band algebra operations using Jiffle\n\nAttend this talk to get a good update on the latest GeoServer capabilities in the Earth Observation field.",
        "description": ""
      },
      {
        "title": "Open EO and FOSS4G serving Sahelian farmers and herders: lessons from the GARBAL programme",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "n the West African Sahel, farmers and herders are critically vulnerable to climate shocks and need access to climate information to secure their livelihoods. Herders use data on pasture and water availability to move their livestock and farmers need weather predictions for planting. While satellite imagery has made much of this information readily accessible to the spatial community, few channels exist to transmit this information to farmers and herders. As a result, climate data has become more powerful than ever before, yet mostly inaccessible to those who depend on this information for their livelihoods.\n\nThis talk will share the lessons of the GARBAL programme, an initiative that seeks to bridge this gap. GARBAL is a call center that uses Copernicus Earth Observation imagery and field data to provide farmers & herders with information on pasture, water and markets in Mali, Niger and Burkina Faso. GARBAL was first developed in 2015 and this talk will provide lessons from several years of practice.\n\nThe GARBAL interface uses an open-source stack including PostGIS and Mapserver to create a user-friendly interface for call center agents, who then use that interface to answer questions from callers on pasture conditions, market prices and weather forecasts (among others).  \n\nThe talk will share lessons from the technical and programmatic aspects of the project. The technical side will go over the architecture of the data treatment, demo the interface, talk about successes and failures and show how you can play with the data yourself. The programmatic side focuses more on how the user needs evolved over the years, techniques for translating GIS data into information useful to farmers and herders, operating in areas of active conflict and how EO data fits into existing centuries-old traditional data collection systems in the Sahel.",
        "description": ""
      },
      {
        "title": "Open source mapping library shoot out",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "MapLibre GL JS, Leaflet, Esri Leaflet, OpenLayers, and Cesium JS are all great mapping libraries. However, it can be difficult to decide which one to use for different applications. In this talk, I compare the strengths and weaknesses of each library based on different criteria. The criteria include the following:  \n\n1. Library footprint and modularity. \n2. Load times for vector tile and image tile layers.  \n3. Rendering performance of GeoJSON data.  \n4. Styling and rendering features.  \n5. Viewport performance and screen size responsiveness.",
        "description": ""
      },
      {
        "title": "State of deegree: The 2023 update",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Initiated in 2002, the OSGeo project deegree has developed to an important and mature building block for Spatial Data Infrastructures (SDI) over the last 20 years. The project provides 9 official Reference Implementations of OGC Standards such as GML, WFS, WMS, and OGC API - Features.\n\nIn this talk, we will focus on the recent improvements available in deegree webservices v3.5 and the updated roadmap for the next version which lists support of Java 17. We will also  show how the OGC Standards OGC API - Features Core and CRS have been implemented and can be used with existing configurations.\n\nFinally, we will present the future directions of the project and what developments are currently planned.",
        "description": ""
      },
      {
        "title": "GeoHealthCheck - QoS Monitor for Geospatial Web Services",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Keeping (OGC) Geospatial Web Services up-and-running is best accommodated by continuous monitoring: not only downtime needs to be guarded, \nbut also whether the services are functioning correctly and do not suffer from performance and/or other Quality of Service (QoS) issues.\nGeoHealthCheck (GHC) is an Open Source Python application for monitoring uptime and availability of OGC Web Services.\nIn this talk we will explain GHC basics, how it works, how you can use and even extend GHC (plugins).\n\nThere is an abundance of standard (HTTP) monitoring tools that may guard for general status and uptime of web services. \nBut OGC web services often have their own error, \"Exception\", reporting not caught by generic HTTP uptime\ncheckers. For example, an OGC Web Mapping Service (WMS) may provide an Exception as a valid XML response or\nin a error message written \"in-image\", or an error may render a blank image. \nA generic uptime checker may assume the service is functioning as from those requests and an HTTP status \"200\" is returned.\n\nOther OGC services may have specific QoS issues not directly obvious. A successful and valid \"OWS GetCapabilities\" response may not \nguarantee that individual services are functioning correctly. For example an OGC Web Feature Service (WFS) based on a dynamic database may \nreturn zero Features on a GetFeature response caused by issues in an underlying database. Even standard HTTP checkers supporting \"keywords\" \nmay not detect all failure cases. Many OGC services will have multiple \"layers\" or feature types, \nhow to check them all?\n\nWhat is needed is a form of semantic checking and reporting specific to OGC services!\n\nGeoHealthCheck (GHC) is an Open Source (MIT) web-based framework through which OGC-based web services can be monitored. GHC is written in \nPython (with Flask) under the umbrella of the GeoPython GitHub Organization. It is currently an OSGeo Community Project. \n\nGHC consists of a web-UI through which OGC service endpoint URLs and their checks can be managed, \nand monitoring-results can be inspected, plus a monitoring engine that executes scheduled \"health-checks\" on OGC service endpoints. \nA database stores results, allowing for various forms of reporting.\n\nGHC is extensible: a plugin-system is available for \"Probes\" to support an expanding number of \ncases for OGC specific requests and -checks. Work is in progress to provide a GHC API for various integrations.\n\nInfo, sources, demo: https://geohealthcheck.org",
        "description": ""
      },
      {
        "title": "OpenMapTiles - vector tiles from OpenStreetMap & Natural Earth Data",
        "type": "Talk",
        "track": "State of software",
        "abstract": "OpenMapTiles is an open-source set of tools for processing OpenStreetMap data into zoomable and web-compatible vector tiles to use as high-detailed base maps. These vector tiles are ready to use in MapLibre, Mapbox GL, Leaflet, OpenLayers, and QGIS as well as in mobile applications.\n\nDockerized OpenMapTiles tools and OpenMapTiles schema are being continuously upgraded by the community (simplification, performance, robustness). The presentation will demonstrate the latest changes in OpenMapTiles. The last release of OpenMapTiles greatly enhanced cartography and map styling possibilities, especially the enrichment of Points of Interest and improvement of land use or land cover layer. The new version of Natural Earth brought updated data to upper zoom levels and included a new OSM OpenMapTiles style, showing all features in well know colors for vector tiles. OpenMapTiles is also used for generating vector tiles from government open data secured by Swisstopo.",
        "description": ""
      },
      {
        "title": "Monitoring Inland water bodies",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This talk describes the creation of a water quantification dataset for the entire world. Tracking changes of water-bodies over time helps in timely action to combat drought and floods. The tools used to build this dataset are all free and open source (postgis, gdal, geopandas, scipy) and are built on top of data from OpenStreetMap.\nThe dataset is updated everyday with new measurements of lake water extent across the globe. The solution to detect and track water bodies involved fetching satellite data using STAC API, pre-processing it to remove cloud cover and invalid pixels, identifying water bodies using band ratio, converting to vector and applying post-processing filters to avoid false-positive detection to finally serve it through an API. This solution has allowed us to track and quantify changes in a lake's water extent over time with high accuracy.",
        "description": ""
      },
      {
        "title": "Implementing Copernicus services at the Norwegian Water and Energy Directorate with Airflow and actinia",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "At the Norwegian Water and Energy Directorate (NVE), the OSGeo Community project actinia was introduced together with the Open Source Apache Airflow software as a platform for delivering operational Copernicus services at national scale.\nIn the presentation, we will illustrate how Airflow and actinia work together and present current and future applications operationalized on the platform.\n\nThose applications cover currently: \n-\tAvalanches\n-\tFlooding\n-\tsnow cover\n-\tlake ice\n\nMore services related to NVE`s area of responsibility are being investigated, like landslides, slush flows, glacier lake outburst floods, or specific land cover changes...",
        "description": ""
      },
      {
        "title": "The Swiss geometadata catalogue: new version (GeoNetwork V4) & first results of a usability study",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "At the end of 2022 the Swiss geodata catalogue, geocat.ch, was migrated to GeoNetwork version 4. A more modern user interface as well as a more powerful search based on Elasticsearch makes it easier to search the more than 14000 geometadata contained in geocat.ch.\n\nThis new version of geocat.ch has been the subject of a usability study focusing on geodata search. Some developments based on the results of this study have been proposed to the GeoNetwork developer community. To discuss these proposals with the other users of GeoNetwork, a GeoNetwork user community should be founded and could be helpful in the further developments of GeoNetwork. In addition, the usability study showed that the search for geodata is very dependent on the quality of the information entered into the catalog.\n\nThe geometadata in geocat.ch come from different organizations (direct entry or harvesting), have different spatial extents, are multilingual and some have different data models. The harmonized entries of the most important information are essential and form the basis for efficient searches. The Swiss geometadata standard (GM03), which is currently under review with the aim of simplifying and updating the Swiss geometadata model, always based on international standards.",
        "description": ""
      },
      {
        "title": "Correlation between the greening rate of a city and local climate zones using free and open source data and tools (case study: city of Tirana)",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Albania is one of the most vulnerable countries in terms of the trend of climate change in the Western Balkans. Changing weather patterns have already been observed over the last 15 years with increasing temperatures, decreasing precipitation, and more frequent extreme events like floods and droughts. Among the most affected cities is Tirana, where a time series analysis was done using FOSS data and tools. Our aim was to provide accurate map representations of local climate zones (LCZs) to track the changes of the last decade based on an open online platform running on Google Earth Engine. This is called LCZ generator and aims to use free data sources from the Copernicus Hub (Demuzere et al. 2021). The satellite data based analysis was done by using 5-15 training areas for each LCZ types. It provided a 100 by 100 m ground resolution supervised classification for the entire municipality of Tirana. The analysis shows that the quick urbanization process resulted in a decreasing proportion of green areas, and unpaved surfaces in the municipality of Tirana, which consequently increased the vulnerability of the city to extreme weather events.\nA large-scale map was also compiled using a free and open source Geographic Information System (QGIS), which seems to be the most effective in identifying the varying urban climate zones on the city planning level, since it shows the city's structures and even highlights the role of a building or small park (Cenameri, 2021).",
        "description": ""
      },
      {
        "title": "Implementing OGC API - Processes with prefect and pygeoapi",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The Open Geospatial Consortium API family of standards (OGC API) are being developed to make it easy for anyone to provide geospatial data to the web, and are the next generation of geospatial web API standards designed with resource-oriented architecture, RESTful principles and OpenAPI. In addition, OGC APIs are being built for cloud capability and agility.\n\nThe OGC API - Processes standard supports the wrapping of computational tasks into executable processes that can be offered by a server through a Web API and be invoked by a client application. The standard specifies a processing interface to communicate over a RESTful protocol using JavaScript Object Notation (JSON) encodings. Typically, these processes execute well-defined algorithms that ingest or process vector and/or coverage data to produce new datasets or analyses.\n\npygeoapi is an open source Python server implementation of the OGC API suite of standards. The project emerged as one of the most effective reference implementations that provides the capability for organizations to deploy OGC API endpoints using OpenAPI, GeoJSON, and HTML. pygeoapi is built on an extensible plugin framework in support of clean, adaptive data integration and easy customization.\n\nPrefect is an open source data workflow orchestration platform developed in Python. It provides robust orchestration of workflows and offers a large set of features that range from monitoring to supporting cloud storage, to periodic execution, etc. It is a robust and very capable workflow engine, which is a perfect fit for managing execution of OGC API – Processes requests in pygeoapi.\n\nThis presentation will provide an overview of the prefect process manager plugin for pygeoapi and will demonstrate:\n\n- How to use pygeoapi for handling OGC API - Processes use cases\n- How the pygeoapi prefect plugin is a good match for managing the execution of processes and what are its main strengths as a geospatial data processing platform",
        "description": ""
      },
      {
        "title": "BBOX – a modular OGC API server",
        "type": "Talk",
        "track": "State of software",
        "abstract": "[BBOX](https://github.com/sourcepole/bbox) is a new OGC API Open Source implementation, with support for established OGC services driven by MapServer or QGIS Server. BBOX is implemented in Rust, with a built-in high-performance web server.\n\nSupported OGC API Services:\n* OGC API - Maps, with support for OGC WMS 1.3\n* OGC API - Tiles, with support for WMTS and XYZ endpoints\n* OGC API - Features\n* OGC API - Processes, with multiple processing engine backends\n\nEnterprise ready:\n* Authentication / Authorization\n* Instrumentation + Monitoring\n* First class Docker support\n\nSimple usage:\n* bbox-server serve –map alaska.qgz",
        "description": ""
      },
      {
        "title": "Mastering Security with GeoServer, GeoFence, and OpenID",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The presentation will provide a comprehensive introduction to GeoServer's own authentication and authorization subsystems. The authentication part will cover the various supported authentication protocols (e.g. basic/digest authentication, CAS, OAuth2) and identity providers (such as local config files, database tables, and LDAP servers). It will also cover the recent improvements implemented with the OpenID integrations and the refreshed Keycloak integration.\n\nIt will explain how to combine various authentication mechanisms in a single comprehensive authentication tool, as well as provide examples of custom authentication plugins for GeoServer, integrating it in a home-grown security architecture. We’ll then move on to authorization, describing the GeoServer pluggable authorization mechanism, and comparing it with an external proxy-based solution. We will explain the default service and data security system, reviewing its benefits and limitations.\n\nFinally, we’ll explore the advanced authorization provider, GeoFence. The different levels of integration with GeoServer will be presented, from the simple and seamless direct integration to the more sophisticated external setup. Finally, we’ll explore GeoFence’s powerful authorization rules using:\n\n- The current user and its roles.\n- The OGC services, workspace, layer, and layer group.\n- CQL read and write filters.\n- Attribute selection.\n- Cropping raster and vector data to areas of interest.",
        "description": ""
      },
      {
        "title": "The role of FOSS in mining sector in Malawi",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The extractive sector in Malawi has been marked as one of the development enablers to achieve the 2063 Agenda established by African nations. As the mining sector continues to develop, open-source software such as QGIS has been a vital and cost-effective tool in monitoring mining activities for the purpose of tracking the effects of mining on the environment and human populations and encouraging accountability from stakeholders in relation to the Malawi government regulations. Open-source software and data have also been vital in resolving compensation issues in communities that exist in mining areas and allow for geoscientists to give needed to advice to affected stakeholders.",
        "description": ""
      },
      {
        "title": "Creating The Red Book of Disaster Response for FOSS4G Community",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "As well trained and experienced members of the free software community from Turkey, we were caught off guard, when the earthquakes happened on February 6, 2023. We started mapping campaigns with HOT, we aggregate different data sources on a GeoServer installation, we did several visualizations on QGIS, but we always felt like something was missing.\n\nIf we had a guideline of disaster response for free software communities, we would feel better at the beginning.\n\nThis session's aim is, to prepare a dynamic guideline of disaster response actions for geospatial communities, focused on free software and open data.",
        "description": ""
      },
      {
        "title": "Unlocking the potential or Earth Observation combining Optical and SAR data",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "We are currently living in an era for Earth Observations that maybe 20 years ago we could not image. Petabytes and Petabytes of data are being created, having so much data it is a good problem to have but the next question is how we can make sure that the Data created it is really being used to solve the challenges we are facing on Earth. The Copernicus Program has given us the opportunity of having Open Data from a variety of diverse sensors but at the same time more and more companies are part of the New Space era in the one commercial companies are launching Optical and SAR satellites that are complementing the Open Data sources.\n\n\n\nIn my daily job doing Partnerships in the industry, I have the chance to work together with most of the New Space companies trying to find the best way to promote how we all can take advantage of all the data we have available from the Open Data sources and the Commercial sources, this can be optical data working together with SAR and how it can be a game changer in many future projects in Earth Observation.\n\nMy presentation will go around how in the last few years there are more options to be able to build products helping to solve earth's challenges by taking advantage of the resources we have in the New Space Industry.",
        "description": ""
      },
      {
        "title": "Earthquakes and OpenStreetMap",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The substantial reduction of disaster risk and life losses, a major goal of the Sendai Framework by the United Nations Office for Disaster Risk Reduction (UNISDR), requires a clear understanding of the dynamics of the built environment and how it affects, in case of natural disasters, the life of communities, represented by local governments and individuals. The framework states that communities participating in risk assessments should increase their understanding of efficient risk mitigation measures. \n\nEarthquakes are threatening many regions in the world with constantly increasing risk due to rapid urbanization and industrialization. Earthquakes do not kill people, buildings do. Thus, the main threat of earthquakes comes from building damage and collapse. To improve resilience and preparedness, we need to estimate the risk, the possible damage of buildings and the related human and financial losses. This requires not only the position, size and class of buildings, but also the reconstruction value and the number of people inside the building at any time. For this, exposure models are used that translate the physical earthquake hazard to building damage, human and financial losses. Exposure models usually describe the built environment of administrative regions as groups (aggregates) of different building classes and their frequency.\n\nWe present our open, dynamic, and global approach to describe, model, and classify every building on Earth with the greatest level of detail. Our model is based on the building data from OpenStreetMap and engineering information from open exposure models, combining these two sources to a building-by-building description of the exposed assets. We retain the aggregated descriptions where the building coverage in OpenStreetMap is incomplete and describe every building separately where building data is available. Due to the near-real-time computations of our model, it directly profits from the growth of OpenStreetMap and with about 5 million buildings added each month (or approx. 2 per second), the areas of incomplete coverage are constantly shrinking, making way for our building-specific exposure model.\n\nHere, we introduce shortly the earthquake phenomenon, how it affects the built environment, why a high level of detail is necessary for useful assessments of the impact and the consequences of earthquakes, how OpenStreetMap and other open data helps us to achieve this goal and how communities can benefit for the model for their own risk assessments.",
        "description": ""
      },
      {
        "title": "How to improve OpenStreetMap for the production of a hiking map",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "In preparation for a new Alpine Club map by the Institute of Cartography of the TU Dresden around Mt. Ushba in Georgia in the Great Caucasus, the decision was made to use OpenStreetMap as the primary data source for the map. As a result, the fieldwork in place contributed to OpenStreetMap to use gained information for map production by using OpenStreetMap. In the past, data import and organized mapping had already happened, leaving gaps only fillable by fieldwork.\n\nMapping campaigns took place in 2021 and 2022. In preparation, it was necessary to identify missing or uncertain information. The catalogue of objects which should be mapped was derived from existing Alpine Club maps and the feature tags of OpenStreetMap. Several trails currently missing in OpenStreetMap were identified by collecting and comparing openly available GPS tracks, hiking guides, and old maps. The comprehensive information collection summarized the knowledge of all the sources. It became central for planning the office work on the data and organizing the extensive on-site mapping.\n\nBased on the collected information, the routes were planned in advance and during the fieldwork assigned to the mapping teams. On tour, new data was collected, which could not be obtained from aerial images such as small paths, hiking routes, guideposts, and POIs.  \n\nThe collection of geographical names worked similar to the collection of missing paths. After reviewing and selecting various sources, an updated set of names has been compiled. Old maps play an important role because they sometimes contain names that need to be added or allow updates for more recent documents. Combined with background literature on the region, uncertainties in assigning geographical features can frequently be solved. Asking locals helped in finding the ideal spelling. The result is a much more consistent toponym base both in the OpenStreetMap database and in the derived produced map.\n\nThe presentation will share the knowledge on preparing and organizing the fieldwork for such a project. Significant aspects are how to identify missing ways and to collect geographic names.",
        "description": ""
      },
      {
        "title": "The state of OpenStreetMap buildings: completeness assessment using remote sensing data",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "OpenStreetMap (OSM) is the largest crowd-sourced mapping effort to date, with an infrastructure network that is considered near-complete. The mapping activities started as any crowd-sourced information platform: the community expanded OSM anywhere there was a collective interest. Initial efforts were found around universities or hometowns of mappers. Events, such as natural disasters can also trigger a major update. The recent earthquakes in Turkey and Syria lead to a massive contribution by the Humanitarian OSM Team (HOT) of more than 1.7 million buildings in the region in less than a month after the event1. This type of activities result in a map that is of non-uniform completeness, with some areas having all building footprints in, while other areas remain incomplete or even untouched. Currently, with 550 million footprints, OSM identifies between a quarter and half of the total building footprints in the world, if we estimate that there are around 1-2 billion buildings in the world.\n\nA global view on the local completeness of buildings in OSM did not yet exist. Unlike other efforts, that only look at a subset of OSM building data (Biljecki & Ang 2020; Orden et al., 2020; Zhou et al., 2020), we have used the Global Human Settlement Layer (GHSL) to estimate completeness of the entire dataset. The remote sensing dataset is distributed onto a grid of approximately 100x100 meter tiles. In each tile of the grid, the built area of GHSL is compared to the total area of OSM building footprints. The computed ratio is measured against a completeness threshold that is calibrated using areas that were manually assessed. \n\nUsing information derived from remote sensing datasets can be problematic: GHSL does not only measure building footprints: it includes any human-built structures, including infrastructure and industrial areas. Next to that, due to sub-optimal input data or failing algorithms, the dataset is not of the same quality as the crowd-sourced data in OSM in areas that are complete. Even with these limitations, a comprehensive global completeness assessment is created. The assessment should not be used as ground truth, but rather as reflection on the OSM building dataset as is and as a guideline for priorities for the future. Statistics on regional completeness can be created and the quality of GHSL could be assessed on countries that are considered to be complete, such as France or the Netherlands.",
        "description": ""
      },
      {
        "title": "Building heights: From open data to open maps",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "In the US, less than 20% of OpenStreetMap (OSM) buildings have a height tag (less than 10% globally). Providing buildings with height tags helps several use cases including 3D map visualization. At Meta, we have begun using open mapping data to estimate building heights and providing them back to the community. At the end of 2022, we used data from city GIS departments to estimate millions of heights and release them to the public through the Daylight Map Distribution (https://daylightmap.org/2022/12/02/building-heights.html). In 2023, we are using publicly available USGS/3DEP aerial lidar and releasing to the public through the Overture Maps Foundation – processing millions of square kilometers. This talk will cover the challenges, algorithm, QA process, and accuracy metrics from this effort. It is our hope that over the course of the year, we can estimate and publish heights for the majority of the buildings in the US and begin work on non-US open data sources as well.",
        "description": ""
      },
      {
        "title": "ST_LUCAS reference data for online automated land cover mapping",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "ST_LUCAS is an open-source system designed for providing harmonized space-time aggregated LUCAS data. LUCAS (Land Use and Coverage Area frame Survey) is an activity managed by Eurostat that performs in-situ surveys (points in 2x2km grid) over Europe every three years since 2006. For each LUCAS point, the land cover and land use classes are examined, five photos taken, and various agro-environmental attributes collected. Eurostat is providing data in plain CSV files. LUCAS nomenclature is changing each survey year, some attributes were removed, added or renamed.\n\nST_LUCAS was created with the goal to provide harmonized (each LUCAS survey is translated into common nomenclature) and space-time aggregated (for each LUCAS point, a single location and set of harmonized attributes for each survey year are provided) data. The ST_LUCAS system offers analysis-ready data through the Python API and QGIS plugin (“ST_LUCAS Download Manager”), which minimizes obstacles to use the data by the wider audience. Users may easily access land cover/use information about 1 350 847 points covering 28 EU countries measured from 2006 till 2018 by Eurostat. LUCAS points are retrieved from the ST_LUCAS system based on specified spatial, temporal, attribute, and thematic filters. The Python API and QGIS plugin also allow retrieving photos (one facing photo and four landscape photos in the cardinal compass directions) for each LUCAS point. Additionally, two analytical functions are available: user-defined LUCAS land cover classes aggregation and the possibility to translate LUCAS nomenclature into other nomenclatures.\n\nSee ST_LUCAS website https://geoforall.fsv.cvut.cz/st_lucas/ for detailed information.",
        "description": ""
      },
      {
        "title": "New lane-detailed OpenDRIVE datasets (HD maps) from Germany openly available",
        "type": "Lighting talk",
        "track": "Open Data",
        "abstract": "Various disciplines such as traffic simulations, driving simulations and applications in autonomous driving require highly detailed road network datasets. OpenDRIVE evolved as an open industry standard for modelling of lane-level road networks (HD maps). Acquiring such datasets is very expensive tough because it has to be done through mobile mapping in most cases. We want to introduce to the FOSS4G community two recently and openly published road network datasets from Brunswick (https://doi.org/10.5281/zenodo.7071846) and Wolfsburg (https://doi.org/10.5281/zenodo.7072631). Investment in both datasets has been funded by German authorities and covered more than 100.000 Euro. We will also give a short appetiser on how to use this data with free and open GIS tools.",
        "description": ""
      },
      {
        "title": "Providing a libOpenDRIVE-based GDAL driver for conversion of lane-detailed road network datasets commonly used in automotive engineering into GIS tools",
        "type": "Lighting talk",
        "track": "State of software",
        "abstract": "Various applications with the need of highly detailed road network models emerged within the last decade. Apart from traffic simulations in context of urban planning, especially the automotive industry plays an important role in geodata consumption for development, testing and validation of autonomous driving functions. In this domain, human-centred driving simulation applications with their realistic 3D virtual environments pose the highest demands on real-world data and lane-level road network models. It is not uncommon for such road network data to not only be mathematically continuously modelled, but also to contain all the necessary topological links and semantic information from traffic-regulating infrastructure – such as signs and traffic lights. Schwab and Kolbe [1] give a compact overview of the requirements of such fields of application and describe different domain-specific road data formats, which are commonly used for such tasks. Of these peculiar road description formats, OpenDRIVE [2] evolved as an open industry standard. In 2017 we proposed a driver for conversion of OpenDRIVE’s continuous road geometry elements into standardized GIS geometries according to OGC Simple Features Access [3] via the free and open-source Geospatial Data Abstraction Library (GDAL) [4]. By then, this was the first open source conversion tool from OpenDRIVE into more GIS-friendly encodings. Since then, other OpenDRIVE conversion tools have popped up, such as [5], [6], [7], [8]. But none of those allows such a comfortable integration into common GIS tools like our proposed GDAL extension by, for example, simply dragging and dropping an OpenDRIVE dataset into QGIS. We now present a refurbished version of our OpenDRIVE GDAL driver which is based on the novel C++ library libOpenDRIVE. It integrates well in GDAL’s new CMake building process and offers a more convenient starting point for developers and researchers who want to bring OpenDRIVE data easily into context with other geodata such as with aerial images, OpenStreetMap or cadastral data. Apart from OpenDRIVE, other specialized road network description formats are crucial to the automotive engineering and research domain. Where Road2Simulation [9] and laneLet2 [10] already come along in GIS-friendly encodings, RoadXML and NDS Open Lane Model [11] could also profit from such a GDAL-based conversion approach. By bringing the domains of automotive engineering and GIS closer together we hope to stimulate interdisciplinary knowledge transfer and the creation of an interconnected research community.\n\n[1] https://doi.org/10.5194/isprs-annals-iv-4-w8-99-2019\n[2] https://www.asam.net/standards/detail/opendrive\n[3] https://www.ogc.org/standards/sfa\n[4] https://elib.dlr.de/110123\n[5] https://doi.org/10.5281/ZENODO.7023152\n[6] https://doi.org/10.5281/zenodo.7771708\n[7] https://doi.org/10.1109/itsc48978.2021.9564885\n[8] https://doi.org/10.5281/zenodo.7702312\n[9] https://doi.org/10.5281/ZENODO.3375525\n[10] https://doi.org/10.1109/itsc.2018.8569929\n[11] https://olm.nds-association.org",
        "description": ""
      },
      {
        "title": "Virtual Constellations-as-a-Service and Virtual Image Catalogs",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Virtual Constellations-as-a-Service and Virtual Image Catalogs\n\nSharing remote sensing assets among multiple tenants is crucial to unlock the value of new space earth imaging constellations. In these schemes, a tenant has access to a so-called virtual constellation consisting of dedicated access to a number of assets as well as automated mechanisms to procure additional imagery from other assets. Access to this virtual constellation is mediated through a virtual catalog client-side that looks to the user as if it comes from its own dedicated assets and is fully interoperable with open-source standards for cloud optimized pipelines, such as STAC and COG.\n\nSatellogic Inc., a leader in sub-meter resolution Earth Observation data collection recently reached a three-year agreement with the Government of Albania to develop a Dedicated Satellite Constellation. This unique program derives from Satellogic's Constellation-as-a-Service model and will provide Albania with responsive satellite imagery capabilities across its sovereign territory. Two satellites, ALBANIA1 and ALBANIA2 were launched in January 2023, to provide imagery for national map generation to support emergency response, land use management as well as environmental monitoring of sustainability goals.\n\nTo support this government effort we have developed a secure, encrypted end-to-end data platform, continuously updated archival imagery in dedicated client-side cloud along with support for open source standards such as STAC and COG. We also discuss future directions in terms of the resulting ability to build integrations with external image processing platforms and open source data exploitation projects.",
        "description": ""
      },
      {
        "title": "Notebooks in (geo)datascience",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In the FOSS4G 2021 programme, the word 'notebook' appeared ten times and the word 'jupyter' ten times too in the abstracts of four workshops and four presentations.\n\nIn 2022, 'jupyter' and 'notebook' appear in two workshops and two presentations abstracts.\nMore discreetly, at least three workshops and one scientific paper used notebooks without mentioning them.\nAs we can see, notebooks are becoming increasingly common in data science and the geospatial world.\n\nBut what is a notebook? What is it useful for? What are its limitations? \nAre there other platforms than Jupyter?\nCan we do anything other than Python? What about geospatial? Are these tools FOSS?\nThese are some of the questions that this presentation will try to answer.\n (TL;DR: yes!)\n\nIf you have never heard of Quarto, Observable or Org-mode, this presentation is for you.",
        "description": ""
      },
      {
        "title": "Navigate urban scenarios with MapStore 3D tools",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This presentation focuses on the use of MapStore to navigate urban scenarios using its 3D tools and capabilities. Latest versions of MapStore include improvements and tools related to the exploitation of 3D data such as Map Views, Styling, 3D Measurements and more. Support for 3D Tiles and glTF models through the Cesium mapping library has also been greatly enhanced to provide support for more powerful integration.\n\nAttendees will be presented with a selection of use cases around the following topics: visualization of new projects for urban planning, relations between different levels of a city and descriptions of events inside a city. At the end of the presentation attendees will be able to  use the presented workflows to replicate them on different urban scenarios using the 3D tools of the MapStore WebGIS application.",
        "description": ""
      },
      {
        "title": "Development of QGIS based topographic data management system",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The National Land Survey of Finland (NLS) is rebuilding its topographic data management system using open source components. The new system will be based on QGIS and PostgreSQL. The goals of the renewal are: \n- Utilization of new technologies and standards \n- Advancement in the transition from producing map data to producing spatial data \n- Enhancement of the quality and timeliness of data \n- Enhancement of the production through automation and better tools \n\nThe current system has been in use for over 20 years and has been developed throughout its lifespan. NLS is planning to replace the current production system after the first phase of development in 2025. \n\n \n\nIn this talk, I will talk about the status of the development, elaborate the main objectives of the first phase and introduce the published OS components so far. In the first two years of the development the focus was on concurrent data management by 100 operators and on the integration of the stereo mapping tools (proprietary). In addition, we have designed and implemented OS quality assurance tools to ensure the logical consistency of the features concerning the attributes, the geometries and the topology. These tools also include a topological rule set for topographic data management in PostgreSQL. \n\n \n\nWe have also published some plugins for the operators to improve the digitizing workflow. To facilitate the development work, we have contributed some development tools for QGIS plugin developers. The OS publications of the service and client components of the concurrent data management tools are not yet on the roadmap although our final goal. \n\n \n\nThe current process of maintaining topographic data includes some field work too. QField is the chosen OS tool for that purpose. Now, we are defining the additional functionalities needed to make the field work efficient enough and to smooth out the data transfer between the main system and the mobile application.  \n\nAfterwards, we have yet to make significant progress in the integration of TDMS with the systems that produce and provide products. In relation to our products, we need to find a way to easily maintain base topographic data and its enriched cartographic derivates and place names, as part of the production process.",
        "description": ""
      },
      {
        "title": "Kobo Toolbox Automation with Geonode for Risk Management",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Based on the implementation of a set of forms in Kobo Toolbox, an information flow for the Fire Management Commission of the Argentine Republic was created to be able to integrate from the field the fire reports (on line / off line) in a simple way and their different stages of evolution. The automation of the ingestion to a Geonode, as a geospatial data manager allows the integration with weather forecast data, near real time information, fire incidences, hot spot detection and predictive fire indexes.\nThe integration is done with the Airflow tool, which guarantees integration and monitoring of information flows, simplifying the process during incidents.",
        "description": ""
      },
      {
        "title": "Teaching GI with FOSS tools: an update for higher education teachers and trainers at public organizations",
        "type": "Side event long",
        "track": "Education",
        "abstract": "In recent years, the combination of technological advances and spatial data abundance revolutionised the field of geoinformation (GI). New methodologies and techniques established in other fields of knowledge proved to be relevant to keep up to date and fully benefiting from all this technological richness. Consequently, new areas of knowledge have emerged, such as geospatial artificial intelligence (GeoAI) or big geodata. Simultaneously, the formulation in 2015 of the 2030 Agenda for Sustainable Development and its multiple goals by the United Nations (UN), impose a specific framework for the application and further development of geoinformation science. Furthermore, the recent COVID-19 pandemic accelerated the transition towards different modalities of distance education as well as the arrival of multiple digital instruments to fulfil this purpose. At the same time, the use of free and open-source software (FOSS) keeps gaining momentum, standing out as the best technological solution to attain sustainable and democratic approaches to geospatial problems.\n\nAll these factors have profoundly impacted the way of teaching with GI and about GI. Both technical and socio-emotional skills required to successfully perform as a GI scientist in the near future are changing. And so are the means to learn those skills. As a result, the training curriculum for educators in this field is being revised and updated. In this presentation, we will first discuss the challenges currently faced by educators in the field of GI and explore new didactic and pedagogical proposals to overcome them. We will analyse how teaching GI science in academic settings (i.e., high school, university) differs from teaching it to staff members at public organisations. We will then explore how to successfully implement the ADDIE (i.e., Analyse-Design-Develop-Implement-Evaluate) model of instructional design in both settings. Finally, we will explore together in detail a recently designed refresher course on geodata analysis and dissemination that combines state-of-the-art pedagogical approaches and the use of FOSS4G.\n\n**Schedule**  \n\nDate: June 30th, 2023\n\n- 09:00 – 10:15 Welcome and introduction   \nPresentation I: Teaching GI with FOSS today: challenges and opportunities\n- 10:15 – 10:30 Break\n- 10:30 – 11:30 Presentation II: Differences between academic and organisational training. How to properly design education for different educational settings?\n- 11:30 – 11:40 Break\n- 11:40 – 12:00 Case study: Online refresher course “Geo-web application building with FOSS”\n- 12:00 – 12:30 Plenary discussion\n- Closing\n\n[Register here](https://forms.gle/PVp5F12CrDwmt4qs9) for participating in the event.",
        "description": ""
      },
      {
        "title": "Lake bottom DEMs from open data with GDAL and GMT",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Finland is reputed to be the Land of a Thousand Lakes, but a more precise estimate is that Finland has 57000 lakes which are larger than one hectare. The precise shorelines of all the lakes have been available as open data since 2012 but the situation with the bathymetric data is not as good. Depth contours are available for about 80% of the total lake area, but oldest soundings are from the end of the 19th century. Bathymetric data of the lakes has not been considered particularly important and the old measurements have not been systematically updated and verified. Therefore, the most common acquisition method in the existing bathymetric data is still manual measurement with a plumb line through the ice. Because the depth points are frequently 75-100 meters apart, such data are only usable for creating rather approximate depth contours.\n\nHowever, since mid 1980s the Finnish Environment Agency, the Finnish Transport and Communications Agency Traficom, and their predecessors, have been mapping lake bathymetry with sonar sounding. In recent years these agencies have published their depth point data as open data under the CC-BY 4.0 license. These new datasets are essentially XYZ point clouds. Thanks to open source GIS programs anybody can take these datasets and create digital elevation models (DEM) of the lake bottoms, colored hillshade visualizations, 3D-models, and even traditional depth contours.\n\nThis presentation will dig into the nature of the data that is collected with sonar soundings and how it affects the selection of the interpolation method. A complete open source workflow that is using GDAL and Generic Mapping Tools (GMT) will be presented. The workflow begins from raw point measurements and lake shoreline vectors, and yields a DEM, hillshade visualization with a color table, and depth contours. Results for more than 1800 Finnish lakes will be available online, but the main outcome is the workflow itself. Because only command line tools which can be scripted and parameterized are used, it is simple to tune the process so that the output will suit different needs.",
        "description": ""
      }
    ]
  },
  {
    "year": 2024,
    "conference_title": "FOSS4G 2024",
    "start_date": "2024-12-04",
    "end_date": "2024-12-08",
    "total_events": 131,
    "total_presentations": 131,
    "academic_presentations": 0,
    "non_academic_presentations": 131,
    "types": {
      "Talk": 116,
      "Keynote": 8,
      "Sponsor": 4,
      "Lighting talk": 2,
      "Side event": 1
    },
    "tracks": {
      "Use cases & applications": 51,
      "State of software": 25,
      "Community & Foundation": 12,
      "Transition to FOSS4G": 12,
      "Open Data": 8,
      "Open Standard": 7,
      "Education": 5,
      "Applications and solutions for the Amazon region": 5,
      "AI4EO Challenges & Opportunities": 5,
      "Open source geospatial ‘Made in Latin America’": 1
    },
    "presentations": [
      {
        "title": "OPENING FOSS4G 2024",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "Welcoming speech and opening of FOSS4G 2024",
        "description": ""
      },
      {
        "title": "“The relationship between FOSS4G and GIScience: The first 40 years and beyond”",
        "type": "Keynote",
        "track": "State of software",
        "abstract": "The launch of GRASS GIS in 1984 marked the beginning of open-source geospatial software. Four years later, in 1988, the National Center for Geographical Information and Analysis (NCGIA) established the foundation for GIScience as a distinct academic discipline. Now, after four decades, this talk explores the dynamic interplay between geographical information theory and geospatial software. Like other fields, scientific and technological advancements in geospatial studies are interdependent yet distinct. This discussion will reflect on past interaction points and consider how the relationship between research and practical applications in the geospatial field may evolve.\n\nOne of the most significant contributions of GIScience to FOSS4G was the development of point-set topological operators. This work culminated in the dimensionally extended 9-intersection model (DE-9IM), which became the foundation for the Open Geospatial Consortium's (OGC) simple features model. The resulting standardisation of vector GIS was crucial in preventing the spatial data market from being dominated by proprietary solutions. Open-source tools like PostGIS, Python geopandas, and R-sf emerged as viable, competitive alternatives. The OGC geopackage standard has also been widely adopted for information storage and transfer. Leading researchers engaged directly in developing user-driven tools for spatial analysis, such as GeoDa and R packages like spdep and gstat, further driving innovation in vector GIS.\n\nHowever, not all GIScience research has had the same practical impact. Topics such as geospatial ontologies, spatial database accuracy, cognitive foundations, and spatiotemporal reasoning have primarily remained within academic circles. While they have enriched theoretical knowledge, their practical applications have been limited. Even concepts that could have benefitted FOSS4G — such as geospatial algebras and abstract spatial data types — were overlooked by developers. This talk will explore potential reasons for this disconnect.\n\nConversely, FOSS4G has made significant contributions to GIScience. Tools like GDAL have enabled researchers to tackle critical scientific questions, while QGIS has become an indispensable tool for scientists. The R spatial packages offer a reliable foundation for new research, and a similar robust foundation is likely emerging in Python.\n\nThat said, the relationship between vector-based FOSS4G and GIScience appears to have reached a plateau, with little significant progress on either side in recent years. In contrast, raster-based GIS is undergoing rapid innovation. The availability of petabytes of open Earth observation (EO) data has spurred a new wave of discovery. Space-borne sensors continue to drive technological breakthroughs, but no comprehensive theory exists for modelling and analysing large-scale EO data. This has resulted in fragmented developments like Pangeo, Open Data Cube, OpenEO, and R-sits. The talk will advocate for stronger collaboration between FOSS4G developers and researchers in big EO data analytics, offering potential paths forward.",
        "description": ""
      },
      {
        "title": "💎 Diamond Sponsor Talk  💎 Re:Earth: Revolutionizing WebGIS - From Data to Insights Without Coding",
        "type": "Sponsor",
        "track": "State of software",
        "abstract": "Discover Re:Earth, the cutting-edge open-source WebGIS platform by Eukarya Inc. that's transforming spatial data interaction. This innovative solution combines powerful visualization, seamless data management, and efficient processing in one user-friendly platform.\n\nIn this presentation, we'll explore:\n\n- The genesis of Re:Earth and its mission to democratize GIS\n- How Re:Earth enables the creation of sophisticated online map applications - no coding required\n- Real-world applications and success stories from Japan\n- Our comprehensive approach to the entire data lifecycle - from acquisition to insight\n- Re:Earth's trajectory and our bold vision for the future of WebGIS\n\nJoin us as we redefine the boundaries of WebGIS and unveil how Re:Earth is set to revolutionize spatial data analysis across industries. Whether you're a GIS professional, data scientist, or decision-maker, this talk will showcase how Re:Earth can transform your approach to geospatial challenges.",
        "description": ""
      },
      {
        "title": "From field biology to the GRASS GIS board: A Journey of Open Source Discovery and Nurturing a New Generation of Contributors",
        "type": "Keynote",
        "track": "Education",
        "abstract": "My journey from field biologist to GRASS GIS board member exemplifies the accessibility of open source contributions. Curiosity and a willingness to learn propelled me into the world of geospatial analysis, leading to a deep appreciation for the power of open source tools and community. I had the chance to witness firsthand the transformative power of open collaboration and this was really inspiring and engaging. However, I also recognized the need to bridge the gap between the global open source community and regions such as Latin America.\n \nIn this keynote, I will explore and reflect on strategies for breaking down barriers and fostering a more inclusive open source community, emphasizing the importance of mentorship, education, and accessible resources. By drawing on my personal experiences and lessons learned, I aim to inspire and empower attendees to become active contributors and leaders to build a more sustainable open source ecosystem!",
        "description": ""
      },
      {
        "title": "Forest Carbon Monitoring: A New Era of Real-Time Insights for Collaborative Forest Protection",
        "type": "Keynote",
        "track": "Applications and solutions for the Amazon region",
        "abstract": "Effective monitoring of forest carbon is essential for accurately estimating carbon emissions, detecting forest degradation, and enhancing forest management practices. This is particularly critical in biodiversity hotspots like the Amazon Rainforest, which not only sequesters vast amounts of terrestrial carbon but is also under constant threat from illegal deforestation and degradation.\nIn 2023, Planet launched Forest Carbon Diligence, a product that delivers annual global maps of canopy height, cover, and aboveground carbon (AGC) at a spatial resolution of 30 meters, covering the years 2014 to 2023. This initiative has already supported various applications, including a study published by Monitoring by the Andean Amazon Project (MAAP), revealing that the Amazon Rainforest stores approximately 56.8 billion metric tons of aboveground carbon (AP News, 2024; Mongabay, 2024).\nBuilding on this foundation, in September 2024, Planet introduced Forest Carbon Monitoring, a groundbreaking product that offers a global map of canopy height, cover, and AGC with a remarkable spatial resolution of 3 meters, updated quarterly. This enhanced capability allows for more detailed analysis of forest dynamics, providing timely insights into forest conditions and changes.\nThis talk will address two primary objectives:\nTechnical Insights: I will cover the development and validation of Forest Carbon Monitoring, highlighting its intercomparison with other existing products. Beyond providing quarterly snapshots, this system enables near-real-time monitoring of landscape changes, comparable to alert systems like RAdar for Detecting Deforestation (RADD) and Global Forest Change datasets. I will demonstrate how this tool is instrumental in identifying deforestation activities and directing conservation efforts to the most vulnerable areas of the Brazilian Amazon.\nCommunity Engagement and Collaboration: In this segment, I will illustrate Planet's commitment to fostering community engagement and collaboration through several key initiatives:\n1.\tOpen-Source Contributions: Planet actively collaborates with the Free Open Source Software for Geospatial (FOSS) community by using and contributing to open repositories such as pystac, GDAL, xarray, and Dask. This open-source approach encourages innovation, enhances tool accessibility, and supports diverse applications for the geospatial community.\n2.\tPartnerships for Impact: Our collaboration with Santiago & Cintra Consultoria (SCCON) in Brazil exemplifies the power of partnership. In the past year, Planet data assisted Brazilian Federal police agents in executing over 3,000 interventions to combat illegal activities, resulting in a remarkable 50% reduction in deforestation rates. This partnership not only strengthens law enforcement efforts but also builds local capacity for sustainable forest management.\n3.\tInclusive Access Programs: Planet offers initiatives like the NICFI and Education and Research programs, which provide users with free access to Planet data. By democratizing access to critical information, we empower researchers, conservationists, and local communities to utilize our data for informed decision-making and collaborative conservation efforts.\nThrough this presentation, I hope to engage the audience in a dialogue about the crucial intersection of technology, conservation, and community involvement, underscoring the vital role of precise forest carbon monitoring in addressing global environmental challenges.",
        "description": ""
      },
      {
        "title": "World Soil Information Service (WoSIS): Practical Applications of Open Source Technology in Soil Data Management",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The World Soil Information Service (WoSIS) aims to serve users with standardized and harmonized soil profile data, underpinning a wide range of global assessments and applications. As an important component of ISRIC – World Soil Information’s data infrastructure, WoSIS is fully open and accessible, adhering to the principles of open data and open standards. \n\nWoSIS infrastructure is powered entirely by Free and Open Source Software (FOSS), promoting transparency, reproducibility, and community-driven innovation. This talk will cover the latest developments in WoSIS, focusing on: \n\n- Spatial Data Infrastructure: Use of PostgreSQL, Mapserver, integration of Open Geospatial Consortium (OGC) standards, and the new spatial GraphQL interface for flexible and efficient data queries. \n\n- ETL Processes: Efficient extraction, transformation, and loading (ETL) methodologies for harmonizing soil profile data from diverse sources and ensuring data quality using mainly PostgreSQL and PostGIS. \n\n- Open Data Accessibility: Strategies to ensure data remains fully open, tools for accessing and utilizing soil data, and case studies highlighting its impact. \n\n- Community Collaboration: Contributions and enhancements by the global community, collaborative projects, and opportunities for engagement. \n\n- Future Directions: Upcoming features, exploration of emerging technologies, and the vision for leveraging open soil data to address global challenges. \n\nBy leveraging open source technology and adhering to open data standards, WoSIS demonstrates the importance of transparent and accessible data infrastructures in driving scientific advancements and supporting sustainable development goals. This presentation will provide insights into the challenges and approaches in the use of FOSS4G in data harmonization for global data products and global data dissemination with WoSIS as a use case.",
        "description": ""
      },
      {
        "title": "Community Standards and Satellite Tasking",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "Community standards are specifications that are created through informal organization and are then widely adopted by a larger group. The STAC specification and Cloud-Optimized GeoTIFFs are examples of community specifications that have become de facto standards for geospatial interoperability. This talk will examine how the process of developing community standards differs from traditional standards development and how to drive adoption. Numerous examples of successful community standards will be presented.\n\nIn addition, we will provide a case study of a current effort - that of STAPI - a specification for satellite tasking, or more specifically, an API for how users can order data from the future from satellite platforms. We have been spearheading an effort to develop such a specification and after two sprints we presented at the last FOSS4G in Kosovo.  This prompted a third sprint in Europe, bringing together an even larger community.  Working with government groups, commercial satellite operators, and data integrators, these sprints have worked toward developing a specification as well as implementations for several commercial providers, as well as ordering APIs for public datasets.\n\nThis talk will dive into what worked for STAC and other community standards and how we are taking those lessons to develop a standardized way for collecting future geospatial data.",
        "description": ""
      },
      {
        "title": "State of GRASS GIS",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GRASS GIS is an open source geoprocessing engine for efficient spatio-temporal data management, analysis, and modeling. The software comes with a Python API, command line and graphical user interfaces, and additional APIs for C and R.\n\nIn this talk we will give a comprehensive overview of the latest GRASS GIS developments and upcoming new features. We will cover several improvements to the graphical user interface aimed at increasing the usability and ease the adoption of GRASS GIS. We will also highlight a number of improvements and existing features relevant for industry and academic users to facilitate the integration of GRASS data processing and analysis tools into their workflows using Python or R, either on the command line or in the cloud. Finally, the latest community activities, as well as contribution and funding opportunities will be presented.",
        "description": ""
      },
      {
        "title": "pygeoapi project status",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pygeoapi is an OGC API Reference Implementation. Implemented in Python, pygeoapi supports numerous OGC APIs via a core agnostic API, different web frameworks (Flask, Starlette, Django) and a fully integrated OpenAPI capability. Lightweight, easy to deploy and cloud-ready, pygeoapi's architecture facilitates publishing datasets and processes from multiple sources. The project also provides an extensible plugin framework, enabling developers to implement custom data adapters, filters and processes to meet their specific requirements and workflows. pygeoapi also supports the STAC specification in support of static data publishing.\n\npygeoapi has a significant install base around the world, with numerous projects in academia, government and industry deployments. The project is also an OGC API Reference Implementation, lowering the barrier to publishing geospatial data for all users.\n\nThis presentation will provide an update on the current status, latest developments in the project, including new core features and plugins. In addition, the presentation will highlight key projects using pygeoapi for geospatial data discovery, access and visualization.",
        "description": ""
      },
      {
        "title": "How CMS Will Empower GIS Data with Flow: Building a Next-Generation Content Management System",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Join us for an in-depth, technical discussion about our journey in developing a specialized Content Management System (CMS) designed to handle the unique challenges of GIS data, powered by our custom workflow engine, Flow.\n\nWhat to Expect:\n\nThe GIS Data Dilemma: Limitations of traditional CMS platforms and why we chose to build our own solution\nSeamless GIS Data Handling: Our vision for streamlined ingestion to deployment\nCMS Architecture: Key architectural decisions and technical challenges overcome\nMeet Flow: An inside look at our custom workflow engine's architecture and data processing role\nCurrent Challenges: Technical hurdles we're tackling in Flow's development\nFuture Roadmap: Expanding capabilities and integration plans\nWho Should Attend:\n\n\nDevelopers, GIS specialists, and system architects interested in the challenges of building specialized content management systems are invited to join the conversation. We'll share our experiences, lessons learned, and ongoing development challenges in creating a purpose-built CMS for GIS data.",
        "description": ""
      },
      {
        "title": "Exploring OGCAPI with GeoServer and GeoNetwork",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The development community is really excited about having modern (REST and JSON) protocols for geospatial data infrastructure. While we love the traditional WMS and WFS protocols, the heavy use of XML is a barrier to today’s developers and tool chains.\n\nGeoServer is enthusiastically pursuing these standards thanks to the efforts of Andrea Aime and other volunteers at the joint OSGeo / OGC code sprints. The GeoNetwork team has also attended these events to work on record management.\n\n* This presentation looks at the approach used by the OGCAPI family of protocols.\n* We also look at how OGCAPI Features works in more detail, as one of the first standards to be finalized.\n* We check in with OGCAPI Records and the progress made by the GeoNetwork team\n* We look at the OGCAPI standardization process, and what features we are looking forward to being finalized\n* Looking ahead to what is needed for OGCAPI support in GeoServer to be successful\n\nJody and Gabe are enthusiastic GeoServer developers really using this presentation as an excuse to work on these new protocols.\n\nAttend this talk to learn about the transition to OGCAPI Protocols and help your organisation plan for the future.",
        "description": ""
      },
      {
        "title": "Approaching Security with Kindness and Compassion",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Wow it has been a busy year for security vulnerabilities. While FOSS4G software is unlikely to result in global “blue screen of death” outages - we are getting caught up in the general push to regulate IT and impose “security” on the technology that powers society.\n\nThis talk unpacks what this can look like for foss4g projects using real world examples. \n\n* Built around the experience of the GeoServer project, and the resulting security policy and practices that can serve as a template for our foss4g community.\n* Public institutions can attend this talk to learn how their security policies interact with and affect foss4g technologies.\n* Vendors and service providers can learn how open source supply chains affect their products.\n* FOSS4G projects can attend to learn how to approach security reports with compassion, and a bit of boundary setting, to take care of your codebase and community.\n\nThis talk explores the tensions, expectations, terrors and triumphs on this hot button topic.",
        "description": ""
      },
      {
        "title": "Integrating STAC with QGIS",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "The ever-increasing volume of geospatial data presents challenges in discovery, access, and utilization. The Spatio-Temporal Asset Catalog (STAC) standard offers a standardized approach to describing and accessing spatiotemporal assets. This talk explores the integration of STAC with QGIS.\nWe describe the benefits of STAC integration, including improved data discoverability through user-friendly search functionalities, streamlined access to remote data sources, and potential for automation in data download and processing workflows within QGIS.\nThe talk outlines the technical approach for integrating STAC, which involve developing a new servevice provider within core QGIS. We discuss potential challenges, such as ensuring compatibility with various STAC implementations and data providers.\nFinally, the talk highlights the potential impact of STAC integration. By leveraging the STAC ecosystem, QGIS users can benefit from a wider range of geospatial data resources, enhancing their ability to conduct spatial analysis and create insightful maps.",
        "description": ""
      },
      {
        "title": "Mergin Maps: an open source platform based on QGIS for data collection and collaboration",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Mergin Maps simplifies field data collection, offering an open-source platform built on the power and familiarity of QGIS. Capture, share, and publish your geospatial data seamlessly with intuitive mobile apps and robust web tools.\n\nMergin Maps (MM) has the following components:\n- Desktop: QGIS to set up and design your field survey\n- QGIS MM plugin: to upload/download your data to/from your cloud service (Mergin Maps server)\n- Mergin Maps mobile: an app based on QGIS with synchronisation tool allowing you to open your QGIS project and edit/capture data in the field\n- Mergin Maps server: a service allowing you to store and synchronise the data between QGIS and mobile app.\n\nThere are other tools and APIs available to handle the data transfer programmatically. For full list, see:\nhttps://github.com/MerginMaps",
        "description": ""
      },
      {
        "title": "FAIR Principles for Geospatial Data Curation",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "FAIR principles are a set of best practices aimed at making data findable, accessible, interoperable, and reusable to both humans and machines. The growth of spatial data infrastructures and discovery catalogs (aka ‘geoportals’) have highlighted the importance of geospatial data management, metadata, and systems architecture. These data are invaluable to public, private, and academic sectors for use in decision-making, policy development, research, as well as subsequent data production. However, the costs and overall effort associated with the curation of data throughout their lifecycle can be substantial. The Stanford Spatial Data Infrastructure implements FAIR principles to its collections of geospatial data in order to better meet the needs of its researchers and worldwide user community. These principles are applied to data, metadata, and infrastructure, and serve as a guide for collecting, organizing, and managing data that are produced through research endeavors, published by public entities for open consumption, or created by vendors for commercial purposes.\n\nIn this presentation, the design and implementation of a geospatial data curation strategy utilizing FAIR principles will be described. Additionally. we will discuss our efforts around automation in data wrangling and metadata, as well as access, licensing, and digital preservation.",
        "description": ""
      },
      {
        "title": "QGIS 3D, point clouds and elevation enhancements",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "QGIS 3d capabilities keep improving. In this talk, we will look at the recent changes and enhancements in QGIS to handle point clouds and elevation data. There is a new data provider (quantized mesh) and also a major enhancement to view \"globe\" in 3D. We will also look at the new vertical filtering tool in QGIS.",
        "description": ""
      },
      {
        "title": "Shortbread - the new OpenStreetMap vector tile schema",
        "type": "Talk",
        "track": "State of software",
        "abstract": "[Shortbread](https://shortbread-tiles.org/) is an open schema for OSM vector tiles. It is intended to be a basic, lean, general-purpose vector tile schema for OpenStreetMap data.\n\nFor creating and extending OSM vector tiles, workflows using [Tilemaker](https://tilemaker.org/), [Planetiler](https://github.com/onthegomap/planetiler) and [osm2pgsql Themepark](https://osm2pgsql.org/themepark/) are compared.\n\nSince vector tiles are styled in the browser, the styling can be changed at runtime. This talk shows the tooling for customizing styles, but also goes into extending vector tile content with additional data for special interests.",
        "description": ""
      },
      {
        "title": "DigiAgriApp, second year update",
        "type": "Talk",
        "track": "State of software",
        "abstract": "**DigiAgriApp** is an free and open-source client-server application designed to manage a wide variety of data. This data can be collected either manually or directly from sensors. \nThe application is composed  of various open-source components such as PostgreSQL/PostGIS and  Django for the back-end, Flutter for the front-end, and a plugin for QGIS to manage geometries. \nOver the past year, since its first presentation in Kosovo, DigiAgriApp has undergone many improvements and gained new features. Users can now save data from measurements and observations made in the field. Moreover, a new component has been added to manage production data, particularly data collected by sorting machine, allowing the possibility to perform some simple analysis. Other improvements on the  client-side include the possibility of assigning different aspects (such as death, an observation or measurement) to one or more plants. The QGIS plugin allows to manage the geographical components of the database, primarily fields, subfields, rows and plants. The latest innovation is the integration of artificial intelligence, thanks to a new project funded by the Fondazione Valorizzazione della Ricerca Trentina. Specifically machine vision algorithms have been incorporated to analyze images and provide extrapolated values. The first application of the model is able to recognize the presence of Scaphoideus Titanus, a vector responsible for spreading the Flavescence dorée disease, which poses a potential threat to vineyards, on a pheromone photochromic trap.\nDuring our presentation we will showcase the first adoption of the application within FEM covering about 15 hectares and more than 20000 plants to demonstrate the application’s functionalities.",
        "description": ""
      },
      {
        "title": "Open Forest Observatory: Open-source drone-based forest mapping tools and data for ecologists",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Forest ecology research often requires detailed forest inventory data at the individual tree level, but such data are time-consuming and costly to collect using traditional ground-based manual survey methods. Recent advances in uncrewed aerial vehicles (UAVs, or drones), image processing, and deep learning are enabling a new era of forest research in which individual trees can be mapped, measured, and taxonomically identified across broad areas without extensive ground surveys. The Open Forest Observatory (OFO; openforestobservatory.org) is a new multi-institution organization that makes cutting-edge forest mapping tools and data accessible to ecologists and practitioners without extensive specialized computing background. Open-source OFO tools simplify and automate tasks including: (a) processing drone imagery into 3D canopy models and stitched imagery mosaics, (b) performing individual tree detection, geospatial crown delineation, and height measurement from drone-derived canopy height models, and (c) obtaining taxonomic classification of detected trees from raw drone images (including multiple views of each tree) using deep learning and 3D geometric reasoning. The OFO also hosts an extensive public database of raw and processed drone imagery from western U.S. forests (> 35 km2) across broad gradients in forest structure, species composition, and disturbance history, and > 100 field-based individual tree maps used for developing and validating the drone-based mapping tools. The growing database is available to host community-contributed datasets from forests globally. In relatively challenging (dense and structurally complex mixed-conifer forest conditions, current OFO overstory tree detection algorithms achieve precision and recall of 70-90%, and current tree height estimation achieves R2 of 0.95. In a challenging cross-site task, preliminary tree species classification using OFO multi-view computer vision tools achieved 76% accuracy across five species, compared with 54% accuracy of a baseline using a single top-down view from a stitched imagery mosaic. All tools and data are free for use by anyone to address ecology questions or build on the tools, and the OFO welcomes collaborations and contributions to data and code. Some current development priorities include (a) expanding multi-view mapping tools to support tree detection using computer vision, (b) optimizing tree detection and species classification algorithms across broad gradients of forest structure and species composition, and (c) developing cloud-native workflows for automated cataloging and processing of contributed drone-based and field-based forest data.",
        "description": ""
      },
      {
        "title": "QField Plugins to the rescue - Natural catastrophe rapid mapping in 2024",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The summer of 2024 saw extreme floods and landslides in Switzerland, significantly affecting the Ticino, Grisons and Valais regions.\nThis talk examines the critical role of QField's new plugin framework in complementing the aerial rapid mapping effort with crowdsourced oblique (terrestrial) imagery. We will discuss a quickly built proof-of-concept (POC) project that used QField's new customizable plugins, allowing first responders, civil protection agencies, and citizens to map and report damages efficiently. The session will introduce the plugin framework, its easy deployment through QFieldCloud, and its possible impact on field data collection workflows in enterprise and community settings.",
        "description": ""
      },
      {
        "title": "State of PgHydro - Hydrographic Extension for PostgreSQL/PostGIS",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The PostgreSQL extension for Hydrographic Applications Project (PgHydro) represents the first innovation of intelligence as an extension of spatial database management systems for use in water resources management that uses the sub-catchment network model and the logic elements present in the Pfafstetter basin coding system. PgHydro aims for an add-on implementation of a spatial database management system performed by a series of tables, queries, functions, or views that can be used individually to assist in water resources decision-making. These objects are the hydrography core of the intelligence system developed using free open-source software that can be used by anyone dealing with water resources management. To this end, this new conceptual model was implemented in the object-relational spatial database management system PostgreSQL/PostGIS, respecting the integrity constraints related to the geometry of the mapped objects. These user-defined constraints respect the logical objects based on the Pfafstetter basin coding system and integrity constraints linked to the spatial relationships between objects, which follow the ISO SQL/MM specifications. The main advantage of using the pghydro extension is the possibility to process large datasets and complex queries using a simpler hydrography model and the tools and languages already available in spatial database management systems that work as a framework for the future development of new extensions related to water resources.\nThe pghydro functionalities can be run using a GUI developed in a QGIS plugin called PgHydro Tools. After the physical implementation of the pgHydro Scheme in the spatial database management system, the construction of the Pfafsteter hydrography dataset is started using the hydrography objects that make up the pgHydro Tools. The construction of this base is divided into seven stages: 1) Creation of the spatial database and creation of the pghydro extension; 2) insertion of the drainage lines and the drainage areas in the spatial database; 3) verification of the consistency of the drainage network geometries and topologies; 4) verification of the consistency of the drainage areas geometries and topologies; 5) verification of the consistency of topology between the drainage network and the drainage areas; 6) Pfafstetter basin coding and other information, finally; 7) export of the final Pfafstetter hydrography dataset. Optional steps are the systematization of river names and the management of the multiuser edition.\nLast year, the pgh_raster was developed, which allows the insertion of the digital elevation model and products derived from it, such as drainage direction and flow accumulation. Functions allow retrieving the downstream or upstream pixel given a position as well as the elevation profile along a given specific geometry.\nAnother extension developed for pghydro was the pgh_hgm, a hydrogeomorphometric extension used to calculate information such as minimum and maximum elevation, slope, concentration time, time travel, and others related to drainage geometry and digital elevation. The pghydro project is officially and widely used by the National Water and Sanitation Agency of Brazil as a reference for the Brazilian Water Resources Management.",
        "description": ""
      },
      {
        "title": "State of GeoServer",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GeoServer is a web service for publishing your geospatial data using industry standards for vector, raster and mapping. Choose additional extensions to process data (either in batch or on the fly) and catalog records.\n\nGeoServer is widely used by organizations throughout the world to manage, disseminate and analyze data at scale. GeoServer web services power a number of open source projects like GeoNode and geOrchestra.\n\nThis presentation provides an update on our community as well as reviews of the new and noteworthy features for the latest releases. In particular, we will showcase new features landed in 2.26 and 2.25.\n\nWe will also check-in on the challenges highlighted on the 2024 Development Roadmap and provide a score card\n\nAttend this talk for a cheerful update on what is happening with this popular OSGeo project, whether you are an expert user, a developer, or simply curious what GeoServer can do for you.",
        "description": ""
      },
      {
        "title": "Fieldwork data collection for agriculture: tips, tricks, and lessons from Mozambique",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Collecting data in the field has become an essential task in most geospatial workflows. Geospatial data collection using mobile applications is expensive because it involves human effort, and several considerations need to be taken into account. Geospatial professionals should ensure high-quality data so that the capital effort employed yields reliable results, as collected data often represents an input for downstream processes. \n\nThis talk will share practical insights and experiences from a fieldwork project conducted in Mozambique for agricultural applications using QField and QField Cloud. It will illustrate from practical examples how to effectively set up forms, capture EXIF information from photos, and navigate the constraints encountered during fieldwork. The presentation will provide user-level tips and tricks to maximize the potential of these powerful tools for geospatial data collection and management.\n\nThe talk will also summarize the most important lessons learned from a recent experience in Mozambique by describing common issues encountered during fieldwork in remote areas,  solutions or workarounds for overcoming these challenges, and some recommendations for fieldwork data collection design. The showcase focuses on agriculture; however, practical examples apply to any tool that uses QGIS forms.",
        "description": ""
      },
      {
        "title": "Adding GeoParquet to a Spatial Data Infrastructure: What, Why and How",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "GeoParquet (https://geoparquet.org/) is a cloud-native format created to address the geospatial interoperability issues between data warehouses. It is based on the widely supported columnar storage Apache Parquet (https://parquet.apache.org/), extending it to add support for geometry data types (e.g.: points, lines and polygons). Although a relatively recent format, it already has a good ecosystem of tools, and it took Standardisation very seriously by joining the path to become an OGC Standard (https://github.com/opengeospatial/geoparquet).\n\nThe eMOTIONAL Cities project (https://emotionalcities-h2020.eu/) aims to understand how the natural and built environment can shape the feelings and emotions of those who experience it. At its core, lies a Spatial Data Infrastructure (SDI), which combines a variety of datasets from the Urban Health domain (https://emotional.byteroad.net/). These datasets should be available to urban planners, neuroscientists and other stakeholders, for analysis, creating data products and eventually making decisions based upon them. To support an efficient analysis, especially of the larger datasets, we have decided to offer GeoParquet as an alternate encoding. In this talk we share our experience, converting and publishing the +90 datasets of the eMOTIONAL Cities SDI using a stack of FOSS/OSGeo software (GDAL, gpq, pygeoapi).\n\nWe will show that there is already a set of (FOSS) tools in place (e.g.: readers, writers, validators) to support this task and to encourage others to add a Standards-based cloud-native format to their SDIs.",
        "description": ""
      },
      {
        "title": "The State of STAC",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Over the past few years, the STAC community has witnessed an huge increase in adoption and implementation across various sectors. With its focus on interoperability and extensibility, STAC has successfully addressed the long-standing challenge of data fragmentation in the geospatial domain. By providing a unified framework for describing and accessing geospatial assets, STAC has empowered users to effortlessly discover and analyze vast amounts of Earth observation data.\n\nMoreover, the emergence of an open-source ecosystem around STAC has been instrumental in its widespread adoption. A myriad of tools and libraries have been developed, enabling seamless integration of STAC into existing geospatial workflows. These tools encompass data providers, data processors, visualization platforms, and more, fostering a vibrant community-driven approach to solving complex geospatial challenges.\n\nThis presentation will provide insights into the current state of the STAC specification, including changes in 1.1 and the current set of STAC extensions with guidance on the use of extensions based on their maturity. In addition, we will provide an overview of the current STAC ecosystem, with a focus on the Python projects available in the stac-utils GitHub organization.",
        "description": ""
      },
      {
        "title": "Enhancing Geographic Data Accuracy: Convolutional Neural Networks in Urban Mapping",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "This presentation introduces an innovative project designed to enhance the precision of urban nuclei mapping using advanced artificial intelligence techniques. The focus is on improving the urban nuclei layer managed by the Institut Cartogràfic i Geològic de Catalunya (ICGC) through the application of convolutional neural networks (CNNs). The project involves training a CNN model on a curated collection of images featuring diverse urban nuclei. This training process enables the model to learn intricate patterns and characteristics unique to urban areas. Once the model is adequately trained, it can autonomously detect and categorize urban nuclei in new images with high accuracy.",
        "description": ""
      },
      {
        "title": "GeoAI for all: Helping answer the most common questions in geo",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Every geospatial project begins with a quest for answers. Large Language Models (LLMs) are revolutionizing how we can directly understand user needs through techniques like natural language to data structure conversion. Over the couple years, we have been exploring how AI could be used for working with geospatial data. What started as figuring out how to use natural language to make STAC queries to find public data has led to much more, including natural language geocoding to contextual image searching of public data such as NAIP and Sentinel-2.\n\nIn this talk we will explore how AI can be used to help automate some of geospatial’s most tedious tasks using open data, and how open vision models can be combined to create powerful tools for search & discovery of earth imagery.\n\nThis talk will include an overview of AI for use in geospatial analysis, with a focus on using open data and open models. We will show some live demos to create accurate AOIs with natural language, as well as for advanced searching of landscape features in public datasets. Additionally we will give an overview of techniques like Retrieval Augmented Generation and LLM Agents and the potential for how these may be used to transform geospatial data science.",
        "description": ""
      },
      {
        "title": "Free and Open source GIS architecture for low cost inventory mapping of urban water supply network",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "In the planning and expansion of water supply schemes, there needs to be detailed mapping and documentation of existing pipeline network and their assets. However this is usually not the case, especially where the construction of these pipelines predates advances in mapping, geoinformation and database where they in prior existed as drawing and engineering plans. In order to migrate to a fully documented inventory, digitalisation and management of a water supply network database to estimate demand and supplies to plan expansion and population growth, there needs to be an inventory of existing scheme. Historically, mapping has been done with expensive mapping and survey equipment that can pose a challenge for small organisation’s budget, making it difficult to have a complete mapping inventory of its network.\nThis article presents a geographical information system–based free and opensource software architecture for the mapping and inventory of urban water supply network. This architecture is especially useful where budget is tight and decision relating to meeting the water and sanitation-related Sustainable Development goals needs to be made. The architecture consists of data management, data collection, data analysis and project host environment tools and software.\nPostGresSQL with PostGIS was used for design and management of water supply network GIS database, basing the creation and design of features and attributes on prior knowledge of what exists on water supply networks. Features created are transmission and distribution pipelines, hydrants, valves, chambers, junctions, leaks, encroachments, pumps, pump stations, reservoirs, bulk flowmeter, treatment stations with attributes across that include diameter, pipeline material, operational status, condition, encroachment, photo; sizes, capacity, models, manufacturer etc. The PostGIS database was connected to a QGIS project environment where custom forms to were designed to capture attributes created in PostGIS. The QGIS project was linked to an android based mobile app data collection software called Qfield, hosting custom forms designed in QGIS to capture the content of the water supply features, location and attributes. Using the form on Qfield, the water supply network is mapped and attributes captured and once data capture has been carried out using Qfield software, data from field capture is synchronised to QGIS project and following edits to the data captured, it is updated to the PostGresSQL PostGIS database. QGIS software acting as the project host environment also functions as the software for mapping, visualising and analysis of data hosted and managed\nThe architecture presented is an opportunity for any organisation seeking a free and open source GIS option in capturing and documenting and managing their water supply network data. As one of the weaknesses, is that data captured using Qfield has the inherent horizontal and vertical accuracy acquired from android devices which is less accuracy than that from a survey equipment. However, Qfield has the option of connecting to GNSS equipment by blue tooth, inheriting the sub cm horizontal and vertical accuracy it offers and thus improving locational and elevation information and still offering a higher accuracy free open source option.",
        "description": ""
      },
      {
        "title": "Advanced Integration of Hydraulic Models for Water and Wastewater Networks using Giswater with Epanet-SWMM and QWC2 Web Client",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Overview\n“Aigües de Manresa”, a public water management company near Barcelona, Spain, serves 17 municipalities for a total of around 150.000 inhabitants (2022). The company manages a 1400 km water network, 127 tanks and 12 drinking water treatment plants, as well as a 400 km wastewater network and 18 wastewater treatment plants. To improve the efficiency of its water cycle management, the company secured funding from the Next Generation EU funds under the Recovery, Transformation, and Resilience Plan (PRTR). This project, benefiting all 17 municipalities, was selected as one of 30 out of 158 submissions in the first call for the PERTE (Strategic Projects for Economic Recovery and Transformation) for Water Digitization. The project's scope includes using open-source and non-licensed hardware, providing replicable solutions across the water management sector.\nObjectives\nThe primary objectives of this project are:\n1.\tTo integrate and improve open-source GIS tools and IoT technologies for comprehensive water resource management.\n2.\tTo develop a scalable, cost-effective platform for data visualization, analysis, and decision support.\n3.\tTo replace existing SCADA systems with open-source alternatives and incorporate IoT databases.\nMethods\nThe implementation strategy includes:\n1.\tExisting Tools: Utilizing QGIS Desktop and the Giswater plugin for inventory management and automatic generation of hydraulic models integrated with Epanet and SWMM.\n2.\tSCADA and Telecontrol Replacement: Replacing SCADA with a new open-source SCADA system and Telecontrol using non-licensed hardware.\n3.\tIoT Integration: Creating an IoT database to manage data from various sensors and devices, supporting advanced data analytics.\n4.\tOGC SensorThings API: Incorporating the OGC Standard SensorThings API by using FROST to facilitate seamless integration and management of sensor data.\n5.\tHydraulic Algorithms: Incorporating new hydraulic algorithms for Epanet. through Qgis Plugin Giswater to improve system modeling and performance.\n6.\tWeb Client: Using QGIS Web Client 2 (QWC2) for web-based data visualization and interaction.\n7.\tCustom Plugins: Developing customized services and specific plugins to extend QWC2 functionalities with the ones provided by Giswater to view inventory and hydraulic model data and interact with the data; for another hand there is a proposal for a new QWC2 plugin for visualizing the data provided by the Sensorthings Api server. \nConclusions\nThe integration of open-source GIS and IoT technologies under the PERTE project will significantly improve the water resource management capabilities of “Aigües de Manresa”. This project demonstrates the practical benefits of using open-source solutions in utility management, promoting sustainability and cost-efficiency. By using these technologies, “Aigües de Manresa” sets a standard for other water utilities, showing the potential for widespread adoption and replication of these solutions.",
        "description": ""
      },
      {
        "title": "GeoPortal PMPV",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "GeoPortal PMPV is a WebGIS platform developed by the Department of Geoprocessing – GEO of the Municipal Secretariat of Planning, Budget and Management – ​​SEMPOG of the City of Porto Velho, which aims to access, consult, analyze and disseminate geographic data from the Municipality of Porto Velho, based on the Municipal Urban and Territorial Information System – SMIUT of Article 45, of Complementary Law Nº 838, of February 4, 2021, which addresses the Participatory Master Plan of Porto Velho.\n\t“Article 45 – The Municipal System of Urban and Territorial Information will be established using a georeferenced digital cartographic base of the Municipality, progressively integrating the various databases of the City Hall to create the Municipal Multifunctional Technical Registry, which will be used for planning and management by all sectors of the Administration and as a tool for transparency of municipal information.” (LEI COMPLEMENTAR Nº 838, 2021, p. 20).\nThe GeoPortal holds relevance and significance grounded in socioeconomic, technical, and professional aspects. It contributes to the dissemination of municipal information for various uses, ranging from the general public to academics and professionals in different fields. This platform offers several functionalities, including: geoservices such as Web Map Service – WMS and Web Feature Service – WFS, and access to 53 geospatial layers accompanied by their respective metadata. Additionally, the GeoPortal offers compendium services that cover the geographic boundaries, historical context, and laws regarding the creation and modification of the 13 districts and the main administrative area that make up the municipality of Porto Velho, as well as the 72 neighborhoods within the municipal seat.\nIn this way, the PMPV GeoPortal provides easy access to georeferenced information for the municipality of Porto Velho, covering data on drainage, neighborhoods, schools, health units, among others. This platform promotes data transparency for the population, encouraging citizen participation in the municipality's planning and management projects. This not only increases the transparency of government actions, but also strengthens the relationship between public administration and citizens, creating a collaborative environment for building a more inclusive and sustainable city. Furthermore, the public availability of this information allows municipal planners and managers to make more assertive and updated decisions about territorial space, both urban and rural, promoting more effective development.\nThese functionalities were developed using a relational database management system, created through an open-source project, namely PostgreSQL with the PostGIS extension, along with the use of Geoserver software to develop web mapping solutions and make them available on the GeoPortal.\nAmong the main features of GeoPortal PMPV are WMS and WFS geoservices. WMS offers the visualization of dynamic maps on the web, providing fast and efficient access to several layers of geospatial data, which facilitates interactive spatial analysis and visualization of information in real time. In turn, WFS facilitates the querying and extraction of vector geospatial data, allowing users to perform detailed analyzes and integrate this data with other information systems. In this way, promoting more integrated and precise territorial management.\nWith 53 layers available, the GeoPortal PMPV is configured as an environment rich in information from different areas, including urban and territorial data, urban zoning, mobility, environment, public services, among others. Each layer includes detailed metadata, ensuring data quality and reliability. This information is essential for users who need to understand the origin, accuracy and timeliness of data for their analysis. Thus, GeoPortal PMPV is consolidated as a fundamental tool for planning and management of the municipality of Porto Velho, promoting efficiency and transparency in public administration.\nFrom this perspective, all technological resources used by the public administration of Porto Velho were open source, in order to guarantee rapid implementation, eliminating bidding processes, and development of advanced functionalities, when comparing the current status of the city hall with other Geoportals of units already renowned for their environments for providing geographic data.\nBenefits Summary\nGeoPortal PMPV provides numerous benefits for the urban management of Porto Velho, including:\nAccess to updated and accurate geospatial information, essential for effective urban management;\nSupport for strategic decision-making based on reliable data, promoting sustainable urban development;\nTransparency in government actions by making data available to the public, encouraging citizen participation in management processes;\nEase of collecting and analyzing data for research and projects, benefiting researchers, students and companies.\nIntegration of different information systems, allowing a more holistic and coordinated approach to territorial management.\nThese advantages consolidate GeoPortal PMPV as an indispensable tool for the planning and administration of Porto Velho, promoting more efficient and inclusive management.\nFuture perspectives\nGeoPortal PMPV plans future updates and improvements to the platform, aiming to expand its functionalities and integrate with other smart city initiatives. Future prospects include:\nContinuous updates of geospatial data to keep the platform always up to date and relevant;\nDevelopment of new functionalities that meet the emerging needs of users and municipal management;\nIntegration with other smart city technologies and platforms, promoting even more integrated and efficient urban management;\nExpanding access and use of the platform to a wider audience, encouraging the participation of more citizens and institutions in the use of geospatial information.",
        "description": ""
      },
      {
        "title": "End To End Tech for Humanitarian Response and Disaster Relief",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "At  the Humanitarian OpenStreetMap Team (HOT), we are working on a group of free and open source tools for community creation of maps. Following a process that involves aerial imagery gathering with UAVs, remote volunteer mapping, community in-the-field data collection, AI assisted remote sensing, and finally, downloading and using the map in disasters and humanitarian work. This is an end-to-end (E2E) solution that benefits everyone, from a small community to a big organization. We want to tell the story about how we’re creating and using these tools and what could be the future of humanitarian and disaster mapping from an open tech and data perspective. \n\nWe hope that you leave this talk inspired and excited about becoming part of the end to end mapping journey!",
        "description": ""
      },
      {
        "title": "QField 3 - Fieldwork redefined",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Discover QField, the professional mobile data collection app for QGIS with over 1 million downloads and 350K active users. Recognized as a Digital Public Good, QField supports essential UN Sustainable Development Goals, such as Clean Water, Sustainable Cities, Climate Action, and Life on Land. \n\nThis powerful tool combines minimal design with advanced technology, enabling intuitive data viewing and editing. Seamlessly synchronized with QFieldCloud, it ensures efficient and pleasant fieldwork sessions. \n\nJoin us to explore how QField 3 can redefine your fieldwork, making it more effective and impactful for addressing both daily tasks and global challenges.",
        "description": ""
      },
      {
        "title": "One million reasons to use QField",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Join us for an in-depth look at how QField is transforming fieldwork for individuals and organizations worldwide. With over 1 million downloads and 350K active users, QField is recognized as a Digital Public Good supporting key UN Sustainable Development Goals.\n\nIn this session, we will showcase real-world use cases that demonstrate how QField empowers teams to tackle daily tasks and global challenges efficiently and effectively. \n\nLearn from success stories across various industries and discover how QField 3's seamless integration with QFieldCloud is making a tangible impact on fieldwork around the globe.",
        "description": ""
      },
      {
        "title": "The Digital Module of the IS_Agro Project: Using the medallion architecture as a basis for automating pipeline execution routines in Apache Airflow",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The IS_Agro project is an initiative focused on the critical evaluation and subsequent adaptation of methodologies designed in global forums, with a view to their application in the national context based on the development of new agro-socio-environmental metrics and indicators (IASs) that aim to provide a more accurate and authentic representation of the agricultural landscape in the national territory. IASs are measures used to monitor and evaluate agricultural performance related to social, economic and environmental aspects, thus having great importance in guiding more sustainable political strategies and agricultural practices, whether by the public or private entity, serving “to evaluate the performance of agriculture in terms of its environmental, social and economic performance, providing comparative data and information between federative entities or countries, among several other applications” (EMBRAPA SOLOS, 2023). In this project, IASs are developed by different teams specialized in the proposed themes, whose works are previously approved and published in the scientific arena. To automate data collection, allocation, calculations and constant updates of the IASs, there is a team called the Digital Module, which develops solutions for each indicator, transforming them into digital algorithms. Structured, semi-structured and unstructured registration data are collected and stored in a data lakehouse, requiring a great deal of organization within the repository so that the data is always available and easily accessible. It was decided to implement the medallion architecture (medal architecture), which consists of allocating data in three layers with different purposes, while an open source platform was used for pipeline management and automation.\n\nThe conception of this project as a digital platform linked to the Brazilian Agricultural Observatory aims to publish indicators and parameters derived from well-founded technical and scientific data, capable of evaluating the effective performance of the national agricultural sector at the municipal or state level, contributing to sectoral policies and planning and management processes aimed at building sustainable agriculture and the correct positioning of the country on the international scene. Thus, the general objective is to develop an intelligent environment that automates and manages the IAS pipelines in a data storage organization environment based on the medallion architecture to be the basis of the data panel for publishing the indicators.\n\nA data pipeline is a succession of connected phases that enable the collection, storage, modification, analysis, and representation of data, with the purpose of acquiring meaningful insights and supporting informed choices (CALANCA, 2023). A data lakehouse, the destination of the project pipelines, is “like a modern data platform built from a combination of a data lake and a data warehouse” (ORACLE CLOUD INFRASTRUCTURE, 2023), using “the flexible storage of unstructured data from a data lake and the management capabilities and tools of data warehouses, and then strategically deploying them together as a larger system” (ORACLE CLOUD INFRASTRUCTURE, 2023). The medallion architecture is the sequential structuring of data storage that aims to logically organize the data in the lakehouse, aiming to incrementally and progressively improve the structure and quality of the data as it flows through the three layers of the architecture (ARQUITETURA medallion, 2024). The terms bronze (raw data from the source), silver (transformation and validation of the data), and gold (refined and enriched data for use in projects) describe the quality of the data during the process (SKAYA et al, 2024) . Pipeline management is performed by Apache Airflow (version 2.44), an open-source platform for developing, scheduling, and monitoring batch-oriented workflows based on the Python programming language, which allows you to create workflows connected to virtually any technology (WHAT is Airflow™?, 2023). The Airflow execution environment was structured in Docker, an open-source platform that allows you to create and manage containers as modular virtual machines that contain the essentials for their execution. The developed image is available on GitHub.\nTo be confirmed, the routines will be executed once a month. Raw data is collected by downloading and maintaining its original format, with a hash of each file being recorded to indicate that the data has been updated and download it again in the event of a change. This data is cleaned and processed as needed. At the end of the silver phase, a tabular structure will be created with geocode (integer, IBGE code of municipalities or states), date (timestamp, ISO 8601), source (text) and value (floating point, real number) and will be saved in the data lakehouse as .parquet, an open-source columnar storage format designed for highly compressed storage and efficient data retrieval, providing improved performance for handling complex mass data (OVERVIEW, 2022). The .parquet files saved in the data lake are available for use in the gold tier with one-to-many cardinality. In this last phase of the architecture, the necessary calculations are performed for each source of the indicators, with some sources that do not require calculations. The final phase is with the export of the gold data to tables in a project database in PostgreSQL, being ready for use by an API developed internally that allows the provision of data for the data panel to be developed (by another team) and published to society from the project website.\n\nThis model has been adjusted and corrected throughout the development of the project in the Digital Module. Flexible, it is now considered ready to receive any indicator developed by other teams, as well as the development of the data panel for publication for use by society.",
        "description": ""
      },
      {
        "title": "Creating Web-Ready QGIS Plugins: Insights from Giswater for Effective QWC2 Compatibility",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In the ever-evolving realm of geospatial technology, developing QGIS plugins that are both reliable and adaptable for web environments is essential. This talk will provide practical insights into creating QGIS plugins that perform well across both QGIS desktop and QGIS Web Client (QWC2).\n\nKey areas of focus include:\n\n- Database-Driven Design: Explore how a database-driven approach can enhance both the functionality and user interface of QGIS plugins. This method simplifies development by allowing dynamic configuration and management of plugin features and UI elements based on backend data, ensuring seamless integration with various data sources and use cases.\n\n- Qt Forms as a Service: Learn how to implement Qt Forms as a service, where forms are dynamically generated and customized according to backend configurations. This approach facilitates the creation of adaptable and maintainable user interfaces that respond efficiently to different data inputs and user needs.\n\nThrough the Giswater plugin case study, this talk will showcase these concepts with practical examples. Attendees will gain valuable insights into building web-ready QGIS plugins that are robust, flexible, and user-friendly across diverse platforms.",
        "description": ""
      },
      {
        "title": "It's not broken... but fix it anyway. Customizing FOSS4G Tools for Government: The Inteligeo Case Study",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "This talk presents a case study of Inteligeo, an adaptation of GeoNode for Brazilian government agencies. We'll share our journey in taming a 170TB raster dataset and other adventures in customizing open-source geospatial tools. We outline the experience of four agencies: The National Water Agency (ANA), the Center for the Surveillance of the Amazon (CENSIPAM), the Chico Mendes Institute for Biodiversity Conservation (ICMBIO), and the Federal Police (PF).\n\nThe project, which began in 2009 using proprietary software, transitioned to open-source in 2017. To address limitations and support multiple agencies, we ported Inteligeo version 4 functionality to GeoNode, creating Inteligeo 5 in 2022. Each agency has a unique perspective: PF was involved from the start, CENSIPAM deployed for internal use, ANA for external use, and ICMBIO is integrating it into their processes.\n\nWhy customize at all? Why not just use the software as it is? Why not develop it from scratch? There are several reasons why one should (and shouldn't) customize, and then there are several ways how to do it right once you commit to it. We share our experiences: the good and the bad, and the lessons learned, when customizing GeoNode for the Brazilian government.\n\nWhy customize?\n- Get shiny new functionality!\n- Jump-start development\n- Optimize processes with tailored deployments and workflows\n- Integrate with existing systems and infrastructure (authentication, 170TB raster storage, Brasil Mais imagery)\n- Comply with internal and government standards\n\nChallenges:\n- Non-standard deployment\n- Training and documentation\n- Balancing customization with community support\n- Syncing with upstream changes\n- Managing a huge codebase with extra stuff that you don't need\n\nHow to do it right (aspirational):\n- Seek sponsorships (SGD/MGI, JICA, FINEP, INTERPOL)\n- Selfless and selfish reasons to contribute back to the community\n- Have a clear strategy for upstream syncing\n- Keep it simple. Minimize customization to essential features\n- Design independent, standalone components\n- Engage upstream developers when possible\n\nOur experience is particularly relevant to the Amazon region, as the tool directly supports the Federal Police surveillance and conservation efforts in the area and is being integrated by the agencies of the other speakers.\n\nWe welcome feedback and collaboration ideas from the FOSS4G community during the Q&A session!",
        "description": ""
      },
      {
        "title": "Benchmarking Zonal Stats for Wildfire Resilience",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The State of California has a recurring issue of devastating wildfires that spread out of control, causing significant damage to the well-being of its citizens, environment, and economy.\n\nDue to this recurring issue, the State of California and the USDA—United States Forest Service committed to treating one million acres of land (roughly 400,000 hectares) to reduce the occurrence and severity of these wildfires. Planscape was created to prioritize and guide this one million-acre effort.\n\nPlanscape is a freely available tool built to help landscape planners, public agents and the general public prioritize and learn about landscape interventions that minimize fire risk and cost while maximizing ecological benefits. \n\nPlanscape utilizes zonal statistics data gathered from over 200 raster layers, applying Linear Programming techniques to determine the best locations for planners to intervene in the landscape by performing treatments such as mastication, prescribed burns, etc.\n\nThis hands-on session will discuss several options for obtaining Zonal Statistics for Planscape. In this benchmark, we can compare a diverse range of technologies and techniques utilized to reduce the necessary time to obtain zonal statistics for roughly 24 million regular polygons covering the state of California and, over 200 million zonal statistics records.\n\nThis talk benchmarks the following methods: PostGIS + PostgreSQL, Python RasterIO, rasterstats, custom code.",
        "description": ""
      },
      {
        "title": "Shooting for Photorealistic 3DCG with Navara: Our Journey Begins",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Join us on the ground floor of an exciting journey as we develop Navara, a revolutionary mapping engine designed to shatter the boundaries of 3D visualization. In this candid presentation, we'll share our early progress, tackle the tough questions, and map out the road ahead.\n\nWhat to Expect:\n\nRethinking 3D Mapping: Why we're shaking things up and addressing the limitations that have held us back\nNavara's Vision: Our technical philosophy and core principles driving innovation\nUnder the Hood: Early architectural decisions and their implications\nOvercoming Obstacles: Real-world challenges we're tackling to achieve photorealism\nFuture Focus: Immediate priorities and long-term ambitions\nWho Should Attend:\n\n\nDevelopers, graphics engineers, and anyone passionate about 3D visualization are invited to join the conversation. Come prepared to dive into the technical nitty-gritty and explore the challenges of building a cutting-edge mapping engine from scratch.",
        "description": ""
      },
      {
        "title": "Soar online open platform to make public and open maps accessible and shareable, and its QGIS Plugin",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Soar is an online atlas of maps, free and open. Our mission is to make maps publicly accessible.  \nOur goal is to bring together every map, satellite, and drone image that has ever existed, or will ever exist, in one place. \nTherefore, it is aimed at both lay people who appreciate maps and professionals.\nSoar is a collaborative platform: maps come from both organizations and individuals.\nFor greater interactivity, we have created map editing tools and a QGIS-Soar integration plugin.\nWe propose to show in this short talk how to access and build upon the multitude of open data maps on Soar catalog using Soar-QGIS Plugin.",
        "description": ""
      },
      {
        "title": "Earth-Search: A STAC API of Open datasets on AWS",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "Earth-Search is a publicly-accessible SpatioTemporal Asset Catalog (STAC) index and API providing data discovery and access for several major geospatial data collections as part of the AWS Registry of Open Data (RODA), including Sentinel-1, Sentinel-2, Landsat Collection 2, and NAIP imagery. Items are backed by data assets accessible in cloud-native formats such as Cloud-Optimized GeoTIFF (COG).\n\nThis talk will provide an overview of the Earth-Search STAC catalog, how to search it to discover items, and how best to access the backing data assets. We'll look at recent changes to catalog and discuss the progress and challenges of the Sentinel-2 reprocessing/reindexing effort. We'll also briefly discuss the architecture of the data orchestration pipeline and what open source tooling underlies its operation.",
        "description": ""
      },
      {
        "title": "GeoHealthCheck - QoS Monitor for Geospatial Web Services",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Keeping (OGC) Geospatial Web Services up-and-running is best accommodated by continuous monitoring: not only downtime needs to be guarded, \nbut also whether the services are functioning correctly and do not suffer from performance and/or other Quality of Service (QoS) issues.\nGeoHealthCheck (GHC) is an Open Source Python application for monitoring uptime and availability of OGC Web Services.\nIn this talk we will explain GHC basics, how it works, how you can use and even extend GHC (plugins).\n\nThere is an abundance of standard (HTTP) monitoring tools that may guard for general status and uptime of web services. \nBut OGC web services often have their own error, \"Exception\", reporting not caught by generic HTTP uptime\ncheckers. For example, an OGC Web Mapping Service (WMS) may provide an Exception as a valid XML response or\nin a error message written \"in-image\", or an error may render a blank image. \nA generic uptime checker may assume the service is functioning as from those requests and an HTTP status \"200\" is returned.\n\nOther OGC services may have specific QoS issues not directly obvious. A successful and valid \"OWS GetCapabilities\" response may not \nguarantee that individual services are functioning correctly. For example an OGC Web Feature Service (WFS) based on a dynamic database may \nreturn zero Features on a GetFeature response caused by issues in an underlying database. Even standard HTTP checkers supporting \"keywords\" \nmay not detect all failure cases. Many OGC services will have multiple \"layers\" or feature types, \nhow to check them all?\n\nWhat is needed is a form of semantic checking and reporting specific to OGC services!\n\nGeoHealthCheck (GHC) is an Open Source (MIT) web-based framework through which OGC-based web services can be monitored. GHC is written in \nPython (with Flask) under the umbrella of the GeoPython GitHub Organization. It is currently an OSGeo Community Project. \n\nGHC consists of a web-UI through which OGC service endpoint URLs and their checks can be managed, \nand monitoring-results can be inspected, plus a monitoring engine that executes scheduled \"health-checks\" on OGC service endpoints. \nA database stores results, allowing for various forms of reporting.\n\nGHC is extensible: a plugin-system is available for \"Probes\" to support an expanding number of \ncases for OGC specific requests and -checks. Work is in progress to provide a GHC API for various integrations.\n\nInfo, sources, demo: https://geohealthcheck.org",
        "description": ""
      },
      {
        "title": "Using Openstreetmap and its technological ecosystem for integrated and community-based territorial management: the Amazon Mappings",
        "type": "Talk",
        "track": "Applications and solutions for the Amazon region",
        "abstract": "HOT's LAC hub dedicates an important part of its activities to foster mapping and collaborative projects in the Amazon, with different academic and civil partners, and with the regional Openstreetmap community including its large student community, in continuity with OSM-Latam's Mapazonia initiative. \nThe Amazonia Program is a multifaceted initiative that addresses mapping gaps and promotes social impact mapping in the region. Partnerships with disaster risk management, civil protection and municipal development authorities aim to fill mapping gaps and improve the risk management and sustainable development, including preservation, of this critical region for the world. Early mapping identifies vulnerable areas to improve planning and response and make degradation processes visible. The community projects teach mapping and support environmental monitoring in several Amazonian cities and communities in Brazil, Colombia, Ecuador, Peru and Bolivia, fostering local engagement and open data ownership.\nThe set of projects develops technical competencies with the great diversity of actors that have a role in the management of their territory (governments, communities, ancestral governments, civil society, supported by universities), based on the use of free and collaborative geographic data (OSM) and “low tech” mapping and monitoring tools, also free, culturally relevant, and with low connectivity requirements. \nIn this talk we will explain the long term strategy to map the Amazon, will exemplify different projects, show the challenges for the region, and show the different mappings that the audience can join and invite their own communities.\nThis talk will serve as an introduction to a practical activity that will be proposed to contribute to the mapping of Belém.",
        "description": ""
      },
      {
        "title": "Open Commercial Fisheries Monitoring Systems for Management: from Data Collection to Web Visualization",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Fisheries' sustainability should be achieved by considering biological, social and economic aspects. To this end, different strategies should be followed, including co-management, which encourages scientists, governments, fishers and civil society to jointly manage the ocean resources. Since 2014, several co-management strategies have been implemented in Catalonia, a region with about 580 km of coast in the NW Mediterranean Sea. With the aim to transfer scientific knowledge to better manage the ecosystem, we present here the end-to-end (E2E) system from ICATMAR, the Catalan Institute of Research for the Governance of the Seas. The E2E system includes data collection, processing, analysis, publication and web visualization of bottom trawling and purse seine fisheries sampling data along the Catalan coast. In 2023, all these fisheries represented 85% of the total catch and 77% of the total fisheries revenue of the region.\n\nDuring 5 years of data collection (2019-2023), the sampling program created a dataset of over 1,500 onboard samplings and 1 million sampled specimens of more than 470 different species. As the combination of environmental data with fisheries monitoring brings new approaches to assess the status of the ecosystem, the collected fisheries data, jointly with the daily fishing landings and Vessel Monitoring System (VMS), are all visualized in combination with georeferenced sea habitats (EMODnet), and climate and sea conditions (CMEMS) on the web browser. An open website (www.icatmar.cat) offers the following data visualizations: geolocalized fisheries samplings together with the mentioned data sources, biomass distribution per port or season, and length-frequency charts per species (https://icatmar.github.io/VISAP). To better implement fisheries management strategies, these E2E information systems may be used as a tool to access high-quality open data, facilitate their comprehension and ease the dialogue between science, fisheries, policymakers and civil society.",
        "description": ""
      },
      {
        "title": "How to Bridge the Gaps Between Remote Sensing AI Research and Real-World Industry Challenges",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "### Introduction\n\nArtificial Intelligence (AI) is transforming remote sensing by enabling the analysis of vast datasets with unprecedented accuracy and efficiency. Despite the progress, significant gaps remain between academic research and practical industry applications. This talk explores these gaps, focusing on the challenges and strategies for transitioning AI research into viable industry solutions, and how open science can play a pivotal role in bridging these gaps.\n\n### Academic Research: Objectives and Challenges\n\nAcademic research in AI and remote sensing aims to push the boundaries of knowledge, often focusing on developing novel algorithms and theoretical models. Researchers prioritize innovation and publication, with less emphasis on immediate practical applications. Challenges in academia include limited access to high-quality data, shared computational resources, and the need for interdisciplinary collaboration. These constraints can hinder the scalability and robustness of research outcomes, making them less suitable for direct industry implementation.\n\n### Industry Applications: Objectives and Challenges\n\nIn the geospatial industry, the primary goal is to solve real-world problems efficiently and effectively. Companies require AI solutions that are robust, scalable, and cost-effective. Challenges include managing vast amounts of heterogeneous data, ensuring real-time performance, and meeting regulatory standards. The industry prioritizes practical methodologies that integrate seamlessly into existing workflows and deliver actionable insights.\n\n### Bridging the Gaps\n\n1. **Data Accessibility and Quality**: Enhancing collaboration between academia and industry can improve access to high-quality, labeled datasets, which are essential for training and validating AI models. Open science initiatives can facilitate this by promoting data sharing and transparency.\n2. **Computational Resources**: Joint initiatives can help share and optimize computational resources, leveraging both academic high-performance computing facilities and industry cloud infrastructure. Open science can further this by encouraging the development and use of open-source tools and platforms.\n3. **Scalability and Robustness**: Academic models must be adapted to handle the complexity and variability of real-world data. This requires close collaboration to test and refine models under operational conditions. Open science practices, such as sharing code and methodologies, can accelerate this adaptation process.\n4. **Integration and Compatibility**: Research prototypes need to be re-engineered to fit into industry workflows. This involves interdisciplinary teams of researchers, engineers, and user experience designers working together. Open science can aid in this by providing a common platform for collaboration and knowledge exchange.\n5. **Ethical and Legal Considerations**: Addressing ethical and regulatory issues through joint frameworks ensures that AI applications are transparent, fair, and compliant with legal standards. Open science principles, like open access and public engagement, can help maintain ethical standards and regulatory compliance.\n6. **Accelerated Innovation**: Open sharing of research findings and tools accelerates the pace of innovation, enabling faster development and deployment of AI solutions in remote sensing.\n7. **Capacity Building**: Open educational resources and open-source tools help build capacity in both academia and industry, ensuring a skilled workforce that can effectively utilize AI technologies.\n\n### Conclusion\n\nBridging the gaps between remote sensing AI research and industry applications is crucial for maximizing the potential of AI. By fostering collaboration, focusing on practical challenges, and embracing open science, we can develop AI-driven solutions that address the complex needs of the geospatial industry. This talk will provide insights and strategies for achieving this integration, highlighting case studies, best practices, and the transformative role of open science.",
        "description": ""
      },
      {
        "title": "What is Digital Earth Pacific",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Digital Earth Pacific (DEP) is a transformative project aimed at developing an operational Earth observation infrastructure for the Pacific. With a focus on empowering Pacific communities, DEP strives to simplify access to Earth and ocean observation data and deliver new data products that cater to specific regional needs. This presentation provides an overview of the motivations, objectives, and methodologies of DEP, highlighting its user-centric approach and the potential impact on informed decision-making and sustainable development in the Pacific.\nBy democratising access to data and building upon advancements in technology, DEP aims to enhance accessibility and streamline data retrieval processes. Through a co-design process involving stakeholders from various sectors, DEP ensures that the infrastructure and data products align with the specific requirements and challenges faced by the Pacific region. Ultimately, DEP seeks to empower Pacific communities to harness the power of Earth and ocean observation data, driving resilience and informed decision-making",
        "description": ""
      },
      {
        "title": "Modern Geospatial Data Science in the Cloud with Nebari",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The modern Python geospatial stack encompasses several tools and libraries that allow scientists and developers to write more efficient and scalable data science workflows, from data access and preparation, to analysis and visualization. It provides a great ecosystem for reading and writing cloud-optimized and chunked data formats, accessing data catalogs, handling labeled N-dimensional arrays, parallel and distributed computing, statistical analysis, machine learning, and interactive computing and plotting.\n\nAs data scientists increasingly work in teams and tackle bigger and more complex problems, there is a growing need for collaborative platforms that can support sophisticated workflows and large-scale data processing. However, platforms for effective collaboration still have significant challenges, including deployment, configuration, graceful scaling, and environment and dependency management. Addressing these challenges is not trivial and it often requires some DevOps expertise.\n\nIn this talk, we’ll introduce Nebari, a cloud-based open source data science platform built on top of Kubernetes, Dask and the Jupyter ecosystem. Nebari enables organizations to quickly deploy a collaborative platform on any of the major cloud providers. Once deployed, teams can easily access single-user Jupyter Notebook and VS Code servers from their web browsers and start writing and running reproducible and scalable geospatial data science workflows. Integrated with conda-store and Dask, it provides users not only the possibility to build, share and access conda environments from their servers, but also to launch short-lived clusters to handle their compute-intensive tasks.\n\nWe’ll demonstrate how Nebari can be leveraged to develop compute and data intensive applications in the cloud using packages from the modern Python geospatial stack. By the end, we hope to equip organizations with the tools and knowledge to promote better and more effective collaboration in geospatial data science. Organizations can choose to adopt Nebari as an out-of-the box platform for their teams, or use it as a blueprint for developing a custom platform built on top of open source libraries.",
        "description": ""
      },
      {
        "title": "MAPi: Web Mapping Platform for Education and Urban Planning",
        "type": "Talk",
        "track": "Education",
        "abstract": "MAPi is a web platform for online mapping conceived as a public participation tool for schools and organized civil society. Its main functionalities of visualization, analysis, and collection of spatial data were developed to encourage participatory urban planning, especially focusing students and teachers from basic education.\nThe Center for Metropolitan Studies (CEM), which is the research center that hosts MAPi´s project, has been responsible for producing and openly sharing databases (cartographic and non-cartographic) since its inception 20 years ago.  More recently, CEM expanded its activities in this technology transfer area to develop interactive web systems, like ReSolution and Schools portals, built from the interaction with the center's academic research work and agendas. So the idea is to bridge the gap between highly complex research and technology transfer to academic and non-academic society, respecting the principle of open data and software. All data produced and all softwares developed, including its source codes, are available at no cost.\nIn this context, however, there is a great challenge to make the knowledge produced by the CEM, especially the cartographic databases, continuously accessible to a portion of society that does not master geospatial analysis techniques. Although the ecosystem that provides tools for this type of analysis is increasingly more accessible, including through the web, they are still far from being widely adopted in non-specialized contexts, such as classrooms.\nTo tackle this challenge, MAPi arises to perform as a tool that can expand the use of spatial data in primary and secondary schools. So it has the twofold challenge of gathering simplicity in its navigation to ensure a friendly experience with complexity of technique specificities of exploring and visualizing spatial data,  especially for the non-specialized public.\nThus, MAPi has, since its first version, native integration with CEM's spatial data repository, GeoCEM, which is developed based on the GeoNode software, with its metadata stored on Geoserver..\nThrough Geonode, MAPi queries the layers that are published on GeoServer using the Web Feature Service (WFS). WFS specifies an interface for accessing and manipulating geographic elements using the HTTP protocol. The advantage and reason for choosing this standard is its ability to extract only the desired data at the element and attribute level. In this way, we can build a dynamic platform that creates thematic maps in real-time from the parameters chosen by the user, characterizing it as an interactive mapping platform.\nMAPi presents as its main tool for spatial data analysis the interface for creating choropleth thematic maps. This type of map is used in MAPi to represent data collected from geographic units such as census tracts and districts. For this functionality, the platform allows the choice between some color palettes (sequential, divergent, or qualitative), which are associated with the polygons of the geographic units according to the class division (quantile or natural breaks). The selection of multiple spatial data layers that are added as a Vector Tile layer in OpenLayers, makes it possible for the user to analyze and compare different themes or periods simultaneously. To serve non-specialized audiences, we maintain only the most essential tools of thematic mapping so that users can in a few steps build visualizations capable of extracting analyses, maintaining some level of parameterization to adapt to the type of phenomenon. Thus, through the OpenLayers and WFS resources, we make parameterizable for each layer its visibility, the attribute that will be associated with the fill color, the data classification method, the color palette, and the attributes that will be displayed in the dynamic map legend (based on attribute values).\nFinally, the third axis of functionality, collaborative mapping, aims to provide, in a simplified way, tools for the production of spatial data. We understand here that this functionality can be a tool that enables the expansion and updating of data coverage that official agencies are unable to perform. In this way, we intend to create an area within the platform that allows the creation of forms with location fields to create spatial data to be viewed and analyzed together with the other data on the platform. Currently, only the integration with Mapillary partially accomplishes this goal through the consumption of the visualization of the cover layer, and visualization of the photo sequences available in Mapillary by its users.\nOne potential application in education from collaborative mapping functionalities is the support in the discussion about neighborhood plans, that are  instruments foreseen under master plan development, like the example from the megacity of São Paulo. The tool is under test with basic education teachers that want to promote mapping activities with MAPi in urban geography classes, and also within mathematics classes where they can address methodological issues of the results of this mapping to support basic concepts related to statistics in the classroom. Our next challenges are the optimization of performance for large volumes of data and the implementation of more complex collaborative mapping functionalities in a Agile methodology, with constant interaction with teachers that envision MAPi´s potential to foster engagement of students in urban planning.",
        "description": ""
      },
      {
        "title": "Geo-Data Analytics and Technology Industry: Current trends, challenges, and opportunities",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "The purpose of this talk is to clarify the trends, challenges, and opportunities that exist in the geo-data analytics and technology industry. With a focus on the critical importance of data visualization, we will examine the cutting edge tools and approaches that are driving this field ahead, especially those found in the open-source ecosystem.The combination of inovation and technology with geo-data is creating a paradigm-shifting environment in the realm of geospatial analytics.\n\nUnprecedented progress is being made in geo-data, the foundation of spatial analysis, by incorporating advanced analytics. The present developments highlight the mutually beneficial interaction between geo-data and technology innovation, which promotes improved strategic planning and decision-making. Even if the problems are difficult, there are chances for creative solutions that make use of all the data that is accessible. With even from a single petal of a flower, we can visualize a vast data with proper analysis and with geo-data and we can carry out all sort of visualization interms of map with multi purpose graphs and charts.\n\nThe talk will explore the vast array of free and open-source resources that enable participants to turn unprocessed data into visually engaging stories. Through the utilization of platforms like QGIS, Openlayers, Maplibre, and D3.js, we can effectively transform intricate geographical data into easily understood and useful insights. These visualizations provide important interpretive value to data and clarify spatial relationships, facilitating well-informed decision-making in a variety of industries.\nThis talk will also highlight the ground-breaking potential of geo-data analytics to transform sectors and improve societal results. We will illustrate how geo-data analytics may promote innovation, efficient resource allocation, and sustainable development by analyzing case studies and real-world applications.",
        "description": ""
      },
      {
        "title": "Mapillary 2.0 - How street-level imagery helps us understand the world",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This talk will cover some of the following areas:\n1. An overview of Mapillary.\n2. Who is contributing and some interesting case studies.\n3. Some of the recent updates including 2.0 mobile apps, NeRFs, and improved upload.\n4. How to contribute including cameras, upload tools, and best practices.\n5. How to download data using Mapillary's web interface and API.\n\nSince Mapillary launched in 2013, over 2 billion images have been contributed from places as far afield as Antarctica and Zimbabwe. Images can be uploaded from any device that creates geotagged images, from affordable smartphones to commercial grade 360° cameras. \n\nEvery image is processed with computer vision to recreate the world in 3D and extract features that are useful for map making. These capabilities have attracted all sorts of map builders including advocates for pedestrian safety, humanitarian agencies, state and local transportation departments, OpenStreetMap contributors, ridesharing companies and more.\n\nIn this talk we’ll recap Mapillary for those that are less familiar, sharing some more recent case studies to help crystalize the utility of street-level imagery. We’ll then cover some of the platform changes of 2024, including the launch of the revised mobile apps (2.0). This leads into our latest recommendations for how to capture and upload street-level imagery effectively. We’ll conclude with a look at how you can download map features using the web interface and Python tools.",
        "description": ""
      },
      {
        "title": "Humanitarian response through collaborative data and Opensource tools in Rio Grande Do Sul",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "HOT's humanitarian program brings together different types of actions that leverage the OpenStreetMap database and its ecosystem of tools to provide local actors, including organizations and governments, with data required in the immediacy of crisis response to natural disasters and other humanitarian situations.  Data creation aims to be timely through a rapid but accurate assessment with local sectors of their most pressing needs for immediate response, intermediate response and then recovery, a phase that can last more than a year after a disaster.  It is a comprehensive process co-designed with local stakeholders and collaborative with all local, regional and global mapping communities interested in this humanitarian issue invited to participate. It includes a certain type of standardized phases, but revisited and prioritized according to needs with high care to people's vulnerability and to the privacy of certain information. \nThis talk will exemplify the support that HOT is giving in the state of Rio Grande do Sul in Brazil and in particular in the city of Porto Alegre since July 2024. \nIn this area of the country, the city government in particular has been helped to have a faster recovery of its educational and health social services, an indirect but concrete way to improve the welfare of the population, with geographic data and open technologies.",
        "description": ""
      },
      {
        "title": "Applying a Human Rights-Based Approach to Open-Source Geospatial and Remote Sensing Data: Enhancing Inclusivity and Accountability in Sustainable Development",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The integration of a Human Rights-Based Approach to Data (HRBAD) with open-source geospatial and remote sensing data presents a powerful tool for achieving the Sustainable Development Goals (SDGs) while ensuring no one is left behind. This talk/presentation will explore how HRBAD principles can be applied to open geospatial data, focusing on use cases and applications that enhance our understanding of human rights issues and support evidence-based policy-making.\nI will introduce the concept of HRBAD, emphasizing its six key principles: participation, data disaggregation, self-identification, transparency, privacy, and accountability. I'll then try to demonstrate how these principles align with and can be implemented in open-source geospatial and remote sensing data initiatives.\n\nThe presentation will showcase several key use cases and applications:\n\nParticipatory Mapping for Inclusive Development: Examples of how platforms like OpenStreetMap enable marginalized communities to contribute to data collection, improving accuracy and empowering advocacy efforts.\nDisaggregated Geospatial Analysis for Inequality Assessment: Demonstrating how fine-grained spatial analysis using open satellite imagery can reveal patterns of inequality, supporting the HRBAD principle of data disaggregation.\nTransparent Earth Observation for Environmental Justice: exploring the applications of open satellite data for monitoring environmental changes and holding actors accountable for degradation affecting marginalized communities.\nCapacity Building through Open Geospatial Education: exploring initiatives that use open data and tools to educate and empower communities in using geospatial technologies for advocacy.\n\nI will discuss how these applications relate to HRBAD principles and consider some of the challenges in using open geospatial data for human rights applications. This includes touching on issues of data quality and accessibility.\n\nThe presentation will suggest ways to incorporate HRBAD principles into open geospatial data initiatives, offering practical considerations for data collection, analysis, and dissemination that align with human rights principles.\n\nThrough these examples, I hope to illustrate the potential of combining open-source geospatial data with a human rights-based approach, and how this could contribute to more inclusive and effective sustainable development efforts.\n\nThis presentation aims to interest professionals from various fields, including geospatial science, human rights, development, policy, and data science. It hopes to encourage further dialogue on the use of open geospatial data in human rights and sustainable development contexts and on applying HRBA to open-source geospatial data.",
        "description": ""
      },
      {
        "title": "How open-source GIS drives better results for children",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "UNICEF is leveraging geospatial technologies in order to geo-enable UNICEF’s data, systems and processes to transform data into easily accessible, readily available and actionable geospatial information. Such information is essential to address key questions, such as: “How many children are affected by climate change?”, “Where children have limited access to schools AND limited access to health services?” to support evidence-based advocacy and decision-making for better results for children.\nUNICEF has adopted a hybrid geospatial architecture, benefiting from both commercial and open-source GIS solutions. Open-source approach allows us to have more flexibility and enables cost-effective scalability of corporate GIS systems, making them available to all UNICEF users.\nGeoSight is an open-source web geospatial data platform developed by UNICEF for easy data visualization and analysis. It is specifically designed to simplify the creation of online maps for visualizing multiple indicators at a subnational level to support evidence-based decision-making. Using GeoSight, UNICEF users and partners can easily overlay multiple indicators representing various thematic areas, such as natural hazards, climate-related risks, but also conflicts, health, education, poverty and other socio-economic indicators.\nGeoSight is developed using Django backend and React at a frontend. It has a robust backend interface where users can manage indicator data, basemaps, contextual layers, styles as well as create new projects (dashboards) for publishing data. A dashboard is the main GeoSight product for end-users to interact with the data. It is consists of an interactive map (developed using MapLibre) with multiple indicator layers representing various statistics, typically at national or subnational levels. Indicator layers can be queried and analyzed at different administrative levels (e.g. province or district) at a specific date and time. Additionally, users can cross-query multiple indicators using filters. The map may also contain contextual layers (which can be any point, line, polygon or raster layers) as well as custom basemaps. \nGeoSight has a robust API that allows for system-to-system integrations. This is a powerful feature, which is used for creating automated data pipelines that feed in data from multiple sources and make them available for all users.\nGeoSight is a self-service platform that equips UNICEF users at all country offices with an easy to use and powerful geospatial analytical system. The platform has been already used to support UNICEF response in many emergency contexts, including Ukraine and Gaza. It is also used as a dissemination tool for global and regional initiative such as Child Climate Risk Index-Disaster Risk Model (CCRI-DRM), WASH Insecurity Analysis (WIA) and many others.\n“UNICEF has over 70-year history of innovating for children and believes that new approaches, partnerships, and technologies that support the realization of children’s rights are critical to improving their lives. Early on, UNICEF established guiding principles for innovation and technology in development, which influences the Principles for Digital Development. One of these – Use Open Standards, Open Data, Open Source, and Open Innovation – explicitly advocates for the licensing of open source software to enable greater impact in international development and cooperation. This Principle has guided UNICEF’s approach in creating, investing in, and supporting innovations” (https://www.unicef.org/innovation/dpg-pathfinding-countries). Following this principle, UNICEF has made GeoSight source code publicly available under the terms of an AGPL-3.0 license.\nOpen source approach is also one of the key elements of the Frontier Data Network - UNICEF’s strategic initiative designed to promote innovation and capacity building in data science, helping us make data-driven decisions that truly benefit children. Open source solutions are part of how we offer the underlying capabilities that are necessary to actually enable the production and provision of solutions.\nWe believe that GeoSight can benefit many organizations, both private, NGOs and public, not only in the humanitarian and development sectors. We would like to encourage organizations and individual developers to contribute to GeoSight project and help us and our partners leverage open-source geospatial technology to support the lives of children around the world.",
        "description": ""
      },
      {
        "title": "Generative AI in your FOS applications",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "The integration of Artificial Intelligence (AI) into various operational and strategic sectors marks a significant shift in the way data is managed and analyzed. In the geospatial context as in many others, usage of generative AI radically changes the way the user can interact with the platform, bringing new use cases and decoupling the value that we can get out of the data catalogs.\n\nIn this presentation, we will quickly cover the generative AI principles (Natural Language Processing, Large Language Models, etc…) and what it can bring to the geospatial ecosystem in terms of usages. \n\nWe will then dive into more specific examples, explain how it works and how we can set them up into existing applications\n- Enrich data readability and findability from unstructured data\n- Improve search with Semantic or Hybride search\n- Bring natural language conversation for easy interactions within the platforms\n- Analyze data for a better understanding and outcome\n\nThese examples and demonstrations are based on Free and Open Source geospatial solutions and rely on OGC standards. \n\nHopefully, the presentation will give your a new perspective about what generative AI can bring to your data and your catalogs, with great insights and concrete applications.",
        "description": ""
      },
      {
        "title": "Work with Re:Earth Visualizer, a new WebGL application based on Cesium",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Visualizer, developed by the Eukarya team, is a next-generation WebGIS tool that has already been deployed across multiple projects, including the PLATEAU initiative, and is utilized in various industries such as education, disaster prevention, transportation, and architecture. In this presentation, I will introduce the primary features of Visualizer and the considerations behind its design. Key topics include:\n\n- Executing Complex Operations with a Simplified UI\n- How we streamlined Visualizer’s user interface to enable complex operations with ease.\n- New Layer System\n- The approach to abstracting geometry in the New Layer System to support diverse GIS formats and the efficient use of style code to manage layer styles.\n- Sketch Layer\n- How the Sketch Layer allows users to freely draw and store 2D and 3D shapes within layers.\n- Scroll-Based Storytelling System\n- Leveraging a scroll-based page design to create engaging story-driven projects.\n- Expanding Functionality through Plugins\n- How to extend the system dynamically through plugins, allowing real-time interaction with layers and the 3D globe.\n- Future Plans and Goals for Visualizer\n- An overview of Visualizer’s public roadmap and our ambitions for future development.\n\nThrough this presentation, we hope to give you a comprehensive understanding of our application and encourage you to join our community to share your valuable feedback.",
        "description": ""
      },
      {
        "title": "Unifying Standards for Water Data Exchange: Leveraging OGC API - EDR and pygeoapi",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "The effective management and utilization of water data are critical for both community engagement and scientific analysis. To address the challenges associated with disparate data formats and standards, this presentation explores the use of unifying standards such as WaterML2.0 and OGC API Environmental Data Retrieval (EDR). These standards offer a cohesive approach to data exchange and interoperability, bridging gaps between diverse use cases and technical requirements.\n\nWater data is highly diverse—spanning vector geospatial features, time series, raster data, and combinations thereof. The Internet of Water (IoW) Coalition advocates for data equity and interoperability, emphasizing the adoption of standardized formats and web services to drive both data science and equitable decision making. Using pygeoapi as a testbed, this session will focus on two primary use cases: community engagement through web applications and modeling & analysis for water resource management.\n\nUse Case 1: Web App Development for Community Engagement and Education\nFor public-facing applications, ease of use and accessibility are paramount. Here, JSON-based formats like GeoJSON and CoverageJSON are evaluated for their effectiveness in representing water data. The session will demonstrate how CoverageJSON provides a streamlined approach to handling time series and geospatial data, aligning with web development needs and enhancing user engagement through interactive, map-oriented interfaces.\n\nUse Case 2: Scientific Analysis for Water Resource Management\nScientific platforms require comprehensive data exchange and metadata standards to support robust analysis. WaterML2.0, with its detailed metadata and extensive schema, is well-suited for this purpose. However, its XML serialization poses challenges for modern software development workflows and analytical tools. We discuss potential solutions, including a proposed JSON serialization of WaterML2.0 and its integration with OGC API - EDR for data queries and access. \n\nWe will address the trade-offs between these standards, considering factors such as data complexity, metadata requirements, and web compatibility. Recommendations will be made for future development, including enhancements to existing standards and the creation of best practice specifications for effective water data exchange.",
        "description": ""
      },
      {
        "title": "Plugin LFTools - A \"Made in Brazil\" Geospatial Solution!",
        "type": "Sponsor",
        "track": "Open source geospatial ‘Made in Latin America’",
        "abstract": "The presentation \"Plugin LFTools: A 'Made in Brazil' Geospatial Solution!\" at FOSS4G 2024 aims to showcase the innovative tools of the LFTools plugin, emphasizing its significance in the context of geospatial technologies and its substantial impact on various fields. During the presentation, newly developed tools of LFTools will be revealed, further enhancing the capabilities of QGIS. These tools include advanced solutions for surveying, land regularization, engineering, and the environment, enabling the automation of complex processes and greater accuracy in results.\n\nPromoting LFTools at an international event like FOSS4G is important for showcasing the excellence and innovation of a free and open-source Brazilian project, encouraging collaboration and knowledge sharing within the global GIS community, and highlighting the advantages of LFTools and QGIS to promote the adoption of accessible and powerful open-source tools.\n\nLFTools offers practical solutions for various fields, including topography with tools for creating topographic maps, analyzing GNSS data, and generating descriptive reports; land regularization by automating cadastre and geoprocessing processes; engineering by providing support for civil, environmental, and other engineering projects; and the environment through spatial analysis and environmental data processing, facilitating resource management and monitoring.\n\nThis presentation will not only showcase the new tools of LFTools but also emphasize the importance of promoting innovative Brazilian solutions at an international event. The initiative reinforces the commitment to developing accessible and high-quality geospatial technologies, fostering collaboration, and advancing open science.\n\nLearn more about the LFTools plugin for QGIS: https://geoone.com.br/lftools-o-plugin-para-topografia-no-qgis/",
        "description": ""
      },
      {
        "title": "Wildfire Surveillance and Tracking in Protected Areas of Pará: A Serverless Solution with Open Data and Software",
        "type": "Talk",
        "track": "Applications and solutions for the Amazon region",
        "abstract": "The serverless solution for wildfire monitoring in protected areas of Pará, hosted on AWS, addresses two main challenges. It provides high-level managers with a dashboard that offers a comprehensive view of affected areas. For firefighting brigades, it delivers alerts via WhatsApp and a webGIS to assist with planning and executing firefighting efforts. The solution employs technologies based on GDAL and PostGIS, integrating data from INPE, NASA, GFS, IBGE, and ICMBIO for efficient, near-real-time analysis.",
        "description": ""
      },
      {
        "title": "DSGTools Geospatial Data Quality Assurance Toolbox: An Automated Workflow Suite for Quality Assurance",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Since 2013, the Brazilian Army Geographic Service has been committed to migrating its geospatial production into Free and Open Source Software (FOSS). DSGTools, a QGIS plugin released in 2015, is a result of this effort. Currently in version 4.14.0, DSGTools’ features have been used to produce massive amounts of spatial data and have been consolidated as the Brazilian Army’s official geospatial production suite.\n\nDSGTools offers features such as database creation according to the Brazilian cartographic legislation, layer loading with resolved domains and easy WMS service access from BDGEx, the Brazilian Army Geographic Service SDI. In addition, DSGTools offers a wide range of extraction tools such as the right angle digitalization tool, the free hand digitalization tool, the generic selection tool, the raster selection tool, the feature inspection tool, the DSGTools Processing Algorithm Provider and the Geospatial Data Quality Assurance Toolbox (QA Toolbox), which is one of the standout features of DSGTools.\n\nThe QA Toolbox runs processes called workflows, which consists of a series of interconnected sequential tasks based on QGIS models executed in a predefined sequence. For each step in this sequence, the QA Toolbox executes a geospatial data processing task, ranging from data cleaning and identification of data inconsistencies.\n\nMoreover, the primary objective of the QA Toolbox is to automate the identification and correction of geospatial data issues introduced during the data extraction process. The execution of a workflow is carried out sequentially, stopping only in specific cases, depending on the configuration chosen by the user. A workflow can have its execution halted when spatial inconsistencies called flags are produced after a model is executed. If a flag is raised during the execution of a workflow, the process halts immediately, prompting users to address the identified issue before proceeding. The QA Toolbox also prevents users from forcing the execution of followup tasks without fixing the flags raised in the current step. By forcing the users to correct the errors pointed out before continuing the process,  the propagation of unhandled inconsistencies is prevented.\n\nThe DSGTools Geospatial Data Quality Assurance Toolbox usage can reduce the required time and effort invested on geospatial data production. Users can also create complex workflows that operate independently. The use of a predefined sequence of models ensures that each step in the data processing is performed consistently, following standardized procedures like the Brazilian Standards of Geospatial Data Production set by the National Infrastructure of Spatial Data (INDE). This consistency is crucial for maintaining high-quality geospatial datasets, especially in large-scale projects. The flagging mechanism is a key component of the Brazilian Army Geographic Service’s production line, ensuring that errors are promptly addressed and preventing the accumulation of issues that could compromise the overall quality of the dataset.\n\nAdditionally, the execution status of the DSGTools Geospatial Data Quality Assurance Toolbox can be saved within the user’s project, allowing the QA process to be carried out over multiple days. Since DSGTools workflows are built using QGIS models, they harness the full power of the QGIS processing toolbox and various plugins, including the 159 processes available in the DSGTools Processing Algorithm Provider.\n\nIn this talk, we will showcase all the DSGTools Geospatial Data Quality Assurance Toolbox features and highlight its usage in real-world use cases of geospatial data production. DSGTools is available at the QGIS Plugin Repository, and its code is hosted on GitHub at https://github.com/dsgoficial/DsgTools.",
        "description": ""
      },
      {
        "title": "Serving live maps with vector tiles",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Vector tiles are an efficient way to serve maps with a high volume of live data. IoT applications or maps with weather, traffic or other live data require a compact format for transmitting updated data. Vector tiles with their powerful styling capabilities are supported by several Javascript map viewers.\n\nThis talks shows how to disply live data with MapLibre and OpenLayers. On the server side [BBOX](https://www.bbox.earth/) is used to serve tiles from a PostGIS database.",
        "description": ""
      },
      {
        "title": "State of PDAL",
        "type": "Talk",
        "track": "State of software",
        "abstract": "PDAL is Point Data Abstraction Library. It is a C/C++ open source library and applications for translating and processing point cloud data. It is not limited to LiDAR data, although the focus and impetus for many of the tools in the library have their origins in LiDAR. PDAL allows you to compose operations on point clouds into pipelines of stages. These pipelines can be written in a declarative JSON syntax or constructed using the available API. This talk will focus on the current state of the PDAL Pointcloud processing library and related projects such as COPC and Entwine, for pointcloud processing. Coverage of the most common filters, readers and writers along with some general introduction on the library, coverage of processing models, language bindings and command line based batch processing. First part will be covering new features for current users. Some discussion of installation method including Docker, binaries from package repositories, and Conda packaging. For more info see https://pdal.io",
        "description": ""
      },
      {
        "title": "Planscape - optimize landscape interventions",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The State of California has a recurring issue of devastating wildfires that spread out of control, causing significant damage to the wellbeing of its citizens, its environment and economy.\n\nDue to this recurring issue, the State of California and the USDA—United States Forest Service committed to applying treatments to one million acres of land (roughly 400,000 hectares), with the goal of reducing the occurrence and severity of these wildfires. Planscape was created as a tool to prioritize and guide this one million-acre effort.\n\nPlanscape is a freely available tool built to help landscape planners, public agents and the general public prioritize and learn about landscape interventions that can minimize fire risk and cost while maximizing ecological benefits. \n\nPlanscape uses linear optimization algorithms and over 200 datasets produced by the California Wildfire & Forest Resilience Task Force to determine the priority locations for applying landscape treatments such as mastication, prescribed burns, and others.\n\nThis talk aims to outline why and how Planscape is being built by Spatial Informatics Group, with an in-depth description of the technologies in use, software architecture, data handling (pre-processing, usage, post-processing), and an overview of our current goals and challenges.\n\nUltimately, we will demonstrate how government agencies can leverage Planscape to implement similar optimizations in your community, county, state, or country.",
        "description": ""
      },
      {
        "title": "Gleo Feature Frenzy",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Gleo is a nascent javascript WebGL mapping library. It aims to find a niche alongside Leaflet, OpenLayers, MapLibre and Deck.gl.\n\nThis library was presented at FOSS4G 2022. The \"feature frenzy\" highlights all of the features developed during the last year with live examples, including its extensible OOP paradigm, (re)projection support, symbols for XYM geometries, clustering, colour spaces, and vector field handling.",
        "description": ""
      },
      {
        "title": "OpenLayers Feature Frenzy",
        "type": "Talk",
        "track": "State of software",
        "abstract": "18 years ago, OpenLayers was the first alternative to Google Maps, with the ability to display layers from open data sources. Today, Leaflet, Mapbox GL JS and MapLibre can do the same. OpenLayers has found its niche as a full-featured, flexible, and high-performance geospatial JavaScript library that users can count on for the long haul, especially when their mapping needs get more complex.\n\nThis talk will provide you with a tour of the latest features in the library, including daring live demonstrations. We will present our recent and ongoing work on adding new features and making the library more fun to work with.\n\nWhether you're a developer or decision maker, come to this talk to learn about the current status of OpenLayers. We’ll provide you with a glimpse into the future of the library and leave you motivated to get mapping with OpenLayers.",
        "description": ""
      },
      {
        "title": "FOSS Applications in the Amazon through the GeoRondônia Project with the GeoINCRA Plugin in QGIS",
        "type": "Talk",
        "track": "Applications and solutions for the Amazon region",
        "abstract": "Introduction:\n\nFOSS4G (Free and Open Source Software for Geospatial) 2024 will be a platform to present innovative and accessible solutions that use free and open-source software for geospatial information management. In this proposal, we highlight the importance of using FOSS to reduce costs, automate processes, and simplify operations in QGIS, with a special focus on applications for large and environmentally sensitive areas such as the Amazon. Our presentation will detail the implementation of the GeoRondônia project and the use of the GeoINCRA plugin in QGIS, emphasizing its benefits in land regularization and environmental sustainability.\n\nGeoRondônia Project - Land Regularization and Environmental Sustainability:\n\nThe GeoRondônia project is a partnership between the Federal Institute of Education, Science, and Technology of Rondônia (IFRO) and the National Institute for Colonization and Agrarian Reform (INCRA), through the Decentralized Execution Term (TED) No. 20/2021/DF/SEDE/INCRA-IFRO. With a duration of 48 months, the project aims to title rural properties in 120 Settlement Projects (PAs), covering a perimeter of 31,000 km and benefiting more than 20,000 families in 26 municipalities. The actions include georeferencing, perimeter demarcation, parceling, occupational supervision, and the Rural Environmental Registry (CAR). With an investment of approximately 23 million reais, GeoRondônia has already benefited about 15,000 families, standing out as a model of efficiency and economy.\n\nImportance of FOSS in Geospatial Projects:\n\nRondônia has 222 Settlement Projects, with approximately 66,000 rural properties, and a large part is being served by the GeoRondônia project. However, the biggest obstacle is data processing, which involves large volumes and complexity. Therefore, the project sought ways to optimize processes and speed up the achievement of goals within established deadlines, finding in QGIS+GeoINCRA the tools needed for this objective.\nAdopting free and open-source software (FOSS) is essential to reduce costs and increase the accessibility of geospatial technologies. Software like QGIS offers a robust and flexible platform for implementing georeferencing and land registration projects, eliminating the need for expensive licenses and allowing the customization of tools according to the specific needs of the projects. With the savings generated in the GeoRondônia project using QGIS and the GeoINCRA plugin, it was possible to direct resources and optimize other processes in the project. Additionally, the solution created using these applications can be replicated to all Brazilian states.\n\nGeoINCRA Plugin: Simplifying Georeferencing in QGIS:\n\nThe GeoINCRA plugin was developed to optimize the georeferencing process of rural properties according to INCRA's technical standards. Implemented in Python, the plugin integrates with QGIS's processing framework, offering functionalities that automate data querying, attribute filling, and document generation required for land certification. The main tools of the plugin include:\nLoad Vertex Layer - Loads selected features from a point layer into the vertices layer of the GeoINCRA database; Download ODS Spreadsheet from SIGEF - Generates an empty ODS spreadsheet for later filling; Query INCRA Database - Connects to INCRA's database to query land assets and generate vector layers; CSV from INCRA to PointZ Layer - Transforms CSV vertex files from INCRA into PointZ layers; GeoINCRA to TopoGeo - Copies features from the GeoINCRA database layers to the TopoGeo database, facilitating the generation of descriptive memorials and topographic maps; Generate TXT for ODS Spreadsheet - Creates a text file with data needed to fill the SIGEF ODS spreadsheet; Fill Vertex Code - Automatically fills the vertex code attribute in the GeoINCRA database's vertex layer, easing the work of the georeferencing professional.\n\nMethodology and Results:\n\nThe adopted methodology includes the modeling of geospatial data in a Geopackage database, the implementation of a plugin in QGIS, and the integration of these elements to optimize the georeferencing workflow. The GeoINCRA database was designed to store topographic data in a standardized and integrated manner, while the GeoINCRA plugin automates processes that traditionally would be manual and prone to errors.\nThe results achieved with the use of QGIS+GeoINCRA are: Automation of ODS spreadsheets that were previously done manually; Reduction of the time to prepare the spreadsheets by 80%, as the spreadsheets are generated in bulk, i.e., hundreds of plots can be generated at once; Elimination of topological errors that were only seen when trying to insert the data into SIGEF (INCRA's georeferencing platform); Elimination of errors in textual attributes (spelling errors); Savings for the project, without the need to purchase licenses.\nThe results obtained demonstrate that the combined use of the GeoINCRA database and the GeoINCRA plugin results in greater productivity, better data standardization, and elimination of software license costs. This approach facilitates access for small companies and professionals to the georeferencing market, aligning with federal government policies on the use of FOSS and promoting independence and public resource savings.\n\nWho Benefits from the GeoINCRA+QGIS Solution:\n\nThe presentation at FOSS4G 2024 will highlight how the use of FOSS, exemplified by QGIS and the GeoINCRA plugin, can transform georeferencing projects for a diverse range of stakeholders. This includes government agencies, environmental organizations, and small surveying companies, particularly in vast and environmentally critical areas like the Amazon. By reducing costs, automating processes, and ensuring high-quality results, these solutions significantly contribute to land regularization, environmental sustainability, and socio-economic development in the region. This approach ensures that even resource-constrained entities can efficiently manage geospatial data and meet regulatory requirements.",
        "description": ""
      },
      {
        "title": "Farewell Web Mercator",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Most of today's web maps are using the [Web Mercator projection](https://en.wikipedia.org/wiki/Web_Mercator_projection). A major issue of Web Mercator is the distortion of area sizes far from the equator.\n\nIn 2018 Bojan Šavrič, Tom Patterson and Bernhard Jenny published their work on the [Equal Earth map projection](https://www.equal-earth.com/), an equal-area projection for world maps.\n\nThis talk shows how to use the Equal Earth map projection for web mapping with different kind of data sources.\n\nA growing collection of information about using Equal Earth is available at [equal.bbox.earth](https://equal.bbox.earth/).",
        "description": ""
      },
      {
        "title": "State of fAIr: Free and Open Source AI-assisted Mapping for Humanitarian",
        "type": "Talk",
        "track": "State of software",
        "abstract": "fAIr is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that aims to improve the efficiency and accuracy of mapping efforts for humanitarian purposes. The service uses AI models, specifically computer vision techniques, to detect objects such as buildings, roads, waterways, and trees from satellite and UAV imagery. However currently focused on buildings only . \n\nThe name fAIr is derived from the following terms:\n\n    f: for freedom and free and open-source software\n    AI: for Artificial Intelligence\n    r: for resilience and our responsibility for our communities and the role we play within humanitarian mapping\n\nIn this talk we will talk about the recent developments in fAIr , Our experiment with Yolo model and RAMP model for community mapping and couple of test results . fAIr recently made public version deployment : https://fair.hotosm.org/",
        "description": ""
      },
      {
        "title": "Topology for Spatial Distribution Networks",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The proposal consists of the presentation of the benefits that can be obtained by the implementation of a Topology for Spatial Distribution Networks. The concepts of Topology, the steps necessary for its implementation and the benefits for an organization are presented. Finally, the project is presented where, from a Plugin developed in Python in QGIS, the implemented resources and the results obtained are demonstrated. The full content of the Project can be viewed in these two articles:\n\n•\thttps://www.linkedin.com/pulse/topologia-de-rede-espacializada-rodrigo-paschoal-do-valle-wx7hf/?trackingId=gNxyrwDYR1ahkTMHJEN%2FzA%3D%3D\n\n•\thttps://www.linkedin.com/pulse/topologia-rede-de-saneamento-rodrigo-paschoal-do-valle-uxblf/?trackingId=1UOrKCOaQcmffR9BqAnqcQ%3D%3D\n\nInitially, the concepts and steps necessary for the elaboration of Topology will be presented in a simple and objective way. A Water Distribution Network (Sanitation) will be used as an example, as its elements are easy to see. Three main elements will be addressed:\n\n•\tSegments: representing the distribution network.\n•\tValves: representing the point at which the flow can be interrupted.\n•\tReservoir: representing the starting point of the network (start of supply).\n\nFor each element, its characteristics (SYNTAX) and behaviors (SEMANTICS) will be presented. The analysis focuses on simplifying the model, meeting what is necessary for the development of the resources that will be explored in Topology.\n\nAfter the contextualization of the Topology, some challenges faced during its implementation will be presented. Emphasizing the eventual necessary adjustment in the registered elements, as examples:\n\n•\tUnconnected segments: adequacy of the initial and final vertices to establish the connection.\n•\tValves superimposed on segments: division of the segment and insertion of valve between them.\n•\tUnconnected reservoirs: adequacy of the segment vertex to establish the connection.\n\nThe benefits obtained and the ability to increase the maturity of an organization are addressed, as static records are transformed into active elements, enabling maneuvers, validations and behavior analysis. Thus, we have the development of Network Intelligence. These and other benefits will be addressed in preparation for the presentation of the results developed in the Project.\n\nThe project had two stages: In the first, a Natural Gas Distribution Network was used on which the Network Isolation mechanism was implemented; in the second, a Water Distribution Network was used and new resources were implemented: Network Analysis, Network Status and Opening and Closing of Valves. The presentation will focus on the advantages obtained by the implemented resources, demonstrating the characteristics and applications of each one.\n\nFor the presentation of the implemented resources, images and videos will be used, as well as made in the published articles.\n\n<u>Step 1: Contextualization of the Gas Distribution case</u>\n•\t<b>Network Isolation:</b> need to isolate the section in an Emergency situation (Leak)\nPresentation of the results and concepts involved: Selected Segment, Isolated Section, Affected Section and Valves to be closed.\n\nEmphasis also on future benefits related to isolation:\n•\tIdentification of Customers Affected in the maneuvers performed.\n•\tIdentification of Critical Regions in which a one-off isolation affects a vast region.\n•\tAnalysis of existing resources in a Graph structure: (1) identification of critical paths, (2) analysis of points without supply redundancy, (3) automatic assignment of the status of segments from the manipulation of valves.\n•\tAssistance in Supply Continuity Strategies, identifying Critical Customers (such as Hospitals) that are in vulnerable regions.\n\n<u>Step 2: Contextualization of the Sanitation case and implemented resources</u>\n•\t<b>Network Analysis: </b>an analysis is made on the imported elements to identify inconsistencies, thus validating the model. The following situations were analyzed: (1) Valves connected to only one Segment, (2) Valves connected to more than two Segments, (3) Segments isolated from the Network, (4) Segments not connected to Reservoirs, and (5) Isolated Valves and Reservoirs. This analysis can be expanded, according to the elements and rules used by Topology, and can also help in estimating the effort required in adjusting records.\n\n•\t<b>Network Status: </b>from the connections between Reservoirs, Segments and Registers (open and closed) the points of the Network that are Supplied and Not Supplied are presented. With this functionality, the status of a certain stretch is automatically defined, thus not requiring the intervention of an operator to update the Network. The feature has the same behavior for Gas Networks, respecting the elements of Topology.\n\n•\t<b>Opening and Closing of Valves: </b>after analyzing the Network, it is possible to select the Valves and open/close them, automatically updating the network. This feature is especially important for situations where we need to analyze the impact on the Network of maneuvers performed. This feature can be used in conjunction with Network Isolation to study the behavior of the distribution fabric.\n\n<b>Next steps</b>\n•\tInclude the list of Affected Customers in the resources already developed.\n•\tExplore opportunities for new resources, some with room to be used by any Distribution Network, others that make sense only for certain contexts;\n•\tUse of the model (BDGD) – Geographic Database of the Distributor, required by ANEEL (National Electric Energy Agency) for the delivery of data from the concessionaires, including an Electric Energy Distribution Network to the project;\n•\tProspect any use case related to Telecommunications Networks.\n\n<b>Conclusion</b>\nIn this second example, the Water Distribution Concessionaire urgently needed to resupply Hospitals in a certain region of Porto Alegre. As alternatives were implemented, a number of houses and buildings also had their supply normalized. In this situation, only critical points should be supplied, directing the scarce resource only to emergency points. Network Topology enables this maneuver and the creation of strategies for network segmentation.",
        "description": ""
      },
      {
        "title": "Overture Maps: Unleashing the Power of Open Data for Interoperable Solutions in a Connected World",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "In an era where geographic data underpins critical decision-making across various sectors, the need for open, interoperable data systems has never been greater. This presentation explores the transformative potential of open data through the lens of Overture Maps Foundation. \n\nOverture Maps Foundation, established under The Linux Foundation in 2022, is developing enterprise grade, open map data as a shared resource. This fast-growing open data community enables companies, government agencies, and other entities to collaboratively build and maintain high-quality foundational map data in a more efficient and cost-effective way. With these base data available for use under open licensing, organizations can then focus on building their business-specific services and applications. Overture Maps' datasets act like a shared freeway, enabling innovation across sectors like local search, logistics, disaster recovery, environmental analysis, augmented reality, and automotive.\n\nThis initiative emphasizes interoperability, ensuring that data can be seamlessly integrated and utilized across different platforms and applications. Both interoperability and conflation are vital in collaborative environments where multiple organizations contribute data to ensure that data can be shared and accessed effectively as a unified resource. By leveraging open data and technologies, Overture Maps enables developers, researchers, and policymakers to build advanced geospatial solutions without the constraints of proprietary data silos. \n\nJoin this discussion to learn about:\n\n- Overture and its genesis. Mapping the entire world — with every community, street, building, home and attractions (even as they change) — is challenging for any single organization to tackle, and the cost to build and sustain that data continues to grow as the demand for better data increases. \n\n- Overture’s project scope. Overture is collaboratively building six global, open foundational data layers that can link to a near-infinite catalog of spatial data through a stable ID system. \n\n- Overture’s Global Entity Reference System (GERS). Data conflation and enabling data interoperability are the biggest obstacles in realizing the full value of the abundant open map data resources available today. Conflation in particular is a challenge given spatial and attribute ambiguities and discrepancies across datasets. With GERS, datasets built by different organizations can reference the same real-world map features in a simple, unambiguous way. \n\nThis session will delve into the foundation's core principles, including the use of open-source tools, community-driven data contributions, and the implementation of best practices for data quality and governance. Attendees will gain insights into the technical architecture of Overture Maps, including its approach to data interoperability, and learn how to contribute to and benefit from this growing ecosystem. Come join us and learn about creating and using a single, coordinated, base map for the world!",
        "description": ""
      },
      {
        "title": "All MapLibre projects, present and future, in one status update",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Present everything MapLibre community has been working on, including tile serving, fonts and sprite handling, to visualizations for both web and native, to new types of tools and format standards.",
        "description": ""
      },
      {
        "title": "Use of Open Source Software in the ESA Planetary Science Archive",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The European Space Agency (ESA) has adopted a variety of open-source software tools to manage, visualize, and distribute planetary data, with a particular emphasis on Mars. These tools are essential for both internal operations and for providing crucial data access to the global scientific community. Below, we detail the use of these technologies, collaboration on open-source projects, and the underlying GIS architecture developed by the Planetary Science Archive (PSA). [Link](https://psa.esa.int/psa)\n\n## Tools Used\n\n1. **OpenLayers**:\n   - **Functionality**: A JavaScript library for creating interactive maps in web browsers.\n   - **Application**: Used to build web user interfaces that allow scientists to visualize geospatial data of Mars and other planets, offering an intuitive and accessible platform for the exploration and analysis of planetary data.\n\n2. **GeoServer**:\n   - **Functionality**: An open-source map server that enables the sharing and editing of geospatial data.\n   - **Application**: Used to serve spatial data via standard protocols like WMS (Web Map Service). This facilitates the visualization of footprints with different base maps.\n\n3. **Three.js**:\n   - **Functionality**: A JavaScript library for creating 3D graphics in web browsers.\n   - **Application**: It is employed to generate three-dimensional visualizations of the Rosetta comet.\n\n4. **PostgreSQL and PostGIS**:\n   - **Functionality**: PostgreSQL is an open-source relational database management system, and PostGIS is an extension that adds support for geographic objects.\n   - **Application**: Are used to store and manage complex geospatial data. PostGIS allows for advanced spatial queries, facilitating the analysis of large volumes of geospatial data and its integration with other GIS tools like GeoServer.\n\n## Collaborative Projects and Data Access\n\n1. **Astroquery**:\n   - **Description**: A Python library that facilitates access to online astronomical databases.\n   - **Collaboration**: ESA contributes to Astroquery to ensure that planetary data is easily accessible to researchers. This includes data from planetary exploration missions and astronomical observations, integrating these data into scientific analyses efficiently.\n\n2. **Antimeridian**:\n   - **Description**: Tool for processing spatial data crossing the antimeridian (the 180° line of longitude)..\n   - **Collaboration**: Open Source project, and the PSA plans to collaborate with the project by contributing code. This tool is crucial for planetary data where coordinates can be extended beyond the traditional range of 0° to 180° longitude, allowing for continuous and accurate representation of planetary maps..\n\n## New Interface and GIS Architecture\n\nESA has developed a new interface for the Planetary Science Archive, integrating the aforementioned tools into a cohesive and user-friendly platform. This interface allows scientists to:\n- **Explore Interactive Data**: Navigate through interactive maps of Mars, Phobos and other planets, applying filters and visualizing different layers of geospatial data. Users can overlay geological, topographical, and spectral data layers to gain a more comprehensive view of the terrain and use the different functionalities, such as changing the projection (polar, equirectangular), extracting information by region of interest.\n- **3D Visualization**: Thanks to Three.js, users can explore the the 67P(Churyumov-Gerasimenko) comet in 3D for the Rosetta mission, rotate, and zoom into features for more detailed analysis. Ultimately, we use Three.js to represent irregular bodies such as comets, asteroids, and asteroids.\n- **Real-Time Data Access**: Researchers can access the latest information and perform real-time queries to obtain specific data according to their needs.\n- **Data Download**: Scientists can download datasets directly from the interface for use in their own analyses and studies, selecting and downloading specific subsets of data based on defined search criteria.\n\n## GIS Architecture\n\nThe GIS architecture behind this new interface relies on a robust combination of open-source technologies:\n- **GeoServer Base Maps**: Acts as the distributor of base maps of Mars, Phobos, Cassis. They are cached using GWC to optimize access in all available projections.\n- **Frontend with OpenLayers and Three.js**: Provides 2D and 3D visualization capabilities, offering a rich and interactive user experience. OpenLayers is used for 2D interactive map visualization, while Three.js is employed to generate three-dimensional visualizations of planetary surfaces.\n- **Database with PostgreSQL and PostGIS**: Used to store and manage complex geospatial data. PostgreSQL and PostGIS enable advanced spatial queries, facilitating the analysis of large volumes of geospatial data and its integration with other GIS tools.\n- **Integration with Data Access Tools**: Projects like Astroquery and Antimeridian are integrated to facilitate the access and manipulation of specific data, solving complex issues like the management of data crossing the antimeridian. This integration allows scientists to access and analyze planetary data more efficiently and accurately.\n\n## Benefits for the Scientific Community\n\nThe use of advanced technologies and a robust GIS architecture developed by ESA offers several significant benefits for planetary research:\n- **Open and Transparent Access**: Although the code is not public, ESA uses open-source tools that ensure data and resources are available to the entire scientific community. This promotes collaboration and knowledge sharing, allowing researchers to access information without restrictions and work together more efficiently. Another benefit for the scientific community is to be able to cross different instruments/missions in a single interface, e.g., give me all the CaSSIS and HRSC data of this particular crater. For more information about ESA projects, you can visit their [GitHub repository](https://github.com/esa).\n- **Solutions to Specific Problems**: Tools like Antimeridian [Antimeridian GitHub](https://github.com/gadomski/antimeridian) address unique technical challenges, ensuring precise and continuous representation of planetary data. This facilitates the analysis and interpretation of geospatial data, ensuring that visualizations and maps are accurate and reliable.\n\n## Conclusion\n\nThe adoption of open-source software and the development of an advanced GIS architecture enable ESA to offer a powerful and accessible platform for planetary research. This benefits not only its own scientists but also the global scientific community, promoting knowledge sharing and collaboration in the exploration of the Solar System. Tools such as OpenLayers, GeoServer, Three.js, PostgreSQL, and PostGIS, along with collaborative projects like Astroquery and Antimeridian, are fundamental for the efficient management and precise visualization of planetary data.\n\nWith all this, the summary of the talk is to show how free software is used in the PSA for planetary data and more specifically in Mars data.",
        "description": ""
      },
      {
        "title": "Unleashing the Power of OpenStreetMap Data in QGIS with Essential Plugins",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "There are multiple ways to download OpenStreetMap (OSM) data, this talk delves into the realm of OSM plugins for QGIS, empowering participants to leverage this valuable free and open source geospatial data in their projects. We'll explore core plugins like QuickMapServices and QuickOSM, guiding participants through the process of adding basemaps and downloading specific features using Overpass API queries. In addition, we'll discover the functionalities of third-party plugins like OSMDownloader, which enable users to download data by area or with custom queries. \nWe will look at some use cases within the humanitarian sector where OSM plugins can be used to download data for disaster response, mapping vulnerable communities, and monitoring refugee camps. We will also look at some use cases for climate change actions where these plugins can be used to download forest cover for mapping and monitoring deforestation by tracking changes over time, assessing climate risk by downloading data e.g. elevation, land cover, proximity to water body which can be used to assess vulnerability to the impact of climate change such as floods, Sea level rise etc.\nThis talk will be beneficial to both novice and advanced QGIS users, it will equip participants with the tools to harness the potential of open source data using free and open source software in their geospatial workflows.",
        "description": ""
      },
      {
        "title": "Swedish National Regional analysis - a general applikation for Community planning.",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The 'Regional Analysis' application is a robust tool for decision-making, offering seamless access to the expansive Pipos (Pinpoint Sweden) database. Built with open-source components, it features a straightforward and user-friendly web interface. The Pipos database contains 600,000 geographical tiles, each measuring 250 meters, and blankets the whole of Sweden. These tiles are detailed with socioeconomic and accessibility attributes. The application aids users in the efficient analysis and presentation of this data. With a goal to engage 10,000 users involved in community planning across various public sector levels, ease of use and integrated communication functions are crucial. The presentation will start with a short introduction and proceed to showcase the application's features.",
        "description": ""
      },
      {
        "title": "osmlanduseR: An R package for the analysis of landuse data contributed to OpenStreetMap",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Over the past 30 years, Argentina has experienced changes in its agricultural and urban development model that have drastically altered land use practices and patterns. (Palmisano, 2018; Pintos and Narodowski, 2012)\n\nThese changes have been associated with flooding in several regions (Pal et al., 2021; Pattison and Lane, 2012). In particular, the Lujan River basin has historically experienced extreme events.\n\nIn the global scale, prevails the expansion of capitalism by a process of accumulation by dispossession,  characterized by land privatization, expulsion of farmers, conversion or suppression of rights to the commons (Harvey, 2004).\n\nThe analysis of these changes in landuse requires information typically obtained from remote sensing, that is validated and complemented with sampling programs. The development of an updated landuse map is a basic tool to study territorial phenomena such as flooding events or land use change. Such a methodology would allow the information to be compared with previously collected data at different scales and time periods.\n\nBut geographic information, its format, or processing tools do not generally allow for its reuse or improvement, and are not necessarily openly/freely available. This type of data can be considered a digital commons and can be the subject of mercantilization processes. In fact, publicly produced information and processing tools should be publicly available to enforce knowledge construction. (Arsanjani, 2015; Duféal and Noucher 2017).\n\nOpenStreetMap is the main framework for volunteered geographic information, and because it constitutes a standardized database, it also allows to be a preferred repository for contributions originating from research programs of universities and public organizations.. Recently has been registered as a public good by an agency affiliated to the United Nations (https://blog.openstreetmap.org/2024/02/05/osm-named-as-a-digital-public-good-by-un-affiliated-agency/)\n\nData contributed to OSM has already been used to create and validate land use and land cover maps in various regions (Arsanjani et al., 2013; Shultz et al., 2017).\n\nIn Argentina,  the local community of OSM users has added a significant amount of geographic information that could be used for land use analysis, which could be further expanded, especially in non-urban areas.\nSince 2016, land use data in the middle basin of the Lujan river have been added to OSM as part of projects conducted by the National University of Lujan. Land use was visually assessed using satellite imagery and representative polygons were added with appropriate tags. Geometries were added preferably as multipolygons. Boundaries were drawn to avoid sharing nodes with the road and rail network.\n\nThe aim of this work is to present the development of an R package for the analysis of landuse data contributed to OSM. Subsequently, the goal is to increase the contribution of publicly generated information and its analysis tools in an open access format, such as those provided by OSM and R (R Core Team, 2023).\n\nThe package can be installed from its github repository https://github.com/aduhour/osmlanduseR.\n\nThe package is in an early stage of development and the features included are aimed at 1) download a set of land use related data from OSM using the overpass API. 2) Remove overlaps and measure polygon area. 3) Classify polygons by mapping OSM tags to user-defined classes that can be assimilated to Corine Land Cover classes or the FAO Land Cover Classification System (Schultz et al., 2017; Volante 2009) 4) Create a land use classification map.\n\n\nReferences\n\nArsanjani, A. J.; Helbich, M.; Bakillah, M.; Hagenauer, J. & Zipf, A. 2013.Toward mapping land-use patterns from volunteered geographic information. International Journal of Geographical Information Science.\n\nArsanjani, J. J.; Zipf, 2015. A.; Mooney, P. & Helbich, M. (Eds.) OpenStreetMap in GIScience\nSpringer. \n\nDuféal, M. and Noucher, M. 2017. Des TIC au TOC. Contribuer à OpenStreetMap: entre commun numérique et utopie cartographique. Communs urbains et équipements numériques, 31 \n\nHarvey, D., 2004. The ‘new’ imperialism: Accumulation by dispossession. Socialist Register 40,\n63–87\n\nPal, S., Dominguez, F., Bollatti, P., Oncley, S. P., Yang, Y., Alvarez, J., and Garcia, C. M. (2021). Investigating the effects of land use change on subsurface, surface, and atmospheric branches of the hydrologic cycle fin central argentina. Water Resources Research, 57(11)\n\nPalmisano, T., 2018. Tierras de alguien. Teseo. URL: https://www.teseopress.com/tierrasdealguien.\n\nPattison, I. and Lane, S. N. (2012). The link between land-use management and fluvial flood risk: a chaotic conception? Progress in Physical Geography, 36(1):72–92.\n\nPintos, P. and Narodowski, P. (Eds.), 2012. La privatopía sacrílega. Efectos del urbanismo privado en humedales de la cuenca baja del río Luján. 1era ed., Imago Mundi.\n\nR Core Team. 2023 R: A Language and Environment for Statistical Computing. R Foundation for  Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.\n\nSchultz, M.; Vossa, J.; Auera, M.; Carterb, S. and Zipf, A. 2017. Open land cover from OpenStreetMap and remote sensing. International Journal of Applied Earth Observationd and Geoinformation.\n\nVolante, J. N. 2009. Monitoreo de la Cobertura y el Uso del Suelo a partir de sensores remotos. Instituto Nacional de Tecnología Agropecuaria, Instituto Nacional de Tecnología Agropecuaria.",
        "description": ""
      },
      {
        "title": "Streamlining GIS Workflows: Developing a Collaborative QGIS Plugin Repository",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "A QGIS plugin repository functions as a centralized repository of tools designed to automate and simplify the daily tasks of GIS users. It serves as a hub where company employees and collaborators can access, share, and utilize these tools. This initiative has the additional benefit of enhancing productivity while also cultivating a culture of collaboration and innovation within the organization. In any organization that places a significant reliance on GIS, the efficiency and accuracy of geospatial data processing are of paramount importance. Geographic information system (GIS) teams encounter a number of challenges, including the necessity to perform repetitive tasks, to complete time-consuming data processing steps, and to develop custom tools that are not readily available in standard GIS software. These issues underscored the necessity for a solution that could automate repetitive tasks, provide custom tools tailored to specific needs, and facilitate easy sharing and collaboration among team members. As an open-source GIS platform, QGIS offers a robust plugin architecture that allows users to extend its functionality. \nTo develop QGIS plugins, a development environment was established, comprising Python (the primary language for QGIS plugin development), QGIS itself, and the requisite libraries, including PyQt for GUI development, pandas, and requests. A version control system based on the Git distributed revision control system was implemented to facilitate the effective management of the codebase. With the development environment prepared, the coding of the plugins commenced. Each plugin was designed to address a specific task or workflow and they are organized by projects. For example, the data standardization plugin was developed to meet the requirements of Resolução ANM n° 142/2023, the automatic caves CAD file download was created through the use of a database PostGRESQL/PostGIS and an AWS connection, and a Drainage tool was developed to delineate and estimate flow given a pour point of interest and elevation data. The initial step in developing the plugin repository involved understanding the specific needs of the team. A series of meetings and surveys were conducted to gather requirements and identify the most pressing issues. Several team members were involved in the testing process, gathering feedback and making necessary improvements. This iterative process helped refine the plugins and make them robust.\nThe plugin repository was organized in a manner that was both clear and intuitive. Each plugin was organized in a directory, which contained the source code, documentation, and example datasets. This structure facilitated user navigation of the repository, enabling them to readily identify the tools they required. The Bitbucket platform was selected for hosting the repository due to its widespread use and intuitive interface, which provides a collaborative environment where team members can access the plugins, report issues, suggest enhancements, and contribute to the development process.\nTo integrate the repository with QGIS, a custom plugin server was incorporated into the QGIS software. The server enabled users to peruse the available plugins, install them with a single click, and receive updates automatically, thus ensuring that users could readily access and utilize the tools without leaving the QGIS environment. To optimize the adoption and efficacy of the plugins, training sessions and workshops were conducted for colleagues. These sessions encompassed the installation and utilization of the plugins, best practices for geospatial data processing, and strategies for integrating the tools into daily workflows.\nThe QGIS plugin repository has had a profound impact on the organization, delivering key benefits such as increased efficiency, enhanced accuracy, enhanced collaboration and knowledge sharing, and enhanced scalability. The automation of repetitive tasks has resulted in a notable reduction in the time and effort required for data processing, thereby enabling team members to direct their attention to tasks of greater critical importance. The implementation of bespoke analytical tools has enhanced the precision and dependability of geospatial analyses. The repository has fostered a collaborative environment in which team members can share their tools and expertise, thereby facilitating continuous improvement and innovation. The modular structure of the repository allows for the straightforward incorporation of new plugins, thereby ensuring that emerging needs and challenges can be addressed.\nConsidering the repository's success, several prospective developments have been proposed. These include the expansion of the plugin library through the development of new plugins to address additional tasks and workflows, the incorporation of advanced geospatial analytics such as machine learning and spatial statistics to further enhance capabilities, the linking of the QGIS plugins with other enterprise systems such as databases and web services to create a more integrated and efficient geospatial data infrastructure, and the encouragement of participation from the broader FOSS4G community through the sharing of plugins and contributions to the global repository of geospatial tools and solutions.",
        "description": ""
      },
      {
        "title": "G3W-SUITE: a framework for publishing and managing QGIS projects as WebGIS services",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "G3W-SUITE is a modular, client-server application based on QGIS-Server and Django used for managing and publishing interactive QGIS cartographic projects of various kinds in a independent, simple and fast way.\nThe suite has an administration component that is used to setup WebGIS services privileges, editing functions and so on.\nThe suite is made up of two main components: G3W-ADMIN (based on Django and Python) as the web administration interface and G3W-CLIENT (based on OpenLayer and Vue) as the cartographic client that communicate through a series of API REST.\nThe application, released on GitHub with Mozilla Public Licence 2.0, is compatible with QGIS LTR versions and it is based on strong integration with the QGIS API.\n\nThis presentation will present how the G3W-suite works, what it can do and some practical examples.\n\nThe talk, accompanied by examples of application of the features, is dedicated to both developers and users of various levels who want to manage their cartographic infrastructure based on QGIS.",
        "description": ""
      },
      {
        "title": "ZOO-Project: news about the Open Source Generic Processing Engine",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The ZOO-Project is an open-source processing platform released under the MIT/X11 Licence. It provides the polyglot ZOO-Kernel, a server implementation of the Web Processing Service (WPS) (1.0.0 and 2.0.0), and the OGC API - Processes standards published by the OGC. It contains ZOO-Services, a minimal set of ready-to-use services that can be used as a base to create more useful services. It provides the ZOO-API, initially only available from the JavaScript service implementation, which exposes ZOO-Kernel variables and functions to the language used to implement the service. It contains the ZOO-Client, a JavaScript API that can be used from a client application to interact with a WPS server.",
        "description": ""
      },
      {
        "title": "Visualizing Overture Maps Data with Lonboard in a Jupyter Notebook",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Visualizing and analyzing large-scale geospatial datasets, such as Overture Maps' comprehensive global dataset containing more than 3 billion features grows more challenging as these datasets continue to expand in size and scale. This presentation introduces Lonboard, a cutting-edge open-source Python library designed to address this challenge by enabling fast, interactive geospatial vector data visualization within Jupyter notebooks.\n\nLonboard's exceptional performance and ease of use stem from its innovative architecture, built on four key technologies: deck.gl for GPU-accelerated rendering, GeoArrow for efficient in-memory representation, GeoParquet for optimized file storage, and anywidget for seamless Jupyter integration. This powerful combination allows Lonboard to move data from Python to JavaScript and then to the GPU with unprecedented efficiency.\n\nUnlike existing solutions that rely on slower GeoJSON encoding, Lonboard employs a fully binary pipeline. It leverages GeoPandas as the primary user interface, internally managing conversions to GeoArrow and GeoParquet for efficient data transport and rendering. This approach not only accelerates data processing but also significantly reduces the data size transferred to the browser.\n\nWe will demonstrate Lonboard's capabilities using Overture Maps data, which is provided in GeoParquet format as monthly releases. This showcase will highlight how Lonboard's simple interface allows researchers and data scientists to effortlessly visualize and interact with cloud-native, optimized geospatial data at a global scale.",
        "description": ""
      },
      {
        "title": "Scaling up OpenStreetMap data validation in the open",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Every day, almost 2.5 million edits are made in OpenStreetMap. To maintain the high quality and reliability of OSM data, keeping track of changes is crucial. In 2024, a new OSM data pipeline was built for [OSMCha](https://osmcha.org/), one of the main OpenStreetMap data validation tools. This pipeline is fully open-source, and it allows us to visualize and run data quality checks on each edit made on OSM. Besides the set of open-source tools and the Kubernetes deploy infrastructure, the resulting data is available for free under the AWS Open Data program. We’ll share how this new pipeline streamlines data integrity and enables developers to build downstream cloud-native applications to monitor the changes happening in OpenStreetMap. We will also demo Gradient, a web application that displays OSM edits using this new real-time OSM pipeline.",
        "description": ""
      },
      {
        "title": "Enhancing Geospatial Data Processing with Python: A Case Study using IBGE data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In today's fast-paced, data-driven environment, organizations that use geospatial data for analysis are challenged by managing complex datasets and frequent updates. Geospatial data provides valuable context and information, enhancing applications in various domains such as logistics, urban planning, environmental monitoring, and marketing.\nTraditionally, many organizations have relied on no-code geospatial software with click-based interfaces. Due to their accessibility and user-friendly interface, these tools allow team members to visualize and manipulate geospatial data without the need for programming knowledge. However, as data starts to become more complex, these tools often present scalability limitations, restricting the full potential of geospatial data applications.\nThis paper explores the benefits of transitioning to a hybrid approach by integrating Python and open-source geospatial libraries into the data processing phase of geospatial analysis. By presenting the possible advantages gained and providing a hands-on example of Python use in geospatial data, the aim of this paper is to demonstrate how Python can play a pivotal role in overcoming the limitations of no-code solutions.\nPython can enhance the data extraction phase, enabling integration with various data sources and APIs and connecting to external databases and web services. This capability supports consistent data exchange and real-time data integration. This phase can also be automated, summarizing all steps into a script that can be applied to every new dataset.\nThe processed data can be visualized using various libraries in a Python environment or used as input for traditional geospatial software. The hybrid approach leverages the user-friendly visualization tools of no-code software while enabling more sophisticated data processing capabilities.\nThe output data can also be used as input for developing custom algorithms, including the integration of machine learning models and artificial intelligence. This step enables a wide range of applications, such as urban feature prediction, classification or segmentation of remote sensing data, and clustering of spatial data.\nAdopting a hybrid approach significantly enhances an organization's analytical capabilities. These advanced analyses provide deeper insights into spatial patterns and trends that manual methods alone may not reveal.\nThe hands-on example, based on census data from the Brazilian Institute of Geography and Statistics (IBGE), demonstrates geospatial data processing with Python and the GeoPandas library, both open-source solutions. This example will include the use of Python in geospatial data processing through the following steps: data extraction, data processing, and customized algorithm application.\nIn conclusion, integrating Python into these workflows enhances flexibility and analytical capabilities, allowing organizations to innovate in their solutions and create new opportunities for products and services. Coding elevates data-driven decision-making and enables more sophisticated and scalable analyses, particularly when dealing with large and complex datasets. This case study serves as an inspiring example for organizations and researchers aiming to maximize the potential of their geospatial data, highlighting the significant benefits of combining traditional geospatial software with powerful open-source tools.\n\nThis work received financial support from the State of São Paulo Research Foundation (FAPESP) (grant 2023/15663-7, 2024/05553-2, 2024/05727-0, 2024/05481-1).",
        "description": ""
      },
      {
        "title": "UNICEF, FAO, ONU, OSGEO, FOSS4G Meeting [GUESTS ONLY]",
        "type": "Side event",
        "track": "Community & Foundation",
        "abstract": "Private Meeting.\nMembers from UNICEF, FAO, ONU, OSGEO, and FOSS4G",
        "description": ""
      },
      {
        "title": "Tethys Platform: An Open-Source Geoscience Web Application Framework",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "Join us for an insightful exploration of the Tethys Platform, a feature-rich tool for developing and deploying web applications for the Earth sciences. Tethys Platform addresses technical barriers by offering a curated suite of free and open-source software (FOSS) accessible through the Tethys SDK - developers can access powerful tools like OpenLayers, CesiumJS, Bokeh, and Plotly for visualization, and GeoServer, THREDDS, and PostGIS for hosting geospatial and gridded datasets. We will explore the versatility of open-source web GIS and how Tethys Platform enables effective communication of scientific information to enhance the decision-making processes. In collaboration with leading regional organizations worldwide, we will showcase several Tethys portals and web applications that utilize satellite data and geospatial technologies effectively for managing water resources, climate risks, and land use. Further, Tethys promotes interoperability and collaborative decision making while meeting diverse organizational requirements. Our talk aims to provide a holistic understanding of how Tethys Platform uses open-source GIS and web applications to significantly augment decision-making capabilities, stimulate innovation, improve data interoperability, and foster collaboration among GIS professionals, researchers, and policymakers. Additionally, we will discuss the challenges of portability of web applications between different organizations' web servers - government organizations often require hosting and managing web applications on their own servers due to political, branding, and security motivations, rather than relying on third-party websites. Our approach ensures that organizations can leverage advanced environmental analysis tools while maintaining sovereignty over their web applications' deployment and management. Tethys Platform’s many capabilities have garnered significant international interest, evidenced by diverse applications developed by users worldwide.",
        "description": ""
      },
      {
        "title": "Project PLATEAU from Japan: Tackling Local Issues with Nationwide 3D City Model",
        "type": "Lighting talk",
        "track": "Use cases & applications",
        "abstract": "PLATEAU is a digital twin initiative led by Japan's Ministry of Land, Infrastructure, Transport and Tourism (MLIT) that collaborates with various stakeholders to develop, utilize, and open “3D city models” across the country. Launched in fiscal 2020, this digital twin data of cities was created for around 200 cities across Japan by fiscal 2023 and continues to expand coverage. We have developed over 100 use cases in various areas, such as urban planning, disaster prevention, and environmental management. PLATEAU has been working to develop an ecosystem to promote open innovation in urban areas. The datasets are openly available in various formats. In addition, we release a wide range of knowledge, including guides, technical material, and source code for our website.",
        "description": ""
      },
      {
        "title": "From the collaborative to the common good : a journey towards open mapping",
        "type": "Keynote",
        "track": "Community & Foundation",
        "abstract": "In this talk, I will colloquially go through the path of my person through values that, between discoveries, projects, initiatives, themes, and very broad fields of applications, have led me to settle in the universe of OpenStreetMap for the collective good, particularly in the humanitarian field and sustainable development. \nThis talk aims to express to the general public, to the technical public, but in particular, to the young public, an itinerary that shows a broad panorama of the possibilities of contribution from all kinds of actions, specialties, and knowledge located from humility, and the immense benefits of this trend.",
        "description": ""
      },
      {
        "title": "Cartografía 4.0: Gemelos Digitales Urbanos",
        "type": "Keynote",
        "track": "Use cases & applications",
        "abstract": "La presentación será en español y exploraremos las posibilidades que ofrece el software de código abierto para la creación de un gemelo digital utilizando la información disponible. También abordaremos el desarrollo de motores de inteligencia artificial que permitan generar datos para predecir comportamientos, destacando casos de aplicación en Latinoamérica.",
        "description": ""
      },
      {
        "title": "Script for database: SQL or Python",
        "type": "Keynote",
        "track": "Use cases & applications",
        "abstract": "The purpose of this work is to show the benefits and difficulties of using scripts, whether in Python or SQL.\nThe work is the result of the author's experience in developing sophisticated Python scripts to build a thematic model for the heat spot data from the “BD Queimada program” of Brazil's National Institute for Space Research.\nThe thematic model consists of generating tables in the database (Postgres) that represent the themes with the heat spot data. The themes are represented by area of interest, such as Conservation Units, Indigenous Lands, Settlements, Biomes, ...\nThe aim of the work was to import the heat spot data and generate aggregated tables with themes in an automated way, making it easy to add new themes.\nAfter the script went into production, it was necessary to add another theme, and although I made an effort in the script to make it easy to add a new theme, things didn't work out the way I wanted them to.\nBy writing the script in SQL, and putting it into production, the author can observe the details of when we should use Python or SQL to automate the thematic model with hotspots.\nThe script is being used to help monitor forest fires in the main areas of operation of The National Center to Prevent and Combat Forest fires.",
        "description": ""
      },
      {
        "title": "FOSS4G Developer Stories",
        "type": "Keynote",
        "track": "Community & Foundation",
        "abstract": "Working on geospatial software is amazing - a real understand and change the world. Working on free and open source geospatial software is even better - putting that power in the hands of so many people is inspiring and amazing.\n\t\n\tDrawing on experience from foss4g conferences, open source projects, multiple software foundations, and some great employers - I would like to share a bit of what I have learned.\n\n\tThis keynote will provide a little bit of amusement, some fun photos, entertaining challenges that have been overcome, and ideas for what is next.",
        "description": ""
      },
      {
        "title": "WMO and FOSS4G: a new horizon of open geospatial tools for weather/climate/water data",
        "type": "Keynote",
        "track": "Transition to FOSS4G",
        "abstract": "Open-source software has become increasingly crucial for the World Meteorological Organization (WMO) and its Members, particularly those in developing countries, Least Developed Countries (LDCs), and Small Island Developing States (SIDS).  The potential of open source can enable climate action for adaptation, while also addressing the challenges and opportunities presented by the current landscape of open-source development within the WMO.\n\nOpen-source solutions also play a pivotal role as accelerators for the implementation of Early Warnings for All, a key UN initiative in which WMO is playing a critical role, aimed at protecting every person on Earth with life-saving early warning systems by 2027. By providing accessible, customizable, and cost-effective tools, open-source software enables WMO Members to rapidly deploy and adapt early warning systems to their specific needs and contexts. This approach is particularly crucial for developing countries, LDCs and SIDS, where resource constraints often hinder the implementation of proprietary solutions.\n\nMoreover, open-source initiatives serve as powerful catalysts in supporting WMO Members' efforts towards digital transformation. As National Meteorological and Hydrological Services (NMHSs) worldwide strive to modernize their operations and services, open-source tools offer a flexible and scalable foundation for innovation. They enable Members to leverage cutting-edge technologies, collaborate on development, and share best practices, thus accelerating their digital transformation journeys while optimizing resource utilization.\n\nFor many years, WMO has been actively involved in developing and supporting open-source software as a low-barrier, low-cost solution for its Members. These efforts, combined with comprehensive training and mentoring activities, have been met with enthusiasm and success.\n\nThis presentation will provide an overview of Open Source at WMO, its significant use, current status and future plans for increased development and use of Open Source software to help lower the implementation barrier to data exchange of weather/climate/water/environmental data.\n\nJoin us as we explore how WMO is transforming global weather, water, and climate data sharing, and discover how this initiative, along with FOSS4G tools, is fostering collaboration, innovation, and societal benefits on a global scale, with a particular focus on supporting Climate Action and Early Warning for All.",
        "description": ""
      },
      {
        "title": "CLOSING GENERAL SESSIONS",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "- Closing speech and acknowledgments\n- Statistics of participation and collaborations\n- OSGeo community and projects.\n- Sol Katz Award Ceremony\n- Sponsors\n- Invitation to the State of the Map.\n- Torch passing to FOSS4G 2025 hosts.",
        "description": ""
      },
      {
        "title": "AGM OSGEO",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "Meeting AGM OSGEO at FOSS4G Belém",
        "description": ""
      },
      {
        "title": "FOSS4G & Indigenous Art: Showcasing Marajoara Symbols in an Interactive Map of Belém",
        "type": "Talk",
        "track": "Education",
        "abstract": "Ever since the year 25,000 B.C.E, when the first known map was carved into a Mammoth tusk, map makers have relied on geometric symbols derived from nature in order to represent a geography. However, even though modern digital cartography brings about endless possibilities for map customization and visualization, the art of utilizing geometric symbols in maps has been all but forgotten in the era of modern map design.\n\nIn order to resurrect, appreciate, and preserve the art form of geometric patterns derived from nature, and to incorporate them in a nuanced form within modern digital cartography, we present Mapajoara: a self-contained free and open source interactive map of Belém integrating indigenous Marajoara iconography into the underlying map interface, using ancient drawing patterns to represent different geographic and urban elements.\n\nEach Marajoara pattern appearing on the map has been studied and carefully selected to symbolize rivers, forests, urbanized areas, and other significant aspects of Belém, offering a visually distinct and culturally enriching experience to users. Our project not only highlights the beauty and importance of Marajoara art, but also promotes interaction and learning about the geography and culture of the region through a modern and accessible technological platform.",
        "description": ""
      },
      {
        "title": "pygeometa project status",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pygeometa provides a lightweight and Pythonic approach for users to easily create geospatial metadata in standards-based formats using simple configuration files (affectionately called metadata control files [MCF]). Leveraging the simple but powerful YAML format, pygeometa can generate metadata in numerous standards. Users can also create their own custom metadata formats which can be plugged into pygeometa for custom metadata format output.\n\nFor developers, pygeometa provides a Pythonic API that allows developers to tightly couple metadata generation within their systems and integrate nicely into metadata production pipelines.\n\nThe project supports various metadata formats out of the box including ISO 19115, the WMO Core Metadata Profile, and the WIGOS Metadata Standard.\n\npygeometa has minimal dependencies (install is less than 50 kB), and provides a flexible extension mechanism leveraging the Jinja2 templating system.\n\nThis presentation will provide an update on recent enhancements, use in high profile projects as well as future plans and roadmap.",
        "description": ""
      },
      {
        "title": "Rendering OGC API Compliant Vector Tiles on the Fly with pygeoapi + Elasticsearch",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "Tiled maps are well-known for their performance and have been present in web map applications for more than twenty years. Vector tiles combine all the benefits of map tiling with the ability to access attributes, enabling client side attribute-based rendering. This makes them one of the most efficient ways of visualising vector data, and they are present in many interactive web maps that we see on the web today.\n\nHowever the proliferation of web map applications has often resulted in a lack of interoperability between tile servers and clients. This was the motivation for the OGC API - Tiles Standard (https://tiles.developer.ogc.org/), published in late 2022. The core of this Standard is very simple, adding some formality to what people have been doing for years with XYZ tilesets, while specifying some metadata elements that help clients do a better job at creating maps (e.g.: title, description, zoom levels, custom projection). \n\nIn this talk we will present a software stack to render OGC compliant vector tiles. This stack includes pygeoapi (https://pygeoapi.io/), an OSGeo project and a Reference Implementation for OGC API - Tiles (https://www.ogc.org/resources/product-details/?pid=1663). The architecture of pygeoapi supports backend plugins, which use different software for storing and accessing geospatial data. For the purpose of creating vector tiles, we will present the MVT-elastic plugin (https://github.com/geopython/pygeoapi/blob/master/pygeoapi/provider/mvt_elastic.py), which leverages the Elasticsearch capability of rendering vector tiles on the fly, from geospatial data stored in an Elasticsearch index. Elasticsearch (https://github.com/elastic/elasticsearch) is a distributed, RESTful search and analytics engine. Recently, this plugin also became capable of exposing the attributes associated with the data, enabling client side styling of attributes. These capabilities can be demonstrated by creating a Leaflet map that consumes and styles the pygeoapi+elastic vector tiles (https://emotional-cities.github.io/vtiles-example/demo-oat.htm). \n\nWe hope that this presentation can make the creation of fast, expressive and interoperable maps, accessible to anyone.",
        "description": ""
      },
      {
        "title": "Wagtail CMS + pygeoapi = Modern SDI for the current needs",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The current state of the art of Spatial Data Infrastructures (SDIs) is limited to solutions that are already becoming stagnant in terms of their approach and technology.\n\nToday, we need more robust and modern SDIs, beyond the logic of flat data tables with a spatial component, with data access that is either all open or all closed, lacking granularity.\nWagtail is originally used as a content management system, I propose a modern SDI where data management can be granular, with access roles and version control.\n\nThanks to the pygeoapi library, this application also allows the publication of modern REST web services, according to OGC API specifications.\n\nThe premise of this proposal is to have a platform with extensive features, but also with current and modern standards, and as simple as possible, without the need to integrate multiple platforms and synchronization agents, etc.\n\nBasically, this is a simple Django, but enhanced for the management and publication of spatial data with a modern and scalable approach.\n\nWagtail is well known for content management but its use for spatial data management has not been explored until now. On the other hand, pygeoapi is an agnostic library but primarily intended only for publication, not for data management, even less for data that changes day by day.\n\nFor ending this proposal precisely resolves both the granular and versioned management of spatial data as well as their publication according to the new OGC (OGC API) standards.",
        "description": ""
      },
      {
        "title": "Leveraging Geospatial Street Data for Effective Urban Mobility Policies: A Comprehensive Methodology for Road Safety Analysis in Brazilian Cities Through Geoprocessing",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Cities can be characterized as an extremely complex and dynamic environment, which integrates multiple interdependent factors and presents a significant challenge in the development and monitoring of public policies. To deal with this complexity and ensure truly effective policies, data-driven decision-making is essential. Cities produce a large volume of data that is crucial for informing these policies. However, many cities face significant challenges in collecting and analyzing quality data due to a lack of technical and human resources. Furthermore, while tabular data is important, it often fails to capture the complex contextual layers present in the urban territory. Geospatial information provides a deeper and more contextualized understanding of the urban context, enriching the decision-making process and promoting more effective public policies.\nWithin the complexity of city management, safe and sustainable mobility increasingly stands out as an area demanding special attention, primarily due to the challenge of addressing the issue of premature road traffic deaths and injuries. The World Health Organization (WHO) indicates that more than 1.19 million people die in traffic worldwide every year — in Brazil, there are over 30,000 annual victims.\nThe Safe System and Vision Zero Approach, which advocate that no death or serious injury in traffic is acceptable, illustrate the complexity of urban mobility. The concept involves several areas of action that must be worked on in an integrated manner, including safe road infrastructure and urban design. Therefore, understanding all layers of the territory is essential for identifying risk areas, planning effective interventions, and monitoring results, ensuring that mobility contributes to building a safer urban environment.\nThe complexity of cities and the need to deeply understand the various aspects that comprise them make territorial analysis an indispensable component in the data and evidence-based decision-making process. By collecting and using data to identify critical areas, for example, it is possible to effectively propose road safety actions and public policies aligned with the Safe Systems approach. For this reason, the Cordial Institute sought to create a methodology for interpreting the territory to facilitate road safety analyses and offer valuable data-based information. This way, it is possible to directly assist municipalities in developing public policies that promote safer mobility in Brazilian cities.\nThis methodology, named ‘Structurals’, assumes that the road system is not uniform but divided between intersections and mid-blocks (structurals), each having different interactions in the urban environment. Road intersections, for example, are areas of many encounters between different road users, which can sometimes lead to conflicting situations. These conflicts can result in traffic incidents and, therefore, this dynamic deserves significant attention. On the other hand, mid-blocks have different behaviors, such as increased vehicle acceleration or pedestrians crossing outside designated areas, so it is also important in analyses in a complementary way to intersections’.\nThe processing of structurals requires a few spatial databases that are commonly developed by municipalities: road blocks, central median/divider (if the road block base does not contain this information), and road axis. With this, it is possible to generate a tool with significant analytical impact using easily processed data from municipalities.\nTo georeference this dynamic, it is necessary to trace the road axis to identify where they meet (intersections) and where they are continuous (mid-blocks). Using PostGIS, the geospatial data extension of PostgreSQL, both free and open-source, a geospatial processing is performed where perpendicular lines to the roadblock polygons are drawn, meeting at an equivalent geometric distance point. Lines are drawn every meter along the roadblock face, resulting in several points located at the road's central axis. These points are connected by lines forming the road axes. When they intersect, they are identified as a road intersection.\nFrom the geoprocessing, intersection areas are drawn in an open-source Geographic Information System (GIS) program, QGis. These areas are ‘buffers’ created from the points generated by the geometric operations, and have their design adapted if they intersect, not being limited only to the geometric point created by the axis crossing — it is not a simple buffer around each crossing but is dissolved to adapt to the road's morphology. This process identifies a primary characteristic of the road system intersections that can be included in territorial analyses: the intersection profile, or how many approaches each intersection has, making it more or less complex proportional to the number of approaches.\nOnce the areas considered intersections are established, the street axes not present in this area are extracted and become mid-blocks. In other words, mid-blocks are defined after the intersections' geometric definition, as their geometric opposite.\nFrom the structurals' design, it is possible to pair with all available geographic territorial information. The more data provided by municipalities, the greater the territory's knowledge and the possibility of relating these variables. Each structural has characteristics that make more sense to be deepened. In intersections, traffic lights, bus or cycling infrastructure, road hierarchies, and pedestrian crossings can be paired. In mid-blocks, the road width, road hierarchy, speed limit, electronic surveillance presence, speed reducers, block face length, etc., can be paired.\nPairing information with structurals helps identify profiles of these spaces and create insights into road crashes in Brazilian municipalities. With this pairing, it is possible to analyze the distribution of road events between intersections or mid-blocks and their characteristics, the severity of occurrences, and identify critical points that should be prioritized in public policies. Additionally, it is an essential tool for effectiveness analyses and creating comparison groups for monitoring interventions. For example, with intersection structurals, it was possible to identify crossings with similar characteristics in São Paulo to evaluate whether those that received the \"Frente Segura\" intervention had a reduction in traffic incidents. The same was done with the \"Melhor Uso do Leito Viário (MULV)\" interventions carried out in mid-blocks of the city. These and other uses of the structurals can be seen on the publication page of the Brazilian Mobility Panel, from the Cordial Institute.",
        "description": ""
      },
      {
        "title": "Geochicas. Building communities of geofeminism.",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "In this dialogue, we take a journey through Geochicas’ eight years of impactful work within both the OpenStreetMap (OSM) and OSGeo communities. From our early days to our growth as a network of diverse voices, Geochicas has forged connections, expanded reach, and set new standards for inclusivity in open mapping and geographic information systems. Along the way, we’ve collaborated with other collectives, amplifying our impact across continents and platforms, and launched projects that bring a fresh perspective to digital mapping through the lens of equity and representation. This conversation uncovers the milestones we've reached, the partnerships we’ve cultivated, and the strategies we’re developing to build even more inclusive and supportive geospatial communities. Join us as we share stories, insights, and our vision for the future, exploring how we can continue inspiring positive change in the communities we care deeply about.",
        "description": ""
      },
      {
        "title": "OpenGeoSGB: State of the art of Transition to an Open Source, semi-automated, FAIR-ready Geological Spatial Data Infrastructure of the Geological Survey of Brazil",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "The Geological Survey of Brazil (SGB) is under a digital transformation process. One of the pillars of this process involves speed, scalability, security and availability of data produced. Furthermore, CPRM is creating a favorable environment for the adoption of cloud-based architecture.\n\nThis paper aims to present an overview of a new developed Geological Spatial Data Infrastructure for the SGB. The GeoSGB is the main source for internal mapping, infographic and dashboard applications. It will assimilate legacy services, that runs on isolated map servers, such as self hosted node of OneGeology OGC Services.\n\nThe solution adopted is GeoNode 4.2, which brings together map, data services, metadata catalog and spatial database. It is free and open source software with a very active community. In addition, GeoNode has a good content management system, a rich API, and it’s fully customizable. To meet data access demands, it was customized to run in Kubernetes-based environments and each mapped area produces its own geoservice, exportable to different formats, such as shapefile and geotiff.\n\nHowever, it became necessary a complete separation between the production and publishing environments. SGB’s production pipeline is composed by internally developed data management software. Some of these systems are being modernized, with updates on business rules, frameworks and security. GIS work is carried out in ArcGIS Enterprise®, with some exceptions in QGIS and GeoServer. With this background, it should be considered as a hybrid GIS model.\n\nAbout database structures, a process of harmonization was necessary, mainly those produced from proprietary GIS. For legacy reasons, the proprietary structures were maintained, as long as possible to export to OGC WKT or WKB. Exported geometries are analyzed for compliance with Simple Features Standard (OGC/ISO19125). The information eligible for publication were consolidated in database views and is literally replicated to GeoSGB, by script.\n\n\nThe metadata production for continuous databases is carried out semi-automatically – templated - in accordance with the mapping program. This is possible by integrating GeoNode's APIs with internal databases, delivering associated metadata and resources directly to the authors. The contact with (meta)data authors were managed by GeoNode.\n\nThe symbolization of thematic layers involved the development of interoperable libraries, based on SVG glyphs inserted in OpenType fonts (ISO/IEC 14496-22:2007), with near equal rendering among different multi-platform GIS software.\n\nData and metadata pipelines were implemented using Python scripts, with specific libraries associated with GeoNode APIs. Apache Airflow manages the entire process of extracting internal bases, quality tests, structure analysis and loading on the GeoSGB database server, including being responsible for notification activities.\n\nSo, GeoSGB now is a continuous development platform, with focus in increase quality in delivered data to customers.\n\nThe future perspectives involve the transformation itself into research line in geotechnologies and high-performance IT services. It shoud envolve plug-in development for data management, processing and visualization including use of artificial intelligence. In operational terms, adoption of OGC APIs, data internationalization and harmonization, associated with adoption of OGC specific standards, such as GeoSciML and WaterML contributes to become SGB a global supplier of geoscientific data.",
        "description": ""
      },
      {
        "title": "QGIS - Ask me anything!",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "QGIS Chairman Marco Bernasocchi and core developer  Matthias Kuhn (remote) will be available for an hour to answer any QGIS-related questions. With the two, interested parties have access to over 20 years of combined expert knowledge in the development, use and organisation of QGIS and QGIS-based products.",
        "description": ""
      },
      {
        "title": "The FOSS4G Observatory",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "The FOSS4G Observatory is the initiative to map the complex, dynamic and ever-expanding ecosystem of the free and open source software used in any geospatial related process, activity or field, from storage to processing to visualisation, from operational to scientific endeavours. The community-led initiative has come a long way since 2016 and, starting with 2023, it has received support from the European Space Agency, leading to significant improvements.  \n\nIn this talk, the authors present the long winding road to a successful community collaboration that lead to the documentation of almost 500 free and open source projects for geospatial and furthermore, the highlights will be on the outlook for the FOSS4G Observatory for the years to come: licence interoperability, standardisation compliance automatic identification, health of FOSS4G projects and more.",
        "description": ""
      },
      {
        "title": "Deploying GeoNode in Production: Lessons from Brazilian Government Agencies",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "This talk presents case studies of deploying GeoNode, an open-source geospatial content management system, in production environments within two Brazilian government agencies: the Geological Survey of Brazil (SGB) and the Brazilian Federal Police (PF). We'll explore how these agencies have successfully implemented and customized GeoNode to meet their specific needs, addressing common challenges in large-scale FOSS4G deployments.\n\nKey points we'll cover:\n\n1. SGB's approach:\n   - Developing a Helm chart for automated GeoNode 4 installation on Red Hat OpenShift\n   - Addressing security requirements like rootless execution and random UID support\n   - Implementing autoscaling for most components based on CPU and memory utilization\n   - Exploring cluster implementation of GeoServer for improved scalability\n\n2. PF's customizations:\n   - Creating a dedicated \"inteligeo-deploy\" repository for enhanced deployment features\n   - Implementing centralized configuration and logging\n   - Improving security by separating credentials and using Podman instead of Docker\n   - Integrating with internal systems and scheduling data updates\n\nWe'll discuss the challenges faced, solutions implemented, and lessons learned from both approaches. These case studies demonstrate that FOSS4G solutions like GeoNode are ready for production use in government agencies, providing flexibility, scalability, and security.\n\nBy sharing our experiences, we aim to help other organizations successfully deploy GeoNode and other FOSS4G solutions in production environments. We welcome questions and discussions on best practices for large-scale FOSS4G implementations.",
        "description": ""
      },
      {
        "title": "Convolutional Neural Network-Based Detection of Erosion Rills on Aerial Imagery Combined with Hydrological Model SMODERP Outputs",
        "type": "Talk",
        "track": "AI4EO Challenges & Opportunities",
        "abstract": "Extreme precipitation events lead to rapid surface runoff, causing sheet erosion and the formation of rills, increasing the risk of flash floods. This combination of processes pose a threat to agricultural and rural areas and the sediment-laden water can infect even the urban zones or cause damage to infrastructure. Detecting and predicting the formation of erosive rills on agricultural land is, therefore, crucial for effective land management and disaster prevention in rural areas.  \n\n  \n\nThe contribution presents a research on the utilisation of convolutional neural networks (CNN) to detect enhanced erosion using remote sensing data combined with the SMODERP hydrological and erosion model.   \n\n  \n\nWhile most tools for semantic segmentation (such as random forests) work only with single-pixel values, CNNs consider also the relationship with its surroundings and between the bands. As the erosion rill patterns are visible especially when compared to the surrounding soil, it is a valuable feature for their detection.   \n\n  \n\nHowever, if we also have the digital elevation model, we can use geospatial tools and algorithms to enhance the imagery input to the neural networks with knowledge-based indices. In this case, it is the SMODERP model.  \n\n  \n\nSMODERP is a hydrological model designed to simulate surface runoff and erosion processes. It considers various factors such as soil type, land cover, slope, and rainfall intensity to predict the movement of water and sediments throughout the landscape. The model calculates the critical height of sheet runoff as a rill formation threshold, which is essential to understand where erosion is likely to occur. The SMODERP is developed as a GIS tool, available through GRASS GIS and QGIS. More details about model on smoderp.fsv.cvut.cz or on GitHub.  \n\n  \n\nThe methodology begins with data collection and preparation, utilising high-resolution orthophoto aerial images of spatial resolutions of only a few centimetres. Additionally, hydrological data from the SMODERP model are incorporated to the CNN's input to capture erosion dynamics. The talk will discuss the effect of the SMODERP's output inclusion on the CNN's accuracy in rill detection.",
        "description": ""
      },
      {
        "title": "Open source tech for fast vector webmaps: Brazilian farms use case",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "How do you show a big Vector dataset on a map on the web? Not showing it at all! Just show small parts at once with a little help from open source geospatial tech. \nWhile image basemap tiles and pyramids system is well know, Vector Tile Layers and API's still among the bleeding edge technologies that allows to show big vector datasets on the web efficiently. On this talk we will discuss how to move from a slow to a fast vector map rendering on the web. \nLast but not least: a word on how to run it at home with spare parts you might have helping you go online while avoid high cloud computing costs.",
        "description": ""
      },
      {
        "title": "OSGeo and OGC MoU in full swing!",
        "type": "Talk",
        "track": "Open Standard",
        "abstract": "Open Software and Open Standards are complementary pieces of the geospatial ecosystem. In January 2022, OSGeo and OGC signed a new and updated version of the Memorandum of Understanding (MoU) that aims to maximize the achievement of the mission and goals of both organizations. Execution of joint Code Sprints, identifying free and open source technologies that could be used as Reference Implementations for OGC Standards and validating OGC compliance tests are examples of activities that can take place within the scope of the agreement.\n\nIn the first year after the agreement was signed, we established the basilar stones for the OSGeo membership within OGC and promoted the related activities within OSGeo. Now we start to see an increasing interest from both sides and some outcomes which are important to highlight.\n\nThis presentation will provide an overview of all activities accomplished under the MoU over the last year, as well as discuss future plans. For those who have been distracted, it will reiterate the benefits of the agreement, which allows OSGeo charter members to represent the priorities of OSGeo in the development of OGC Standards and supporting documents and services.\n\nMoU: https://www.osgeo.org/wp-content/uploads/MOU_OGC_OSGeo_2022_signed.pdf\nJoint Code Sprint 2024: https://developer.ogc.org/sprints/23/\nTeamEngine: https://www.osgeo.org/projects/teamengine/",
        "description": ""
      },
      {
        "title": "Uber's Open Source H3 Index in Open Source Projects: Simplifying Distance Calculation and Data Storage",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "## Introduction\n\nThe ability to efficiently and accurately represent geographic data is a cornerstone of many modern applications, from navigation systems to environmental monitoring. Uber’s H3 index offers a transformative approach to handling spatial data, making it a valuable tool for developers and researchers alike. In this talk, we will explore how the H3 index can be utilized across a variety of open-source projects. We will delve into its advantages, such as ease of distance calculation, reliability at extreme latitudes, and the benefits of storing data as areas rather than points. Additionally, a live demo will illustrate the practical applications of the H3 index in real-time.\n\n## Main Points\n\n1. **Ease of Distance Calculation**\n   \n   Calculating distances between geographic points is a fundamental task in many applications. Traditional methods, relying on latitude and longitude, can be computationally intensive and complex. The H3 index simplifies this process by using a hexagonal grid system. Each hexagon, or cell, has a unique identifier that allows for straightforward distance calculations. This system reduces the computational overhead and enhances the performance of applications that require frequent distance measurements, such as ride-sharing services and delivery optimization platforms.\n\n2. **Reliability at the Poles**\n\n   Geographic coordinates (latitude and longitude) become less reliable and more distorted as one moves towards the poles due to the curvature of the Earth. The H3 index mitigates this issue through its hexagonal grid, which maintains consistent cell shapes and sizes across the globe, including polar regions. This characteristic ensures that spatial analyses and operations are accurate and reliable, regardless of geographic location. For instance, environmental monitoring projects can benefit from this consistency when tracking climate change indicators in polar areas.\n\n3. **Storing Data as Areas**\n\n   Traditional spatial data storage often relies on point-based representations, which can lead to inefficiencies and inaccuracies, particularly when dealing with large datasets or areas. The H3 index allows for data to be stored as areas rather than points, leveraging its hexagonal cells. This approach offers several advantages:\n   - **Efficiency:** Hexagonal cells cover areas more uniformly, reducing data redundancy and improving storage efficiency.\n   - **Accuracy:** By representing regions as collections of hexagonal cells, spatial analyses can be more precise. This is especially useful for applications such as urban planning and resource management.\n   - **Scalability:** Hexagonal cells can easily aggregate or disaggregate, facilitating scalable solutions for various spatial resolutions.\n\n## Conclusion\n\nUber’s H3 index is a powerful tool that enhances how we handle and analyze geographic data. Its ease of distance calculation, reliability at extreme latitudes, and efficient area-based data storage present significant advantages for a wide range of open-source projects. By adopting the H3 index, developers and researchers can achieve more accurate, efficient, and scalable solutions for their spatial data needs.\n\nDuring the talk, we will demonstrate these benefits with a live demo, showcasing how the H3 index can be implemented in a real-world scenario. Whether you are a developer seeking to optimize your application’s performance or a researcher aiming for precise spatial analysis, the H3 index offers a versatile and robust solution.",
        "description": ""
      },
      {
        "title": "Enhancing Resilience to Pluvial Flooding in Pacific Island Nations: A Novel Approach to Rapid Rainfall Modelling and Risk Assessment.",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Flooding caused by rainfall poses a significant hazard to Pacific Island nations that are at risk of cyclones and large rainfall events. Particularly, island nations such as Samoa, which are characterised by high mountains and steep river gullies, are highly susceptible to flash flooding events. Despite this recognised threat, a notable absence of reliable, high-quality data hinders the application of traditional hydrodynamic models that typically rely on precise river flow values and often disregard rainfall factors. Furthermore, there is an absence of flood hazard inundation maps that can provide a basic indication and outline the potential consequences of rainfall-induced flooding on both the populace and the environment within these regions.\n\nTo address these challenges, the Pacific Risk Tools for Resilience Phase-2 (PARTneR-2) project, co-led by the New Zealand National Institute of Water and Atmospheric Research (NIWA) and the Pacific Community (SPC), has introduced a novel approach to rapid rain-on-grid modelling. This methodology uses a combination of open-source geo-spatial software, including QGIS, RAS Mapper, and 2D hydrodynamics software BG-Flood, on limited geo-spatial data to rapidly produce indicative flood depth inundation maps.\n\nOur study has concentrated on two Pacific Island nations: Samoa and Vanuatu. For these two countries, we have developed a series of flood depth inundation maps at national coverage for 10-, 50-, and 100-year Annual Recurrence Intervals (ARI). The model used to produce the maps requires limited input data, is reasonably easy to set up, and can be run in under 12 hours. The maps produced are useful in providing a basic indication of areas that are at the most risk of flooding in a specified rainfall event. The method is cost-effective and can be implemented using entirely open-source software.\n\nFurthermore, by integrating the flood maps produced by this study with geo-spatial asset data sets, particularly focusing on buildings, our study shows how we can quantify the exposure and potential losses to flooding events, employing the Riskscape multi-hazard risk modelling software. The outcomes generated by this innovative method offer valuable insights that can inform resource allocation decisions during the critical hours following a significant rainfall event, particularly in the context of a Post Disaster Needs Assessment (PDNA) study.",
        "description": ""
      },
      {
        "title": "Gema & Sisdai: open data & free software projects by the Mexican government",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "INTRODUCTION\n\nThe development of Gema (Map Manager) and Sisdai (System of Design and Accessibility for Research) is based on two main premises:\n\n1) The information generated with public money should be public and for free.\n\n2) The Mexican government is moving towards technological autonomy and independence; therefore, free software components should be conceived for development and use in the federal public administration.\n\n\nINSTITUTIONAL CONTEXT\n\nGema and Sisdai have been developed by the Center for Research in Geospatial Information Sciences, A.C. (CentroGeo) and have been coordinated by the National Council of Humanities, Sciences and Technologies (Conahcyt), which is the Mexican government institution responsible for establishing public policies on humanities, sciences, technologies and innovation throughout the country.\n\n\nARTICULATION WITH ENI\n\nConahcyt created the National Informatics Ecosystems (ENI) to make available the results of research funded by the state, publishing open data, information visualizations, analysis and maps that help citizens better understand the country in which they live. \n\nENIs are collaborative and open access spaces that contribute to local and regional knowledge to address Mexico's priority problems by storing, processing, analyzing and disseminating humanistic, scientific and technological information. The topics addressed are: toxic agents and polluting processes, water, culture, education, energy and climate change, health, human security, socio-ecological systems and sustainability, food sovereignty and housing.\n\nGema and Sisdai are articulated with the ENIs, as contributions to open science. All of them are available in public portals.\n\n\nGEMA\n\nWe will begin this talk by asking: is it possible to have a country where the government, academia, civil society, private sector and media collaborate to generate, publish and consume open data? And the answer is simple: this would be ideal, but today it remains a utopia.\n\nGema is moving towards institutional interoperability, but also exploring the incorporation of data outside the scope of government. \n\nConahcyt manages research projects with academia and we all know that historically scientific production has been shared mainly in specialized journals, books and research articles that generally involve payment.\n\nAdditionally, what happens to the data used as input in research? It usually remains within the research teams and is not published, hindering replicability, interoperability and methodological contrast.\n\nThis project contributes to open science by promoting free access to scientific research products (data, methodology, code, etc.).\n\nIn Mexico there are different national instances of data collection, integration and visualization, for example the National Institute of Statistics and Geography (INEGI), the Ministry of Health, the National Population Council (Conapo), the Executive Secretariat of the National Public Security System, among others.\n\nTo support the analysis and visualization of data, Gema has loaded layers of information from all of the above-mentioned agencies, making it possible to cross-reference official data with information derived from research projects. \n\nGema (in English Gem) takes its name from the combination of the first letters of the words “gestor” and “mapas” in Spanish, thus alluding to a precious stone, as well as to the gradual process it takes to become a real precious jewel, such as the constant transformation of data into information and knowledge, which results in an input of great value and importance.  \n\nAs a Geospatial Knowledge Infrastructure, created in an open science environment and with accessibility criteria, Gema integrates a free data model, as well as tools so that users can explore, compile, visualize and share geospatial information related to humanistic, scientific and technological activity. \n\n\nSISDAI\n\nBeyond being a free software project, it is a design system that allows to establish rules, patterns and practices to ensure the consistency of complex, flexible and constantly evolving digital products. \n\nSisdai is built in an interdisciplinary way, considering criteria for accessibility, usability, data visualization, good code practices and user experience. Its structure is based on the atomic design methodology, which proposes that from simple elements -atoms- complex components and functions -organisms- are formed and, in turn, these form functional and robust templates and user interfaces under the same logic to be able to decompose them if necessary.\n\nBy using Sisdai you will be able to explore buttons, menus, graphs, maps, components, and others that will allow you to develop accessible interfaces. Also, if your native language is not English, you will have the opportunity to access documented code in Spanish! Sisdai libraries are developed using open source technologies, such as the Vue.js Javascript framework, OpenLayers, D3.js and Git.\n\nSisdai enhances the social impact of research projects by promoting technological autonomy and independence with the use of free software components. The Sisdai portal aims to enable as many people as possible to perceive, understand, navigate and interact correctly. This includes those with different disabilities: visual, hearing, motor, cognitive or neurological, as well as older adults and those unfamiliar with the Internet. The code repositories, libraries and documentation that comprise it are developed by Mexican work teams where the Spanish language is privileged. \n\nThere are currently 5 public code libraries in institutional repositories available for research teams.\n\n\nCONCLUSION\n\nENI-Gema-Sisdai are open projects in favor of the nation, in which collaborations were carried out with more than a dozen research teams, institutions and public centers. There are more than twenty public portals of the ENI project, about 400 geographic layers with free format downloads in Gema and 5 open source libraries in Sisdai. All this is available in public sites.",
        "description": ""
      },
      {
        "title": "Reconstructing literary geographies on the margins of São Paulo using open GIS resources",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This presentation shows how QGIS, OpenStreetMap, and other open resources were used to reconstruct narrative maps conveying one woman’s experience with space and identity in mid-20th-century São Paulo. In 1960, the edited diary of Carolina Maria de Jesus, Quarto de Despejo, become one of the best-selling books in Brazil. It vividly portrayed the struggles of a single mother scavenging the streets of the city looking for anything to sell so she could feed her family. Some Brazilians did not believe that a Black woman from a “favela” neighborhood could have composed such a poetically poignant manuscript, but more recent scholarship has confirmed the authenticity of Carolina’s authorship.  \n\nIt is clear that Carolina saw and felt lines of division between her neighborhood and the city not visible on any printed maps of the time. When going into “the city” she felt like she was in paradise, or a beautiful guest room; whereas her neighborhood on the precarious margins of the Tietê River was viewed as the backyard trash heap, subject to environmental hazards and government neglect. In this talk, I use narrative maps to show how these different places in Carolina’s life were constructed and what they each meant to her.  \n\nIn order to better understand Carolina's relationship to the spaces she called “the favela” and “the city”, I carefully constructed a list of every place mentioned in Quarto de Despejo. Employing a combination of archival research, Internet searches, onsite visits, and modern GIS databases, our research team located and mapped as many of these locations as possible, using QGIS software. For a base layer, we fashioned a historical street grid by starting with OpenStreetMap and modifying the geometries to conform to old aerial photographs freely available online. We then used our newly-created databases to compose narrative maps of Carolina’s São Paulo, focusing on areas where she did and did not go, as well as places where she experienced different kinds of emotions, challenges, access to resources, and interactions with the state. These maps demonstrate how geographic and social barriers both seen and unseen influenced the daily lives of Paulistanos in the 1950s, a challenge that persists in Brazil today.",
        "description": ""
      },
      {
        "title": "TerraBrasilis:  an open-source solution for disseminating information about the Brazilian Biomes vegetation cover",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "Brazil’s National Institute for Space Research (INPE) developed the TerraBrasilis web portal as the main link between the land cover data generated by its projects and a wide range of users. In this talk, we will discuss TerraBrasilis' development, updates, and improvements. We built the portal's front-end and back-end using free and open-source software.\nINPE’s program named BiomasBR is responsible for long term projects that generate the official data about the Brazilian biomes. These projects process Earth observation satellite images with the goal of generating accurate and timely data to support the country in monitoring and controlling deforestation, forest degradation, forest fire, and other environmental impacts on the Brazilian Amazon forest and other biomes. In this talk, we address specifically the projects PRODES and DETER. The Real Time Deforestation Detection System (DETER) uses Earth observation satellite imagery to map and report changes in forest cover across the Amazon and the Cerrado biome. As a result, DETER produces, every day, sets of polygons, referred to as alerts, that delineate areas where deforestation or degradation (such as mining, forest fire scars, or logging) can be perceived. Each DETER alert includes the image date that generated it, the type of change, and other attributes. DETER uses images from sensors with high temporal resolution and large swath width, with the appropriate spectral bands to detect vegetation cover change. Currently, DETER uses mainly data from the Wild Field Imager (WFI) on board the Amazonia-1 and the China-Brazil Earth Resource Satellite (CBERS-4A). Command-and-control organizations use the data produced by DETER to plan and qualify field actions.\nThe Legal Amazon Deforestation Satellite Monitoring Project (PRODES) maps, annually, the suppression of native vegetation in Brazil. Initially, the project restricted its scope to the Legal Amazon region, conducting an annual inventory of forest loss dating back to 1988. In 2001, the project expanded its scope to include the monitoring of Cerrado biome. PRODES uses medium resolution imagery with the appropriate spectral bands to detect vegetation cover change. It uses images from the Operational Imager (OLI) sensor on board the Landsat-8 satellite, the Multispectral Camera (MUX) from CBERS-4 and CBERS-4A, and the Multispectral Instrument (MSI) from Sentinel-2. PRODES data has supported long-term public policies to contain native vegetation removal in the Amazon and the Cerrado. Currently, the project has expanded to encompass all Brazilian biomes. PRODES data support public policies, greenhouse gas emission estimates, and can help release international resources associated with conservation, climate, and biodiversity agendas. \nPRODES and DETER generate a large volume of geospatial data useful to a several organizations. Given the strategic importance of these projects, and INPE’s commitment to geospatial open source and open data, the TerraBrasilis portal is the one-stop point to access PRODES and DETER data.  The portal is used by the Brazilian government, academic and private sectors, non-governmental organisations, and the general public interested in environmental issues. As such, TerraBrasilis is a way to ensure INPE’s transparency regarding the environmental data it generates. It allows anyone to explore land cover data without requiring further knowledge.\nThe main functional requirements and characteristics that guide the development of TerraBrasilis are: multiplicity of users, with different technological backgrounds and different interests; data might be subject to temporal embargo before being public; temporal embargo might be overcome by authorized users; data is related to the same process, but has a distinct production timetable and distinct time granularity; data has to be presented as a web map with the conventional operations of zooming, pan, layer selection, etc.; data summaries should be available with common visualization tools such as graphs, charts, and tables; users need to make comparisons over time and different spatial units of interest, such as municipalities, states, indigenous lands, or conservation units; data can be downloaded in open formats; data can be accessed through open geospatial services.\nConsidering these requirements TerraBrasilis is designed as a set of multiple independent panels, to expose different views of its geospatial database. A series of scripts connect to the production database and prepare the data for publishing in TerraBrasilis. The publishing database contains over 2 PB of raster and vector data, and this volume increases daily. It contains data for Brazil's entire territory. Some datasets date back to 1988.\nPRODES and DETER data are available in distinct web mapping panels that allow the visualization of multiple layers, spatial and temporal filtering, as well as the download of data in open formats such as GeoTIFF and Shapefile. They are also available in interactive dashboards where users can make comparisons over time and different spatial units of interest, such as municipalities, states, or conservation units.\nTerraBrasilis uses a service-oriented paradigm that follows interoperable international spatial data standards and the specifications of the Brazilian National Spatial Data Infrastructure (INDE). TerraLib offers Open Geospatial Consortium (OGC)-compliant services such as Web Map Service (WMS), Web Feature Service (WFS), Web Service Coverage (WCS) and Catalog Service for the Web (CSW).\nTerraBrasilis publishes all of the data generated in DETER and PRODES, and it is constantly evolving. Recently, the portal has developed and integrated some panels that integrate data from PRODES and DETER, along with other data sources such as INPE's vegetation fire monitoring project in Brazil. The Situation Room panel, for example, allows the observation of critical areas of deforestation and vegetation fire, through a series of indicators based on data from DETER and other environmental indicators. In this talk, we will provide a general overview of the portal, the technical aspects of its development, and the data presented in TerraBrasilis.",
        "description": ""
      },
      {
        "title": "Leveraging AI and Open Source Geospatial Software to Combat Illegal Gold Mining in the Amazon",
        "type": "Talk",
        "track": "Applications and solutions for the Amazon region",
        "abstract": "Illegal gold mining in the Amazon rainforest has escalated dramatically, resulting in deforestation, pollution, and violence against indigenous communities. This talk will explore how AI and open-source geospatial software are being used to combat this environmental crisis. We will delve into the methodologies and technologies behind Amazon Mining Watch's (AMW) efforts to monitor and mitigate illegal mining activities. By integrating machine learning models with satellite data and open-source tools, AMW provides insights that empower local activists and influence policy changes. \n\nThis session will cover the following topics:\n- Overview of the environment and social impacts of illegal mining and challenges of detection\n- The technological approach, beginning with purpose built machine learning models and tracing the evolution to the foundational models used today. \n- Current architecture and open source components for data processing\n- The use of open data, data processing strategies and information on accessing (for free!) over 100TB of processed imagery and embeddings generated to date\n- Live demo of the next-gen platform\n- Q/A\n\n**Links:** \n\n- https://Amazonminingwatch.org\n- https://infoamazonia.org/2024/07/09/eis-o-que-a-inteligencia-artificial-achou-na-amazonia-areas-de-garimpo-dobraram/\n- https://news.mongabay.com/2024/07/gold-mining-in-the-amazon-has-doubled-in-area-since-2018-ai-tool-shows/ \n- https://foundation.mozilla.org/en/blog/a-gold-rush-worth-stopping-ai-does-its-part \n- https://pulitzercenter.org/blog/amazon-mining-watch-expands-use-ai-monitor-illegal-gold-mining \n\n**Target Audience**\nThis talk is aimed at geospatial professionals, software developers, environmental activists, policymakers, and anyone interested in the application of AI and open-source geospatial technology for environmental protection.",
        "description": ""
      },
      {
        "title": "Utilizing R for Open-Source GIS in Brazilian Governmental Institutions: Enhancing Public Policy Through Detailed Mapping",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Geographic Information Systems (GIS) have become indispensable tools in public administration, providing essential insights for decision-making. My talk will showcase the application of R, a free software environment for statistical computing and graphics, in creating detailed maps that reveal critical socio-economic and environmental patterns across Brazil. Our work primarily focuses on public health, education, environmental issues, and economic development, demonstrating the transformative power of open-source GIS in governmental institutions.\n\nThe data sources for our analysis are primarily open data repositories provided by the government, NGOs, and private institutions. We integrate various datasets, including public expenditure, municipal influence, population, and hospital locations. Statistical tools such as clustering, centrality measures, correlation, chi-square tests, and distribution measures are employed to identify patterns and produce significant graphical elements. We utilize R packages like {geobr} and {spdata} for accessing thematic shapefiles, including states, municipalities, municipal seats, and biomes. Our mapping projects often stem from requests by institutions such as the National Treasury Secretariat and the Vice-Presidency of the Republic, as well as spontaneous contributions to the data science ecosystem.\n\nOne of our key findings relates to healthcare access. Cities with low management capacity and influence are typically associated with significant travel distances for hospital care. In the realm of education and human development, public education policies have reshaped the municipal HDI map of Brazil between 1991 and 2010, particularly in the Northeast. In Ceará, education improvements were pivotal in enhancing HDI from 1991 to 2010. The impact of COVID-19 has also been a critical area of focus. By the first anniversary of the epidemic, numerous Brazilian municipalities reported death tolls nearing the total deaths observed from 2014 to 2018.\n\nClimate change exacerbates natural disaster issues in Brazil, notably floods and droughts in Rio Grande do Sul and the Amazon region. Despite most Brazilian homes having at least one bathroom, maps reveal that the North region and Maranhão still have many municipalities with low proportions of homes with bathrooms. Conversely, the South and Southeast regions, especially São Paulo, show high proportions of residences with access to central water supply networks. When considering environmental pressure, many Global South countries see their HDI positions improve, while the Global North, notably Canada, experiences a significant decline.\n\nEconomic analysis reveals that in Brazil, five clusters of municipalities are formed based on municipal GDP components. Maps highlight patterns associated with agricultural strength in the West, wealth in São Paulo, and significant economic deprivation in the Northeast and northern Minas Gerais. On a broader scale, Brazil stands out with the largest GDP growth in South America from 1960 to 2023, while Argentina and Venezuela have seen their shares plummet, and Chile remains stable.\n\nThe maps generated serve multiple purposes. They are integral to storytelling, dashboards, and publications on platforms like Medium. Some maps are intended solely for internal use by the requesting institutions. Many projects have their codes available on GitHub or Gist, promoting transparency and collaboration within the data science community.\n\nOur work demonstrates the profound impact of using open-source GIS tools in governmental institutions. The ability to visualize and analyze complex data sets in a spatial context has enabled more informed decision-making and public policy development. By sharing our methods and findings, we aim to inspire other governmental bodies to adopt similar practices, ultimately fostering greater transparency and efficiency in public administration.",
        "description": ""
      },
      {
        "title": "State of deegree: Server-side open source software for spatial data infrastructures and the geospatial web",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The OSGeo project deegree is open source software for spatial data infrastructures (SDI) and the geospatial web, which mainly focuses on the server-side. It implements standards of the Open Geospatial Consortium (OGC) and the ISO Technical Committee 211. The project provides 9 official Reference Implementations of OGC Standards such as GML, WFS, WMS, and OGC API - Features.\n\nThis talk will give an overview of the latest stable release of deegree (3.5) as well as the recent developments of version 3.6 that provides support of Java 17 and Tomcat 10. Additionally, the deegree implementation of the OGC API - Features standard will be presented.\n\nAt last, the future directions and planned core developments of the project will be presented.",
        "description": ""
      },
      {
        "title": "Scaling FOSS4G for National Environmental Monitoring: Inteligeo and Brasil MAIS",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "This talk explores how the Brazilian Federal Police leverages FOSS4G to integrate and visualize data from Brasil MAIS, a multi-million dollar environmental monitoring program, using our custom-built Inteligeo platform.\n\nBrasil MAIS monitors over 2.9 million km² weekly - about 34% of Brazil's vast 8.5 million km² territory. To manage the resulting massive amounts of satellite imagery and environmental change alerts, we developed Inteligeo, a GeoNode-based solution currently in production at the Federal Police and in various stages of adoption across multiple government agencies.\n\nWe'll discuss:\n1. Building Inteligeo on FOSS4G principles (GeoNode, PostGIS, GDAL, GeoServer, FastAPI) to enable seamless data integration across government agencies.\n2. Implementing user-friendly tools for non-specialists and integrating with Gov.br authentication.\n3. Using FastAPI for complex integrations, such as streamlining access to the XYZ data provider for Brasil Mais.\n4. Technical challenges in scaling Brasil MAIS to handle 46 million monthly tile views and serve 500+ public institutions and 100,000+ users.\n5. The crucial sponsorship of the Ministry of Management and Innovation in Public Services (MGI) in developing Inteligeo 5 since 2021, and their plans for its independent use.\n\nBrasil MAIS has already demonstrated significant impact, contributing to over R$16 billion ($2.8 billion USD) in environmental crime-related fines and asset freezes. While Inteligeo is still ramping up adoption, it aims to compound this impact by enhancing the efficiency of data utilization. We anticipate measuring Inteligeo-specific results in the coming year, showcasing how FOSS4G solutions can amplify the effectiveness of large-scale environmental monitoring programs.\n\nWe'll conclude by examining how this integration serves as a model for similar global initiatives in environmental management and welcome input from the FOSS4G community on further improvements.",
        "description": ""
      },
      {
        "title": "Use of Open-Source Software in Census Cartography Production - IBGE's Case",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "To showcase the application of free and open-source software (FOSS) in the census cartography works produced within the scope of the Brazilian Institute of Geography and Statistics (IBGE). \n\nCurrently linked to the Ministry of Planning and Budget, IBGE is a federal public administration entity. Since its foundation in 1936, it has become the main provider of data and information in the country, meeting the needs of various segments of civil society as well as federal, state, and municipal government entities. Its institutional mission is \"to portray Brazil by providing the information required to the understanding of its reality and the exercise of citizenship\". To achieve this, it identifies and analyzes the territory, counts the population, shows economic evolution through people’s work and production, while also revealing how they live. \n\nTo support this purpose, IBGE maintains the Territorial Base (TB), a spatial information system supporting the operations of collection, processing, analysis, tabulation, charts, cartograms, and publications of its surveys and censuses. It is also used for evaluations of population estimates by identifying, monitoring, and representing the evolution of the territory, mainly through the development of its census sector network. \n\nThe TB consists of a graphical base of georeferenced information representing territorial structures, census tracts (called sectors) and other elements. These structures can be legal, such as the political-administrative division; analytical, linked to territorial patterns or specific population groups; or operational, intended to guarantee access to and coverage of census units. \n\nAmong the territorial structures that comprise the TB, the census sector underpins the operational organization of censuses and some sample surveys. It is a territorial interview collection unit whose size and number of households and establishments allow the census taker to complete their activities within a specified timeframe. Moreover, it is the basic unit for disseminating census information, allowing public access to statistical results at the smallest spatial range. \n \nIn mid-2014, IBGE was moving from a data update model based on a complete review conducted just before a census operation to a continuous update model, which was more complex and required more agile tools and systems for operators. At that time, however, the software used by the TB was commercial, with the update and customization processes for the entity's specific objectives being slow and difficult, as they were tied to contracts with the supplier. This represented a considerable, often prohibitive, expense for public coffers.  \n\nConcurrently, a staff of analysts had formed within IBGE, coming from civil service exams held in previous years for specialized technical jobs. These servers had two synergistic qualities: methodological knowledge of the activities performed by the institution, and some had technical capabilities in systems development. \n\nGiven the scenario, it became feasible to explore an internal alternative for TB update activities. Such a system should meet these technological premises: \n\nBe based on a free platform without licensing costs, preferably with an open-source codebase to avoid future dependencies; \n\nOperate with low computational and network requirements to meet the diverse realities of users; \n\nBe fully customizable to adapt to the needs of the TB and IBGE. \n\nGiven such requirements and considering the technical knowledge and stage of evolution of geoprocessing tools at the time, the system was defined as an external plugin for QGIS (using pyQGIS API) with a graphical interface modeled in Qt framework (using pyQt API). \n\nThe Geographic Information System for the Territorial Base (SIGBT), therefore, is the technological solution that emerged in 2014. Its conception, implementation, and homologation stages were always characterized by constant collaborative development, where TB technicians and related areas of IBGE could participate and contribute in several ways, from signaling problems to suggesting improvements and even writing the code itself. The development of SIGBT was designed modularly, to meet the methodological and conceptual priorities defined by IBGE. \n\nIts code was primarily written in Python, and the handled data is managed by an SQLite database (with SpatiaLite extension), made available to internal users for download and offline editing from a versioned Git repository. \n\nAmong the main procedures performed by SIGBT are the graphical operations on census sectors (divisions, aggregations, adjustments, and ensuring comparability between previous sector grids) and intra-sector layers (localities, roads, blocks, and block faces). It also ensures the topological and methodological consistency of the edits made. Additionally, the production of maps used in census operations is carried out on SIGBT. \n\nSince the advent of SIGBT, three major data update cycles aimed at census operations have been conducted using the system: the 2015 Population Count (which was eventually canceled), the 2016 Census of Agriculture, and the 2022 Demographic Census (which prompted two update cycles due to two postponements). \n\nCurrently, SIGBT is in use at the IBGE Coordination of Territorial Structures, in all 27 State Superintendencies, and in most of its over 560 agencies. It comprises about 50 specific functions that automate processes, facilitate edits, and enable the continuous updating of the TB. The system also integrates information from other databases from different areas that interact with the TB, enabling integrated editing and visualization of all data, thus facilitating the construction of territorial occupation indicators at the national level. \n\nLooking towards the future of SIGBT, the development of new modules for public access is being considered. It is projected that these modules will leverage QGIS navigation resources to offer citizens easier and quicker access to geographic data published by IBGE. \n\n  \n\nFinal Considerations \n\nThe explanation above showcases, through IBGE's experience with SIGBT, the feasibility of using a free software architecture for complex geographical projects with a wide range of geographic applications.  \n\nFrom a budgetary perspective, the superiority of free software was also verified, as SIGBT has contributed estimated savings that would exceed six figures since its creation, solely from software licensing that is no longer used. \n\nThe SIGBT initiative, therefore, presents itself as a successful example of a system based on free and open-source software, developed by public administration, showing that the use of this type of structure is not only viable but desirable.",
        "description": ""
      },
      {
        "title": "The Copernicus Global Land Cover and Tropical Forest Mapping and Monitoring service - a free and open dynamic global land cover service at 10 m resolution for the years 2020-2026.",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The newly established Copernicus Global Land Cover and Tropical Forest Mapping and Monitoring service (LCFM), coordinated by the European Commission's Joint Research Centre (JRC) and implemented by a consortium led by VITO, marks a significant advancement in land cover mapping. Building on the 100 m Copernicus Global Land Cover layers (2015-2019), LCFM integrates insights from ESA’s WorldCover and Horizon2020 REDDCopernicus projects to deliver a dynamic global land cover service at 10 m resolution for the years 2020-2026.\n\nLCFM will provide global sub-annual (i.e., monthly) land surface features and categories at 10 m resolution, consolidating them into annual land cover maps and tropical forest monitoring products. These data products will be of high quality, free, and open to all users in line with the principles of the Copernicus program. Additionally, LCFM will also release the training data used in its workflows, supporting transparency and fostering further research and development within the community.\n\nA key aspect of LCFM is its foundation on open-source software. Production processes will be conducted on the Copernicus Data Space Ecosystem (CDSE) and will be based on the Python ecosystem and the openEO platform. The processing workflows and developed software will be open-sourced and released together with the data products ensuring transparency, reproducibility, and collaboration.\n\nIn this presentation, we will introduce the LCFM service and showcase the first data products which will be publicly released in the beginning of 2025.  We will highlight the improvements the service brings and explore its potential for downstream applications.",
        "description": ""
      },
      {
        "title": "Elevating GeoServer to the Cloud: Production-Ready Features for Optimal Performance",
        "type": "Talk",
        "track": "State of software",
        "abstract": "## Abstract:\n\nUnlock the full potential of your geospatial data with GeoServer Cloud! This presentation will showcase how GeoServer Cloud delivers high availability, scalability, and cost-effective mapping solutions, positioning itself as a robust alternative to proprietary mapping platforms. Learn how to optimize your GeoServer deployment for production, ensuring reliability and efficiency without breaking the bank.\n\n[ GeoServer Cloud ](https://geoserver.org/geoserver-cloud/) is a GeoServer distribution (alongside war downloads and docker), that leverage microservices for deployment. \n\n## Session Description:\n\nAs the demand for reliable and scalable geospatial solutions grows, GeoServer Cloud emerges as a leading choice for organizations looking to enhance their mapping capabilities without incurring excessive costs. This session will explore how GeoServer Cloud addresses these needs through its core features and advanced functionalities.\n\nIn this session, you will learn:\n\n1. **High Availability and Scalability:** Discover how GeoServer Cloud ensures high availability and scalability, allowing you to confidently deploy your geospatial applications.\n\n2. **Improved Catalog scalability:** Say goodbye to lengthy load times. GeoServer Cloud leverages a new PostgreSQL back-end for the GeoServer Catalog and Configuration objects, ensuring a cloud-friendly, stateless, scalable storage solution. Experience near-instantaneous pod start-up times, allowing you to spawn as many pods as needed and be ready in seconds.\n\n3. **Advanced Authorization with GeoServer ACL:** Secure your data with GeoServer ACL, an advanced authorization system. This independent application service manages data access rules and per-workspace administrative rights, providing granular control over your geospatial data.\n\n4. **Comprehensive Observability:** GeoServer Cloud integrates with observability tools to collect traces, metrics, and logs. This integration provides invaluable insights into your system’s performance, supporting proactive monitoring and troubleshooting. Working with open-source tools like Jaeger and Prometheus or commercial offerings, and ensuring adaptability to diverse cloud environments and existing infrastructure.\n\nDuring this presentation, we will walk you through these features and discuss their positive impact on productivity, reliability, and efficiency. Learn how Camptocamp’s customers have successfully implemented GeoServer Cloud and how you can achieve similar results in your organization.\n\nJoin us to explore the cutting-edge capabilities of GeoServer Cloud and transform your geospatial data management.",
        "description": ""
      },
      {
        "title": "Cartographic Transition: From Map Library to Geoprocessing - Exploring the Codes Behind the Project. A Model for Public Agencies",
        "type": "Talk",
        "track": "Transition to FOSS4G",
        "abstract": "In 2015, the City of São Paulo launched GeoSampa, a platform providing access to a variety of cadastral data, maps, satellite images, and information on zoning, land use, urban infrastructure, land subdivision, and public areas for both municipal technicians and the general public. GeoSampa serves as a data repository, allowing different departments to share their information and keep it updated on the platform. A notable example is the Licensing Department, which in 2015 presented a geographic database solution using open-source software, with Linux, Postgres, PostGIS, Python, and QGIS, for land subdivision processes. These initiatives solved major problems and streamlined work processes, highlighting the importance of a data-sharing culture among public agencies for urban development.\nThe work developed by the Licensing Department was summarized in the experience cataloged by CEBRAP in a program called COPI-COLA, whose content can be seen in full on the CEBRAP website  https://cebrap.org.br/wp-content/uploads/2023/09/Guia_5.pdf. This program adopted an innovative approach by using open-source software to develop and maintain the system, significantly reducing implementation costs and making team work more efficient and integrated. Other areas also benefited from the organized and available information on GeoSampa, serving as an example for public agencies across the country.\nThis work specifically aims to show two technical examples of codes used in the Geographic Database mentioned above, which I developed, highlighting the practical benefits of collaboration and technological innovation in the public sector using open-source software.\nExample 1\nAccording to Ordinance No. 957/GC3 of 2015, issued by the Brazilian Aeronautics Command (COMAER), guidelines are established for the creation and management of Airport Protection Zones (ZPA). The objective is to ensure the safety and efficiency of air operations by preventing obstacles and activities that could interfere with air navigation. The ordinance details the competencies of municipalities and other bodies in monitoring and controlling these areas, as well as defining specific restrictions for land use near airports.\nDue to the large volume of licensing requests, if all enterprises were subject to DECEA's evaluation, there would be a need to increase DECEA's team, resulting in approval delays. To mitigate this problem, the implementation of a geographic system, to be used by the City of São Paulo, could select only cases in regions where DECEA's action was necessary.\nA system using Linux, Postgres, PostGIS, and QGIS was developed by the department's technicians with validation from DECEA's technicians. This optimized processes, reducing DECEA's workload and shortening approval times. Our objective is to briefly present how this system was developed and its impact on time savings and information security.\nExample 2\nThe work consisted of creating an algorithm to comply with Article 131 of Law No. 16.402/2016. \"The use of properties, for the purposes of land subdivision, use, and occupation discipline, is classified as permitted or not permitted and as compliant or non-compliant. § 2º Use not permitted in the location is that which cannot be implemented or installed on the property due to the zone and the street width.\" Street width is defined as the distance between property alignments, including the carriageway and public walkway.\nWe used the cartographic bases of the City of São Paulo, surveyed in 2004, for geoprocessing. The tool used was the QGIS software (Geographic Information System - GIS), the official project of the Open Source Geospatial Foundation (OSGeo). The Licensing and Urban Development Department's (SMUL) database on street widths at the time (2018) covered between 35% and 40% of the city's streets. The database had low reliability, as measurements were taken randomly at three different points on the street segment, making it visually impossible to pinpoint where the width is smallest, thus preventing proper application of the above law. Measurements not included in this database would be taken in loco by subprefecture technicians and sent to the Licensing Department's Cadastre Division for completion. This database gathered measurements taken over time and was not available to the public.\nThe work presented here was developed using scripts in PyQGIS, which combines the Python programming language with QGIS APIs, allowing for automated geometry tracing. The created code used all street segments and traced all elevations from the nodes created in the block layers, choosing the minimum elevation. This work does not eliminate the need for in loco measurements, but like in Example 1, selects the necessary cases, significantly reducing the number of visits and increasing reliability for proper law enforcement.\nConclusion\nThe success of GeoSampa and associated projects reveals that using open-source tools promotes transparency and accessibility of geographic information, strengthening citizen participation and facilitating urban planning, making cities smarter and more sustainable.\nAs a developer of this project, I feel immense pride in having contributed to this digital and technological transformation in public administration. Seeing the positive impact of our work is extremely gratifying.\nUltimately, São Paulo's example can inspire other cities to adopt similar practices, contributing to a future where technology and innovation serve efficient and inclusive urban development. Thus, the cartographic transition from map library to geoprocessing marks a new era in public administration, where geographic information is a valuable resource for decision-making and improving quality of life in cities.",
        "description": ""
      },
      {
        "title": "Humanitarian data collection in browser-based Postgres",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In a humanitarian context, data collection can be divided into two main categories: proactive collection of data that may be useful for disaster response and recovery; reactive collection that is required to assess the situation on the ground during an event.\n\nThe Humanitarian OpenStreetMap Team has supported both types of mapping through the Tasking Manager platform. Going forward we will also be able to collect drone imagery collaboratively and add collected field-data to complement and match to the remote data, with the Drone TM and Field Mapping TM (FMTM) respectively.\n\nThere are often major hurdles for field-based data collection:\n1.  \tHow to effectively collaborate with multiple data collectors at the same time.\n2.  \tHow to work when there is poor connectivity in an area.\n\nWeb applications may be an acceptable choice to solve the first issue, but typically perform poorly when subjected to the second.\n\nWith a new paradigm in web development, local-first applications, this may no longer be an issue.\n\nWe can develop web-based applications that allow for both:\n-          Real-time update for users undertaking collaborative data collection campaigns.\n-          Fully offline data collection capability, with syncing and conflict resolution once connectivity is restored.\n\nThese capabilities have been achieved through some major landmarks over time:\n-          Addition of WASM to the browser in 2017.\n-          Implementation of databases in the web-browser (SQLite, Postgres), using WASM.\n-          Introduction of smart data reconciliation mechanisms such as CRDTs.\n-          Continual improved access to mobile phones globally, particularly in the introduction of high-performance smart phones.\n\nThis talk explores our journey implementing a local-first field mapping flow, with an example and demo to demonstrate its efficacy.",
        "description": ""
      },
      {
        "title": "Geoservercloud in a nutshell",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In this presentation, we will explore a practical example of using GeoServerCloud within a local cluster, specifically focusing on catalog configuration in the database. We'll simplify the complex process of deploying an application in a Kubernetes cluster using Helm. This task is challenging, especially when the application has a microservices architecture. Our goal is to bridge the technical gap, ensuring developers don't need to become DevOps experts to integrate GeoServerCloud into their applications.",
        "description": ""
      },
      {
        "title": "Assessing the gap: how to empower climate policy with open source data",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "The integration of open-source data into climate policy has immense potential to enhance climate adaptation and mitigation efforts globally. This talk addresses the existing gaps in the utilization of open-source data by decision-makers and explores sustainable funding models for these initiatives. \n\n Case Studies:\n\n1. Assessing Climate-Related Disaster Damage Using EMDAT:\n   The Emergency Events Database (EMDAT) provides critical open-source data on the damage caused by climate-related disasters. This data is invaluable for developing countries to assess their vulnerabilities and prioritize areas for climate adaptation funding from the Green Climate Fund (GCF). By using EMDAT, countries can make informed decisions to prevent future damage and allocate resources more effectively.\n\n2. Using OpenStreetMap (OSM) Data for Small Island Developing States:\n   In small island developing states (SIDS), the use of OSM data has been instrumental in assessing average travel times to urban centers. This analysis highlighted the high transactional costs due to fragmented geographies, providing policymakers with concrete evidence to address these challenges. By leveraging OSM data, these countries can improve infrastructure planning and reduce costs, enhancing their resilience to climate impacts.\n\n Focus Areas:\n\n1. Gaps in Competencies:\n   Despite the availability of valuable open-source data, there remains a significant gap in its usage by policymakers. Data specialists often conduct in-depth analyses that are not easily accessible or understandable to policymakers. Bridging this gap requires improved collaboration and communication between data specialists and policymakers.\n\n2. Enhancing Data Literacy:\n   Policymakers need better data literacy to fully utilize the benefits of open-source data. Training and capacity-building initiatives can empower them to make data-driven decisions, fostering a culture of evidence-based policy-making in climate adaptation and mitigation.\n\n3. Sustainable Funding for Open Source Initiatives:\n   Many open-source projects suffer from intermittent funding, threatening their sustainability. Innovative funding models, such as public-private partnerships, crowdfunding, and subscription-based services, can ensure continuous support and development of open-source data platforms.\n\nChallenges for Small Landmass Countries:\n   Countries with small landmasses, particularly SIDS, often face limitations in accessing open-source data due to satellite coverage gaps and network connectivity issues. Addressing these challenges requires tailored solutions, such as deploying drones for localized data collection and improving internet infrastructure.\n\nConclusion:\n   To maximize the benefits of open-source data for climate action, it is crucial to close the competency gap between data specialists and policymakers. By enhancing data literacy among policymakers and ensuring sustainable funding for open-source initiatives, countries can better harness these tools to develop robust, data-driven climate policies. This talk aims to shed light on these critical issues and propose actionable solutions to bridge existing gaps and promote effective climate decision-making.",
        "description": ""
      },
      {
        "title": "Catching up with GeoTools, JTS and friends",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The very first open source geospatial foundation diversity statement … was about representation for the “C Tribe” and the “Java Tribe”!\n\nAttend this presentation to check in with the Java crew with updates from:\n\n* GeoTools - open source Java library that provides tools for geospatial data.\n* JTS Topology Suite - Java library for creating and manipulating vector geometry.\n* ImageN - image and raster processing\n\nHappy mapping everyone please enjoy FOSS4G.",
        "description": ""
      },
      {
        "title": "Bridging the gap: How to help communities leverage open-source tools for crowdmapping?",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "Codeando Mexico is a Mexican non-profit organization, with 11 years of experience, that works with technology and data for the public good. Our goal is to build better societies by building and contributing to sustainable and useful tech / data projects. \nWe strongly believe in participatory, community-led projects that leverage tech and data to find solutions for our societies’ most pressing issues. We have spent the last decade building sustainable digital tools, opening up our tech and data, collaborating with other teams and contributing to amazing open-source projects. \nAs part of our work, we act as technical partners to communities that require assistance. We are aware that the communities themselves are the experts in the problems we are trying to solve. To do this, we mostly take advantage of great open-source digital tools that are already available. \nFor this talk, we would like to present three projects, talk about the lessons learned and provide insights on how to better collaborate between communities. In these collaborations we face similar challenges: How to integrate different skill levels to crowdmapping projects? How to structure participation to ensure data quality? How to help our partners better communicate and amplify the job they are undertaking? How to find a common language? How to create better relationships between tech, data, and problem-focused communities?\nThroughout the presentation we will talk about 3 projects:\n8M Map - Collaboration to improve the workflow from mapping to visualization of the activities around the globe for the 8M Geochicas International Women's Day 2024. \nMapea tu Cuadra (map your block) - A collaboration with URBE, a local collective of urbanists, to map security and micro mobility infrastructure in their neighborhoods. \nEcoZonas - Ongoing collaboration with WRI and the Wuppertal Institute to help neighborhoods map and identify climate disaster risks in their communities. \nWe will showcase the technologies and lessons learned, as well as useful strategies to build better collaborations.",
        "description": ""
      },
      {
        "title": "K8S Operator for Geoserver: Quarticle's approach to automation",
        "type": "Sponsor",
        "track": "Use cases & applications",
        "abstract": "As geospatial data systems grow in scale and complexity, automating the deployment, scaling, and management of geospatial servers becomes increasingly critical. This talk presents Quarticle’s innovative solution to automating GeoServer management using Kubernetes (K8S) operators. GeoServer is a powerful open-source server for sharing geospatial data, but its deployment and management in cloud-native environments can pose challenges.\n\nWe will dive into how Quarticle, a company specializing in cloud-native applications and geospatial data solutions, developed a Kubernetes Operator tailored for GeoServer. This operator simplifies the management of GeoServer instances, enabling seamless automated deployments, scaling, updates, and configuration management in a Kubernetes environment.",
        "description": ""
      },
      {
        "title": "Mapping Locally, Globally: A YouthMappers Perspective on Open Mapping",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "Since 2015, thousands of university students across the world have joined the YouthMappers network, creating and using open geospatial data to address local and global development challenges. At over 410 universities in 78 countries—including 60 chapters in 18 countries in Latin America and the Caribbean—student-led YouthMappers chapters leverage free, open-source geospatial tools to map their communities and priority regions. This presentation explores the confluence of factors that have enabled university youth spanning diverse identities, academic backgrounds, and languages to successfully engage in YouthMappers activities and with the broader open mapping community. The presentation also discusses the challenges that YouthMappers students face in applying open mapping methods and tools to their work. It features the lived experiences of YouthMappers students, particularly in the Latin American context, and foregrounds topics of Diversity, Equity, Inclusion, and Accessibility.\n\nYouthMappers chapters collaborate with the US Agency for International Development, humanitarian organizations like the Humanitarian OpenStreetMap Team, local and federal governments, NGOs, and other partners to address gaps in geospatial data. Students map areas both in person using open survey tools and remotely using satellite imagery. Ultimately, the open data that they collect and publish power development interventions, disaster response, and digital ecosystem growth, largely in low- and middle-income countries. The key to the YouthMappers network’s expansive growth and impact is its sense of community. For many students, YouthMappers is their initial introduction to open GIS software and its real-world applications. Students build their open mapping skills through peer-to-peer learning within the network and with mentorship from individuals in the larger FOSS4G ecosystem.\n\nAs a testament to the power of the network, YouthMappers students have collectively contributed over 24 million edits to OpenStreetMap since 2015. Advancements in open mapping software, such as improved options for low-connectivity environments and non-English-speaking users, have enabled YouthMappers students from a greater range of backgrounds to participate in open mapping. The YouthMappers network has also contributed to building FOSS4G tools, such as OSM Teams. This presentation highlights these successes and calls for continued mutually beneficial partnerships between YouthMappers chapters and the broader FOSS4G community.\n\nThis talk is rooted in a book chapter of the same name co-authored by the three presenters. The chapter will be published in the forthcoming open-access book Open Movements: Recognizing Challenges and Building Connections.",
        "description": ""
      },
      {
        "title": "Mapping Kenya: 15 Years of Map Kibera and beyond",
        "type": "Talk",
        "track": "Education",
        "abstract": "Map Kibera arose from a desire to expand OSM beyond the confines of Europe and North America. In 2009, it pushed the boundaries of what then-new technologies could do. What have the mappers learned over the years? This talk will welcome you to Nairobi and through the ups and downs of mapping in Kenya - from the history of mapping in 20th Kenya, through Map Kibera’s start, into slums and rural parts of Kenya, and finally to current-day Kibera, where mappers are mapping street lights, waste disposal, schools, and more. How has Map Kibera and OSM had a community impact even as drones, satellite technology and AI are revolutionizing mapping? What has changed, and what has remained the same?  We will discuss the global impact of Map Kibera, on community-based mapping in OSM and on the general application of technology in developing countries. \nThis talk will include a discussion of mapping in Kenya dating back to the colonial era, the establishment of Kibera as a region of Nairobi, and its growth into a massive informal settlement. Kibera has been viewed as a place to develop by the Kenya government, International aid agencies, charities, and missionaries. It was a flashpoint of the post-election violence of 2007/8.\nMap Kibera’s Kenyan leaders will discuss the most recent mapping and local impacts made by the use of OSM. Mapping of street lights in Kibera led to new and more street lights installed in Kibera. Mapping of waste management in Mukuru led to the placement of dumping waste bins. Data on schools has led to a pilot project to install solar panels on selected schools. None of these impacts have been easy, but we will share lessons learned about OSM, open data and communities. Finally, we conclude with a discussion of emerging and ever-changing technology, and the fate of the techno-optimism of the early 21st century.",
        "description": ""
      },
      {
        "title": "Easily publish your QGIS projects on the web with QWC2 - news from the project",
        "type": "Talk",
        "track": "State of software",
        "abstract": "QWC2 (QGIS Web Client 2) is the official web application of QGIS, that allows you to publish your projects with the same rendering, thanks to QGIS Server. The environment is composed of a modern responsive front-end written in JavaScript on top of ReactJS and OpenLayers, and several server-side Python/Flask micro-services to enhance the basic functionalities of QWC2 and QGIS Server.\n\nQWC2 is modular and extensible, and provides both an off-the-shelf web application and a development framework: you can start simple and easy with the demo application, and then customize your application at will, based on your needs and development capabilities.\n\nThis talk aims at introducing this application and to show how easy it is to publish your own QGIS projects on the web. An overview of the QWC2 architecture will also be given. It will also be an opportunity to discover the last new features that have been developed in the past year and ideas for future improvements.",
        "description": ""
      },
      {
        "title": "Integrating Earth Observation Data for Enhanced Health Response Systems: The EODCtHRS component of HARMONIZE Project",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The lack of an integrated understanding of the connections between extreme weather events, environmental degradation, socioeconomic disparities, and their impacts on infectious disease outbreaks heightens the risk of disease spread. This issue is particularly critical in Latin America and the Caribbean (LAC) region, where vulnerable communities have been more frequently affected by these events. The HARMONIZE project goal is to create digital toolkits that stakeholders in climate change hotspots can use to combine data about the environment, climate and health cost-effectively to monitor and send out alerts about a set of diseases that are affected by its effect.\n\nThis talk will give an overview of the Earth Observation Data Cube tuned for Health Response Systems (EODCtHRS), an HARMONIZE Project component. The EODCtHRS presents a technical-scientific proposal termed HARMONIZE Instance composed of back/front-end solutions developed using free and open-source software for integration and interoperability between specific sets of health, environmental and climate data and the digital infrastructure of the Brazil Data Cube (BDC) project of the National Institute for Space Research (INPE). \n\nThe development of this proposal was divided into four working streams, Drone, Health, Climate, and Data Science Environment modules. Furthermore, we developed a custom version of the web platform for data visualization and analysis of these sources based on BDC Explorer 3.0 (https://brazildatacube.dpi.inpe.br/portal/explore), which presents improved capabilities for discovering, visualizing, and downloading data cubes from remote sensing images (https://brazildatacube.dpi.inpe.br/harmonize/dev/portal/explore). An Harmonize Instance ALPHA Version has been generated.\n\nThe core background of this platform is the SpatioTemporal Asset Catalog (STAC) specification which defines a way to store and search data using spatial and temporal operations. The STAC enables the harmonization of data from different sources and maintains interoperability between all system parts. The solution utilizes a suite of technologies from  Python and R environments in addition to PostgreSQL/PostGIS and GeoServer needed to store and publish data collections.\n\nBelow we present a brief description of each working stream:\n\nModule 1 - Drone image: The main goal of drone image integration in the context of EODCtHRS is to provide a data infrastructure that meets the demands of health surveillance, especially in areas considered hotspots of climate change. Consequently, we started exploring the integration of the images generated by fieldwork campaigns in some locations of Pará State. The processing of these images is based on auxiliary information (course angle and flight height) and EXIF and TIFF metadata tags to support the conversion of the raw images into Cloud Optimized GeoTIFF (COG) files ideal for integration with STAC specification implemented by BDC infrastructure. Besides that, mosaics were created using the OpenDroneMap application.  The Alpha version of these data collections (scenes/mosaics) has been published as layers with GeoServer and associated metadata available in STAC catalogs.\n\nModule 2 - Health data: This module integrates health data for the EODCtHRS, including information from different stakeholders, mainly Fiocruz's Health Information Laboratory (LIS) and the InfoDengue initiative. Both projects produce health indicators, considering the impacts of environmental and climate change on the Brazilian population. The module also covers the development of two main packages.\n\nThe first, called EODCtHRS Health Indicator Processing (EHIPR), was developed in Python to obtain health indicators from CSV and Parquet files, aggregate them spatially and temporally, spatialize them and accommodate them on the HARMONIZE platform . Second, called EODCtHRS Data PUblisher (EDPU), is a package developed in Python to publish the HARMONIZE datasets as a layer in GeoServer and its metadata in STAC Catalog to make available at the HARMONIZE Explorer. All sources of data (drone images, climate and health indicators) used the EDPU package to publish the ALPHA version of collections produced in the context of the HARMONIZE project. \n\nModule 3 - Climate data: This module integrates climatological data for EODCtHRS, enabling direct query execution via access interfaces, and eliminating the need for data transfer. Within the project's scope, we consider products produced by Fiocruz team from the Copernicus Climate Change Service (C3S), which the European Centre implements for Medium-Range Weather Forecasts (ECMWF) ERA5-Land reanalysis dataset and available by the Center for Weather Forecasting and Climate Studies (CPTEC/INPE): SAMeT and MERGE.\n \nThis module developed the EODCtHRS R Climate Processing Package (rclimpr) to generate climate indicators. The rclimpr uses scripts to extract indicators like temperature and precipitation from netCDF files through spatial and temporal aggregations (epidemiological weeks and months). It outputs raster files in COG format and vector formats like GeoJSON and Shapefile, providing suitable data formats for analysis and visualization.\n\nModule 4 - The Geospatial Data Science Environment (BDC-Lab) aims to provide a set of geospatial data analysis tools integrated with BDC data, avoiding the necessity to download large amounts of Earth Observation data and allowing researchers to produce deep analysis using tools such as RStudio, QGIS, Metview, VSCode and Jupyter Notebooks with several R and Python geospatial libraries pre-installed. Currently, it is in an experimental phase, where some users are testing its functionalities and providing feedback for its improvement. \n\nThis talk proposal presents an overview of a software environment developed to harmonize Earth observation, environmental, climate, and health data aiming to provide ways to visualize, analyze, monitor, and alert for spreading diseases in climate change hotspots in LAC region. The development of the HARMONIZE Instance has demonstrated the utility of geoservices and technologies, with standard infrastructure and protocols, as an effective way to harmonize different data formats from diverse data sources in the health context.\n\nThe HARMONIZE project is financed by the Wellcome Trust (https://wellcome.org) grant number 224694/Z/21/Z, through the  Foundation for Scientific and Technological Development In Health (FIOTEC)  ID Project: ICICT-002-FEX-22 and coordinated by Prof. Rachel Lowe leader of the Global Health Resilience Team in the Earth Sciences Department from  Barcelona Supercomputing Center (BSC).",
        "description": ""
      },
      {
        "title": "Speckle: your geospatial & 3d data hub",
        "type": "Talk",
        "track": "Open Data",
        "abstract": "As geospatial data users and developers, we often encounter industry standards for data formats, posing challenges for collaborative projects with sectors like Architecture, Engineering, and Construction, which have diverse data formats and conventions.\n\nProfessionals in these fields often struggle with transferring data between QGIS, Rhino, Revit, Grasshopper, and other platforms and web services. Speckle, an open source platform simplifies data and model exchange between urban design, architecture, and engineering software, fostering collaboration and automation; it enables this to all happen as a result of a single click! In this presentation, we will share simple workflows using Speckle  to unlock the power of your GIS data.\n\nSpeckle's open nature serves multiple purposes:\n\n- GIS Users: Focus on your tasks and avoid technical data misalignments. Align data location-wise and interoperably, and collaborate in real-time via an interactive web interface with a 3D viewer.\n\n- Developers: Extract real-time data insights, automate workflows and data checks, and build custom apps and integrations using Speckle's infrastructure (3D viewer, SDKs, data access, and authentication).\n- Managers: Gain full control of your data with location- and provider-agnostic server setups, custom data access and permissions, time efficiency gains, change tracking, and dashboards.\n\nSpeckle's flexible schema facilitates easy conversion to and from other native formats and enables  data querying. A recent update includes integration with the OGC API standard, making Speckle data accessible to any client using the OGC API without extra plugins or scripts. This means that a master plan drawn by an architect in AutoCAD can be instantly mapped as a WFS layer via Speckle.\n\nJoin us this December at FOSS4G to learn more! For those interested in exploring further, please visit our GitHub repository.",
        "description": ""
      },
      {
        "title": "The Geospatial Data Science Certificate for High School students that uses FOSS4G tools and Project Based Learning (PBL) techniques to solve spatial problems",
        "type": "Talk",
        "track": "Education",
        "abstract": "Reflections from a research study on GIS in secondary schools in South Africa resulted in the development of a collaborative certification for high school students. The study showed a need for more research into how GIS can be used more in secondary school pedagogy.  Results of a study done by the author on the status of GIS teaching in secondary schools in South Africa is investigated with the aim to determine if the use of Open Source software such as QGIS and open data such as OSM would facilitate the use of GIS as a teacher intervention. Teachers who participated in the study overwhelming agree that there are numerous benefits to using GIS in the classroom. They also expressed a keen willingness to attend GIS courses and learn more about FOSS4G tools. This study also showed how FOSS4G empowers teachers with the means to create exciting, real and relevant teaching content. A sample group evaluated how practical GIS lessons using QGIS and OSM can be used to teach geospatial skills.\nWhat this research study concluded is that comprehensive teacher training is required to make GIS practical lessons more effective in the classroom. This resulted in the development of a certified GIS course for teachers and the development of a coordinated Project Base Learning (PBL) task, which was named the Geospatial Data Science Certificate (GDSC). The GDSC is sponsored by Kartoza, endorsed by the Independent Examination Board (IEB), the Southern African Geography Teachers’ Association (SAGTA) and the University of Pretoria at a nominal cost to make it accessible to all students.  A pilot project was conducted over 2002 and launched in 2023 and the author believes it is ready to evolve into an international ‘Geolympiad’ whereby students can collaborate globally to solve real, pertinent issues.",
        "description": ""
      },
      {
        "title": "Exploring OpenData: dynamic rendering of OvertureMaps with GeoServer and WFS without local storage",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In an era where data is increasingly becoming open and accessible, leveraging this wealth of information effectively is crucial. Our presentation, \"Exploring OpenData: dynamic rendering of OvertureMaps with GeoServer and WFS without local storage,\" showcases an innovative proof of concept that addresses this need.\n\nThis project demonstrates how to efficiently consume and dynamically render geospatial data from OvertureMaps without the necessity of local data storage. By utilizing a Web Feature Service (WFS) gateway, we access OvertureMaps data directly from its original source. This approach eliminates the need for intermediate data storage, reducing overhead and streamlining the process of rendering dynamic maps.\n\nThe integration of OvertureMaps with GeoServer through WFS enables real-time data rendering, allowing users to visualize up-to-date geographical information without having to handle large datasets themselves. This method not only enhances data accessibility but also optimizes performance by leveraging the capabilities of GeoServer to render data dynamically on-the-fly.\n\nOur presentation will delve into the technical aspects of implementing this proof of concept, discussing the integration process between OvertureMaps, WFS, and GeoServer.",
        "description": ""
      },
      {
        "title": "pgRouting, state of the project",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The UN OpenGIS initiative along with OSGeo Foundation organized the OSGeo UN Committee Educational Challenge where Challenge 2 was to create Workshop material for pgRouting. \n\nThe challenge supports the objectives of the OSGeo UN Committee, i.e. promoting the development and use of open-source software that meets UN needs and supports the aims of the UN. \n\npgRouting is not only useful for routing cars on roads but it can also be used to analyze water distribution networks, river flow or the connectivity of an electricity network, etc. \n\nCurrently, this presentation talks about pgRouting addressing one of the Seventeen Sustainable Development Goals of the UN.",
        "description": ""
      },
      {
        "title": "GeoNode Cloud: Your Geospatial Data in the Cloud",
        "type": "Sponsor",
        "track": "State of software",
        "abstract": "Discover the design and benefits of GeoNode Cloud, showcasing its scalability, robustness, and versatility in managing large volumes of geospatial data.",
        "description": ""
      },
      {
        "title": "FOSSGIS e.V. - how to run a successfull Local Chapter",
        "type": "Talk",
        "track": "Community & Foundation",
        "abstract": "FOSSGIS e.V. is already 24 years old and has come a long way. FOSSGIS e.V. is very successful. We want to share how we got to where we are.\nWe want to share our story with you and inspire you or give you ideas.\n\nWhat FOSSGIS e.V. does:\n- Runs an annual conference FOSSGIS & other smaller events\n- Networking between users and companies\n- Build a strong & vibrant community\n- Stimulate discussion in the working groups\n- Have fun - community sprints, mapping parties & hacking weekends\n- Supports community and projects\n- And even more\n\nIt would be great to connect and share with other chapters to spread the FOSS4G idea around the world.",
        "description": ""
      }
    ]
  },
  {
    "year": 2025,
    "conference_title": "FOSS4G 2025",
    "start_date": "2025-11-17",
    "end_date": "2025-11-23",
    "total_events": 308,
    "total_presentations": 265,
    "academic_presentations": 18,
    "non_academic_presentations": 265,
    "types": {
      "Talk": 161,
      "Lightning Talk": 48,
      "Workshop": 44,
      "Keynote": 4,
      "Community Day Event": 4,
      "AGM": 2,
      "B2B": 1,
      "Panel": 1
    },
    "tracks": {
      "Lightning talk": 48,
      "Use cases & applications": 37,
      "Community, Collaboration & Impact": 27,
      "Cloud, APIs & Data Infrastructure": 24,
      "Tools, Libraries & Visualisation": 22,
      "Workshop - Beginner Friendly": 21,
      "State of software": 21,
      "Workshop - Intermediate Level": 16,
      "AI, Data Science & Analytics": 15,
      "Desktop GIS & Data Collection": 14,
      "Workshop - Advanced Level": 7,
      "Associated Event": 5,
      "Keynote": 4,
      "AGM": 2,
      "B2B": 1,
      "Panel": 1
    },
    "presentations": [
      {
        "title": "QField & QFieldCloud: Hands-On Fieldwork Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Learn how to do fieldwork with QField and QFieldCloud, the fieldwork apps for QGIS.",
        "description": "QField is the field data collection app for QGIS. It is trusted over million times and used by hundreds of thousands of users every month. QFieldCloud is the synchronization and fieldwork management platform for QField and QGIS.\n\nThis workshop will introduce the basics of QField and QFieldCloud to achieve effortless fieldwork.\n\nWe will walk through the entire fieldwork process, including setting up your QGIS project, publishing the project via QFieldCloud, collecting data using the QField mobile app, and synchronizing field data back into your main dataset at the office.\n\nBasic knowledge of QGIS is desirable but not essential. Participants are asked to bring their own laptops and have the QField app pre-installed on their smartphone or tablet.\n\nThe app is available for Android, iOS, Windows, Linux and MacOS."
      },
      {
        "title": "International QField Day Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Join us for QField Day 2025—a half-day of insights, innovation, and community. Discover how QField streamlines fieldwork through powerful features, plugins, and real-world use cases across various professional sectors.",
        "description": "Join us for the international QField Day!\nDiscover the latest innovations and powerful enhancements in QField. Discover how it streamlines field data collection, simplifies workflows, and empowers professionals and organisations across various industries.\nWhether you're a long-time QField user, just getting started, or simply curious about cutting-edge field operations, this event is your chance to connect with the QField product team and gain valuable insights into how QField can elevate your projects.\nFrom rapid mapping to AI, plugins to community building—and even leading research efforts like the EU-funded egeniouss project—get ready for a half day of inspiration, knowledge sharing, and exploring how QField empowers people to map and understand the world, solving everyday tasks and global challenges alike."
      },
      {
        "title": "AGM - OSGeo Oceania",
        "type": "AGM",
        "track": "AGM",
        "abstract": "OSGeo Oceania Annual General Meeting",
        "description": "OSGeo Oceania Annual General Meeting"
      },
      {
        "title": "Cloud-Native Geospatial for Earth Observation Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "Explore cloud-native geospatial tools through this hands-on workshop utilizing Python Notebooks. This workshop introduces key concepts like STAC and COGs, and walks participants through real-world Earth observation analyses, empowering EO professionals to apply modern tools in their own work.",
        "description": "The advent of cloud computing has revolutionised the capabilities of researchers and professionals globally, helping them to access and analyse Earth observation (EO) data more easily than ever. Despite the well-understood tools and technologies, such as cloud-optimised GeoTIFFs and the Spatio-Temporal Asset Catalog (STAC) and STAC API specifications, many EO professionals have not yet had the opportunity to practically apply these innovations. This workshop aims to bridge that gap by showcasing how cloud-native geospatial technologies simplify the process of working with EO data, using Python as the primary programming language.\nIn part one of the workshop, we’ll give an introduction to cloud native geospatial, and then participants will get hands-on coding an Earth observation data science notebook from scratch, loading and visualising Landsat data.\nIn part two, we’ll talk about Al Gore’s vision for a digital earth, and how we’re on the path to realising that vision, and then we’ll delve into a real-world case study focused on documenting land productivity metrics, a crucial component for monitoring the UN Sustainable Development Goal (SDG) indicators for 15.3.1, also using Landsat data. Then we’ll shift over to another example of a long time series of sea-surface temperature data, accessed from Source Coop, before concluding with a discussion session.\nThroughout the workshop, participants will gain hands-on experience and insights into how cloud-native geospatial technologies have significantly enhanced the ability to access and analyze large volumes of EO data. By the end of the session, attendees will have acquired practical examples and knowledge to further develop their skills in this innovative field."
      },
      {
        "title": "QGIS Graphical Modeler: Build Smarter Workflows with Algorithms and Expressions Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Tired of repetitive GIS workflows that consume time and lead to errors?\nThis hands-on workshop will show you how to automate and streamline spatial data analysis using QGIS expressions, built-in geoprocessing algorithms, and the Graphical Modeler — without writing a single line of code.",
        "description": "🚀 What you'll learn:\n• How to use QGIS expressions to enhance vector and raster analysis\n• How to combine algorithms into automated workflows using the Graphical Modeler\n• How to process and analyze spatial data without scripting\n• How to work with real-world open datasets in a structured way\n\n🛠️ Workshop structure:\n1. Introduction to QGIS Graphical Modeler – we discuss the main elements and logic behind the tool\n2. Vector & Raster Analysis – Learn how to apply expressions and algorithms to perform meaningful spatial operations (e.g., selections, buffer, raster calculations).\n3. Workflow Design – Manually build step-by-step processing chains using QGIS tools and expressions.\n4. Model Automation – Use the Graphical Modeler to convert your workflow into a repeatable, parameterized model.\n\nBy the end of the session, you’ll have practical experience building robust, automated models that improve efficiency, reduce errors, and boost the quality of your spatial analyses — all within the QGIS environment."
      },
      {
        "title": "Diving into pygeoapi Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "pygeoapi is an OGC Reference Implementation supporting numerous OGC API specifications. This workshop will cover publishing geospatial data to the Web using pygeoapi in support of the suite of OGC API standards.",
        "description": "pygeoapi is an OGC Reference Implementation supporting numerous OGC API specifications. Lightweight, easy to deploy and cloud-ready, pygeoapi's architecture facilitates publishing datasets and processes from multiple data sources to the Web. This tutorial will cover publishing geospatial data to the Web, and using the API from QGIS, OWSLib and a web browser. The workshop will cover the following OGC API standards:\n\n- OGC API - Features\n- OGC API - Coverages (OACov)\n- OGC API - Maps (OAMaps)\n- OGC API - Tiles (OATiles)\n- OGC API - Processes (OAProc)\n- OGC API - Records (OARec)\n- OGC API - Environmental Data Retrieval (EDR)\n- SpatioTemporal Asset Catalog (STAC)\n\n**Requirements for the Attendees**\n\nPlease consult the workshop documentation at https://dive.pygeoapi.io, and ensure you are setup accordingly (https://dive.pygeoapi.io/setup) prior to attending the workshop.\n\nA Gitter channel exists at https://gitter.im/geopython/diving-into-pygeoapi for discussion and live support from the developers of the workshop.\n\nAs the installation of all dependencies on all platforms (Windows, Mac, Linux) can be quite involved and complex, this workshop provides all components within a Docker Image.\n\nThe core requirement is to have Docker and Docker Compose installed on the system. Once you have Docker and Docker Compose installed you will be able to install the workshop without any other dependencies."
      },
      {
        "title": "Doing Geospatial with Python",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "This workshop will provide an introduction to performing common GIS/geospatial tasks using Python geospatial tools such as OWSLib, Shapely, Fiona/Rasterio, and common geospatial libraries like GDAL, PROJ, pycsw, as well as other tools from the geopython toolchain.",
        "description": "With a low barrier to entry and large ecosystem of tools and libraries, Python is the lingua franca for geospatial development. Whether you are doing data acquisition, processing, publishing, integration or analysis, there is no shortage of solid Python tools to assist in your daily workflows.\n\nThis workshop will provide an introduction to performing common GIS/geospatial tasks using Python geospatial tools such as OWSLib, Shapely, Fiona/Rasterio, and common geospatial libraries like GDAL, PROJ, pycsw, as well as other tools from the geopython toolchain. Manipulate vector/raster data using Shapely, Fiona and Rasterio. Publish data and metadata to OGC web services using pygeoapi, pygeometa, pycsw, and more. Visualize your data on a map using Jupyter and Folium. Plus a few extras in between!\n\nThe workshop is provided using the Jupyter Notebook environment with Python 3.\n\n**Requirements for the Attendees**\n\nPlease see https://geopython.github.io/geopython-workshop for details on how to setup the workshop before you attend.\n\nA Gitter channel exists at https://gitter.im/geopython/geopython-workshop for discussion and live support from the developers of the workshop.\n\nThe workshop uses Jupyter Notebooks. Jupyter is an interactive development environment suitable for documenting and reproducing workflows using live code.\n\nAs the installation of all dependencies on all platforms (Windows, Mac, Linux) can be quite involved and complex, this workshop provides all components within a Docker Image.\n\nIn addition, geospatial web services like pygeoapi and pycsw in this workshop are provided by Docker images.\n\nThe core requirement is to have Docker and Docker Compose installed on the system. Once you have Docker and Docker Compose installed you will be able to install the workshop without any other dependencies."
      },
      {
        "title": "Introduction to GeoServer Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "This workshop will cover the basics of setting up a GeoServer instance and adding vector and raster data to it, and applying styles to the data to produce a completed web map.",
        "description": "GeoServer is a much loved open-source project and one of the most popular web mapping services in the world. This workshop provides a gentle hands-on introduction in setting up and enjoying GeoServer.\n\nThis workshop covers the advantages of using GeoServer; looking at the abilities of this open-source technology.\n\nThis session is a great way to get started, geared towards those with no prior open source experience. Familiarity with GIS concepts is recommended for attendees, and you are welcome to bring your own data.\n\nWe will start with a demonstration of GeoServer installation and touch on system requirements and installation of extensions\n\n* Hands-on publication of spatial data (vector, raster and database).\n* GeoServer styling and web mapping use\n* Preflight check-lists making sure your datasets, and web services, are ready for use."
      },
      {
        "title": "GeoTools Geospatial Introduction Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "Introduction to spatial concepts for the Java developer. This is a great programmer first introduction to everything from geometry to map features and styling.\n\nWorkshop updated with latest Java 17, ImageN, and CQL2 technologies.",
        "description": "Are you new to GeoSpatial? This GeoTools session is back by popular demand updated with Java 17 examples and the latest ImageN and CQL2 technologies. Offering a visual introduction for Java developers we will explore  how you can integrate GIS services into your next project.\n\nFor those new to the GeoSpatial scene we provide an introduction to spatial concepts and how to avoid common pitfalls.\n\nThe workshop offers a steady series of workbooks introducing:\n\n* Feature creation\n* JTS Geometry\n* Coordinate Reference Systems and Re-projection\n* Geospatial data and spatial queries\n* Accessing large format rasters\n* Rendering, cartography and styling\n* Raster Operations with ImageN\n\nCovering both the concepts and the science of map making the workbooks serve as an excellent reference, but the focus is always on you and the code you need to get the job done.\n\nThis is a hands-on workshop - so bring your Java IDE or command line and a sense of adventure."
      },
      {
        "title": "Microwave Image Processing: Exploring realms of Earth  through spaceborne Radars using Python Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "Synthetic Aperture Radar(SAR) is an Imaging Radar that acquires images of a particular area in the microwave region  and possesses  all-weather,all-time imaging capabilities.This workshop deals with the processing of SAR Images and how these images can be beneficial in a variety of geographical applications.",
        "description": "Intended Audience : The workshop will be aimed at the audience belonging to any level of education. It will introduce them to the wonderful class of SAR images and how are these images useful from the perspective of various applications.\n\nExpected Outcomes (after the workshop, the audience will be) :\n\ni) Able to understand the acquisition of SAR imagery.\n\nii) Able to understand the types of datasets utilized in remote sensing\n\niii) Able to use the GDAL library to perform operations on images\n\niv) Able to efficiently process SAR imagery using Python\n\n v) Able to draw a roadmap in order to utilize SAR imagery for various geographic applications\n\nOutline\n\nThe workshop will be divided into the following sub-sessions :\n\nSub-Session-1: Introduction to Microwave Remote Sensing (30 minutes) - This part will discuss the foundations of Microwave Remote Sensing. Theoretical aspects regarding the acquisition of images, the formation of images encompassing the generation of complex images and ground range detected images will be discussed.\n\nSub-Session-2: Pythonic Way to SAR Image Processing (150 minutes): This part will focus on achieving the following Key points:\n\n1) Basic utilization of GDAL, Numpy and Matplotlib Libraries for opening and visualizing Images(30 minutes)\n\n2) Codes will be developed separately for calibration for each SAR sensor(esp. Sentinel-1, Radarsat-2) from scratch.(45 minutes)\n\n3) Utilization of the codes developed in (2) for various applications such as Oceanography, Forestry, etc.(75 minutes)\n\nDatasets: Free Imagery data sets of Sentinel-1 SAR will be utilized. In addition, Sample Data sets of Radarsat-2, RISAT- 1 which are freely downloadable will be utilized.The sample datasets will be provided. For better understanding of the datasets, the participants may download and utilize the Sentinel-1 SAR free Image dataset initially .Sentinel-1 Free SAR Imagery(https://browser.dataspace.copernicus.eu/)\n\nConduct of the workshop : The workshop will be conducted through the means of Jupyter Notebooks. Along with the sessions, the audience will be provided with the exercises to clear their concepts of SAR Imagery.\n\nTotal Duration\n\nThe duration of the workshop will be  3 hours."
      },
      {
        "title": "Scalable Remote Sensing Workflows with Xarray Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "XArray is a powerful Python package for working with climate and earth observation datasets. This workshop will be give you a structured introduction to XArray with use-cases focused on remote sensing applications.",
        "description": "Xarray is an evolution of rasterio and is inspired by libraries like pandas to work with raster datasets. It is particularly suited for working with multi-dimensional time-series raster datasets. With the growing ecosystem of spatial extensions like rioxarray and xarray-spatial and built-in support for parallel computing with Dask, it has become the de-facto standard for working with large spatio-temporal raster datasets. This workshop will show you how you can use it to effectively process large volumes of earth observation data. We will cover the following topics:\n\n1. Computing Remote Sensing Indicies\n2. Cloud Masking\n3. Extracting Time-Series\n4. Calculating Zonal Statistics\n5. Analyzing Landcover Data\n\n\nPre-requisites:\n- Familiarity with Python programming and Remote Sensing datasets.\n- We will be using [Google Colab](https://colab.google/) as the computing environment for the workshop. Participants will need a Google Account to access the platform."
      },
      {
        "title": "Hands-on DGGS and OGC DGGS-API with DGGRID and pydggsapi Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "This hands-on workshop walks through a full DGGS data pipeline. You'll use the FOSS tool DGGRID to index geospatial data and then publish it using pydggsapi, a new open-source server for the OGC API DGGS standard. Leave with a running web service on your laptop.",
        "description": "Discrete Global Grid Systems (DGGS) are getting more attention, and with the new OGC API - DGGS standard released, it's a good time for the open-source community to get practical, hands-on experience. This workshop bridges the gap between the theory of DGGS and a working implementation.\n\nWe'll show you why DGGS are so useful for integrating and indexing different data sources and spatial data analysis without the usual headache of map projections. Then, we work through a complete, real-world data pipeline using FOSS tools.\n\nIn this workshop, you will:\n\n- take a standard geospatial file (like a GeoTIFF or GeoPackage).\n- use the command-line tool DGGRID (https://github.com/sahrk/DGGRID | https://dggrid.readthedocs.io/latest/ ), generate grids and index this data onto a hexagonal grid\n- we will introduce hierarchical indexing for ISEA3H and ISEA7H with the new Z3 and Z7 indexing systems in DGGRID\n- in the decond part, we set up and configure pydggsapi (https://github.com/LandscapeGeoinformatics/pydggsapi/ | https://pydggsapi.readthedocs.io/en/latest/), an open-source Python server that implements the newly (to be) released OGC API - DGGS standard.\n\n- we then point pydggsapi at the data you just created and launch the service\n- we explore several ways how to interact with your new web service using a browser, or curl/Python notebook, and maybe even QGIS, to make queries and retrieve data.\n\nWho should attend?\n\nThis workshop is for developers, data managers, and generally DGGS and geospatial enthusiasts who want to learn how to publish their data using this new paradigm.\n\nPrerequisites:\n\nParticipants should be comfortable with the command line and have a basic understanding of what geospatial raster and vector data are. We can use Docker or plain Miniconda/Micromamba/Pixi Python environments to ensure the dependencies are easy to set up for everyone. Bring your laptop (Win, Mac, Linux, *BSD might all work).\n\nBy the end, you will understand the concepts of DGGS-indexed data and will have built a functioning DGGS-based web service yourself."
      },
      {
        "title": "Cartography for Rebels: Building Beautiful Maps with Free Tools Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Learn to create a map from start to finish using QGIS and open data in this beginner-friendly workshop. No prior knowledge is required. By utilising open source software and data, participants gain hands-on experience in map creation while exploring cost-effective open-source tools.",
        "description": "In this workshop, participants will learn how to create a map from start to finish using open source software such as QGIS and freely available open data. No prior experience is required - the session is designed to be beginner-friendly and welcoming to all skill levels.\nBy working with open source tools and open data, participants will gain hands-on experience in map creation while exploring the power and flexibility of cost-effective, community-driven solutions.\nDuring the workshop, we’ll cover techniques such as generating hillshades from Digital Elevation Models (DEMs) and integrating them with other layers, including aerial or satellite imagery, to enhance map presentation. We’ll also explore data-driven symbology, rule-based labelling, and final steps to prepare the map for print."
      },
      {
        "title": "Let’s create Interactive Web Maps with the Open-Source WebGIS: Re:Earth Visualizer + Re:Earth CMS",
        "type": "Workshop",
        "track": "Workshop - Advanced Level",
        "abstract": "Learn how to create, publish, and manage interactive web maps using Re:Earth, a data platform with a low-code visualizer and content management system. This workshop is ideal for beginners and professionals looking to streamline geospatial communication on the web without programming.",
        "description": "Experience **Re:Earth Visualizer + CMS** in this hands-on workshop focused on creating and managing interactive web maps — all without writing code. Re:Earth is a data platform that combines a powerful visual map editor (Visualizer) with a flexible content management system (CMS), enabling users to build and publish dynamic maps directly from the browser.\n\nYou’ll learn how to design interactive maps, manage map layers and content through the CMS, and understand how the two components work together to streamline map publishing workflows. Whether you're new to GIS or looking for a modern alternative to traditional WebGIS tools, this session will provide a practical introduction to Re:Earth's low-code capabilities.\n\n### **What to Expect:**\n\n- **Web Map Creation**: Learn to build interactive maps using Re:Earth Visualizer\n- **Content Management**: Manage layers, properties, and metadata with Re:Earth CMS\n- **Integrated Workflow**: Understand how CMS and Visualizer work together\n\n### **Who Should Attend:**\n\nGIS users, educators, local government staff, and non-developers seeking a user-friendly, a data platform for building and managing interactive web maps — low coding required."
      },
      {
        "title": "QGIS PLUGIN DEVELOPMENT Workshop",
        "type": "Workshop",
        "track": "Workshop - Advanced Level",
        "abstract": "This hands-on workshop teaches the basics of QGIS plugin development with Python. Participants will learn to set up their environment, build functional tools, and extend QGIS capabilities. Ideal for GIS users and developers, the session includes a practical project where attendees create and test a simple plugin.",
        "description": "👥 Target Audience:\n\nGIS analysts, researchers, environmental scientists, developers and GIS enthusiasts\n\nAnyone with basic Python knowledge and interest in geospatial tools\n\n🎯 Learning Objectives:\n\nUnderstand the QGIS plugin architecture and how QGIS interacts with Python\nLearn to set up a development environment for plugin creation\nBuild and package a basic functional plugin\nUnderstand GUI design using Qt Designer\nUse PyQGIS API to access layers, features, and perform spatial tasks\nLearn tips for plugin debugging, deployment, and sharing via QGIS Plugin Repository"
      },
      {
        "title": "Build the Thing: A Hands-On Product Workshop for Geospatial Makers Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "Got an idea for a geospatial product? Bring it along (or just bring yourself) and turn ideas into action. This hands-on session is for anyone wanting to explore, shape, or progress a product idea—with guidance from an experienced geospatial product consultant.",
        "description": "This workshop is for anyone with an idea—or the spark of one—for a geospatial product, tool, or app. Whether you’re just starting out or already working on something, this open, supportive session will help you move forward.\n\nLed by Stella Blake-Kelly, a product consultant and founder of spatial studio Cartisan, the workshop will walk participants through key stages of product discovery and design, including:\n\t•\tIdeation and framing your product vision\n\t•\tUser and market research\n\t•\tMapping out functionality\n\t•\tDesigning wireframes or prototypes\n\nYou’ll have time to work independently or in small groups, get feedback and guidance tailored to where you’re at, and learn from others along the way.\n\nBring a laptop, your curiosity, and your creativity. By the end, you’ll come away with a better understanding of the product development process and, hopefully, momentum on your big idea.\n\nAll are welcome: open source experts, newbies, developers, mappers, hackers, and dreamers."
      },
      {
        "title": "Achieving Reproducibility in Geospatial Data Processing with Apache Airflow Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "In this workshop, participants will learn to automate geospatial data processing using Apache Airflow, focusing on designing workflows, integrating geospatial data, and leveraging tools/libraries like GeoPandas, GDAL, MinIO, and python debugger. The session will emphasize efficiency and reproducibility, enabling participants to create reliable and repeatable geospatial data pipelines.",
        "description": "Tools & Technologies\n- Apache Airflow (Workflow management)\n- GeoPandas / GDAL (Geospatial data processing libraries)\n- MinIO (S3 compatible object storage)\n- debugpy (Debugger for Python)\n- GeoNetwork (Geospatial data catalog)\n\n\nKey Takeaways\n- Deploy and manage services MinIO and Apache Airflow with docker container\n- Automate geospatial data processing workflows using Apache Airflow\n- Ensure reproducibility in geospatial data pipelines for consistent results\n- Store and manage geospatial data efficiently with MinIO\n- Debug and troubleshoot Python code in geospatial workflows using debugpy\n- Organize and catalog geospatial datasets with GeoNetwork (if time permits)"
      },
      {
        "title": "Building Geospatial AI Applications: From Data to Insights with Open Source Tools Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Combine deck.gl visualization, data processing, and AI APIs to create intelligent mapping applications. Build a real geospatial AI app from scratch, processing spatial data and generating insights through modern web technologies.",
        "description": "This hands-on workshop guides developers through building a complete geospatial AI application using modern open source tools. Participants will create a functional application that processes spatial data, generates AI-powered insights, and presents results through interactive visualizations - all running in the browser with no backend required.\n\n### Technology Stack\n\n- **Visualization**: deck.gl for high-performance geospatial rendering\n- **App development**: Vercel AI SDK \n- **AI Integration**: AI APIs for intelligent data analysis\n- **Cloud Analysis**: Using MCP to access data and run cloud analysis from CARTO platform\n- **Development**: React with TypeScript\n\n### Workshop Structure\n\n#### Hour 1: Data Processing Foundation\n- Project setup with React, TypeScript\n- Loading and processing geospatial datasets (GeoJSON, CSV)\n\n#### Hour 2: AI Integration\n- Connecting AI APIs for geospatial analysis\n- Prompt engineering for spatial data interpretation\n\n#### Hour 3: Visualization and Integration\n- Creating interactive maps with deck.gl\n- Connecting AI analysis to visual updates\n- Real-time user interactions and analysis\n- Hands-on: Complete geospatial AI dashboard\n\n### Requirements\n\n- JavaScript/TypeScript and React experience\n- Laptop with Node.js and modern browser\n- Anthropic API key\n\nParticipants will leave with a complete, functional geospatial AI application and reusable code patterns for building similar applications."
      },
      {
        "title": "Tile serving with MapLibre/Martin/Planetiler - base and overlays Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Create a tile server with the base map and some custom data. Build a web site with both the base map and custom data using MapLibre GL+Martin+PG+Planetiler+osm2pgsql+...",
        "description": "In this workshop we will generate base map tiles from OSM data using Planetiler, set up Martin tile server, set up nginx to serve our sample web site that will use MapLibre GL JS to show the map. Additionally (time permitting), we will add a PostgreSQL server, and will use osm2pgsql to import extra data from the same OSM dump, and do on-the-fly tile generation from PG.\n\nWhat topics do we plan to cover in your workshop? –\n* generating base maps\n* setting up postgres with data\n* generate overlay tiles on the fly\n* serving tiles\n* visualizing tiles with MapLibre\n* adding data layers\n\nSee https://github.com/maplibre/workshop?tab=readme-ov-file#pre-reqs"
      },
      {
        "title": "Exploring the ZOO-Project-DRU and OGC Application Package Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "This workshop will introduce participants to ZOO-Project-DRU, an open-source implementation of OGC API - Processes, with a focus on Part 2: Deploy, Replace, Undeploy and Common Workflow Language (CWL), used as an exchange format for OGC Application Packages, facilitating interoperability and portability of geospatial applications.",
        "description": "The OGC API - Processes standard provides a RESTful interface for executing geospatial processes, while ZOO-Project-DRU extends this capability with support for Part 2: Deploy, Replace, Undeploy, allowing users to dynamically manage geospatial services. This workshop will guide participants through the installation, configuration, and deployment of ZOO-Project-DRU, with a focus on automating geospatial workflows and encapsulating applications using Common Workflow Language (CWL)."
      },
      {
        "title": "Exploring Cloud-Native Geospatial Formats: Hands-on with Raster Data Workshop",
        "type": "Workshop",
        "track": "Workshop - Advanced Level",
        "abstract": "Dig into three cloud-native raster formats—COGs, Zarr, and Kerchunk—and learn how data access works under the hood with hands-on Python exercises, no image libraries required!",
        "description": "Ever wonder what GDAL is doing under the hood when you read a GeoTIFF file? Doubly so when the file is a Cloud-optimized GeoTIFF (COG) on a remote server somewhere? Have you been wondering what this new Zarr thing is all about and how it actually works? Then there's the whole Kerchunk/VirtualiZarr indexing to get cloud-native access for non-cloud-native data formats, what's that about?\n\nCloud-native geospatial is all the rage these days, and for good reason. As file sizes grow, layer counts increase, and analytical methods become more complex, the traditional download-to-the-desktop approach is quickly becoming untenable for many applications. It's no surprise then that users are turning to cloud-based tools such as Dask to scale out their analyses, or that traditional tooling is adopting new ways of finding and accessing data from cloud-based sources. But as we transition away from opening whole files to now grabbing ranges of bytes off remote servers it seems all the more important to understand exactly how cloud native data formats actually store data and what tools are doing to access it.\n\nThis workshop aims to dig into how cloud-native geospatial data formats are enabling new operational paradigms, with a particular focus on raster data formats. We'll start on the surface by surveying the current cloud-native geospatial landscape to gain an understanding of why cloud native is important and how it is being used, including:\n\n* the core tenets of cloud-native geospatial data formats\n* cloud-native data formats for both raster and non-raster geospatial data\n* the intersection with SpatioTemporal Asset Catalogs (STAC) and how higher-level STAC-based tooling can leverage cloud-native formats for efficient raster data access processing of cloud-native data\n\nThen we'll get hands-on and go deep to build up an in-depth understanding of how cloud native raster formats work. We'll examine the COG format and read a COG from a cloud source by hand using just Python, progressively grabbing data from the image until we can extract a target tile, all without using any image libraries. We'll repeat the same exercise for geospatial data in Zarr format to see how that compares to our experience with COGs. Lastly we'll turn our attention to Kerchunk/VirtualiZarr to see how these technologies might allow us to better optimize data access with non-cloud-native formats.\n\n#### Prerequisites\n\nThis workshop expects some familiarity with geospatial programming in Python. Most of the notebook code is already provided, so any gaps in understanding don't necessarily prohibit completing the exercises. That said, a basic knowledge of STAC and Cloud-Native Geospatial Python tooling and working with rasters as single and multidimensional arrays is quite helpful.\n\nA good primer workshop is Alex Leith of Auspatious's [Cloud-Native Geospatial for Earth Observation Workshop](https://github.com/auspatious/cloud-native-geospatial-eo-workshop). It is recommended to work through those activities or have an equivalent knowledge prior to working through the notebooks in this workshop."
      },
      {
        "title": "Advanced PostGIS: Beyond the basics. Workshop",
        "type": "Workshop",
        "track": "Workshop - Advanced Level",
        "abstract": "Going beyond all the common functions that most folks use PostGIS for. Delving into advanced use cases and using PostGIS in production workloads.",
        "description": "PostGIS, sitting on top of Postgresql, is by most metrics, the most popular spatial database. Many videos are online about how to install and use PostGIS....many are from prior FOSS4G conferences and do a good job in getting you to understand the basics. Most of them, however, only scratch the surface when it comes to the power that can be wielded with PostGIS.\n\nThis workshop seeks to explore a wide array of functions that may be used on a regular basis or are outside the scope of common spatial queries.. These include, but are not limited to:\n- Linear Referencing\n- Clustering\n- Rasters analytics\n- Vector Tiles\n\nThere will also be an emphasis on exposing PostGIS data and functions to the web, to this end, there will be some usage of other software products (for example: Martin Vector tile server or pg_featureserv or pg_tileserv or PostgREST). Consideration will also be given to the ecosystem around PostGIS (For example: ogr_fdw)"
      },
      {
        "title": "Getting Sentinel Data within Seconds with STAC Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Discover how to access and analyze Sentinel satellite imagery in seconds using STAC APIs and Microsoft Planetary Computer. In this hands-on workshop, you'll use Python to fetch data, calculate vegetation indices, and build efficient Earth observation workflows.",
        "description": "This hands-on workshop introduces a modern and efficient way to access and analyze Sentinel satellite data using STAC (SpatioTemporal Asset Catalog) APIs and the Microsoft Planetary Computer. You'll learn how to build scalable Earth observation workflows in Python—without downloading massive datasets manually.\nWe’ll begin with an overview of STAC and the role of the Microsoft Planetary Computer as a cloud-native source for open geospatial data. Participants will learn how to search and access Sentinel-2 and Sentinel-1 imagery based on time, location, and cloud coverage—directly within Python using libraries like pystac-client, odc-stac, and xarray.\nDuring the workshop, you'll:\n- Search and preview Sentinel datasets using STAC\n- Fetch cloud-hosted imagery from Microsoft Planetary Computer\n- Visualize bands and calculate vegetation indices like NDVI, EVI, and RVI\n- Perform pixel-level analysis with minimal compute time\n- Build efficient, reproducible workflows using Jupyter notebooks\n\nBy the end, you’ll have a working pipeline to go from area of interest to actionable insights in minutes—ideal for environmental monitoring, agriculture, and forest analysis. This workshop is perfect for developers, remote sensing analysts, or GIS professionals looking to simplify and accelerate their satellite data workflows."
      },
      {
        "title": "FILM EVENT: I Am the River, the River Is Me",
        "type": "Talk",
        "track": "Associated Event",
        "abstract": "A canoe trip down the Whanganui River in New Zealand, led by a Māori elder, awakens spiritual belief and practice, and becomes a call to action to draw closer to nature and fight climate change through a fundamental value shift.",
        "description": "The Whanganui River in Aotearoa/New Zealand is the first river in the world to be recognized as a legal person, as a living and indivisible being.\n\nMāori river guardian Ned Tapa invites a First Nations Elder from Australia and his daughter, who are activists dedicated to saving their own dying river back home, on a five-day canoe trip down this sacred river. Joining them are Ned’s friends, his family, an international film crew and Ned’s dog Jimmy.\n\nFor the Māori, the Whanganui is a living being – their ancestor. This belief has been institutionalized by New Zealand law as of 2017. Granting the river legal personhood is a way of environmental protection for the river, and as a way of legally validating the Māori worldview.\n\nThe film is an invitation to experience these values: of thinking about our relationship to the world around us – to above all the natural world – as one of intergenerational care and guardianship rather than just ownership/use/extraction.\n\nThe film is the result of a four-year long collaboration with the Māori community in Whanganui.\n\nhttps://iamtheriver.org/"
      },
      {
        "title": "B2B",
        "type": "B2B",
        "track": "B2B",
        "abstract": "Placeholder for B2B Event at FOSS4G 2025",
        "description": "Placeholder for B2B Event at FOSS4G 2025"
      },
      {
        "title": "Mapping with Mini UAVs -- UAV build session Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "The barrier to entry for building and flying small (sub 250g) drones for mapping has flown low enough to step over. Join this workshop to assemble a small mapping drone equipped with flight planning software and a good camera for mapping.",
        "description": "The barrier to entry for building and flying small (sub 250g) drones for mapping has flown low enough to step over. Join this workshop to assemble a small mapping drone equipped with flight planning software and a good camera for mapping.\n\nLearn about existing, e.g. Drone Tasking Manager -- DroneTM, and forthcoming drone mapping tools, e.g. the Community Mapping Drone. We are now mapping entire cities at 5cm resolution with small drones, allowing local pilots to map as well, and often better, than commercial satellite providers.\n\nJoin us outside the workshop, regulations, weather, and logistics willing, for some small demonstration mapping flights.\n\n10 years after that fateful build session at FOSS4G in Seoul, let's do it again! This time, the drones are much easier to build, much smaller and lighter, and the mapping results, that much more satisfying. It should be a fun workshop at the intersections of sophisticated / easy-to-use photogrammetry, community coordinated mapping, 3D modeling, easy hardware builds"
      },
      {
        "title": "Open Data, Open Source, Open Standard: Quickly build your digital twin city with mago3D Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Open Data, Open Source, Open Standard: Quickly build your digital twin city with mago3D",
        "description": "This workshop provides participants with the experience of creating a digital twin city by utilizing open data from the region where FOSS4G is held.\n\nParticipants will collect open data, transform it using open-source tools (mago3DTiler, mago3DTerrainer), and experience the full workflow of serving the data according to OGC (Open Geospatial Consortium) standards.\n\nThey will visualize city buildings and terrain in 3D, overlaying road layers and satellite imagery to construct a realistic urban environment. Additionally, participants will transform and overlay point cloud data, and then plant trees in the city.\n\nThis entire process relies on open data, open source, and open standards, allowing participants to experience the remarkable creation of a digital twin city from scratch within just three hours on an unprepared PC.\n\nThis work is supported by the Korea Agency for Infrastructure Technology Advancement(KAIA) grant funded by the Ministry of Land, Infrastructure and Transport (Grant RS-2022-00143336, NTIS Grant: 2610000396)\n\nThis work is supported by the Korea Agency for Infrastructure Technology Advancement(KAIA) grant funded by the Ministry of Land, Infrastructure and Transport (Grant RS-2025-02317649, NTIS Grant: 2610000447)"
      },
      {
        "title": "Introduction to QField plugin authoring Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "For a little over a year now, QGIS' best field companion QField has gained a plugin framework that allows users to expand the capabilities of QField through QML and Javascript. This workshop introduces the framework and goes through practical examples aimed at empowering the participants into writing their own plugins.",
        "description": "The workshop will introduce participants to QField’s plugin framework and its two main plugin types: app-wide plugins and project-scoped plugins. We will look into the decision-making around settling on Javascript/QML as the scripting language and look at its strengths.\n\nWe will then go through several practical plugin building examples that will cover:\nIntegration with online REST API endpoints;\nGeoreferenced visual map canvas overlays in QML language\nFeature creation and iteration via plugin\nCustomization of QField user interface\n\nThe workshop will also provide participants with resources to further increase their knowledge beyond the workshop session itself."
      },
      {
        "title": "The Deep Magic of QGIS Cartography Workshop",
        "type": "Workshop",
        "track": "Workshop - Advanced Level",
        "abstract": "This QGIS workshop will raise your cartographic skills from \"meh\" to \"magical\". Co-presented by Mathieu Pellerin and Nyall Dawson, these two developers/cartographers are directly responsible for many of the cartographic features in QGIS, and together share over 3 decades experience in designing beautiful maps.",
        "description": "QGIS is packed with powerful tools just waiting to be discovered—tools that can take your cartography to the next level. Why settle for a plain map when you can wow your audience?\n\nIn this workshop we'll be exploring the cartographic features in QGIS that you didn't even know existed. We'll stop along the way to discuss our recommendations for pretty mapping, and what we can learn from our favourite cartographers.\n\nA moderate level of QGIS experience is recommended. You need to be familiar with handling layers, basic styling and comfortable finding your way around QGIS. (We've got a lot to cover, so don't have time to cover the fundamentals!)"
      },
      {
        "title": "FOSS4G with .NET: A Hands-On Introduction for Spatial Developers Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "The .net-platform is gaining ground in the open-source community but is it for us “spatial-junkies”?\nIn this workshop we touch on the subject, going through possibilities and limitations of using .net in foss4g.\nDatabase connections\nUsing existing spatial servers\nBuilding your own spatial server\nFrontend interactive maps",
        "description": "Is .NET Ready for the Spatial World?\nIn 2014, Microsoft announced that the .NET Core platform would become open source. Since then, it has steadily gained traction within the open-source community. But how well does it fit within the FOSS4G (Free and Open Source Software for Geospatial) ecosystem?\nThis workshop aims to explore exactly that—bridging the powerful, modern capabilities of the .NET platform with the rich, well-established tools in the OSGeo stack.\nWhy Consider .NET for Geospatial Projects?\n\nThere are several reasons why you might choose the .NET platform for geospatial applications:\n* Legacy integration: You may be working in an enterprise environment where .NET systems already exist, and you're looking to add spatial functionality.\n* Greenfield projects: You may be considering .NET for its modern tooling, performance, or cross-platform capabilities using .NET 8+.\n* C# language benefits: C# is expressive, type-safe, and comes with excellent async/await support—useful for scalable geospatial applications.\n* Strong IDEs: With Visual Studio and Visual Studio Code, .NET developers benefit from some of the best development environments available.\n\nThis workshop is for anyone curious about using .NET as a viable toolset in geospatial development—whether for new applications or enhancing existing systems.\nWorkshop Structure\nThe workshop will consist of three short, focused programming examples. Each one introduces a different layer of the technology stack—from database interaction to frontend mapping.\n1)Database Access with PostGIS Using Npgsql\nWe’ll start at the data layer, using the Npgsql library to connect a C# application to a PostgreSQL/PostGIS database.\nTopics include:\nConnecting to PostGIS with connection strings\nExecuting spatial queries\nReading and writing geometry types using NetTopologySuite.IO.PostGIS\nMapping results to C# objects\n\n2)Handling Spatial Data in the Backend with NetTopologySuite\nNext, we move to the backend. The NetTopologySuite library provides robust spatial capabilities similar to JTS (Java Topology Suite).\nTopics include:\nRepresenting geometries such as points, polygons, and linestrings\nPerforming spatial operations (e.g., intersection, buffering, distance)\nBuilding spatial APIs using ASP.NET Core\n\n3)Serving Frontend Maps\nFinally, we bring everything to life in the browser. We will demonstrate how to use Blazor (a .NET-based frontend framework) alongside OpenLayers to create an interactive web map.\nTopics include:\nServing GeoJSON data via Web API\nDisplaying spatial data in OpenLayers/Leaflet in a .net-context.\nAdding interactivity: popups, hover info, and basic layer control\n\nThis combination lets you stay within the C# ecosystem across the full stack while integrating with powerful open-source geospatial libraries.\n\nWho Should Attend?\nThis workshop is ideal for:\n.NET developers new to the FOSS4G ecosystem who want to understand how to integrate spatial capabilities\n\nProject leaders or software architects evaluating .NET for geospatial projects\n\nGeospatial professionals who find themselves moving into or collaborating with teams using the .NET platform\n\nWhether you're an experienced backend developer or a GIS expert exploring new toolchains, this session will offer practical, real-world insights.\n\nPrerequisites\nAttendees should have:\n* Basic familiarity with C# and the .NET platform\n* Experience using an IDE such as Visual Studio or Visual Studio Code\n* A general understanding of geospatial concepts (e.g., coordinates, layers, spatial queries)\n\nKeywords\n.NET, PostGIS, Npgsql, NetTopologySuite, OpenLayers, Leaflet,C#, FOSS4G, spatial API"
      },
      {
        "title": "Simulating Sustainable Urban Spaces on AWS Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "Extreme heat poses major threats to human and environmental health and safety. In this workshop, you use Amazon SageMaker AI and open data from Amazon Sustainability Data Initiative (ASDI) to uncover patterns in vegetation and temperature, understand risks to urban areas, and simulate solutions that reduce risk to communities.",
        "description": "Extreme heat poses major threats to human and environmental health and safety. In this workshop, you use Amazon SageMaker AI and open data from Amazon Sustainability Data Initiative (ASDI) to uncover patterns in vegetation and temperature, understand risks to urban areas, and simulate solutions that reduce risk to communities.\n\nIn particular, you will:\n- Investigate the relationship among temperature, vegetation, and other environmental factors (such as water and soil build-up) using Amazon SageMaker Studio \n- Gain hands-on experience working with geospatial data using AWS services\n- Learn about the Amazon Sustainability Data Initiative (ASDI)  to access open satellite imagery data (USGS Landsat)\n- Build and deploy a machine learning model to simulate more sustainable outcomes (less heat risk)\n\nThis workshop is aimed at individuals who want to learn how Artificial Intelligence and Machine Learning can help make predications based on open source data, individuals who want to experiment with the power of satellite imagery, as well as individuals who want to on-ramp to cloud-native geospatial workflows. No specific background knowledge is required. This workshop provides step-by-step instructions along with the code required to run each step. Participants can also elect to make additional, optional coding improvements."
      },
      {
        "title": "GeoServer 3 Developers Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "GeoServer is an amazing project, and an amazing project to work on!",
        "description": "Please attend this workshop to:\n\n* Get Started with the GeoServer 3 codebase\n* Orientation with a Tour of the GeoServer architecture\n* Introduction the service dispatch framework, including creating your own service\n* built chain and test facilities\n* Create a custom function for use with map styling\n* Create a custom process for use with style transformations and web processing service\n* Anatomy of a successful pull request\n\nAttendees will build their own GeoServer, learn a bit about how our community operates, and enjoy extending the base application. \n\nIf you are a developer looking to support GeoServer,  or join us for a sprint or bug-stomp, this workshop is great introduction. \n\nThis course features hands-on development.  We encourage and expect you to bring your favourite Java development environment.\n\nFor a good time with open source join GeoServer today!"
      },
      {
        "title": "Getting Started with GeoNetwork 5: A Hands-On Developer Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "A workshop to introduce the new Spring Boot architecture of GeoNetwork 5",
        "description": "GeoNetwork is the most popular and loved open-source catalog, trusted by organizations to deliver on their geospatial data management objectives. \nGeoNetwork 5 is the future version based on Spring Boot, which will introduce some architectural innovations for developers.\n\nIn this practical session we will show you what is boiling into the pot and you’ll learn how to:\n\n- Set up GeoNetwork 5 locally using Docker and Docker Compose alongside GeoNetwork 4.x for a hybrid test environment\n- Explore the new architecture, based on Spring Boot \n- Test core features such as OGC API Records and the new Formatter architecture, for output with real metadata examples\n- Navigate the codebase, understand how to contribute or where to start to build you custom application on GeoNetwork foundations\n\nJoin us to jump-start your GeoNetwork 5 developments and help shape the future of this open-source metadata platform!"
      },
      {
        "title": "Cloud-based Remote Sensing with QGIS and Google Earth Engine Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "This workshop will give you hands-on experience using new [Google Earth Engine Plugin for QGIS](https://github.com/gee-community/qgis-earthengine-plugin) to combine their deskto-based geospatial workflows with cloud-based datasets.",
        "description": "Google Earth Engine is a cloud-based platform that enables working with large-scale earth observation datasets effectively. The new [Google Earth Engine Plugin for QGIS](https://github.com/gee-community/qgis-earthengine-plugin) brings this power to the desktop and enables QGIS users to combine their geospatial workflows with cloud-based datasets. The workshop will cover the following topics\n\n* Installing and setting up the Google Earth Engine Plugin for QGIS\n* Exploring the Google Earth Engine data catalog\n* Streaming images from GEE\n* Downloading images from GEE\n* Creating a Processing Model to use data from GEE Data Catalog.\n* Creating Maps with QGIS Print Layout\n\nPre-requisites:\n\n* This workshop requires a Google Earth Engine account. Please follow our [step-by-step guide](https://courses.spatialthoughts.com/gee-sign-up.html) to obtain a free account."
      },
      {
        "title": "Participatory mapping field survey and computer lab: QField integration into machine learning landcover classification within Digital Earth Pacific. Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "The workshop will include a) field survey component where participants will be able to walk around Auckland to collect data points in QField for QGIS and b) a computer lab component where participants will use Digital Earth Pacific Python Notebooks to generate a land cover map.",
        "description": "FOSS4G abstracts:\nWorkshop abstract:\nParticipatory mapping field survey and computer lab: QField integration into machine learning landcover classification within Digital Earth Pacific.\n\n3-hour workshop\nThe objective of the workshop is to share a workflow to allow for field data, calibration, and validation of a land cover classification model output. \n\nLand cover classification:\nLand cover classification forms serves numerous functions including land cover accounting, monitoring land use changes, biodiversity and conservation monitoring, measurements of urban and agricultural expansion as well as forest inventories and national greenhouse gas inventories.  \n\nThis workshop will include two main components:\n1.\ta field survey component where participants will be able to walk around Auckland to collect data points in QField for QGIS. \n2.\ta computer lab component where participants will use Digital Earth Pacific Python Notebooks to generate a land cover map based on the data collected in their respective surveys.\nThese participatory mapping workflows enable users from a range of disciplinary backgrounds to contribute to land cover mapping outputs. These outputs may be used for a range of applications including land cover classification. \nThe learning outcomes of this workshop will include the following: \n1.\tParticipants will learn how to collect field data using QField \n2.\tParticipants will also be able to ingest this data into Digital Earth Pacific and through a Jupyter Notebook Environment\n3.\tParticipants will be able to build on introductory levels of Python programming knowledge. \nWithin QField and Python, participants will be making use of the following tools and libraries:\n \n\nQField workflows to be covered:\nPoint data collection\tCollection of points for different land cover classes\nTransects \tTransects \nPolygons\tCollecting polygon areas of interest\nAccuracy assessments\tEnsuring data collected is within set thresholds of horizontal accuracy\n\nPython libraries to be covered:\nPandas / Geopandas\tVector data analysis and plotting\nodc-geo\tWeb map plotting\nRasterio\tRaster data analysis and plotting\nodc.stac\tLoading satellite data through Digital Earth Pacific Spatiotemporal Asset Catalogues."
      },
      {
        "title": "Oxidize to Decarbonize. Building more sustainable geospatial processes with Rust Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Geospatial workflows can be surprisingly energy-intensive. This workshop introduces Rust as a fast, efficient alternative to interpreted languages like Python and R. Learn how Rust’s performance and safety make it ideal for sustainable geospatial analysis, and explore the eorst toolkit for spatial data, raster processing, and modelling.",
        "description": "Even though servers, data centers, and high-performance computers use a considerable amount of energy—and thus contribute significantly to carbon emissions, the environmental impact of computation is often overlooked. This is especially ironic in geospatial science, where much of the work aims to understand and protect natural resources. Remote sensing workflows can be resource intensive, and as data volumes and analysis complexity grows, so too do the computational demands. Efficiency becomes not just a practical issue, but an environmental concern. The tools we use may matter more than we think.\n \nThis workshop explores how programming language choice can meaningfully reduce the environmental cost of geospatial analysis. While interpreted languages like Python and R are the most popular choices due to their accessibility and rich ecosystems, they often fall short at scale. Their reliance on runtime interpretation and garbage collection often translates into longer compute times and higher energy usage, and thus generate more emissions. \n \nCompiled languages like Rust and C++ can offer an alternative path. These languages are significantly more efficient, often completing tasks faster and with less energy consumption. Rust in particular is emerging as a powerful option for scientific computing. It provides memory safety without a garbage collector, using a system of ownership and borrowing to manage memory at compile time. The result is software that is fast and reliable, with minimal runtime overhead. Unlike C or C++, Rust helps prevent common errors such as buffer overflows, null pointer dereferencing, and data races on parallel processes, without sacrifice in performance.\n \nRust’s growing adoption by major tech players, like Microsoft, AWS, Google, and even the Linux kernel project reflects its maturity and reliability. This support, highlight that Rust is more than an academic or niche choice, but a practical, long-term solution.\n \nThis workshop will introduce core Rust concepts relevant to geospatial analysis and explore the growing Rust geospatial ecosystem—including libraries for spatial data handling, raster processing, modelling, and machine learning.\n \nIn this hands-on session, we will explore Rust fundamentals and use eorst to compute cloud frequency over a two-year period from satellite Sentinel-2 imagery.\n \nThe eorst crate is an open-source Rust library designed to simplify and accelerate geospatial raster processing. Inspired by tools like Rasterio, RIOS, Dask, and Open Data Cube, eorst wraps complex geospatial operations in high-level abstractions, while preserving the performance benefits of systems programming. It minimizes the overhead of abstraction, letting developers focus on the science rather than the plumbing.\n \nOriginally developed to support the Spatial BioCondition framework for modeling ecosystem condition, eorst now underpins large-scale operational workflows for Queensland’s Department of the Environment, Tourism, Science and Innovation. The library's main features are:\n \nEfficient geospatial raster I/O\nOn-the-fly projection and alignment.\nTiling and parallel out-of-core processing.\nRaster point sampling.\nZonal statistics.\nMosaicking.\nBand math and time series operations.\nSTAC integration.\nXGBoost and LightGBM inference.\nOptional OpenCV image processing.\n \n \nRust also interoperates well with Python and R, making it a pragmatic choice for hybrid workflows and for teams gradually transitioning toward more efficient computation. As part of the session, we will also demonstrate how to create a simple Python wrapper using the pyo3 library, allowing Rust functionality to be accessed from Python.\n \nIf you are curious about producing more sustainable geospatial analysis, this session will be a practical starting point."
      },
      {
        "title": "Semantic Interoperability made easy with OGC Building Blocks Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "Learn how OGC Building Blocks allow you to use open standards to improve user experience",
        "description": "Hands on opportunity to build data schemas that self-document and deliver them using FOSS4G software.  Will include schema design, server and client options, and introduce you to the open source OGC Building Blocks for defining, testing, composing and publishing data and metadata that means something to end users.  Focuses on JSON, JSON-LD and Linked Data, GeoDCAT and STAC, and OGC API Processes and provenance tracing to make your data and processing chains understandable and trustworthy."
      },
      {
        "title": "Modelling Climate Risks Using NASA Earthdata Cloud & Python APIs Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "This workshop is built around guided computational analyses of two particular climate risk scenarios—wildfires & floods. The goal is to support independent geospatial explorations using publicly available data products from [NASA Earthdata Cloud](https://www.earthdata.nasa.gov/) and FOSS with Pythonic APIs on a cloud computing environment.",
        "description": "Predicting and managing environmental risks of various climate-related disasters—e.g., wildfires, drought, and floods—is challenging and critical worldwide. Part of the difficulty is that historical norms (e.g., from last century) for the frequency of such extreme climate events are no longer sufficient to infer the frequency of future disasters. These natural risks are intrinsically linked to the dynamic distributions—varying both temporally and spatially—of surface water, precipitation, vegetation, and land use. These distributions can be modelled for forecasting and analysis (enabling quantification of these environmental risks) using hundreds of petabytes of relevant Earth science data available through [NASA's Earthdata Cloud](https://www.earthdata.nasa.gov/). With the dramatic growth in the availability of such data, today's earth scientists benefit from a strong understanding of open science practices and of cloud-based data intensive computing that enable reproducibly analyzing and assessing changing risk profiles.\n\nThis workshop provides hands-on examples of using cloud-based infrastructure and data products from [NASA Earthdata Cloud](https://www.earthdata.nasa.gov/) for the analysis of environmental risk scenarios. This involves constructing quantitative estimates of changes in hydrological water mass balance over various defined geographical regions of interest and time windows. The goal is to build enough familiarity with generic cloud-based Jupyer/Python workflows and with remote-sensing data to enable adapting and remixing examples for other region-specific contexts. The workshop's design reinforces best practices of data-proximate computing and of reproducibility (as supported by NASA's [Open Science](https://science.nasa.gov/open-science-overview) and [Transform to Open Science (TOPS)](https://nasa.github.io/Transform-to-Open-Science/) initiatives).\n\nParticipants are expected be familiar with raster data and common geospatial data conventions. Ideally, they are comfortable using a shell or a command-line interface to interact with data & programs. They should also be comfortable using common scientific Python libraries (e.g., NumPy, Pandas) and related Python data structures (e.g., tuples, dicts, lists, NumPy arrays, Pandas dataframes). There is a brief overview of Xarray, Hvplot, & Geoviews; prior exposure to those Python libraries is useful but not mandatory. Prior experience using Jupyter notebooks and writing short snippets of Python code is helpful.\n\n**Approximate schedule**:\n\n+ *minute 0-19*: Introduction & Setup (logging in, configuring NASA Earthdata credentials)\n+ *minute 20-29*: Reminders about GIS prerequisites: coordinate systems, data formats (if required)\n+ *minute 30-49*: Overview of PyData tools for geographic data: [Rasterio](https://rasterio.readthedocs.io/en/stable/index.html) & [Xarray](https://docs.xarray.dev/en/stable/index.html) (if required)\n+ *minute 50-59*: Break\n+ *minute 60-79*: Overview of PyData visualisation tools: [Hvplot](https://hvplot.holoviz.org/) & [Geoviews](https://geoviews.org/) (if required)\n+ *minute 80-99*: Using NASA Earthdata Products (DIST, DWSx)\n+ *minute 100-109*: Using PyStac for retrieving data\n+ *minute 110-119*: Break\n+ *minute 120-144*: Case study: wildfires\n+ *minute 145-169*: Case study: flooding\n+ *minute 170-179*: Wrap-up\n\nThe workshop starts by getting participants logged into the cloud infrastructure and verifying their NASA Earthdata Cloud credentials. This is followed by a quick, non-comprehensive overview of GIS prerequisites and Python approaches to manipulating and visualizing geospatial data. The schedule above will be adapted to suit the audience needs (i.e., by increasing or decreading time allocated in each section as appropriate).\n\nThe hands-on case studies rely on the [OPERA (Observational Products for End-Users from Remote Sensing Analysis)](https://podaac.jpl.nasa.gov/OPERA) suite of data products; in particular, they use two particular categories of data products: [DSWx (Dynamic Surface Water Extent)](https://d2pn8kiwq2w21t.cloudfront.net/documents/ProductSpec_DSWX_URS309746.pdf) and [DIST (Land Surface Disturbance)](https://lpdaac.usgs.gov/documents/1766/OPERA_DIST_HLS_Product_Specification_V1.pdf). The workflows presented extend notebook examples drawn from the extensive [OPERA Applications repository](https://github.com/OPERA-Cal-Val/OPERA_Applications).\n\nThis workshop—co-developed by [MetaDocencia](https://www.metadocencia.org) & [2i2c](https://2i2c.org)—is part of NASA's [Open Science](https://science.nasa.gov/open-science-overview) and [Transform to Open Science (TOPS)](https://nasa.github.io/Transform-to-Open-Science/) initiatives. An important goal is to reinforce principles of reproducibility and open science-based workflows (as exemplified in TOPS OpenCore, the introductory suite of open science curricula including Open Science 101)."
      },
      {
        "title": "Running and Auto Scaling Geoserver and PostgreSQL/PostGIS without managing servers in the AWS cloud Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "This lab will introduce running GeoServer on AWS infrastructure using containerised deployment without needing to manage server infrastructure. GeoServer is an OGC compliant implementation of a number of open standards including Web Feature Service (WFS), Web Map Service (WMS), and Web Coverage Service (WCS).",
        "description": "The workshop will walk you through the steps required to launch the Geoserver standard docker distribution and host this on AWS Fargate. AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers.\nGeoServer can leverage a variety of data sources including PostgreSQL/PostGIS. PostGIS is a spatial database extender for the PostgreSQL object-relational database. With AWS support for PostgreSQL/PostGIS available in Aurora Serverless, we will explore connecting GeoServer to a PostgreSQL source to illustrate a multi-tier architecture. We will also explore scaling out the GeoServer web tier leveraging shared file system using Amazon Elastic File Service (EFS).\nThe intended audience will be Geoserver admins/users that are interested in running Geoserver on AWS in a highly available fashion with minimal server management needs.\nNOTE: You will need a Internet connected laptop with browser to access the AWS console environment."
      },
      {
        "title": "Exploring Cloud-Native Geospatial Formats: Hands-on with Vector Data Workshop",
        "type": "Workshop",
        "track": "Workshop - Advanced Level",
        "abstract": "Dig into geospatial vector formats—including GeoJSON, WKT/WKB, and cloud-native GeoParquet—using Python to see in detail how vector features are stored in each format and to understand what cloud-native means for vector data.",
        "description": "Cloud-native geospatial is all the rage these days, and for good reason. As file sizes grow, layer counts increase, and analytical methods become more complex, the traditional download-to-the-desktop approach is quickly becoming untenable for many applications. It's no surprise then that users are turning to cloud-based tools to scale out their analyses, or that traditional tooling is adopting new ways of finding and accessing data from cloud-based sources. But as we transition away from opening whole files to now grabbing ranges of bytes off remote servers it seems all the more important to understand exactly how cloud-native data formats actually store data and what tools are doing to access it.\n\nThis workshop aims to dig into how cloud-native geospatial data formats are enabling new operational paradigms, with a particular focus on vector data formats. Unlike its raster workshop counterpart, this workshop will be a bit more experimental. Vector data formats tend towards greater complexity than raster formats, so exactly how deep we get into which topics will be dependent on the audience’s interests and the time available. Broad themes to explore might include:\n\n* GeoJSON: what is it, what does it represent, and how it is not cloud-native\n* Well-Known Text/Binary (WKT/WKB): how these vector formats work and why they are important in GeoParquet\n* GeoParquet: how does parquet store data, how geo maps into that paradigm, and what it takes to read some subset of data from a parquet file\n* FlatGeoBuff: what is is, how it works, why it might be “more” cloud-native than GeoParquet\n* Practical considerations when using these formats\n\nThe content of this workshop aims to not only be theoretical: a strong goal is to be as hands-on with these formats as possible by working with them in Python without any specific geospatial format libraries. We’ll look at interacting with object storage directly, to pull down files and fragments and inspect them, to build up working understanding of what common higher-level tooling does under the hood and abstracts away from users.\n\n#### Prerequisites\n\nThis workshop expects some familiarity with geospatial programming in Python and a basic understanding of the vector data model and its utility. Most of the notebook code is already provided, so any gaps in understanding don't necessarily prohibit completing the exercises. That said, some knowledge of the geospatial vector formats and tooling is quite helpful."
      },
      {
        "title": "H-O-T-T-O-G-O: Mobile Apps That Support Disaster Response and Climate Resilience Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "HOTTOGO is a hands-on workshop introducing mobile mapping tools used in disaster response and climate resilience. Learn how to collect, navigate, and act on data using free, offline-ready apps built for fast, community-driven action—wherever you are, even without WiFi.",
        "description": "Disaster doesn’t wait for a stable internet connection—or for your laptop to boot up. That’s why the Humanitarian OpenStreetMap Team (HOT) and its global contributors have developed and adapted a suite of mobile-ready tools designed for field mappers, community organizers, and first responders.\n\nThis workshop introduces participants to the HOTTOGO Toolkit:\n\n🧭 MapSwipe – Tap and swipe to prioritize satellite imagery for mapping.\n\n📍 EveryDoor – Add points of interest or update OpenStreetMap data right from your phone.\n\n📝 KoboToolbox / ODK Collect – Build and deploy surveys to collect local knowledge, damages, needs, and more.\n\n🗺️ Organic Maps / OsmAnd – Navigate and visualize OSM data offline in disaster zones.\n\n✍️ Field Papers – Go analog with printed maps and draw-on mapping for truly offline areas.\n\n🧩 Mapillary / KartaView – Contribute street-level imagery from your mobile for detailed, up-to-date visuals.\n\nThe session will include:\n\nLive demos of each app\n\nReal-world use cases from climate-vulnerable and disaster-prone communities in Asia-Pacific\n\nHands-on mini-exercises\n\nTool selection guide: how to choose the right combo for your local mapping needs"
      },
      {
        "title": "Terra Draw - cross-platform drawing library for all map applications Workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "Terra Draw is a drawing library for cross-platform mapping libraries such as Maplibre, Leaflet, OpenLayers, Mapbox, Google Maps, ArcGIS. It brings advanced drawing features for all web map applications with a unified API. This workshop introduces you how you can develop drawing feature with Terra Draw in your application.",
        "description": "[Terra Draw](https://github.com/JamesLMilner/terra-draw) is developed and maintained by James Milner. The speaker is a author of maplibre-gl-terradraw that is Tarra Draw plugin for maplibre-gl-js. This workshop's proposed agenda includes two parts - presentation and hands-on:\n\nFirstly, introduction of Terra Draw will be delivered in order to let you understand what Terra Draw can bring to your map application.\n\nThe next part will be hands-on exercise. As an example of use of Terra Draw, the workshop will show you how you can integrate drawing feature with Maplibre GL JS. The agenda of exercise will be:\n\n- Installation and setup basic functionality of raw Terra Draw\n- Advanced functinalities of Terra Draw (layer stying, events, adding data, etc)\n- Quick introduction and tutorial of maplibre-gl-terradraw plugin\n\nEach participant is expected to bring a laptop computer installed in [NodeJS v22 LTS](https://nodejs.org) and [VSCode](https://code.visualstudio.com/) to exercise Terra Draw in own computer with provided sample codes.\nThe workshop will use Maplibre as an example, however participants can choose any mapping libraries such as Leaflet, OpenLayers if they prefer using it.\n\nIf time is allowed, the exercise will show you how you can integrate Terra Draw with different map libraries other than Maplibre. Terra Draw has a unified API, so it will be pretty easier for you to adapt it once you will be familiar."
      },
      {
        "title": "Collaborative Geospatial Workflows in Action: A Hands-On Alpha with Re:Earth Flow Workshop",
        "type": "Workshop",
        "track": "Workshop - Advanced Level",
        "abstract": "Try Re:Earth Flow’s visual, browser-native interface to clean, join, and export spatial data—no coding required. In this hands-on alpha workshop, you’ll collaborate live and help shape the future of the platform.",
        "description": "Experience **Re:Earth Flow** hands-on in this workshop where you’ll build and run geospatial data workflows entirely in your browser. Re:Earth Flow is a new open-source, visual ETL platform — currently in alpha — designed to make transforming spatial data more accessible, collaborative, and code-free.\n\nWorking with real datasets, you’ll walk through how to clean, filter, join, and export spatial data using our flow-based UI. You’ll also get a preview of how real-time collaboration works in the browser — with no installs, no command line, and no fuss. This is your chance to test the tool early, give feedback, and help shape its roadmap.\n\n### **What to Expect:**\n\n- **A Guided Walkthrough**: Learn how to build ETL workflows visually using Re:Earth Flow\n- **Real Datasets**: Import shapefiles or CSVs, perform spatial joins, and export to GeoJSON or 3D Tiles\n- **Live Collaboration**: Work with others in real time via our WebSocket-powered backend\n- **Open Feedback Loop**: Share what worked, what didn’t, and what you want to see next\n\n### **Who Should Attend:**\n\nGIS users, planners, analysts, and developers looking for a modern, collaborative way to transform spatial data — right from the browser. No coding experience required."
      },
      {
        "title": "pgRouting basic workshop",
        "type": "Workshop",
        "track": "Workshop - Intermediate Level",
        "abstract": "This basic workshops takes you by the hand to start using pgRouting's basic routing function`pgr_dijkstra`.",
        "description": "1. Prepare Data\n\n    1.1. Prepare the database\n    1.2. Get the Workshop Data\n    1.3. Upload data to the database\n    1.4. Chapter: Appendix\n\n2. Pedestrian Routing\n\n    2.1. pgr_dijkstra\n    2.2. pgr_dijkstraCost\n\n3. Vehicle Routing\n\n    3.1. Routing for vehicles\n    3.2. Cost manipulations\n\n4. Graph views\n\n    4.1. The graph requirements\n    4.2. pgr_extractVertices\n    4.3. pgr_connectedComponents\n    4.4. Preparing the graphs\n\n5. SQL function\n\n    5.1. The application requirements\n    5.2. Additional information handling\n    5.3. Geometry handling\n    5.4. Creating an SQL Function"
      },
      {
        "title": "Create and Customize Your Own 3D Web Maps with TerriaJS Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Learn how to build and brand your own 3D geospatial data explorer using TerriaJS - without writing code!",
        "description": "TerriaJS is an open-source framework for web-based geospatial catalogue explorers.\n\nIt uses Cesium and Leaflet to visualise 2D and 3D geospatial data, and it supports over 50 different Web APIs, file formats and open data portals.\n\nIt is almost entirely JavaScript in the browser, meaning it can even be deployed as a static website, making it simple and cheap to host.\n\nTerriaJS is used across the globe to create next-generation Digital Twin Platforms for open geospatial data discovery, visualisation and sharing - it is used to drive\n\n- [Digital Earth Australia Map](https://maps.dea.ga.gov.au/)\n- [Digital Earth Africa Map](https://maps.digitalearth.africa/)\n- [Pacific Map (Digital Earth Pacific)](https://map.pacificdata.org/)\n- [VIC Spatial Digital Twin](https://vic.digitaltwin.terria.io/) (Australian State Gov)\n- [Tokyo Digital Twin](https://info.tokyo-digitaltwin.metro.tokyo.lg.jp/)\n- and many others\n\nThis hands-on TerriaJS workshop will walk you through configuring maps, adding your own data, and applying custom branding using JSON configuration files. You’ll also learn how to publish your map on GitHub Pages, making it accessible to others with just a shareable link. Perfect for anyone wanting to present spatial data and stories, no programming skills required.\n\nParticipants should bring a laptop with a text editor (such as VSCode) and NodeJS installed. By the end, you’ll have your personalised TerriaJS map running locally and published online via GitHub Pages.\n\nFor more information about Terria:\n\n- https://terria.io/\n- https://github.com/TerriaJS/terriajs"
      },
      {
        "title": "Standalone Web Maps, No Platform Required",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Calling all researchers, land managers, and FOSS4G hobbyists who need a modern web map. Using just a Github account, you can make a web map that is simple and free to deploy, is totally customizable, can be integrated into your CMS or modern web stack.",
        "description": "In this workshop, we will walk through how to build a map inside a GitHub repository, hosted with GitHub Pages, and built entirely with open source tools. Attendees will vote up modules at the end, so that time permitting, we can address the most relevant ways to take things further.\n\nKey concepts / technologies:\n- [PMTiles](https://docs.protomaps.com/pmtiles/): because they work via HTTP, PMTiles (portable map tiles) do not require a map server to deliver tiled map data to your browser.\n- [Protomaps](https://protomaps.com/): is a global basemap stored in PMTiles that can be downloaded from a bounding box and customized to meet your needs.\n- [Overpass](https://overpass-turbo.eu/#): is an API and querying language for OpenStreetMap that can be used to extract select data you want to use to add context and fidelity by styling them within your basemap\n- [GitHub Pages](https://docs.github.com/en/pages): provide a built-in web hosting platform for any Github repo. Provided your repo is under 1 GB of data, achievable for most project or site-level maps, this is all you need to host your map\n- [Maplibre GL JS](https://maplibre.org/maplibre-gl-js/docs/): is an open source, vector tile map rendering library for Javascript. It supports PMTiles and can be used to control how your data from PMTiles and other sources is rendered and displayed.\n\nIndividual modules will include step-by-step instructions for how to:\n1. Set up a GitHub account\n2. Use Overpass to create an OpenStreetMap extract\n3. Define a bounding box to download reference layers from Protomaps\n4. Initialize GitHub Pages to display your map\n5. Use Maplibre to render your map\n6. Customize the stylesheet\n7. Add local fonts and sprites and reference them from your repo\n\nTime permitting and in order of priority, we will cover these additional topics:\n8. (Bonus) Interactivity enhancements (such as custom pop-ups)\n9. (Bonus) When and how to add Terrain (hillshade, contours)\n10. (Bonus) Migrating your tiles to AWS\n11. (Bonus) Create custom icons / spritesheet\n12. (Bonus) Combining with Typescript\n13. (Bonus) Combining with React"
      },
      {
        "title": "Building Spatial APIs in PostgreSQL with PostgREST Workshop",
        "type": "Workshop",
        "track": "Workshop - Beginner Friendly",
        "abstract": "Join this hands-on workshop to learn how to build fully functional spatial APIs using PostgreSQL, PostGIS, and PostgREST. We'll cover setup, data exposure, and using spatial functions—empowering you to build powerful, backend-free geospatial services using just SQL.",
        "description": "This workshop is a deep dive into building geospatial APIs directly from your PostgreSQL database using PostgREST and PostGIS—no backend frameworks required. You'll learn how to expose spatial data (like points, lines, and polygons) as RESTful endpoints, and perform geospatial operations such as distance queries, intersections, and bounding box filters using PostGIS functions.\nWe’ll begin by setting up PostgreSQL with PostGIS and configuring PostgREST to serve your database as a REST API. Then, we'll walk through creating tables, views, and SQL functions to handle various spatial use cases. You'll learn how to query spatial data through HTTP, return GeoJSON responses, and optimize queries for performance.\nParticipants will get hands-on experience with:\nInstalling and configuring PostgREST\nWorking with spatial data types and functions in PostGIS\nCreating and securing spatial endpoints\nBuilding and testing real-world use cases (e.g., find nearby features, filter by geometry)\nBy the end of the session, you’ll have a fully working geospatial API built entirely with SQL—ready for integration with frontend apps, dashboards, or GIS tools. Ideal for developers, GIS analysts, and anyone interested in modern spatial data APIs without the complexity of writing backend code."
      },
      {
        "title": "Māori Maps: ‘To the gate’ of intellectual belonging in Aotearoa",
        "type": "Keynote",
        "track": "Keynote",
        "abstract": "Over five years, and more than twenty thousand kilometres, our small team visited almost every ancestral Māori community of New Zealand to create the first ever map of ancestral marae. Our mission is to reconnect descendants with marae through our web platform – Maorimaps.com.",
        "description": "Over five years, and more than twenty thousand kilometres, our small team visited almost every ancestral Māori community of New Zealand to create the first ever map of ancestral marae. Our mission is to reconnect descendants with marae through our web platform – Maorimaps.com. Now 16 years in existence, Māori Maps now guides the 30 thousand (and growing) monthly visitors to the virtual gateway of 780-plus ancestral marae of Aotearoa.\n\nHirini’s keynote reflects on the genesis of Māori Maps and the founding ethic of “to the gate” that guided its creation. Grounded in the ritual of encounter (pōwhiri), “to the gate” embodies both a philosophical stance and a practical approach to protecting indigenous data sovereignty and intellectual belonging. This talk will examine how this principle shaped the project’s framework and continues to inform its responsibilities to marae communities. It will also consider the pathways in managing restricted and non-restricted knowledge—tapu and noa, public and private—in the context of digital platforms and “free and open source” environments."
      },
      {
        "title": "Keynote 2",
        "type": "Keynote",
        "track": "Keynote",
        "abstract": "The second keynote for FOSS4G 2025 Auckland",
        "description": "The second keynote for FOSS4G 2025 Auckland"
      },
      {
        "title": "QGIS Feature Frenzy",
        "type": "Talk",
        "track": "State of software",
        "abstract": "QGIS is packed full of incredible features! We'll run through a few of our favourites, including highlights from the last few releases, and take a look at what's about to be unleashed in QGIS 4.0.",
        "description": "QGIS is packed full of incredible features! You can work with this software for years, and still discover new, weird, and wonderful tricks on a regular basis. Every new release contains a raft of enhancements and whole new areas of functionality.\n\nIn this talk, Marco Bernasocchi (QGIS.org chairperson) and Nyall Dawson (QGIS software developer) will run through a few of their favourite features in QGIS, including highlights from the last few releases, and take a look at what's about to be unleashed in QGIS 4.0.\n\nWith an eye on the future, they'll talk about how you can contribute to QGIS, and what the future might hold for this community-driven open source project. \n\nStay around for the following 'Ask me anything' session with Marco and Nyall."
      },
      {
        "title": "QGIS \"Ask me Anything\" session",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "This session follows on from the QGIS \"Feature Frenzy\" talk and is your once-in-a-lifetime opportunity to ask Marco Bernasocchi (QGIS.org Chairperson) and Nyall Dawson (QGIS Core Contributor) anything about QGIS.",
        "description": "Start thinking about those burning questions that YOU want answered about QGIS. Quiz us about QGIS features, how the project is run, challenges and what the future holds. Don't hold back!"
      },
      {
        "title": "Is Zarr the new COG?",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Zarr is gaining traction in geospatial workflows—but is it replacing COG, complementing it, or something else entirely? We’ll unpack the formats’ shared foundations, explore their tradeoffs, and offer a path toward better community guidance, tooling, and support.",
        "description": "Cloud-Optimized GeoTIFF (COG) and Zarr have each earned their place in modern geospatial workflows. While often framed in opposition—raster vs. analysis, imagery vs. data cube—they are in fact deeply complementary. In this talk, we’ll unpack how they address similar challenges from different angles, and why they should be considered parts of a shared toolkit rather than competing paradigms.\n\nWe’ll highlight where Zarr and COG overlap, where they differ, and how decisions around chunking, compression, tiling strategies, and metadata design affect both formats. We'll discuss implementation pitfalls, emerging best practices, and the still-unanswered questions that data producers and tool builders face.\n\nMore than a comparison, this talk is a call to action: the community lacks clear guidance and consistent support for practitioners working to produce data in either format. We’ll highlight concrete gaps in the tooling landscape, share ideas from our own work on how to improve decision-making and best practices, and invite others to collaborate on building a healthier, more cooperative open geospatial data ecosystem."
      },
      {
        "title": "State of GeoServer",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GeoServer is a web service for publishing your geospatial data using industry standards for vector, raster and mapping, as well as to process data, either in batch or on the fly.",
        "description": "GeoServer powers a number of open source projects like GeoNode and geOrchestra and it is widely used throughout the world by organizations to manage, disseminate and analyze data at scale.\n\nThis presentation provides an update on our community as well as reviews of the new and noteworthy features for the latest releases. In particular, we will showcase all the new features landed in the 2.27 and 2.28 series.\n\nAttend this talk for a cheerful update on what is happening with this popular OSGeo project, whether you are an expert user, a developer, or simply curious what GeoServer can do for you"
      },
      {
        "title": "Introducing Re:Earth Visualizer – An Open-Source Data Platform Optimized for GIS Use",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Re:Earth Visualizer is an open-source WebGIS platform that enables the creation of interactive geospatial applications directly in the browser. It supports a wide range of formats including GeoJSON and 3D Tiles, and offers flexible customization through plugins.",
        "description": "Visualizer is an open-source, WebGIS-compatible data platform developed by the Eukarya team and built on Cesium. It enables users to create interactive geospatial applications entirely in the browser, with support for multiple data formats suitable for a wide range of use cases.\nThis session will introduce the following key features of Visualizer:\n\n### Intuitive No-Code Interface\nUsers can manage layers, style data, and build maps through a graphical user interface. Even those without GIS expertise can perform advanced spatial operations with ease.\n\n### Support for Multiple Geospatial Formats\nVisualizer natively supports formats such as GeoJSON, 3D Tiles, KML, and CSV. Existing datasets can be integrated seamlessly without conversion or custom code.\n\n### Transparency and Extensibility through Open Source\nThe platform is fully open-source and available on GitHub, allowing anyone to review the code, propose improvements, or customize it to fit their needs. It is well-suited for both public and enterprise applications.\n\n### Real-Time Extensibility via Plugin System\nA JavaScript-based plugin system enables dynamic feature extensions, including API integrations and interactive components, without modifying the core engine.\nVisualizer aims to democratize open data usage and geospatial visualization. It is designed to be user-friendly, extensible, and community-driven—supporting use across public institutions, educational organizations, private enterprises, and more."
      },
      {
        "title": "Modular, Interoperable, Cross-Language Geospatial libraries with GeoArrow",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "DuckDB, GDAL, and libraries like Lonboard can now efficiently share large vector data at low cost, thanks to GeoArrow. This talk will explain what GeoArrow is and how to get the best performance when sharing data between these libraries with practical examples.",
        "description": "[DuckDB](https://duckdb.org/), [GDAL](https://gdal.org/), libraries like [Lonboard](https://developmentseed.org/lonboard/latest/), and more can now efficiently share large vector data at low cost, thanks to [GeoArrow](https://geoarrow.org/), a binary representation for vector geometries that can be shared across libraries without any data copies.\n\nGeoArrow is a relatively low-level technology that tends to be unseen by end-users. This is great! Users just see performance improvements!\n\nFor example, GDAL 3.6 [introduced support](https://gdal.org/en/stable/development/rfc/rfc86_column_oriented_api.html) for exposing the data read by OGR’s vector drivers as GeoArrow. This [dramatically improved performance](https://gdal.org/en/stable/development/rfc/rfc86_column_oriented_api.html#benchmarks): reading from FlatGeobuf or GeoPackage files to a GeoPandas `GeoDataFrame` improved by **20x**.\n\nBut it can be useful to understand the factors behind what makes GeoArrow so performant. This talk will explain what GeoArrow is, how it differs from other new technologies like GeoParquet, and how to get the best performance when sharing data between these libraries with practical examples. It will also give a quick peek under the hood for how advanced users can create a cross-language library from C or Rust, but this talk will aim to be digestible for wide audiences; no deep technical prerequisites are expected."
      },
      {
        "title": "Introducing OSM Clubs in Elementary Schools Building Young Mappers for a Better Future",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "The initiative aims to launch OSM-based clubs in primary schools across Addis Ababa, promoting environmental awareness and digital literacy among children, and building a foundation for future YouthMappers through fun, educational, and community-centered activities.",
        "description": "Youth Mappers has successfully built a global network of university student chapters that contribute to mapping underserved communities through OpenStreetMap (OSM). While this movement has empowered youth in higher education to engage with geography, technology, and civic responsibility, there is a growing need to extend this opportunity to even younger students. This proposal advocates for the expansion of OSM based clubs into elementary schools in Addis Ababa, Ethiopia, to cultivate environmental awareness and digital mapping skills from an early age.\nChildren are naturally curious and observant of their surroundings. Starting OSM clubs at the elementary level will allow students to explore their neighborhoods, learn basic mapping concepts, and develop a sense of ownership and care for their environment. Through hands-on activities such as sketch mapping, local data collection, and digital storytelling, young students can begin to understand their communities in new ways. This not only enhances geographic literacy but also encourages early digital learning, problem-solving, and teamwork.\nThe proposed pilot program, called “Young Mappers Club,” will be launched in selected primary schools in Addis Ababa. The club will follow a simplified and age-appropriate curriculum that introduces key concepts of OpenStreetMap, environmental awareness, and community mapping. Activities will be designed to be fun, interactive, and aligned with existing educational goals. With support from local Youth Mappers university chapters, schoolteachers, and parents, the club will serve as a bridge between university-led initiatives and community-based learning.\nWe believe that empowering students at a young age helps develop a lifelong interest in their environment, technology, and civic responsibility. This initiative will create a pipeline of future Youth Mappers who are already familiar with OSM tools and values before they reach university level. It also contributes to a culture of participation and awareness within families and local communities.\nTo bring this vision to life, we are seeking support from the Youth Mappers and OSM community in the form of training materials, mentorship, and technical guidance. With collaboration and shared resources, Addis Ababa can become a model city for integrating OSM education into primary school systems. Let’s inspire the next generation of mappers, starting from the classroom."
      },
      {
        "title": "Tracking Trash: Mapping Marine Debris Using Earth and Ocean Observations",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Plastic pollution threatens Pacific ecosystems and livelihoods. The CleanSeas project uses Earth and ocean observation, machine learning, and on-ground validation to track marine debris across land, rivers, and sea. This presentation highlights DEP tools, showcasing how open data informs policy and enables targeted, cross-domain responses to pollution hotspots.",
        "description": "Plastic pollution and marine debris threaten marine ecosystems, fisheries, and livelihoods across the Pacific. But how do we track pollution pathways when they cross rivers, land, and sea? Enter the CleanSeas project; a collaborative effort leveraging Earth and ocean observation data to identify, track, and predict the movement of marine debris. This presentation dives into the methodology behind CleanSeas, including using SVM, Random Forest machine learning, the floating debris index and on-the-ground validation. We’ll demonstrate how open data and tools from Digital Earth Pacific are being applied to identify pollution hotspots and inform policy responses. The session emphasizes the power of EO in driving tangible outcomes in marine pollution mitigation and the growing importance of cross-domain integration in environmental monitoring."
      },
      {
        "title": "GPU-native Zarr: Optimizing data throughput for large-scale geospatial machine learning workflows",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Zarr is a cloud-native format, and now it can be GPU-native too! We address one of the main bottlenecks of geospatial machine learning, which is on the data loading stage. Let's see how we can read and decompress data from Zarr directly into GPU memory!",
        "description": "The Zarr format is used in the geosciences for storing time-series and multi-variate data, and was designed with parallel access in mind. Previously though, there was a bottleneck where data from storage disk had to go through an intermediate step of being loaded into CPU memory, before it is then copied to Graphical Processing Unit (GPU) memory. Now, with the proper [GPUDirect Storage (GDS)](https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html) drivers configured, one can read and decode uncompressed data from storage into CUDA GPU device memory directly via [`kvikIO`](https://docs.rapids.ai/api/kvikio/stable) for lower latency. To get even more throughput, compressed data in Zarr can be sent directly to GPU memory, and decompressed in parallel using [`nvCOMP`](https://developer.nvidia.com/nvcomp) that works faster than CPU-based decompression algorithms. The result is a fully GPU-native pipeline where I/O latency is minimized by sending compressed data, and GPU utilization is maximized by having it do most of the processing. We'll show some benchmarks comparing the speedups from a CPU baseline to a fully GPU-native workflow, and show some extra tips and tricks on how to use these technologies for your next geospatial machine learning project!"
      },
      {
        "title": "State of the eoAPI",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "**eoAPI** is an open-source toolkit for building scalable Earth Observation applications. We discuss the state of core components like **pgSTAC**, **TiTiler**, and **STAC-FastAPI**, introduce new tools like **stac-auth-proxy** and **stac-manager**, and highlight recent infrastructure work in **AWS CDK** and **Kubernetes** for deploying production-ready services.",
        "description": "In 2023, Development Seed launched the eoAPI project - a growing collection of open-source tools and infrastructure aimed at making it easier to build, deploy, and scale modern Earth Observation (EO) applications. In this talk, we’ll explore the current state of the ecosystem, highlight new developments, and share what’s coming next.\n\nWe’ll visit some of the core building blocks of eoAPI:\n\n- **[pgSTAC](https://github.com/stac-utils/pgstac)** - A performant, normalized STAC catalog backed by PostgreSQL\n- **[TiTiler](https://github.com/developmentseed/titiler)** - A dynamic tile server for Cloud Optimized GeoTIFFs and STAC Items\n- **[TiPg](https://github.com/developmentseed/tipg)** - A lightweight OGC API - Features implementation built on top of pgSTAC\n- **[STAC-FastAPI](https://github.com/stac-utils/stac-fastapi)** - A high-performance, pluggable STAC API built with FastAPI\n\nWe’ll showcase some exciting new additions:\n\n- **[stac-auth-proxy](https://github.com/developmentseed/stac-auth-proxy)** - A flexible FastAPI-based proxy for adding authentication and authorization to any STAC API\n- **[stac-manager](https://github.com/developmentseed/stac-manager)** - A tool for orchestrating STAC metadata ingestion, validation, and management across pipelines\n\nFinally, we’ll explore recent infrastructure efforts that support deploying and scaling these services:\n\n- **[eoAPI-CDK](https://github.com/developmentseed/eoapi-cdk)** - AWS CDK constructs for cloud-native eoAPI deployments\n- **[eoAPI-K8s](https://github.com/developmentseed/eoapi-k8s)** - Kubernetes Helm charts for containerized, production-grade deployments\n\nTogether, these tools form a modular, interoperable foundation for building next-generation EO platforms. Whether you’re running a small data portal or a high-scale STAC service, eoAPI provides the pieces to get you up and running securely and efficiently."
      },
      {
        "title": "Building a Business with Open Content and Open Source Software",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This talk will take you through my journey of building Spatial Thoughts and share insights into what it takes to build a education business powered by open content and open-source ethos.",
        "description": "This talk will cover my 5-year journey into building [Spatial Thoughts](https://spatialthoughts.com/) - a learning platform for modern geospatial technologies. Based on the idea that learning should be accessible to all, I made a conscious choice to make all of our learning content open-source and available under a very liberal license. Not monetizing the content turned out to be an important decision, and while it may seem counterintuitive, it was key in powering the growth of the company. Hope this talk will inspire you to consider alternative business models and adopt the ethos of open-source software in your work."
      },
      {
        "title": "IGEO7 and DGGRID - Like H3, but an Equal-Area Hexagonal DGGS for Fairer Global Analysis",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Uber H3 has revolutionized spatial indexing, but its cell sizes vary a lot. I will introduce IGEO7, an aperture 7 hexagonal equal-area DGGS with its hierarchical indexing system Z7. It is now implemented in the open-source software  DGGRID and has a handy Python wrapper, dggrid4py.",
        "description": "Uber's H3 has revolutionized spatial indexing, but its cells aren't equal-area, skewing global analyses and visualizations. What if you could have H3's elegant hierarchical indexing and true equal-area cells for statistically sound results?\n\nIGEO7 is a pure aperture 7 hexagonal DGGS with a hierarchical indexing system named Z7. It is implemented in the long-standing open-source DGGRID software and has a handy Python wrapper, dggrid4py.\n\n- https://dggrid.readthedocs.io/\n- https://github.com/sahrk/DGGRID\n- https://dggrid4py.readthedocs.io/\n- https://github.com/allixender/dggrid4py\n\nIn this talk, I'll shortly introduce IGEO7's capabilities and show how to use it with DGGRID and dggrid4py. Come see how you no longer have to choose between handy hexagon indexing and statistically sound analysis with open-source world."
      },
      {
        "title": "Portable CQL2: A Rust Core for Queries Everywhere",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Explore how [**cql2-rs**](https://developmentseed.org/cql2-rs/), a compact Rust library for working with OGC CQL2 expressions, powers validation, conversion, and evaluation across environments — from Rust to CLI, Python, and WebAssembly — and how writing small, reusable Rust utilities can extend impact across languages and platforms.",
        "description": "[CQL2](https://www.ogc.org/standards/cql2/) is a powerful and flexible query language designed by the OGC to support advanced filtering and subsetting of geospatial data, particularly for features and records. It enables expression of complex spatial, temporal, and logical conditions in both text and JSON forms.\n\n[**cql2-rs**](https://developmentseed.org/cql2-rs/) is a Rust library built to work with CQL2 expressions. It provides tools to validate expressions, convert between text and JSON formats, combine multiple expressions, simplify logical trees, and evaluate expressions against JSON input. It is intended as a lightweight and reusable core for working with CQL2 in any setting.\n\nBecause it’s written in Rust, `cql2-rs` can be exposed across many platforms and runtimes. Along with publishing a Rust crate for usage within the Rust ecosystem, we've published a [Python module](http://developmentseed.org/cql2-rs/latest/python/) via PyO3, a [command-line interface](https://developmentseed.org/cql2-rs/latest/cli/), and a [browser-based playground](https://developmentseed.org/cql2-rs/latest/playground/) using WebAssembly; all built from the same codebase.\n\nThis talk will highlight both the functionality of the library and the philosophy behind it: building small, focused Rust utilities that embrace composability and portability. Whether you're scripting, building APIs, or creating interactive tools, `cql2-rs` demonstrates how a single Rust core can power diverse workflows and tools."
      },
      {
        "title": "Measuring Soil Moisture with GPS Multipath and Open Source Software",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "The development and deployment of a low-cost, Raspberry Pi-based GNSS-IR system for field-scale soil moisture monitoring.",
        "description": "By repurposing position, navigation, and timing (PNT) signals as \"signals of opportunity\" for environmental sensing, Global Navigation Satellite System Interferometric Reflectometry (GNSS-IR) transforms navigation infrastructure into a distributed sensing network for snow depth, water surface height, and soil moisture. This presentation explains the development and deployment of a low-cost, Raspberry Pi-based GNSS-IR system for field-scale soil moisture monitoring.\n\nOur system integrates Commercial Off-The-Shelf (COTS) components including a Raspberry Pi, u-blox ZED-F9P multi-band GNSS receiver, and Waveshare 4G LTE modem within a custom-built weatherproof enclosure. The implementation relies exclusively on open-source software for system management, remote access, and data processing. \n\nThe GNSS-IR methodology analyzes interference patterns in the signal-to-noise ratio (SNR) of GNSS signals caused by multipath reflections from the ground. Changes in soil moisture alter the dielectric properties of the reflecting surface, creating detectable variations in these patterns that correlate with volumetric water content (VWC) of the soil. \n\nThis system was successfully deployed at Warkworth Space Center, New Zealand, in May 2025, where it is being used to validate data from the Rongowai and NASA CYGNSS sensing systems. The project demonstrates how cutting-edge geodesy research is now accessible to anyone, turning a simple GNSS receiver into a powerful scientific instrument with open-source software."
      },
      {
        "title": "The Pacific Geospatial Women Network",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "To provide geospatial mapping training and raise awareness on the use of geospatial tools to empower Pacific Women — including young women, women with disabilities, and women from outer islands  in mapping to access, utilize, and apply mapping resources for community development and decision-making.",
        "description": "The Pacific Geospatial Women Network (PGWN) is newly endorsed under the Oceans Management and Literacy Programme at the Pacific Community (SPC). This network reports to the Pacific Geospatial and Surveying Council (PGSC) and was established to promote women capacity in the field of Geospatial Science and Earth Observation (EO) in the Pacific. With the overarching goal of raising awareness, celebrating achievements, and most importantly, creating a support group for women in this field. \n\nIn 2024, the Pacific Geospatial Women Network (PGWN) successfully piloted its initiative with two local women’s groups in Fiji: the Gusunituba Women’s Group of Votua Village in Ba Province and the Daku Women’s Group in Daku Village, Tailevu Province. Both groups are actively engaged in mangrove rehabilitation and agricultural initiatives, supported under the Jobs for Nature funding program administered by Fiji’s Ministry of Economy. This pilot phase aimed to integrate geospatial capacity building into existing community-led environmental efforts. This work recognises that women and marginalised groups often face systemic barriers in accessing technology, technical training, and decision-making spaces. The initiative helps bridge the digital divide and promotes inclusive, equitable participation in climate resilience efforts.  by intentionally engaging women at different levels — including those from rural areas and with diverse abilities.\n\nGeospatial tools have become vital in addressing sustainable development challenges across the Pacific, and through its regional mandate, PGWN is working to ensure that women are not left behind in this digital and data-driven age. This includes access to hands-on learning, digital literacy, and the practical use of Earth Observation (EO) and mapping tools for decision-making, community planning, and environmental resilience. By equipping local women groups with these skills, PGWN is contributing to inclusive development efforts, ensuring that women are active participants in shaping their future and that of their communities.\n\nTo support these roles, PGWN is planning a tailored training and awareness programme for Kiribati Women in Mapping (KWIM) that promotes the integration of traditional knowledge with geospatial technology. This will not only empower local women but also support national and regional resilience efforts through inclusive data practices and adaptive community-led planning.\n\nAt the heart of this initiative lies the application of Geospatial Science and Earth Observation (EO). These tools allow for the collection, analysis, and visualization of spatial data, which is essential for mapping natural resources, land-use planning, and resource management. While the science is already well established in global contexts, the application in rural and local communities of the Pacific is still evolving.\nGeospatial technologies are being explored for community-led decision-making, particularly in the management of natural resources, waste disposal, and sustainable development. These tools enable local women to gather critical environmental data, empowering them to take an active role in their community's planning and development. \nWhile these technologies are powerful, their full potential is still being explored in Pacific contexts, especially for rural women who often have limited access to technical resources. PGWN aims to bridge this gap by providing capacity-building sessions and awareness campaigns to introduce geospatial tools to these communities. \nThe PGWN envisions further collaboration with universities, non-profits, and regional organizations to give additional opportunities to young women entering the geospatial field. One key initiative is creating internship opportunities for female graduates in geospatial science, offering them practical experience and industry exposure. By acting as a catalyst for women’s success in geospatial science, PGWN aims to inspire more women to take leadership roles within STEM fields across the Pacific. Through capacity-building efforts, mentorship, and collaborative projects, the network continues to grow, paving the way for a more inclusive and sustainable future for women in the Pacific"
      },
      {
        "title": "Digital Earth Machine Learning Operations",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Digital Earth is implementing a Machine Learning Operations proof of concept for an automated system designed to support development, training, and release of complex ML models for satellite imagery such as artificial surface detection. Talk will share architecture and learnings, focusing on integration of open-source components with cloud services.",
        "description": "Digital Earth (DE) is implementing a Machine Learning Operations proof-of-concept for an automated system designed to support the end-to-end development, training, and release of complex machine learning workflows and models for satellite imagery, such as artificial surface detection used in Land Cover. The PoC focuses on the discoverability of open-source machine learning models, streamlining data versioning, feature transformation, containerised model training, hyperparameter tuning, governance, model, and artefact management. The lightening talk will share our architecture and learnings, with a focus on the integration of various open-source components and specifications with cloud-services."
      },
      {
        "title": "Predicting Greenhouse Damage from Heavy Snowfall in South Korea Using FOSS4G",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "We are developing a system to mitigate the risks posed by heavy snowfall.\nTo achieve this, we are using remote sensing data from South Korea's radar sensor.\nOur backend comprises GDAL, PostGIS and GeoServer.\nThe front end uses OpenLayers.\nThis system will enable us to predict damage to greenhouses",
        "description": "."
      },
      {
        "title": "MapLibre - from data to tile rendering, in one status update",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Presenting everything MapLibre community has been working on, including tile serving, fonts and sprite handling, to visualizations for both web and native, to new types of tools and format standards.",
        "description": "This talk will cover all aspects of MapLibre efforts - the open source non-profit delivering the ubiquitous map rendering engine plus all tooling to convert data into interactive maps.  The engine is used by organizations of every size, from tiny one person sites to Meta, AWS, and Microsoft as their primary map rendering engine.  Come learn of the products we are developing, the new features we are excited about, the challenges and success, and the collaboration with the FOSS community and companies of all sizes."
      },
      {
        "title": "AI-Powered Wildfire Spread Prediction System Using Open Source Geospatial Technologies",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "An application case of combining open-source technology and AI to develop a Wildfire Spread Prediction platform that supports on-site decision-making.",
        "description": "The frequency and intensity of wildfires are increasing globally due to climate change impacts, and the Republic of Korea is no exception. Particularly during spring seasons, the combination of dry weather and strong winds leads to large-scale wildfires, resulting in national disasters.\n\nIn March 2025, simultaneous large-scale wildfires that broke out across South Korea were recorded as one of the worst in the nation’s history, burning approximately 100,000 hectares of forest an area 1.7 times the size of Seoul, the capital city. It lasted nine days before it was fully extinguished due to strong winds with maximum instantaneous wind speeds of over 25 m/s, making it extremely difficult to extinguish.\n\nTo effectively respond to these large-scale wildfires, which are difficult to predict and cause immense damage, it is essential to have technology that can analyze and predict their paths by receiving real-time data on the three key elements of wildfire spread: topography, fuel, and weather.\n\nAccordingly, we have developed a system that improves upon existing services based on empirical algorithms, precisely predicting wildfire spread using AI deep learning technology. The system processes and analyzes real-time data on the three elements of wildfire spread and utilizes open-source technology to predict, analyze, and visualize the path and speed of the fire.\n\nWe have actively utilized a proven open-source tech stack—including QGIS, PostGIS, GeoServer, OpenLayers, and CesiumJS—to implement an integrated development environment that covers everything from the analysis of prediction data to its 2D and 3D visualization.\n\nIn this presentation, we aim to introduce a practical case study where this technological foundation was used to effectively support on-site decision-making during wildfire events.\n\n---\n\nThis study was carried out with the support of 'R&D Program for Forest Science Technology '(Project No. \"RS\")' provided by Korea Forest Service(Korea Forestry Promotion Institute)."
      },
      {
        "title": "Mapping the Vine: Smart Vineyards with QGIS & QField",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "When wine passion meets GIS expertise, QField 🍇💻 becomes the foundation for a smart spatial vineyard. Discover the story of Jojo’s Vineyard: advanced QGIS symbologies, map themes, and field data collection revolutionize vineyard management. Is this the future of (Q)Wine—even in Aotearoa?",
        "description": "In this talk, discover the journey of Ian Beecher Jones—a passionate British winemaker and OPENGIS.ch who set out to create an open-source workflow enabling wineries to manage, report, plan, and monitor their operations entirely within QGIS, with QField as the backbone for field data collection. From mapping vineyard posts with precision to leveraging advanced symbology and map themes, Ian’s approach transforms how vineyards are visualized and organized. Assigning attributes, defining zones, and integrating spatial data unlock new possibilities for reporting, planning, and harvest optimization—all before a single grape is picked.\n\nCurious how knowing the exact location of every post and grape variety can revolutionize vineyard management? Join us to see how QGIS and QField blend seamlessly with the world of wine—and imagine what this could mean for vineyards in Aotearoa and beyond."
      },
      {
        "title": "Creating 3D Printed Landscape Models with FOSS",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "A behind the scenes look at how a kiwi cartographer uses QGIS, Blender, and Cura Slicer to turn elevation data into 3D printed landscape models.",
        "description": "This talk explores the workflow behind crafting detailed 3D printed landscape models using free and open-source software. From raw elevation data to tangible terrain, attendees will get an inside look at how QGIS is used to scope and prepare elevation datasets, how geometry nodes and modifiers are used to dynamically create 3D geometry in Blender, and how Cura Slicer is used to prepare the digital 3D model for printing.\n\nThis presentation draws from real-world projects by a New Zealand based cartographer working at the intersection of geospatial data and physical modelling. It will discuss challenges like terrain generalization, tile-based printing, and design for both aesthetic and functional outcomes. Whether you're into 3D printing, terrain visualisation, or just love seeing geospatial data come to life, this session offers practical insight and inspiration for mappers and makers alike."
      },
      {
        "title": "Workflow Automation with QGIS: Tips and Tricks",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "This talk will do a deep-dive into QGIS Processing Toolbox for building fast, automated and reproducible workflows. We will cover real-world use cases and share tips and tricks to help you leverage the no-code framework provided by QGIS.",
        "description": "The talk will feature real-world case studies showcasing use of the QGIS processing framework.\n\nQGIS processing framework offers Batch Processing, Model Designer, and the Command Line Utility as the no-code solutions for analysts looking to automate their work. These tools allow you to take hundreds of native and 3rd party processing algorithms and build workflows for spatial analysis and map publishing.\n\nThe talk will also cover advanced features that will make you more productive. Join this talk to learn new tricks to make the most out of the world's most popular open-source desktop GIS."
      },
      {
        "title": "Open Source Solution for Topographic Data Production",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "National Land Survey of Finland has developed Topograhic Data Production System mainly based on Open Source solutions. Most important components are PostGIS, QGIS and QField. The production started successfully in April 2025.  This is very exceptional use case how National Mapping Agency is updating Topographic database.",
        "description": "This talk will discuss\n- the main features of the new solution, especially data management, feature editing and quality tools\n- experiences during the development\n- experiences in the production"
      },
      {
        "title": "AI Coding and the Future of Open-Source Geospatial Software",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "AI may push open-source geospatial software toward fragmentation or stronger, well-governed cores. This talk looks at how AI now — and fully automated code generation in the future — could reshape collaboration, trust, and the enduring value of geospatial expertise.",
        "description": "AI-assisted programming is quickly changing how developers write, test, and maintain code. This talk looks at how AI coding tools could push the open-source ecosystem in two directions at once: making it easier than ever to generate new tools or forks, risking fragmentation and short-lived projects, while also creating strong incentives to build core software with higher quality and thorough documentation that AI agents can build on.\n\nWe’ll look at how AI programming is being used, what developers and maintainers think today, and why deep subject matter expertise — especially in the geospatial domain — is likely to stay vital even as routine coding work becomes easier to automate.  How can the FOSS4G community adapt to keep open-source geospatial software resilient and trusted in an AI-driven era?"
      },
      {
        "title": "deck.gl State of the Union 2025: Globe View, React Widgets, and WebGPU Readiness",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Comprehensive update on [deck.gl](https://deck.gl)'s latest developments from a core maintainer. Discover seamless [MapLibre globe integration](https://deck.gl/examples/maplibre), powerful widget system, WebGPU and more. Learn how these advances position deck.gl as the leading open-source framework for high-performance web geospatial visualization.",
        "description": "This State of the Union talk will showcase how deck.gl continues to evolve as the premier open-source solution for high-performance geospatial visualization on the web. Some highlights:\n\n### Globe View Integration with MapLibre\n\nThe most visually striking advancement is seamless integration with MapLibre's globe view. This collaboration between the deck.gl and MapLibre teams represents a major milestone for open-source geospatial visualization.\n\nThe `GlobeView` has been updated to match MapLibre's camera positioning at equivalent zoom levels, ensuring consistent user experiences. The `MapboxOverlay` component now works effortlessly with maplibre-gl globe maps, eliminating previous integration complexity. This advancement opens new possibilities for global-scale data visualization, from climate modeling to satellite imagery analysis.\n\n### Powerful UI Widget System\n\ndeck.gl's widget system provides developers with reusable UI components that integrate seamlessly with the visualization pipeline. Users can simply add widgets to their applications without being forced into any particular framework, while maintaining full compatibility with React applications when needed.\n\nThe comprehensive widget ecosystem includes ResetViewWidget for navigation controls, ScaleWidget for map scale indication, GeocoderWidget for address search integration, ScreenshotWidget for image export capabilities, LoadingWidget for data loading states, ThemeWidget for consistent styling, InfoWidget for contextual information display, and SplitterWidget for comparative visualizations."
      },
      {
        "title": "State of mago3DTiler & mago3DTerrainer: Open-Source Tools for Standards-Based Digital Twins!",
        "type": "Talk",
        "track": "State of software",
        "abstract": "I will present the current state of mago3DTiler and mago3DTerrainer.\n\nmago3DTiler is a Java-based OGC 3D Tiles creator that has gained wide adoption thanks to its flexibility, high performance, and extensive format support. \n\nmago3DTerrainer is a Java-based quantized-mesh terrain generator designed specifically for Cesium Terrain Tiles.",
        "description": "mago3DTiler (https://github.com/Gaia3D/mago-3d-tiler) is a Java-based OGC 3D Tiles creator that has gained wide adoption thanks to its flexibility, high performance, and extensive format support. It handles over ten 3D formats (CityGML, 3DS, OBJ, FBX, glTF, Collada DAE, IFC/BIM, LAS/LAZ, SHP) and features on-the-fly CRS conversion for seamless integration. It can convert 2D data into extruded 3D Tiles. Additionally mago3DTiler optimizes large point clouds and reality mesh data for smooth web visualization. \n\nmago3DTerrainer (https://github.com/Gaia3D/mago-3d-terrainer) is a Java-based quantized-mesh terrain generator designed specifically for Cesium Terrain Tiles. It efficiently converts GeoTIFFs into high-precision quantized-mesh data with customizable settings (depth range, tile size, interpolation method) and supports batch processing .\n\n1. mago3DTiler Key Features\n- On-the-fly CRS (Coordinate Reference System) transformation during tile generation\n- Extrusion of 2D features into 3D using height attributes\n- Support for massive point clouds (e.g., full-scale city level datasets)\n- Attribute handling for intensity and classification in point clouds\n- Photogrammetry (Reality Model) data conversion\n- Reality mesh data optimization with new triangle reduction and geometry simplification algorithms\n- Mesh quantization for improved rendering performance\n- Instance-based LOD implementation (e.g., trees and forest data rendering with scalable detail)\n- 3D Tiles 1.1 compatibility: unified GLB support across .b3dm, .i3dm, and .pnts\n- Tileset merging: generate parent tilesets from multiple input tilesets\n\n2. mago3DTerrainer Key Features\n- Easy and flexible conversion of GeoTIFFs into quantized mesh\n- High accuracy mesh generation\n- Batch conversion of multiple GeoTIFFs\n- Detailed customization options (e.g., tile depth, size, interpolation)\n- Priority handling for overlapping GeoTIFFs based on resolution\n- Enhanced support for large-scale GeoTIFFs and diverse CRS transformations\n- Large area conversion (e.g., verified conversion of nationwide DEM datasets)\n\nThese tools enable seamless integration of various spatial datasets into digital twin platforms based on OGC 3D Tiles and Quantized Mesh standards. Through continuous improvements and real-world applications, such as national digital twin projects and high-resolution environmental simulations, mago3DTiler and mago3DTerrainer are helping advance the open geospatial ecosystem.\n\nJoin this session to explore how open-source tools can simplify complex geospatial workflows and empower developers and users to build smarter, scalable 3D digital twins.\n\n<Acknowledgments> This work is supported by the Korea Agency for Infrastructure Technology Advancement(KAIA) grant funded by the Ministry of Land, Infrastructure and Transport (Grant RS-2025-02317649, NTIS Grant: 2610000447)"
      },
      {
        "title": "Earthquakes to Everyday: How an Open Geospatial Ecosystem Supports New Zealand’s Lifeline Infrastructure and National Resilience",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Born from Christchurch’s earthquake recovery, an open geospatial platform supports everyday coordination of lifeline infrastructure. Built on federated data, open access, and stewarded by a public-good foundation, it embeds resilience into daily planning and enables smarter, faster responses to natural hazards and infrastructure disruption.",
        "description": "In New Zealand, resilience isn’t optional. \n\nWith earthquakes, landslides, and storms a part of life here in New Zealand, building and maintaining resilient infrastructure is as much about foresight as it is about recovery. What if the tools we need in emergencies were already in everyday use? What if we didn’t have to build new systems during  a crisis because they were already in place? \n\nThat’s the idea behind a growing set of open, federated geospatial tools that are transforming how infrastructure is planned, built, and maintained across Aotearoa. \n\nThe 2011 Christchurch earthquakes fractured the city – literally and organisationally. As agencies mobilised to rebuild, coordination quickly became a critical issue. Who was working where? Which projects were clashing? How could a city be rebuilt efficiently when no one had the full picture? \n\nTo answer these questions, the National Forward Works Viewer (NFWV) was born – a shared map of all planned civil works. It helped teams from multiple organisations to coordinate timeframes, reduce clashes, and avoid rework. It was built fast, out of necessity – but it worked, and saved tens of millions in avoidable costs. From that crucible, the idea emerged that this shouldn’t be a one-time fix. It should be the new normal. \n\nToday, the NFWV is no longer a recovery tool. It’s used by over 500 organisations including councils, utility providers, and transport agencies – as part of their daily operations. Whether it's resurfacing roads, replacing pipes, laying fibre, or planning major events like marathons and parades, the Forward Works Viewer helps agencies see each other’s plans and work together. \n\nBut the NFWV is just the beginning.  \n\nIt’s part of a growing ecosystem of open, geospatial tools. Alongside the NFWV are two other national tools, which have been built not to hoard data – but to connect it. \n\nOne of these tools is the New Zealand Underground Asset Register (NZUAR), which brings visibility to what lies beneath our cities. Piloted in Wellington NZ, it federates subsurface utility data into a common schema giving planners and contractors visibility of underground pipes and cables. It helps prevent asset strikes, protects lives, and improves the information that asset owners receive from the field. Just like the NFWV, it operates on the principle that better decisions come from shared, trusted information – especially when it’s about what can’t be seen. \n\nThe third tool is the upcoming National Geospatial Catalogue, which federates critical contextual data – like ground conditions, contamination risks, heritage overlays, and more – into a searchable map layer available alongside the NFWV and NZUAR. Instead of scattered datasets across siloed agencies, the catalogue will create a nationwide view of risks and constraints that affect infrastructure delivery and resilience planning. \n\nTogether, these three tools – NFWV, NZUAR, and the Geospatial Catalogue – form a shared picture of digital infrastructure for Aotearoa. All are built on open-source geospatial software, chosen for its interoperability, transparency, and freedom from licensing barriers. But this openness isn’t just a technical choice – it’s a civic principle. public data, public tools, and public infrastructure should be available for public good. \n\nThat’s why these platforms are governed by the Digital Built Aotearoa Foundation (DBAF) – a charitable trust formed in 2023 to steward this ecosystem of open data. DBAF operates on a cost-recovery model, reinvesting any surplus into improving the tools, building new features, and keeping subscription costs low. Its independence means it can act as a neutral, non-commercial custodian of data shared between public and private actors – fostering collaboration, not competition. \n\nThe Foundation’s work has been recognised by the NZ Infrastructure Commission as addressing a problem of national importance - and now included on their Infrastructure Priorities Programme for further investigation. The digital ecosystem is also supporting the National Emergency Management Agency (NEMA) in the development of a pilot national electricity outage map to improve emergency readiness and response. These are not side projects – they are signs that this ecosystem is becoming core national infrastructure. \n\nAnd crucially, the model is scalable. None of the tools require centralised databases. Instead, they use data federation and schema alignment to map disparate datasets into a unified view. Organisations retain their own data and control, while contributing to a greater whole. This is what makes the system work – technically, politically, and culturally. \n\nThe map-based user-friendly design makes them accessible to all users – allowing organisations to spatialise their data, validate inputs, and generate insights without needing complex software or deep expertise. It brings the power of spatial analysis to non-spatial users, which in turn improves data quality and engagement across the board. \n\nNew Zealand’s story is a global one in disguise. Every country faces infrastructure challenges. Every community faces disasters, whether acute or slow-moving. But the lesson from Aotearoa is this: if we build the right tools for emergency response, and we embed them into everyday workflows, then we build resilience every day – even when the ground isn’t shaking. \n\nFor the FOSS4G community, this is a story of what’s possible when open-source meets public need. It’s about designing systems that are federated, open, and governed in the public interest. And it’s about recognising that disaster resilience is not a feature – it’s an outcome of everything we do. \n\nThis presentation will take the audience on the journey of how Digital Built Aotearoa’s open geospatial ecosystem evolved from emergency response into essential national infrastructure. It will explore why open principles, federated data, and public-good governance were critical design choices, and how these tools enable coordination across hundreds of organisations without centralising control. The presentation will also discuss the real-world technical and organisational challenges of building federated systems at national scale, including schema design, data standardisation, and stakeholder trust. Finally, it will reflect on New Zealand’s unique context of frequent natural disasters and how this shaped the foundational thesis: systems built for disaster recovery must be pre-embedded in everyday infrastructure planning to deliver true resilience. The underlying principles of openness, federation, and public stewardship, which are widely transferable, offer a model for other jurisdictions looking to improve infrastructure delivery, emergency readiness, and cross-sector collaboration."
      },
      {
        "title": "State of TerriaJS",
        "type": "Talk",
        "track": "State of software",
        "abstract": "TerriaJS is an open-source framework for web-based geospatial catalogue explorers. It uses Cesium and Leaflet, and it supports over 50 different Web APIs and file formats. In this talk, we will cover the background of TerriaJS, its current state, new features, and future plans for the project.",
        "description": "TerriaJS is an open-source framework for web-based geospatial catalogue explorers.\n\nIt uses Cesium and Leaflet to visualise 2D and 3D geospatial data, and it supports over 50 different Web APIs, file formats and open data portals.\n\nIt is almost entirely JavaScript in the browser, meaning it can even be deployed as a static website, making it simple and cheap to host.\n\nTerriaJS is used across the globe to create next-generation Digital Twin Platforms for open geospatial data discovery, visualisation and sharing - it is used to drive\n\n- [Digital Earth Australia Map](https://maps.dea.ga.gov.au/)\n- [Digital Earth Africa Map](https://maps.digitalearth.africa/)\n- [Pacific Map (Digital Earth Pacific)](https://map.pacificdata.org/)\n- [VIC Spatial Digital Twin](https://vic.digitaltwin.terria.io/) (Australian State Gov)\n- [Tokyo Digital Twin](https://info.tokyo-digitaltwin.metro.tokyo.lg.jp/)\n- and many others\n\nIn this talk, I will give:\n\n- Background information about TerriaJS and how it is used by the community\n- Current state of the project for users, developers and wider community\n- New features\n- Future plans!\n\nFor more information about Terria:\n\n- https://terria.io/\n- https://github.com/TerriaJS/terriajs"
      },
      {
        "title": "State of UN Smart Maps Group",
        "type": "Talk",
        "track": "State of software",
        "abstract": "UN Smart Maps Group is testing new technologies for future geospatial operations. This presentation showcase the latest advancements in the UNVT POD (Portable on Demand) and FOIL4G (Free and Open Information Library for Geospatial), integrating Tippecanoe, MapLibre GL JS, go-pmtiles, and Martin with Generative AI and classic UNIX utilities.",
        "description": "The UN Smart Maps Group advances geospatial innovation within the UN Open GIS Initiative as Domain Working Group 7. The group tests emerging technologies including Generative AI, Distributed Web, and IoT devices for future geospatial operations. This presentation showcases recent developments in UNVT POD and FOIL4G projects, demonstrating how FOSS4G tools integrate with cutting-edge technologies to democratize geospatial information access.\n\nThe group operates on three fundamental pillars: Openness, Collaboration, and Innovation. The organization embraces a philosophy of open by default, leveraging open-source technologies, open practices, and open communities to democratize access to geospatial information. Central to this approach is the recognition that AI has become an integral collaborator in the work, with the understanding that we must be prepared to welcome sufficiently advanced AI as collaborators in the near future.\n\nThe UN Vector Tile Toolkit Portable on Demand (UNVT POD) represents a paradigm shift in geospatial infrastructure deployment. Built on Raspberry Pi technology and utilizing FOSS4G tools including Tippecanoe for vector tile generation, MapLibre GL JS for visualization, and Martin for tile serving, UNVT POD creates map servers that operate independently of traditional internet infrastructure. Key capabilities include edge computing that enables high-speed response in remote areas, offline-first design that maintains functionality without network connectivity, solar power compatibility that supports sustainable operation in energy-limited environments, and community management systems that can be operated and maintained by local personnel without specialized technical knowledge. \n\nThe Free and Open Information Library for Geospatial (FOIL4G) integrates Generative AI capabilities with traditional UNIX tools and FOSS4G technologies to create an intelligent geospatial information processing system. \n\nThe group's exploration of Distributed Web (DWeb) technologies, particularly IPFS (InterPlanetary File System), addresses critical challenges in geospatial data sharing and preservation. This approach enables decentralized data storage and distribution, enhanced resilience against network disruptions, reduced bandwidth requirements for large geospatial datasets, and improved data sovereignty for local communities.\n\nThe technical solutions are built on cloud-native principles while maintaining the ability to operate in completely disconnected environments. The architecture emphasizes microservices design with modular components that can be deployed independently, API-first approach ensuring interoperability with existing systems, and GitOps methodology for transparent and collaborative development processes.\n\nThis presentation will demonstrate live deployments of technologies and invite the FOSS4G community to engage with initiatives. The combination of proven open-source tools with emerging technologies like AI and DWeb creates unprecedented opportunities for democratizing geospatial information access and empowering communities worldwide. The UN Smart Maps Group's work exemplifies how international organizations can leverage community-driven innovation to address global challenges while maintaining the open principles that make geospatial technology accessible to all."
      },
      {
        "title": "State of JICA Quick Mapping Project (QMP)",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Japan International Cooperation Agency (JICA) is implementing a co-creation, innovation and circulation initiative for updating its development cooperation by adding geospatial data on OpenStreetMap. This presentation will introduce the use of open source tools developed for OpenStreetMap to create maps useful for activities in JICA.",
        "description": "The Japan International Cooperation Agency (JICA) Quick Mapping Project (QMP) represents a initiative that transforms development cooperation through OpenStreetMap utilization. JICA is developing a new map procurement service model that delivers essential mapping solutions within 2 weeks to 2 months. This co-creation, innovation and circulation initiative renews how development agencies access geospatial information for urgent project needs.\n\nQMP's new approach treats mapping as a service rather than a lengthy project. This paradigm shift enables JICA to rapidly respond to urgent development needs by leveraging OpenStreetMap's collaborative platform and open-source tools specifically developed for OpenStreetMap data handling. The 2-week to 2-month delivery timeframe makes geospatial information accessible for time-sensitive development interventions, disaster response, and infrastructure planning.\n\nCentral to QMP's mission is the systematic enhancement of OpenStreetMap with development-relevant geospatial data. This approach involves strategic data addition through identifying gaps in OpenStreetMap and systematically filling them with development-focused information. Community participation ensures that all contributions work within OpenStreetMap's collaborative framework and align with community standards. Sustainability efforts focus on building local capacity to maintain and update contributed data over time.\n\nQMP utilizes a comprehensive suite of open-source tools specifically designed for OpenStreetMap data manipulation and visualization. Visualization and distribution utilize Tippecanoe for generating vector tiles from OpenStreetMap data, PMTiles for efficient serverless map tile distribution, MapLibre GL JS for creating interactive web maps from OSM data, Martin for serving vector tiles and providing robust map APIs, and MapLibre GL JS for lightweight map interfaces. \n\nThis presentation will demonstrate live workflows using the complete open-source toolchain, showcase real-world mapping outputs, and invite community collaboration on tool development and methodology refinement. QMP represents a model for leveraging OpenStreetMap in development cooperation, and the presentation seeks community input on technical innovations and partnership opportunities. QMP demonstrates that thoughtful integration of OpenStreetMap with development practice can create transformative outcomes for both the global mapping community and development effectiveness, proving that open data and open tools can drive meaningful social impact at scale."
      },
      {
        "title": "Cloud-native spatial data ecosystem in the rise of the National Geospatial Data Center of Timor-Leste",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "The National Center for Geospatial Data (CNDG) of Timor-Leste is implementing a spatial data ecosystem using cloud-native technologies in an experimental basis. Application of go-pmtiles, Tippecanoe, MapLibre GL JS, Martin, and GitHub Pages are introduced with some support from geospatial unit of Japan International Cooperation Agency (JICA).",
        "description": "The National Center for Geospatial Data (CNDG) of Timor-Leste is implementing a spatial data ecosystem using cloud-native technologies in an experimental basis. This project aims to enhance the accessibility and usability of geospatial data in Timor-Leste. The application of Free and Open Source Software for Geospatial (FOSS4G), such as go-pmtiles, Tippecanoe, MapLibre GL JS, and Martin, and developer platforms like GitHub will facilitate efficient geospatial data management and visualization, with support from the geospatial unit of Japan International Cooperation Agency (JICA).\n\nThe project is designed to address the current challenges faced by Timor-Leste in managing and utilizing geospatial data. By leveraging cloud-native technologies, CNDG aims to create a scalable and flexible ecosystem that can handle large volumes of data and provide real-time access to geospatial information by everyone, especially by the partner organizations in the Government of Timor-Leste, among other development partners for the country. The use of go-pmtiles and Tippecanoe will enable efficient data processing and storage, while MapLibre GL JS and Martin will provide powerful tools for data visualization and analysis. GitHub Pages will serve as a platform for sharing and collaborating on geospatial data and applications in a cost-effective manner.\n\nWith the support of JICA's geospatial unit and its consultant teams, CNDG is exploring innovative approaches to improve the quality and accessibility of geospatial data in Timor-Leste. This collaboration aims to build local capacity and foster sustainable development through the use of advanced geospatial technologies. The project is expected to have a significant impact on various sectors, including urban planning, environmental management, disaster response, and infrastructure development.\n\nCNDG has implemented LiDAR survey and spatial mapping of Timor-Leste since 2014. Based on the Decree law no. 68/2023 September 14th article 15, CNDG has the following responsibilities: collecting, organizing, managing, producing, and disseminating geospatial information base and thematic. CNDG maintains GNSS CORS network and conducted several UAV survey projects. Future plans include tide gauge projects. CNDG has an ongoing project with JICA in topographic map creation for Dili and surrounding area.\n\nFor example, go-pmtiles will be used to efficiently process and store large volumes of geospatial data, enabling quick access and retrieval of information. Tippecanoe will facilitate the creation of vector tiles, which are essential for high-performance map rendering. MapLibre GL JS will be employed to visualize geospatial data in an interactive and user-friendly manner, allowing users to explore and analyze the data effectively. Martin will provide a robust backend for serving geospatial data, ensuring reliable and scalable access to information. GitHub Pages will serve as a collaborative platform, enabling stakeholders to share and contribute to geospatial data and applications.\n\nThe project is expected to result in significant improvements in urban planning, environmental management, disaster response, and infrastructure development in Timor-Leste. By providing real-time access to accurate and up-to-date geospatial information, the project will enable better decision-making and resource allocation, ultimately contributing to the sustainable development of the region."
      },
      {
        "title": "Early Action Starts with Local Data, Role of OSM in Community Preparedness",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "In Tanzania, OMDTZ, CartONG, and  Emergency-Response Team built a community-led dashboard using OSM and local knowledge. Instead of high-tech sensors, they used phones, maps, and local insights to collect data. Session shows how anticipatory action starts with trusted community input, enabling faster, informed disaster preparedness and response.",
        "description": "In disaster prone urban areas like Dar es Salaam, acting early can save lives. But anticipatory action requires localized, trustworthy, and community validated data. This talk explores how OpenMap Development Tanzania (OMDTZ),in partnership with CartONG and in collaboration with the Dar es Salaam Mult Agency Emergency Team (DarMAERT), developed a community dashboard powered by OpenStreetMap (OSM) data to support local early warning and preparedness.\n\nInstead of relying solely on remote sensing or expensive early warning technology, this initiative focused on local knowledge and low cost open tools. Community members, especially youth and local leaders, were trained to map key risk features (flood hotspots, drainage points, safe shelters) using ODK tools.\n\nAt the core of the project is the DarMAERT Dashboard, a custom-built platform using OSM data to visualize high risk areas and inform decisions to rescue teams. It allows for, monitoring flood-prone zones with locally collected risk indicators and prioritizing emergency response based on exposure and vulnerability\n\nMore than 45 community members, including 15 local leaders and elders, were engaged in training. Their involvement ensured the data’s relevance and accuracy, while also building community ownership of the results. For example, ten-cell leaders and the elderly helped identify hidden flood-prone zones and accessible evacuation paths that were not evident in satellite imagery through participatory mapping \n\nThis presentation explores how local leaders, youth, and first responders collaborated to produce real time, actionable data focusing on flood preparedness. By combining mobile data collection, maps, and a digital dashboard, the project will enable DarMAERT to anticipate risks and act earlier, not just respond after a disaster.\n\nThis session will offer key takeaways:\n-How community-generated OSM data can inform local early warning\n-The role of local leaders in validating and sustaining open data\n-Challenges in low-tech anticipatory systems and how we addressed them\n-How to turn maps into tools for local governance and advocacy"
      },
      {
        "title": "Utilizing Free and Open-Source Software to Better Share Local Data for Improved Community Decision Making",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "This presentation will share how community members are being trained to leverage the use of measurable local data in decision making using Free and Open-Source software such as QGIS, GeoJSON.io, Google Sheets, GitHub, and Tableau Public.",
        "description": "Recognizing the need for improved access to data vital for addressing vexing community challenges, Iowa State University Extension and Outreach’s Community and Economic Development (CED) unit developed the Community Indicators Program in 2013 and the Data Science for Public Good (DSPG) Young Scholars summer outreach program (https://dspg.iastate.edu/) in 2020. These programs have been successful at producing and sharing demographic, economic and other state data through the curation of meaningful and timely data and informative publications and dashboards. The programs have also worked with communities to utilize the Community Learning through Data Driven Discovery (CLD3) https://cld3.org/our-approach/ framework to address local issues and provide local stakeholders with the ability to make data-supported decisions.\n\nWhile these efforts have been a good start to improve issue awareness and decision-making in smaller and rural communities, many of the indicators that are informative measures in larger communities are not readily available or are at a granularity that is not suitable for local decision-making. Additionally, access to software and the skills to operate this software can be a barrier to working with this data once it is identified.\n\nTo address this issue, a program focused on Science Education and Workforce Development was piloted in 2024. This program prioritized 1) Increasing local capacity for data literacy 2) Increasing technical skills to access data 3) Increasing ability to share local data. The program developers recognized the need for utilizing Free and Open-Source software to limit the cost and access barriers to visualizing and sharing local data with the community. Software skills are developed though hands-on-training using local data examples. The current suite of free and open-source software is accessible and scalable to meet the needs of many small communities. The software included in the program consists of QGIS, GeoJSON.io, Google Sheets, GitHub, and Tableau Public. Used together, this software allows participants to easily make their local community data publicly available at little to no cost. \n\nExamples of some of the data projects include: the number of recurring community events, use frequency of park shelter rentals, number of volunteer organizations, number of youth/adult parks and recreation participants, attendance rates at council/supervisor meetings, number of permits for home improvements, library check outs, or even the number and spatial location of trees planted by the city. During this presentation, the presenter will share several of these examples demonstrating how this software can be used by those new to data science or geospatial technology. Additionally, the presentation will include some techniques that can be used to increase the utility and delivery of spatial data visualizations created with these tools."
      },
      {
        "title": "OSGeo and OGC MoU update",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Open Software and Open Standards are complementary pieces of the geospatial ecosystem. In 2022, OSGeo and OGC signed a new Memorandum of Understanding (MoU) that aims to benefit the mission and goals of both organizations.  This presentation will provide an update on collaboration, reference implementations, code sprints, and future plans.",
        "description": "Open Software and Open Standards are complementary pieces of the geospatial ecosystem. In January 2022, OSGeo and OGC signed a new and updated version of the Memorandum of Understanding (MoU) that aims to maximize the achievement of the mission and goals of both organizations. Execution of joint Code Sprints, identifying free and open source technologies that could be used as Reference Implementations for OGC Standards and validating OGC compliance tests are examples of activities that can take place within the scope of the agreement.\n\nThe current MOU can be found at https://www.osgeo.org/wp-content/uploads/MOU_OGC_OSGeo_2022_signed.pdf"
      },
      {
        "title": "The Fellowship of the Map: Open Mapping for Climate Action, Disaster Preparedness, and Building a New Generation of Gurus",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Inspired by the Fellowship of the Ring, this talk shares how youth and community mappers across Asia-Pacific are rising as Open Mapping Gurus. Learn how regional collaboration, inclusive leadership, and open tools are transforming climate action and disaster preparedness—one map, one mentor, and one movement at a time.",
        "description": "What if we could harness the spirit of The Fellowship of the Ring, but for mapping? Imagine a network of passionate youth and community leaders united by a single purpose: to use open mapping for real-world impact—disaster preparedness, climate action, and community resilience.\n\nThis session dives into the Open Mapping Guru Network—a powerful fellowship of mappers, activists, and mentors across Asia-Pacific, where the maps they create don’t just track roads or buildings—they chart a future of resilience, inclusion, and hope. We'll explore The Origin of the Fellowship: How the Open Mapping Guru Network was born and how it has grown into a cross-border network that empowers youth leaders, supports local communities, and builds strong partnerships. How mappers from different walks of life are using tools like Tasking Manager, MapSwipe, KoboToolbox, and more to respond to climate crises, map disaster-prone areas, and create safe spaces. From the humble beginnings of a mapper to a leader, mentor, and community builder—hear stories of Gurus making a difference in their communities and in the open mapping movement. The power of community solidarity, mentorship, and peer learning in growing the open mapping movement. Why collaboration, not isolation, is key to long-term impact."
      },
      {
        "title": "Improving Climate Data Delivery and Visualisation",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "I am Harris Hudson and I aim to put climate, weather, and earth science, data directly into the hands of the everyday person. This discussion follows my research and recommendations regarding the future of visualisation of NetCDF Climate and Forecast gridded data.",
        "description": "Some GIS (Geographic Information System) technologists present today may remember maps before they became 'slippy'. But young or old, we should now question some prior assumptions that shaped the current GIS web ecosystem. Considering its evolution – based on prior learning and precedents – I suggest we re-examine some of the premises underlying NetCDF gridded data. Are legacy OGC (Open Geospatial Consortium) or tiling services really the best abstractions for delivery of NetCDF or gridded data?\n\nThe following discussion also provides insights into my development of a front-end renderer. I will also briefly mention delivery mechanism services such as TDS (THREDDS - Thematic Real-time Environmental Distributed Data Server) subsetting service. These delivery mechanisms, coupled with modern devices and networks, allow us to explore contemporary, timely and scalable ways of delivering and viewing data that have the potential to improve the end-user experience.  \n\nFollowing are some live demonstrations for your consideration.\n\nMy findings are especially useful to custodians, publishers and climate/earth data researchers who seek better alternatives of delivery and visualisation of this important information. If nothing else, my suggestions will help broaden audiences by delivering important data into the hands of the everyday person. \n\nAnd for the technologists present today, I will share some limitations encountered during my journey and possible future directions. Who knows? Perhaps you can also contribute.\n\nAbout me - I am an unfunded GIS developer who questioned several prior assumptions that still shape today's GIS web ecosystem.  And found some solutions.  I am grateful to FOSS4G 2025 for considering my presentation.  However, as an unfunded GIS developer, and as submitted in prior proposals, my in-person attendance at Auckland depends on full or partial funding.  Otherwise, I am happy to give my presentation online from Australia."
      },
      {
        "title": "Wither Qfield/Mergin? Data collection apps in the age of AI",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "Will the ubiquitous nature of AI spur the decline of Data collection apps?",
        "description": "AI hasn't made hard things easy, it has made long things short. Workflows that once had tedious parts are now a virtual snap. The most difficult part is now often writing the correct prompt. Qfield and Mergin are two apps that have made the process of collecting geospatial data in the field easy and trivial. There is however still a small learning curve, and when there needs to be a method to the data collection madness, setting up those complex form workflows can be problematic. \n\nIn cases where you want the collection workflow to be a cog in a larger process this can be more difficult to achieve. Often, if this is the case, you also need to be managing a dozen devices and app store installs.\n\nHow does AI fit in. by itself, it really does not. But the promise of HTML5 and PWA's has been that with just HTML, CSS and JS you could rule the mobile device world. Most of a mobile phones hardware can be accessed via various HTML5 API's. So it is not a stretch to craft a PWA that can use all the available sensors on a phone to capture the relevant info. Throw in some logic with JS and the most complex form workflows can be realized.\n\nAI just makes this so much faster.  This talk seeks to present these facts and show that while Qfield and Mergin are here to stay (especially for simple collection efforts) AI will most likely spur the creation of bespoke PWA's for more complex data collection workflows"
      },
      {
        "title": "OSGeo tool stack in a container for a team of geospatial analysts/engineers in 2025",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "We put together the most popular opensource GIS dataset in Australia. We've been working on a standardised OSGeo-toolstack for a team of geospatial analysts and engineers to deploy fast. We're here to share some use cases as well as this journey and some of its challenges.",
        "description": "We intensively use a broad range of OSGeo tooling to put together the original source of truth location data for Australia that, among other things, is the most downloaded dataset provided by the Australian government from `data.gov.au`.\n\nOur team takes things like SRIDs very seriously and we work hard to ensure our data is very accurate. Additionally we use these great GIS tools to construct many other data use cases such as Australian roads, buildings, trees, indigenous lands, disaster response and climate reporting information. We have encountered some fascinating problems that these tools help us solve.\n\nWe're constantly working to be more efficient and to better scale, specifically we have been working on \"Standard Operating Environments\" that are expected to be fast to deploy, work off the shelf and stay up to date. These are used by a large team of geospatial analysts and engineers. For our relatively mature use cases our development stack uses Python + PostGIS, GDAL, PROJ, Shapely, Fiona, GeoPandas, jupyter and much more. Making a development environment is a well trodden path, this should have been easy right. Right?\n\nThis talk will briefly touch on some cool uses we have for these tools; outline how we're working to make our processes better and will discuss some of the challenges we encountered in doing this work, in the hopes that others may avoid them."
      },
      {
        "title": "Mapping Community Capital: Using FOSS and Open Data to Reveal Local Gaps in Rural Access and Opportunity",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "We apply the Community Capitals Framework in small rural Midwest communities using FOSS and open data. Our case studies demonstrate how regional access to services varies across rural places—and how, within those places, spatial analysis reveals localized gaps in access to essential assets like healthcare, broadband, and public infrastructure.",
        "description": "The Community Capitals Framework (CCF) offers a structured approach to assessing community strengths and weaknesses across seven forms of capital—natural, cultural, human, social, political, financial, and built. Over the past several years, our team has collected and analyzed data indicators aligned with this framework to evaluate the strengths and weaknesses of cities and counties in the Midwest United States. Drawing on publicly available data, we’ve assessed how various forms of capital—such as built infrastructure, human services, and social networks—vary across communities. While these assessments have offered valuable insights at the county and city level, they often mask important disparities within communities themselves. Our current focus shifts to analyzing those same indicators at a finer spatial scale to better understand how access to critical services and assets is distributed within individual communities.\n\nTo uncover these internal disparities, we apply spatial analysis techniques using free and open-source software (FOSS) and openly available geospatial data. We begin by analyzing access to key community assets—such as parks, healthcare facilities, broadband infrastructure, and educational institutions—across a multi-county rural region, identifying differences in regional availability. We then zoom in on selected case study communities to map the neighborhood-level distribution of those same assets, revealing gaps in access that are often hidden by broader county- or city-level aggregates.\n\nUsing free and open-source tools we perform proximity analysis, service area analysis, and spatial overlays to identify underserved areas—places where residents may face barriers to opportunity, infrastructure, or essential services. Our case studies illustrate how a place-based, spatial lens—grounded in open data tools—can guide more equitable rural planning, investment, and community engagement. \n\nThis session will highlight our methods, case study findings, and how these insights can inform targeted investment in small rural communities. Our reproducible, open-source workflow is designed for use by researchers, planners, and community stakeholders aiming to apply geospatial analysis in low-resource environments."
      },
      {
        "title": "Implementing Interactive Indoor Maps with MapLibre and IMDF",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Learn to render interactive indoor maps with MapLibre and IMDF. This session guides you through the complete workflow, from data processing to creating a map with dynamic features like searching for open stores and locating amenities within large facilities.",
        "description": "While outdoor maps are ubiquitous, navigating large and complex indoor venues like shopping malls and airports remains a significant challenge for users. Traditional solutions, often static image-based maps provided by facility operators, lack the flexibility required for modern applications. These image maps are difficult to update and do not support crucial features like interactive search or real-time data integration, as the locational information is not structured as usable data.\n\nThis presentation tackles these challenges head-on. We will demonstrate how to build a powerful, data-driven indoor mapping solution using open-source tools.\n\nFirst, we will introduce the Indoor Mapping Data Format (IMDF), an international standard by Apple, and explain how to model a venue's floors, units, and points of interest.\n\nNext, we will walk through the technical implementation of parsing this IMDF data and rendering it as an interactive, multi-layered indoor map with MapLibre.\n\nFinally, going beyond simple map display, we will detail the implementation of a practical search feature. Attendees will learn how to leverage the structured IMDF data to allow users to find specific points of interest, such as stores that are currently open or the nearest available restroom, transforming the map from a static image into a dynamic and genuinely useful tool.\n\nKey Takeaways for the Audience:\n\n- A solid understanding of the Indoor Mapping Data Format (IMDF) structure and how to apply it to a real-world venue.\n- A step-by-step guide to processing IMDF data and rendering it as a multi-level indoor map using MapLibre.\n- Actionable techniques for implementing dynamic search and filtering functionalities based on real-time data (e.g., business hours, facility status) over the indoor map."
      },
      {
        "title": "GeoFM with OpenDataCube - From arrays to embeddings",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Groups have wrangled sufficient number of GPU's to scan through a lot of satellite data to create a good set of weights. This talk scans through a few GeoFM's (Geospatial Foundation Models) and how to put data through them while loading it via OpenDataCube.",
        "description": "This talk presents technical implementation and intercomparison of various geospatial foundation models using TerraTorch , the twist is loading satellite data collections via OpenDataCube tooling rather than built in sample datasets. A bit of pre-processing is necessary to make the foundation models comply with the dynamics of the data as-is from cloud native sources, however they stand up quite well and are generalizable for problems they are trained for. In addition, they can also generate embeddings which can be thought of as dimension reduced versions of source arrays that can then be used to fine tune and perform additional tasks the initial foundation model was not trained for.\n\nThis approach is used for demonstrating application of multiple foundation models - Prithvi v2.0, Clay and DOFA for burn scar mapping (Segmentation) and water quality inference (Regression) tasks."
      },
      {
        "title": "1% AEP Current and Future Climate Flood Maps for Aotearoa New Zealand",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Flooding is one of the costliest hazards facing Aotearoa. We present a methodology for creating nationally consistent flood-maps for a range of current and future climate scenarios developed by the Endeavour Mā te haumaru ō te wai: flood resilience Aotearoa. The maps are shared in an open-data repository.",
        "description": "Introduction\nFlood inundation modelling and the resultant flood maps are essential for understanding, planning for, and responding to flood events. Flooding is one of the most costly and impactful hazards facing Aotearoa, and the frequency and severity of flooding is projected to rise under a warmer future climate. \n\nHistorically in Aotearoa, flood hazard products have been produced at the local and regional levels using locally defined methodologies leaving Aotearoa without nationally consistent flood map products available for nation-wide hazard, risk and future climate analyses. This is the aim of the Endeavour project Mā te haumaru ō te wai: flood resilience Aotearoa. \n\nMā te haumaru ō te wai aims to adhere to the principles of open science and access. Our methodology uses open-source software where possible, while also contributing directly to the development of two open-source software projects: BG_Flood and GeoFabrics. Additionally, where possible we use open data sources as inputs into the modelling methodology. Finally, the current and future climate scenario flood maps are published in an open-access data repository. \n\nWe present the automated cascaded modelling methodology used to produce nationally consistent fluvial and pluvial flood inundation maps by integrating climate science, rainfall statistics, hydrology, hydrodynamics and geospatial data for 256 flood domains across Aotearoa New Zealand. We apply this methodology to produce four current and future climate scenarios at a 1% Annual Exceedance Probability (AEP) for current, 1°, 2° and 3°C warmer than current conditions and present a summary of these results. \n\nMethod\nWe created a cascaded modelling methodology to produce nationally consistent flood maps (Figure 1) consisting of five major stages: flood domain definition, topography and roughness generation, storm generation, hydrology modelling, and hydrodynamic modelling. The workflow was fully automated using Cylc [1], an open-source workflow engine, to control the progress between different stages. This allowed the workflow to be run across all flood domains for the current and future climate scenarios in an automated fashion.  \n\nOur modelling methodology begins with the definition of floodplains and associated catchments (Figure A.1.) where we perform our coupled hydrology (Figure A.4.) and hydrodynamic (Figure A.5.) modelling. For each catchment, a design rainfall event is created in the storm generation stage (Figure A.3) which forms another key input to both the hydrology and hydrodynamic modelling stages. The topography and roughness stage (Figure A.2) produces hydrologically conditioned Digital Elevation Models (DEMs) and hydraulic roughness layers across Aotearoa New Zealand which form third key input to the hydrodynamic modelling stage. A flood inundation map showing the maximal flood depths across the current climate scenario is shown for an example domain, the West Coast Fox River (Figure B). \n\nThe catchments and associated floodplains are defined from a set of basic manual outlines, a NZ wide river network, and nationwide population and building information. The manual outlines roughly indicate each floodplain in the country act to ensure appropriate groupings of nearby river courses. The floodplains are defined using a process to propagate up from the river mouth(s) (or downstream reach for inland catchments) to define relatively flat populated areas with available LiDAR where higher detail hydrodynamic modelling is undertaken. During this stage, river injection points are defined at the intersection between the flood plains and the river network; they are used to couple the hydrology model used in the upper catchment with the hydrodynamic model used over the floodplain in the lower catchment. \n\nWe use the Aotearoa-specific High Intensity Rainfall Design System (HIRDS) [2] to generate our rainfall events. HIRDS is an open tool (https://hirds.niwa.co.nz/) for generating rainfall estimates for a specified AEP and duration at any location across New Zealand where the rainfall estimates are derived from historic rainfall observations as well as other climatic and topographical information.  \n\nAll hydrology modelling in our workflow is performed using the Aotearoa New Zealand TopNet model [3]. We hydrologically model rainfall events with durations between 6 and 72hrs to experimentally determine a realistic worst case storm duration for each catchment. In each catchment, the selected worst-case duration 1% AEP rainfall event was used to force the coupled hydrology and hydrodynamic model.  \n\nHydrodynamic modelling was performed using BG_Flood [4] an open-source software (OSS) GPU-enabled adaptive resolution shallow-water solver that supports rain-on-grid. BG_Flood was actively developed as part of this project. The TopNet river flows from the upper catchment are injected into the hydrodynamic model around the edge of the floodplain. The rainfall event over the floodplain is also included directly to the hydrodynamic model as rain-on-grid. The NZ tide model, with open access through an online tool (https://tides.niwa.co.nz/), was used to provide tidal forcings (mean high water spring tide) around the coast.  \n\nThe hydrodynamic modelling also requires a hydrologically conditioned DEM and hydraulic roughness. These were produced using the OSS Python package GeoFabrics [5], which was also developed for this project. GeoFabrics is a Dask enabled Python tool for creating hydro DEMs and hydraulic roughness from LiDAR point clouds, other elevation, natural feature and infrastructure information. This was included within the workflow so that the topography and roughness information could be updated as LiDAR coverage increased across New Zealand from 20% at the project inception to 80% today. \n\nResults\nWe developed our workflow with a focus on iterative improvement. As such, we performed our first nationwide run concluding June 2024. These results were limited to 1% AEP at current climate and an 8m resolution. These were reviewed to identify key areas of improvement. Specifically, we identified: more realistic storm durations, inclusion of inland floodplains, inclusion of lakes and 150 missing culverts, the opening of more than 100 river mouths, and modelling to a resolution of 4m. \n\nWe have completed our second nationwide run to a resolution of 4m concluding June 2025 across four scenarios: 1% AEP at current, 1°, 2°, and 3°C warmer future climate. In our presentation we will cover several catchments in detail and share summary results comparing the current and future climate scenarios.  We will also share the open-data repository where the flood inundation maps across each catchment and scenario can be accessed. Finally, we will highlight how these products can be used to access impact through risk modelling in future studies.  \n\n\n\nReferences\n[1] Oliver et al., (2018). Cylc: A Workflow Engine for Cycling Systems. Journal of Open Source Software, 3(27), 737, https://doi.org/10.21105/joss.00737  \n\n[2] Carey-Smith, T., et al., (2018) High Intensity Rainfall Design System Version 4, NIWA Client Report 2018022CH prepared for Envirolink, retrieved from https://niwa.co.nz/climate-and-weather/hirdsv4-usage \n\n[3] Bandaragoda, C., et al., (2004). Application of TOPNET in the distributed model intercomparison project. Journal of Hydrology. https://doi.org/10.1016/j.jhydrol.2004.03.038  \n\n[4] Bosserelle, C., et al., (2022), BG-Flood: A GPU adaptive, open-source, general inundation hazard model; Australiasian Coasts and Ports 2021. https://github.com/CyprienBosserelle/BG_Flood. \n\n[5] Pearson, R et al., 2023, Geofabrics 1.0.0: An Open-Source Python Package for Automatic Hydrological Conditioning of Digital Elevation Models for Flood Modelling. Environmental Modelling and Software. http://dx.doi.org/10.2139/ssrn.4463610"
      },
      {
        "title": "Free and Open Source AI Assisted Mapping : fAIr",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "fAIr is an open AI-assisted mapping platform developed by the Humanitarian OpenStreetMap Team (HOT). In this talk we will go through current progress and state of the art of usage of AI in humanitarian mapping. We will share our experience and roadmap of fAIr .",
        "description": "The service uses AI models, specifically computer vision techniques, to detect objects in satellite and UAV imagery."
      },
      {
        "title": "A5 Technical Deep Dive: The Mathematics of Pentagonal Perfection",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Journey through the mathematical elegance of  [A5](https://a5geo.org)'s pentagonal spatial indexing system. Discover how dodecahedral geometry, vertex curvature minimization, and equilateral pentagon tiling create superior spatial accuracy. A highly visual exploration of the geometric principles that make  [A5](https://a5geo.org) equal area cells possible.",
        "description": "### Overview\nMathematics and geometry have always been humanity's tools for understanding and organizing space. This presentation takes you on a visual journey through the mathematical foundations of A5, revealing how geometric principles dating back to ancient Greece combine with modern computational techniques to create a new spatial indexing system.\n\n### The Mathematical Quest: Why Break From Regular Polygons?\n\nA fundamental question drives A5's innovation: why restrict ourselves to regular polygons when projecting onto a sphere warps all shapes anyway? Traditional DGGS approaches use regular polygons (triangles in HTM, squares in S2, hexagons in H3) on platonic solids, but projection distorts these shapes significantly.\n\nA5 recognizes that since projection destroys regularity regardless, we should optimize for the final spherical result rather than the initial planar form. This insight led to embracing irregular equilateral pentagons that, while not regular on the plane, achieve superior properties when projected onto the sphere.\n\n### The Five Platonic Solids: Building Blocks of Space\n\nTo understand A5, we must first explore the five Platonic solids - the only three-dimensional shapes where all faces are identical regular polygons. These geometric forms, known since antiquity, provide the foundation for all Discrete Global Grid Systems (DGGSs).\n\nEach platonic solid offers different characteristics:\n- **Tetrahedron** (4 triangular faces): Highest vertex curvature\n- **Cube** (6 square faces): High vertex curvature\n- **Octahedron** (8 triangular faces): Moderate vertex curvature\n- **Icosahedron** (20 triangular faces): Low vertex curvature\n- **Dodecahedron** (12 pentagonal faces): Lowest vertex curvature\n\nThe key insight is that vertex curvature directly relates to distortion when projecting onto a sphere. The dodecahedron, with its twelve pentagonal faces, has the lowest vertex curvature of all platonic solids, making it the most \"sphere-like\" geometric form.\n\n### Hilbert Curves: Elegant Spatial Indexing\n\nA critical feature in A5 is how cells are spatially indexed using Hilbert-like curves - space-filling curves that map one-dimensional integer sequences to two-dimensional spatial arrangements. This elegant mathematical solution ensures that cells with similar locations have similar cell IDs, leading to several desirable properties:\n\n- **Spatial Locality**: Nearby cells in space have nearby IDs in the integer sequence, enabling efficient spatial queries and neighbor finding operations.\n- **Hierarchical Consistency**: Parent cells are guaranteed to overlap with their children, though they don't cover them exactly. This overlap ensures spatial coherence across resolution levels.\n- **Efficient Traversal**: The curve's path through the pentagon hierarchy creates natural ordering for spatial operations and range queries.\n\n### Efficient Encoding: What can fit into 64 bits?\n\nThe mathematical journey culminates in A5's elegant encoding system. Each cell is represented as a 64-bit integer, enabling:\n\n- **Hierarchical Addressing**: Parent-child relationships encoded in bit patterns\n- **Computational Efficiency**: Fast spatial operations using integer arithmetic\n- **Millimeter Precision**: Extraordinary accuracy at the finest resolution"
      },
      {
        "title": "Promoting Collaboration between Local and Global Mappers: Case of  the OSM Student Club in the DRC",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "The talk will share the club's achievements, obstacles, and a vision for spreading student-led open mapping throughout Africa.",
        "description": "In many areas of the Global South, especially in Sub-Saharan Africa, there is an urgent shortage of current geospatial data needed for planning infrastructure, responding to disasters, and promoting sustainable development. This issue is particularly pronounced in the Democratic Republic of Congo (DRC), where rural and conflict-affected regions are notably underserved. To tackle this problem from the ground up, the OSM Student Club was started at the University of Kinshasa with the aim of teaching students how to map openly and connecting local communities with the wider global geospatial network.\n\nThis academic session shares a case study of the DRC’s OSM Student Club and delves into how it has emerged as an example of youth-driven innovation and teamwork in open mapping. Drawing on insights from initiatives like Open Cities Kinshasa, Scaling Missing Maps, the EZO SALA COVID-19 response, and mapping efforts in North Kivu and Ituri, this talk will look at how club members have supported local development goals while also working on international humanitarian projects. It underscores the role of academic institutions as centers for open geospatial knowledge, skill-building, and community strength.\n\nThe session will also discuss the use of tools like QGIS, JOSM, ODK, and Mapillary in student training, and the partnerships with global organizations like HOTOSM, UN Mappers, and UNICEF. By highlighting tangible achievements—such as digitizing health zones, mapping critical infrastructure, and providing disaster data support—the presentation argues for the value of investing in university-based open mapping clubs across Africa.\n\nAttendees will come away with practical tips for encouraging local and global cooperation, creating sustainable mapping groups in academic environments, and involving young people as the next leaders in the open geospatial movement."
      },
      {
        "title": "Did you get that thing I sent you? Simplifying spatial data in Python",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Distributing spatial data is a major challenge. This talk walks-through different strategies you can take to simplify your spatial data for distribution in Python, and under what circumstances you might make each choice, including: reducing precision; filling holes; Ramer-Douglas-Peucker and friends; and raster and vector conversion.",
        "description": "To quote Stewart Brand: On the one hand information wants to be expensive, because it’s so valuable. The right information in the right place just changes your life. On the other hand, information wants to be free, because the cost of getting it out is getting lower and lower all the time.\n\nRegardless of which hand you hold, making sure you can deliver data efficiently and quickly is important. Spatial data is not special - it's just more data - but it poses major challenges in distribution for a number of reasons: competing file formats; multiple data types (vector and raster is just the beginning); and the most common issue: the quantity of data. This talk walks through different strategies for simplifying your data, why you might want to (and why you might not), and what choice you might make to perform the simplification using Python.\n\nTopics covered include:\n\n- reducing precision and delivering data suitable to scale;\n- filling holes and removing noise;\n- the Ramer-Douglas-Peucker algorithm and other common simplification algorithms;\n- and, when to use raster vs vector data for distributing information."
      },
      {
        "title": "maplibre-gl-terradraw - new drawing plugin for maplibre-gl-js",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "This talk introduces a new drawing plugin for MapLibre: maplibre-gl-terradraw as an alternative of historical mapbox-gl-draw. maplibre-gl-terradraw has specifically been developed for Maplbire with a simple API to bring Terra Draw to your Maplibre with a single line of code.",
        "description": "This talk introduces a new drawing plugin for MapLibre: maplibre-gl-terradraw.\n\nPreviously, both Mapbox and MapLibre have relied on an older plugin, mapbox-gl-draw, to provide drawing functionality in maplibre-gl-js. However, mapbox-gl-draw is no longer actively maintained, making it increasingly difficult to use with MapLibre.\n\nAs an alternative, Terra Draw maintained by James Milner, offers advanced drawing features for multiple mapping libraries, including Mapbox, MapLibre, OpenLayers, Leaflet, Google Maps, and ArcGIS, all within a unified user interface. Compared to mapbox-gl-draw, Terra Draw is significantly easier to use. However, integrating its full functionality into MapLibre still requires extensive configuration. I developed maplibre-gl-terradraw, a new plugin that enables a pre-configured drawing feature with a single line of code.\n\nWith maplibre-gl-terradraw, users can easily add drawing controls with just a line of code.\n\nThis immediately grants access to all drawing modes (point, line, polygon, rectangle, circle, etc) powered by Terra Draw. The plugin comes pre-configured with icons and additional functionalities, such as:\n\n- Selecting and deleting features\n- Downloading drawn features\n- Customizing Terra Draw options and styles, as described in the documentation\n\nFurthermore, the measure control allows users to:\n\n- Measure the distance of a line or the area of a polygon\n- Query elevation data from raster DEM sources (MapLibre Terrain, TerrainRGB, and Terrarium)\n\nIn this talk, I will demonstrate the core functionalities of maplibre-gl-terradraw and show how easily you can integrate drawing features into your MapLibre applications.\n\nReferences: \n\nmain project: \n- maplibre-gl-terradraw: MIT License. https://github.com/watergis/maplibre-gl-terradraw\n\ndependencies of the main project:\n- Terra Draw: MIT License. https://github.com/JamesLMilner/terra-draw\n- maplibre-gl-js: https://github.com/maplibre/maplibre-gl-js"
      },
      {
        "title": "Weavingspace: a new way to make multivariate maps",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "The weavingspace python module enables creation of thematic maps using periodic tilings to produce attractive maps that weave together many data attributes in a single map display.",
        "description": "The weavingspace python module enables creation of multivariate thematic maps by generating and overlaying a periodic tiling layer with polygon data. Geometries in the tiling symbolise data by choropleth colouring. Well over one hundred tiled patterns are supported, including a highly configurable tilings that give the appearance of biaxial or triaxial woven materials. Complex tilings with as many as twenty distinct elements yield maps with textures that convey combinations of attribute values as textures, while maps with up to around eight elements make it possible to read maps with that number of variables simultaneously. In this presentation I will discuss the motivation for this work, challenges in implementation, and also present a web app that allows code-free creation of such maps."
      },
      {
        "title": "State of GeoTools, JTS Topology Suite, and ImageN",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The very first open source geospatial foundation diversity statement … was about representation for the “C Tribe” and the “Java Tribe”! Check in with this presentation for an update from the Java crew!",
        "description": "This talk covers three foundation Java Geospatial Projects that do the heavy lifting behind applications you know and love:\n\n* GeoTools - Open-source Java library that provides tools for geospatial data.\n* JTS Topology Suite - Java library for creating and manipulating vector geometry.\n* ImageN - image and raster processing\n\nWe have been having an amazing year, and an amazingly active year. There have been big changes in the Java ecosystems for us to respond to. We have also been revising our community interaction as development culture changes around communication and security.\n\nAttend this talk to learn more about this projects, their project teams, and what we are doing next."
      },
      {
        "title": "Scaling GeoNetwork 4.4.x in Kubernetes: Production Deployment Strategies and Performance Analysis",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Walk through of Helm charts and horizontal scaling approaches for GeoNetwork in Kubernetes, covering volume pitfalls like wro4j cache configurations, plus Locust benchmark techniques for user access testing",
        "description": "GeoNetwork, a widely adopted open-source metadata catalog, faces significant challenges when deployed in modern containerized environments requiring high availability and scalability. This presentation examines current capabilities and limitations for scaling GeoNetwork 4.4.x deployments in Kubernetes environments.\n\nWe present a comprehensive Helm chart implementation that enables both vertical and horizontal scaling strategies for GeoNetwork instances. Our analysis reveals that while vertical scaling (CPU/memory increases) provides straightforward performance improvements, horizontal scaling presents complex challenges due to GeoNetwork's architecture, particularly around session management, database connections, and Elasticsearch cluster coordination.\n\nKey deployment pitfalls identified include persistent volume configuration issues and  inter-pod file sharing. Ingress load balancer configurations that maintain catalog consistency across multiple instances.\n\nPerformance benchmarking using Locust load testing platform on a catalog containing 7,000 metadata records reveals critical capacity thresholds for single GeoNetwork instances. Our tests simulate realistic user behavior including HTML page loading for metadata record viewing, providing concrete metrics for infrastructure planning and resource allocation decisions.\n\nGeoCat provides products to its customers that need to be fast, reliable and scalable. Moving the GeoCat Live product to a Kubernetes environment contributes to this goal.\n\nThe presentation concludes with recommendations for organizations implementing scalable GeoNetwork infrastructure, including when to choose vertical versus horizontal scaling approaches, and operational monitoring strategies essential for production deployments."
      },
      {
        "title": "Data delivery simplified with GeoCat Bridge",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Bridge is a plugin for your favorite desktop GIS that makes it easy to publish your data to map and catalog services. Now it will also allow you to search and consume your OGC services.",
        "description": "If you need to publish your (meta)data to GeoServer and/or GeoNetwork straight from QGIS or even ArcGIS, GeoCat Bridge might just be the tool for you.\n\nIn this talk we'll show you how Bridge works and what it does: from converting native symbology into SLD or MapLibre GL JS, to publishing feature and raster layers to a variety of services, as well as publishing metadata records and linking them to your services.\n\nFurthermore, we're excited to show you a sneak preview of the upcoming integrated Catalog Search functionality, which will allow you to search through OGC API Records from any server - including your own - and add the results directly to your desktop GIS."
      },
      {
        "title": "Manage your fieldwork directly from your app with QFieldCloud API integration",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "Get familiar with the latest QFieldCloud development, QFieldCloud RESTful API and the QFieldCloud SDK.",
        "description": "QField helps people map and understand the world, QFieldCloud helps people automate their data collection and fieldwork.\n\nIf your organization already has an application that manages processes and workflows, or you simply want to run a nightly cron to automate processing on your data, the QFieldCloud RESTful API will help you do that.\n\nWe will look at some of the newest QFieldCloud features and how we can access them via QFieldCloud CLI, QFieldCloud SDK or simply by running a HTTP request against QFieldCloud API endpoint."
      },
      {
        "title": "(Re)Making Cirrus: Five Years Building a Data Orchestration Framework",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "A retrospective on building cirrus, a cloud-native framework for building STAC-based data orchestration pipelines. We'll look at the design and architecture evolution over five years of development and some lessons learned adapting to ecosystem and requirement changes.",
        "description": "Cirrus is an open source, cloud-native framework for orchestrating geospatial data pipelines built using the concept of STAC (SpatioTemporal Asset Catalog) workflows. It provides a flexible and modular approach to deploying and managing serverless pipelines in AWS via python components and a Terraform-based deployment mechanism. Cirrus enables scalable, repeatable data processing workflows in the cloud, and is designed to help teams transform, validate, and catalog geospatial data in STAC-compliant formats at scales both large and small.\n\nOver the past five years, cirrus has evolved from a directory of loosely-organized bits of configuration and components built on top of the Serverless Framework to a robust, open source cloud-native data pipeline management system. In this talk, I’ll share my journey maintaining and evolving cirrus from what I inherited to its current state, and lessons I’ve learned along the way.\n\nTogether we’ll explore cirrus’ origins and the original architecture and its challenges. We’ll examine the decision to shift away from duplicating deployment code via the configuration merge system of the first cirrus CLI, and what benefits and pitfalls that brought along with it. We’ll trace some of the tooling and ideas that spun out along the way, like stac-task and swoop. Finally, we’ll look at the version 1.0 release’s move away from Serverless Framework and the decoupling of the deployment logic from the codebase, the new cirrus Terraform module, and how this 1.0 release has prompted the reconsideration of what actually constitutes cirrus now.\n\nWhether you're maintaining your own internal tooling, building cloud-native data processing pipelines, or just trying to keep an open source project healthy through shifting technical landscapes, this talk will offer practical insights drawn from real-world experience. We'll cover the technical decisions, tradeoffs, and lessons learned—especially those relevant to anyone maintaining cloud-native tooling in a fast-moving landscape."
      },
      {
        "title": "Evaluating LLMs as Intermediaries for FOSS4G CLI-based Geospatial Analysis",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This presentation investigates whether Large Language Models (LLMs) possess adequate knowledge to function as effective intermediaries between non-expert users and FOSS4G CLI tools. We assess the capability of current LLMs to correctly respond to queries requiring geospatial domain-specific knowledge and generate appropriate solutions for spatial analysis tasks.",
        "description": "The complexity of CLI-based geospatial analysis tools presents a significant barrier to widespread adoption of FOSS4G technologies. While tools like GDAL, PDAL, and Python geospatial libraries offer powerful capabilities, their command-line interfaces require substantial technical expertise. This limits effective utilization to technical specialists, despite FOSS4G's promise of democratizing geospatial analysis.\nRecent advances in Large Language Models (LLMs) suggest potential for bridging this technical gap. LLMs could theoretically interpret natural language requests and generate appropriate CLI commands, making these tools accessible to domain experts who possess valuable geospatial knowledge but lack programming backgrounds.\nHowever, effective geospatial analysis requires understanding of domain-specific concepts such as coordinate reference systems, data formats, and regional standards. Our preliminary investigations reveal that current LLMs often fail to correctly handle country-specific geospatial information. \nThis presentation evaluates whether existing LLMs possess sufficient geospatial domain knowledge to serve as reliable intermediaries. We examine their performance and evaluate specific knowledge gaps that prevent LLMs from effectively facilitating FOSS4G CLI tool usage for non-technical users."
      },
      {
        "title": "\"Osmia\" - A quick Openlayers project for OSM data verification",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "\"Osmia\" is an Javascript/Openlayers web app that interrogates OSM metadata to find areas of the road network that need mappers attention. Highways that have not been edited in a long time are highlighted to the user.",
        "description": "Quality control tools greatly benefit map maintainers. The purpose of this tool is to find map features, in this case roads or paths, that have not been updated in a long time.\n\n\"Osmia\" is a tool primarily to help mappers perform quality control analysis of OpenStreetMap data, and add missing data where it is required. Osmia's Obsolete Road Geometry function colours road lines and the line vertices increasingly red if they have not been edited recently. This highlights data that needs reviewing; due to improvements in the quality of aerial photography recently, the theory is that old data could be quite inaccurate. As far as I know, despite all the quality assurance tools available for mappers, none focus specifically on the temporality or recency of the data, so this is fairly novel. Additionally, there is a function that finds roads with no surface tags.\n\nPreviously, the only way to achieve this kind of data visualisation was through manually downloading and styling the vector data in QGIS. This tool fully automates this task in a computationally light, browser-based workflow.\n\nOsmia hinges on the Nominatim and Overpass APIs to retrieve data easily. Openlayers is used for the slippy map and vector data styling."
      },
      {
        "title": "Make It Easier to View GTFS : Building a GTFS Timetable Viewer with SvelteKit",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "A web-based GTFS timetable viewer that processes General Transit Feed Specification (GTFS) data entirely client-side using SvelteKit. Users can upload GTFS ZIP files and instantly visualize transit schedules in an interactive timetable format with real-time status indicators.",
        "description": "Public transit agencies publish schedule data in GTFS format, but accessing this information remains challenging for most users. This application provides a web-based GTFS timetable viewer that transforms complex transit data into readable, interactive timetables with ease of use. Users simply drag and drop GTFS ZIP files directly into their browser, where data is processed entirely client-side without requiring server infrastructure.This application makes viewing GTFS data significantly quicker and easier."
      },
      {
        "title": "Development of Core Technologies for a Metaverse-Based Training Engine in a Scientific Training Environment Platform",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "A metaverse-based training engine is developed to enable high-precision 3D simulations for scientific training environments, enhancing decision-making and spatial analysis through realistic geospatial data and interactive tools.",
        "description": "Development of Core Technologies for a Metaverse-Based Training Engine in a Scientific Training Environment Platform\nJoohyuk Park, Kwangin Han, Hyeongi Min, Sanghak Kim, Geunha Kim, Chaeyoun Lee\nSundosoft Co., Ltd.\n\n1. Introduction\nTraditional 2D geospatial systems present significant limitations in the context of real-time training, simulation, and decision-making. With increasing needs for immersive and spatially rich environments in areas such as disaster response, defense training, and urban planning, metaverse-based platforms leveraging high-resolution 3D data have become essential.\n\nThis paper introduces a core technology platform developed to support metaverse-based scientific training environments. It integrates multi-source 3D spatial data, interactive modeling tools, and web-based visualization interfaces, creating an advanced simulation engine for scenario planning, operational training, and geospatial analysis.\n\n2. Objectives\nThe primary objectives of this project are:\n\nTo overcome the limitations of flat 2D mapping by enabling realistic and immersive 3D simulations.\n\nTo support high-resolution modeling of terrains, buildings, roads, and operational assets.\n\nTo provide web-based access to a 3D simulation environment for use in policy-making, training, and public services.\n\nTo integrate spatial data into metaverse engines for enhanced collaboration and planning.\n\nTo promote public–private–academic collaboration through scalable, shareable data and open APIs.\n\n3. Data Sources and Processing\nA variety of spatial data sources were employed to construct the 3D models, including:\n\nLiDAR point cloud data: Captured via drone or aerial platforms to model surface elevation and object geometry.\n\nOrthophotos and aerial imagery: Used for texture generation and spatial referencing.\n\nVector map data: Used for defining roads, building footprints, and infrastructure.\n\nCustom field survey data: To fill gaps in public datasets and validate terrain accuracy.\n\nThese datasets were fused using a hybrid data processing pipeline. The result was a seamless digital terrain model (DTM) and digital surface model (DSM), which served as the foundation for object and unit placement within the metaverse environment.\n\n4. System Components\nThe system comprises five primary components:\n\n4.1 3D Terrain/Object Editing Module\nThis module allows users to generate and modify terrain and object layouts. Features include:\n\nTopographic surface editing (elevation, slope adjustment)\n\nRoad/pathway creation using spline tools\n\nObject positioning and orientation\n\nExternal model import/export using CSV format\n\n4.2 Unit and Scenario Modeling\nA central feature of the platform is its ability to simulate operational units (e.g., armored vehicles, troops) and define scenario-based behaviors:\n\nEach unit can be assigned movement paths, states, and display attributes.\n\nPainting schemes or external features of vehicles can be modified interactively.\n\nTraining sequences can be saved and replayed for review or evaluation.\n\n4.3 Web-Based 3D Viewer and API\nA web-based 3D viewer was developed using CesiumJS and WebGL technologies. It allows remote users to:\n\nVisualize terrain and unit deployment in real-time\n\nInteract with editable 3D objects via browser\n\nAccess scenario data through RESTful APIs for integration with third-party platforms\n\n4.4 Metaverse Operational Tool (CSCI)\nThe “Metaverse Integrated Operational Tool,” codenamed CSCI, acts as the main interface for simulation control:\n\nDisplays the operational situation in a real-time 3D environment\n\nProvides an overview of unit lists, terrain zones, and scripted events\n\nEnables data exchange with external systems through API gateways\n\nSupports planning, monitoring, and post-exercise analysis\n\n4.5 UI/UX Design\nThe user interface is designed to balance complexity and usability. Major UI features include:\n\nTop Toolbar: For simulation playback, camera control, and object visibility\n\nSide Panels: To manage lists of operational units and active objects\n\n3D Display Area: Central workspace for real-time spatial interaction\n\nExternal System UI: Supports access to other spatial or simulation databases\n\n5. Application Scenarios\nThe platform is intended for cross-domain deployment. The following use cases demonstrate its versatility:\n\nMilitary Training: Realistic simulation of urban combat or tactical missions using real-world terrain and movable units.\n\nDisaster Preparedness: Planning emergency responses for earthquakes, floods, and wildfires with modeled risk zones and evacuation routes.\n\nUrban Planning: Simulation of infrastructure expansion, zoning changes, or traffic flow optimization.\n\nEnvironmental Education: Interactive 3D modules for terrain analysis, land cover study, and climate change impact visualization.\n\nEach scenario benefits from customizable data layers, editable units, and immersive interaction, enhancing training fidelity and decision-making accuracy.\n\n6. Results and Evaluation\nInitial testing demonstrated that the platform can handle large-area terrain datasets (>50 km²) and render them interactively within web browsers. The real-time responsiveness and modularity of the simulation engine were validated through internal scenario drills.\n\nUser feedback from early adopters in the disaster response and defense sectors highlighted:\n\nImproved situational awareness through 3D visualization\n\nEnhanced engagement and learning during training exercises\n\nFlexibility in configuring and deploying custom scenarios\n\nStrong potential for interoperability with digital twin systems\n\n7. Discussion and Challenges\nWhile the platform demonstrates high performance and flexibility, several challenges remain:\n\nData Standardization: Integrating heterogeneous spatial datasets from multiple agencies requires harmonization and metadata control.\n\nReal-Time Scalability: Future deployments will require support for concurrent users and larger datasets, demanding optimization of rendering pipelines and network infrastructure.\n\nAI Integration: The system would benefit from AI-based decision support, such as automated unit behavior or hazard prediction based on real-time data feeds.\n\nInteroperability: Continued effort is needed to align with standards like CityGML, 3D Tiles, and OGC WFS/WMS.\n\n8. Conclusion and Future Work\nThis study introduced a scalable, browser-accessible platform that combines high-precision geospatial data with metaverse-based simulation tools. It enables immersive, flexible training and planning across diverse domains.\n\nFuture plans include:\n\nExpansion of supported data formats and 3D model libraries\n\nIntegration with live sensor feeds and AI-powered analytics\n\nDeployment in public safety agencies and educational institutions\n\nFull support for VR/AR hardware to enhance immersion\n\nBy providing an open, extensible framework, this platform contributes to the growing ecosystem of geospatial metaverse applications, supporting smarter, more informed decision-making in both public and private sectors."
      },
      {
        "title": "Tree shadow modelling in QGIS + GRASS",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Using QGIS, GRASS and Python along with LINZ Dataservice to calculate shadow impact of trees 50 years in the future.",
        "description": "Using QGIS, GRASS and Python along with LINZ Dataservice to calculate shadow impact of trees 50 years in the future, Tim from New Zealand Carbon Farming briefly outlines their approach to maintain compliance with NZ National Environmental Standards for commercial forestry: What we tried; what worked and what didn't."
      },
      {
        "title": "The Open Data Cube is Dead, Long Live the Open Data Cube",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "The Open Data Cube is a big, complicated tool that does a lot of things, and it's a little old. It's been replaced with a small complicated tool, which does less things more simply, which is great. This is a talk about that.",
        "description": "For years, the Open Data Cube has been the go-to framework for managing massive collections of satellite imagery. But with changing cloud-native practices, new open standards, and better APIs, there’s a simpler way forward.\n\nIn this talk, Alex Leith, Executive Director at Auspatious, explains how the heavy Postgres-driven ODC tool is being replaced by flexible, open STAC APIs and elegant Python tools like pystac-client and odc-stac.\n\nThe result? A leaner, easier way to discover and analyze EO data, with just a few lines of code. Long live the Open Data Cube."
      },
      {
        "title": "Fast, Free, and (Mostly) Painless: Getting Started with Open-Source Web Mapping",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Learn how to start using Leaflet, OpenLayers and MapLibre open-source mapping libraries to transform how you build and deliver interactive web maps. We will explore the differences between these libraries, compare their capabilities and go through simple examples to get YOU started lightning fast before you can pronounce FOSS4G.",
        "description": "Open-source web mapping libraries have opened up a world of possibilities for building fast, flexible, and modern geospatial applications without being tied to proprietary platforms. In this talk, we’ll dive into three of the most widely used open-source mapping libraries—Leaflet, OpenLayers, and MapLibre—comparing their performance, developer experience, documentation, and suitability for different types of applications. Drawing from real-world projects, I’ll share why MapLibre was chosen for a recent road safety tool, how OpenLayers offers a great developer experience, and where Leaflet’s simplicity starts to struggle in modern development workflows.\n\nWe’ll cover practical examples to help you get started quickly, as well as also touch on how these libraries can be integrated with a range of spatial data services. If you’re building interactive maps, experimenting with spatial data, or just looking to visualize some locations on a map, this session will help you choose the right tool for the job and hit the ground running.\n\nCode samples will be provided via GitHub."
      },
      {
        "title": "Interactive Simulation for Visualizing Bus Locations Using GTFS Data",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This presentation will introduce a system that leverages GTFS data to visualize bus locations on a map for any chosen time and date.",
        "description": "This proposal utilizes the FOSS4G toolset to encourage bus ridership and enhance the sustainability of urban transportation systems. It specifically showcases an interactive simulation that employs General Transit Feed Specification (GTFS) data to dynamically represent bus positions on a map, corresponding to specific dates and times. This system is ingeniously crafted with FOSS4G tools to enable straightforward tracking of buses' current and planned routes."
      },
      {
        "title": "Finding the Farthest Point: Implementing Longest Path Analysis in QGIS with NetworkX and Python",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "I developed a Python-based solution that extends QGIS routing capabilities by calculating longest paths in road networks using NetworkX and modified Dijkstra's algorithm. This approach transforms traditional shortest-path analysis into maximum distance exploration, could be applied in tourism route design and hydrological analysis.",
        "description": "This presentation demonstrates advanced network analysis methods using Python and NetworkX within QGIS to calculate the farthest reachable point from any starting location. Users can transform road network data into graph networks and visualize complex routing paths across geographic regions. This approach makes advanced network analysis accessible to GIS practitioners without requiring specialized graph theory knowledge."
      },
      {
        "title": "Hitchhiker's Guide to LINZ Open Data",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "A whirlwind tour of LINZ's latest open datasets and useful platform features.",
        "description": "A whirlwind tour across LINZ's data landscape. From Topo maps and Navigation Charts, to Property and Elevation."
      },
      {
        "title": "Outside Track: A Rapid Round-up of Unrepresented Open Source Geospatial Projects",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This talk highlights the diversity of our global community and showcases tools and contributors not otherwise featured at FOSS4G 2025.",
        "description": "FOSS4G is a space where collaboration and innovation converge. But with so many incredible projects, many of them volunteer run, not every maintainer can make it to our global event. In this talk, I’ll take you on a fast-paced, tour of some of the most interesting, impactful, and lesser-known open source geospatial projects that haven’t otherwise found representation at FOSS4G 2025."
      },
      {
        "title": "Research on the development of an image-to-image translation model that outputs SWIR band images using satellite images in the visible band",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "The forest fire index, which can be used to estimate forest fire damage areas using satellite images, Therefore, we conducted a study to increase the possibility of providing a satellite image-based forest fire index by developing a model that generates SWIR band images using multispectral images of visible bands.",
        "description": "The GAN-based image generation model used image-to-image translation, and the training data used RGB spectral band and SWIR spectral band image data from the Sentinel-2 satellite data.\nIn the experiment of the image generation model, visible band satellite images such as Sentinel-2 and Kompsat-3 were used.\nThe accuracy was evaluated by calculating the forest fire index using the generated SWIR image data."
      },
      {
        "title": "Let’s solve the problem of object storage",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Object storage gave us scale, durability, and low cost–the foundation of cloud-native geospatial. But what if it’s also our biggest problem? This talk explores why the object model fails multidimensional datasets, and how the Coalesced Chunk Retrieval Protocol could restore seamless user experience at cloud scale.",
        "description": "Object storage transformed data at planetary scale, giving us cheap, durable, elastic storage. But in geospatial, it also turned chunking into everyone’s problem. Misaligned queries waste time, compute, and money, while producers struggle to optimize datasets for all users.\n\nThis talk reframes our problems with chunking as a limitation of the object model itself. We’ll look at why object storage makes chunking visible, what that means for usability and efficiency, and how the [Coalesced Chunk Retrieval Protocol (CCRP)](https://ccrp.dev) could restore transparent, efficient access — this time at cloud scale."
      },
      {
        "title": "Building Open Geospatial Communities in Nepal: The Role of OSGeo Nepal in Open Data and Youth Empowerment",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "OSGeo Nepal has been building an inclusive and active open geospatial community, with a strong focus on youth empowerment through open data. By forming a network of over 150 members, OSGeo Nepal is equipping the next generation with tools to access, analyze, and contribute to open datasets.",
        "description": "Open geospatial data is reshaping how communities understand and manage their environments, and in Nepal, OSGeo Nepal is leading this change by empowering youth to engage meaningfully with open geospatial tools and data. Since its formal recognition in 2023, OSGeo Nepal has grown into a vibrant, youth-led network of over 150 active members committed to fostering a culture of openness, collaboration, and data accessibility. The community hosts monthly meetups, which serve as collaborative spaces where participants share opportunities, discuss open data practices, and build their capacity to contribute to platforms such as OpenStreetMap, HDX, and QGIS. Senior members actively mentor newcomers, guiding them on how to start their open-source journey and emphasizing the value of contributing to the global open geospatial community. Outreach has been a cornerstone of this movement. OSGeo Nepal has presented at several leading universities, including Tribhuvan University and Kathmandu University, to raise awareness about OSGeo Nepal, open-source geospatial tools, and the importance of accessible data. One of the major programs conducted in collaboration with these university groups is training on QGIS, OSM Hackfest 2024, geo-talk series, and hackathons. Student representatives have been appointed across campuses to sustain engagement and act as catalysts for local geospatial initiatives. Several skill-building sessions and trainings have been conducted on impactful topics like GeoAI, OpenEO, and the application of LLMs in GIS. These events not only provide technical skills but also inspire participants to view open data as a tool for solving real-world challenges. Collaborations with national student groups like NGES and GESAN have helped amplify these efforts. A major milestone has been the launch of OSGeo Nepal’s organizational presence on the Humanitarian Data Exchange (HDX), where volunteers are curating and publishing standardized datasets—such as road networks and administrative boundaries—for public use. This involvement offers young contributors direct experience in the data lifecycle, from acquisition and cleaning to metadata creation and public release. As impactful as the journey has been, OSGeo Nepal is only getting started. In the coming months, the community aims to expand its reach, conducting training programs on open-source software and data stewardship. One of our goals is to build a centralized, accessible data platform where anyone from students to policymakers can discover high-quality geospatial datasets for analysis and application. Further, OSGeo Nepal plans to deepen its collaborations with both national and international organizations, strengthening Nepal’s role in the global open geospatial movement. Through these next steps, OSGeo Nepal envisions a more informed, engaged, and empowered youth community—one that doesn’t just use open data but becomes a driving force in creating, managing, and sharing it for the public good."
      },
      {
        "title": "Mapping Ink: Using Open Source GIS Data to Tattoo a River",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "A friend sent me a message asking if I could extract the Te Arai River outline in Tai Rawhiti for a tattoo idea he had.",
        "description": "Using LINZ Open Data and a few QGIS geoprocessing tools, I was able to extract various river lines for a friend to then pass on to his tattoo artist to add his flare. It was a fun way to use GIS for something personal—turning open data into a design that tells a story."
      },
      {
        "title": "A Year as a Baby OSGeo Local Chapter: OSGeo Nepal",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This talk showcases OSGeo Nepal’s journey as a new OSGeo local chapter, highlighting its grassroots growth, student-led initiatives, and collaborative projects. It demonstrates how a young, volunteer-driven community can make a meaningful impact through inclusive working groups, regular events, and partnerships with academic and professional organizations.",
        "description": "OSGeo Nepal officially became a local chapter of the Open Source Geospatial Foundation (OSGeo) in 2024. This lightning talk will highlight how even a young chapter can make a meaningful impact within a year through grassroots organizing, student engagement, and open collaboration.\n\nOSGeo Nepal is structured into four active Discord-based working groups: Engineering, Data Access, Training & Knowledge Sharing, and Design & Communication. These groups are open spaces where members get to support a wide range of activities like helping each other solve problems, share useful resources, collaborate on tools and datasets, organise events, opportunity sharing, create training materials, and contribute to ongoing projects. It’s a place where beginners can learn from seniors, and seniors can mentor the new contributors in the open-source geospatial space. Regular meetups are held on the first Friday of every month, offering a welcoming space for connection, discussion, learning, and community updates. The chapter has also led community-driven events such as Coding Parties, GIS Day celebrations and Knowledge Sharing Sessions.\n\nThroughout the year, OSGeo Nepal has supported and collaborated with student organizations such as GESAN, GES, and KUCC in events like the OSM Hackfest WRC-2024, Geospatial Meet, NEPGEOM-2024, WebGIS Training, IT Meet-2024, Geo-Talk and Map Design Competition. Chapter members have contributed as mentors, trainers, speakers, judges, and volunteers in such events. This lightning talk will also highlight the expansion of collaboration to professional bodies like NGES through programs like GeoSeries- an online platform that brings together experts and learners to explore applications of geospatial technologies. With student representatives from four institutes and a strong volunteer culture, OSGeo Nepal serves as a bridge between the national and global open geospatial communities. Anyone with a question or idea can drop it in the Discord and someone will always be ready to support.\n\nAlong the way, the chapter has encountered challenges common to emerging communities. Despite these challenges, the commitment of members has remained really strong. Looking ahead, OSGeo Nepal aims to reach more people from underrepresented communities, develop educational content in both English and local languages to improve accessibility, and strengthen collaboration with global OSGeo projects and contributors.\n\nThis talk aims to inspire new chapters, demonstrate the power of grassroots organizing, and focuses on  how even a “baby chapter” can make a meaningful impact within a year."
      },
      {
        "title": "Introduction to OSGeo and ideas for the future",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "The Open Source Geospatial Foundation is a not-for-profit organization whose mission is to foster global adoption of open geospatial technology. This talk introduces the foundation and talks about challenges and ideas for its future.",
        "description": "The Open Source Geospatial Foundation is a not-for-profit organization whose mission is to foster global adoption of open geospatial technology. \n\nIt is the formal host of the FOSS4G global conferences like the Auckland 2025 conference. \n\nIt also acts as an umbrella organization for a large stack of geospatial software applications and libraries.\n\nThis talk will introduce what OSGeo is and does. What are the challenges faced and how can we future proof the foundation?"
      },
      {
        "title": "Navara map engine: Photorealistic Rendering & Lightning-Fast Interaction with a Rust/WASM Headless GIS",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Powered by a newly architected headless GIS engine written in Rust and WebAssembly, Navara combines photorealistic rendering, unconstrained custom-shader freedom, and a high-performance API in a single platform. This session introduces the technology behind Navara and showcases the real-world use cases these capabilities unlock.",
        "description": "Traditional map engines often struggle to (1) expand their visualization capabilities and (2) prevent interactive logic from becoming overly complicated and slow. **Navara**, under development since FY 2024, breaks this mold by *fully decoupling the GIS engine from the rendering engine*. Complex GIS operations—coordinate transformations, LOD management, ellipsoid computations—run in a Rust/WASM headless core, while any renderer (e.g., Three.js) handles drawing.\n\nDuring the current fiscal year we implemented:\n\n- **Visual API** – realistic atmosphere, clouds, night-time lighting, and other photoreal effects.\n- **GIS API** – coordinate conversion, ellipsoid geometry, and other GIS-specific computations.\n\nIn this talk we will dive into these APIs, share architectural insights, and demonstrate the kinds of applications Navara is built to power.\n\n### Intended Audience\n\n- 3D-engine and real-time graphics developers\n- Researchers, municipalities, and enterprises working with GIS data\n- Creators who want to combine photoreal rendering with rich data visualization\n\n### Takeaways\n\n- Design patterns for **headless GIS + rendering-engine** architectures\n- Examples of photorealistic visual effects achievable with Navara\n- How the **GIS API** enables advanced, high-speed interactions"
      },
      {
        "title": "Cut the clutter - clean and clear cartography",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "People love to throw everything including the kitchen sink onto their maps mistakenly thinking more information is better. This leads to a mental burden, making the map harder to read and putting some readers off entirely. This talk will cover designing maps which are clear, uncluttered and support the viewer.",
        "description": "People love to throw everything including the kitchen sink onto their maps mistakenly thinking more information is better. This leads to a mental burden, making the map harder to read and putting some readers off entirely. \n\nThe talk was given at FOSS4G Oceania in 2023 and was inspired by the work of Edward Tufte and how my approach to cartography has changed over the years.\n\nIn this talk, I will cover designing maps which are clear, uncluttered and support the viewer. I discuss how links to other concepts, such as visual hierarchy and layout, the history of thinking about clutter with Tufte and Cairo, cover the idea of the elegant and the invisible, as well as how to keep your viewer foremost in your mind. This talk will be more focused on cartography theory rather than practice with open source tools, however it willl give examples using QGIS."
      },
      {
        "title": "Re:Earth Flow (Alpha): Your Spatial ETL Workspace — In the Browser, Together",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Re:Earth Flow is a new visual ETL tool for geospatial data—fully browser-based and built for collaboration. See the alpha in action, learn how it works, and find out how to get involved early.",
        "description": "Discover **Re:Earth Flow**, a new open-source platform for building geospatial data workflows — visually, collaboratively, and entirely in your browser. Whether you’re cleaning municipal datasets or transforming large geospatial files, Re:Earth Flow lets you connect and configure transformation steps like building blocks, no code required. In this talk, we’ll introduce the tool (currently in alpha), explain its technical foundation, and show how it brings a familiar ETL model to the web — powered by a Rust engine, Go APIs, and real-time collaboration via WebSockets and Yjs.\n\n### **What to Expect:**\n\n- **Why We Built It**: Friction points with desktop tools and the case for a browser-native alternative\n- **How It Works**: A look at the architecture behind our visual ETL engine and collaboration system\n- **Live Demo**: A walkthrough of the alpha release — what’s working, what’s missing, and what’s next\n- **What’s Ahead**: Our roadmap toward beta and full public release, and how you can help shape it\n\n### **Who Should Attend:**\n\nGIS analysts, developers, and open-source enthusiasts curious about accessible, collaborative alternatives to traditional spatial ETL tools — and anyone interested in testing and contributing to an early-stage platform."
      },
      {
        "title": "Enhancing MapProxy with New 2D and 3D Layers Support via Plugins",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "MapProxy is a lightweight geospatial proxy server supporting WMTS, WMS, and TMS. We utilize its plugin system to enable COG, vector tiles, and 3D Tiles support, facilitating integration of modern 2D/3D geospatial formats into open-source enterprise GIS workflows with enhanced interoperability, scalability, and cost-efficiencies in enterprise environments.",
        "description": "As the demand for modern geospatial data formats grows, integrating 2D and 3D content into open-source GIS infrastructures becomes increasingly critical. This talk presents our work on extending MapProxy—a widely used open-source tile and proxy server—to support next-generation geospatial formats through a modular plugin architecture.\n\nWe introduce new plugins that enable seamless support for:\n\n- **Cloud Optimized GeoTIFFs (COG)**\n- **Vector tiles**\n- **3D Tiles**\n\nThese enhancements allow MapProxy to serve as a unified gateway for both traditional and modern geospatial services. They enable seamless integration of high-performance, scalable 2D and 3D data delivery into enterprise workflows, while preserving compatibility with established OGC standards such as **WMS**, **WMTS**, and **TMS**.\n\n**Attendees will gain insights into:**\n\n- The architecture and design of the plugin system  \n- Implementation details  \n- Deployment strategies"
      },
      {
        "title": "AI in QGIS: Hype, Help, or Just a Gimmick?",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "Artificial Intelligence is reshaping GIS, and QGIS plugins are catching up fast. This talk explores how AI and large language models (LLMs) can enhance geospatial workflows. We’ll cover key concepts, demo real tools, and ask: do they work, can we trust them, and what are the trade-offs?",
        "description": "Artificial Intelligence (AI) is no longer just a buzzword - it’s becoming increasingly a useful tool in the world of GIS, and QGIS plugins are quickly becoming more relevant and powerful in this space. This talk explores how AI and integrated LLM’s can enhance geospatial workflows within QGIS.\nWe’ll start by unpacking key terminology: What is AI? What is machine learning? What are large language models (LLMs)? And why do these technologies matter for geospatial professionals?\nThere’s already an abundance of tools available - but which QGIS plugins actually work? Are they helpful in real-world projects? Can we trust their outputs? Who is using them, and how?\nWe’ll explore these questions through practical examples, and also look at the pros and cons of current tools - highlighting key challenges around transparency, usability, and data quality."
      },
      {
        "title": "Introducing OGC Developer Tier membership",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "OGC wants developers more involved in testing and developing standards - and making sure standards have direct benefits in your work.",
        "description": "OGC is introducing a new developer tier of membership to allow developers to contribute and take advantage of OGC resources and activities more easily. This talk will outline the approach and benefits."
      },
      {
        "title": "Shaping a FOSS4G community in Bulgaria",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This lightning talk highlights the progress of forming a FOSS4G community in Bulgaria. After two local FOSS4Gs and one OSM mapathon we are still on our way of establishing the core of the community. These milestones represent significant progress and open up exciting opportunities for collaboration, learning, and growth.",
        "description": "People and companies working with FOSS4G technologies had never gathered together for networking until the first FOSS4G:BG in March 2024. The event had great success bringing together more than 100 participants from both the public and the private sectors. The second edition, held in March 2025, was even more ambitious and featured a full day of workshops followed by a day of talks. Moreover the OSM mapathon in May 2025 sparked interest in many university and high school students. All of this was possible to happen thanks to our informal organisation - QGIS.bg. More than just an organization, QGIS.bg serves as a platform for sharing GIS materials and tutorials in Bulgarian language.\nThis is only the beginning for us in shaping a stable community. While we face many challenges, we believe that we are on the right track. With enough motivation, consistency and shared passion we will eventually grow into the strong and vibrant FOSS4G community that Bulgaria deserves."
      },
      {
        "title": "Filling the Gaps: Mapping Marine Habitats with Divers, Aerial Imagery, and Algorithms",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "ORA Reefs aims to restore degraded rocky reef ecosystems across Tīkapa Moana by removing large-scale kina barrens. To support marine habitat identification and monitoring, we are developing a classification pipeline in Python and QGIS. Using the Depth-Invariant Index to isolate seabed reflectance from water column reflectance.",
        "description": "ORA Reefs is a new initiative developed by Ocean Regeneration Aotearoa (ORA) Trust to actively restore degraded rocky reef and benthic ecosystems across Tīkapa Moana/the Hauraki Gulf, New Zealand. Sea urchin barrens are a dominant stressor on rocky reef ecosystems worldwide due to the overfishing of key predators. Evidence shows that releasing barren rocky reefs from kina (Evechinus chloroticus) grazing pressure in the Hauraki Gulf through large-scale removal enables biodiversity regeneration within 2 years. ORA Reefs’ initial focus is to pilot large-scale kina barren removal alongside the development of artificial reefs on degraded benthic ecosystems. If successful, these interventions, alongside the development of Blue Nature Credits, may offer a way to unlock capital for ecosystems that have historically lacked funding. \n\nWhile dive surveys can provide high-resolution data including species composition and physical characteristics of the subtidal environment, they are expensive, require specialised scientific divers, and only cover small areas. Geospatial classification techniques could better support broad-scale habitat identification and monitoring over a significantly larger area at relatively little cost. However, at present it is difficult to accurately depict biodiversity and physical traits of subtidal marine habitats. Here, we aim to combine aerial imagery with diver surveys using machine learning and deep learning algorithms to classify near-shore broad-scale marine habitat.\n\nWe have begun developing a pipeline using Python and QGIS to classify these marine habitats, mitigating the issue of water column reflectance contribution to seabed reflectance by applying the Depth-Invariant Index (DII). The initial use-case trialled feeding DII layers through a Random Forest model with drone imagery that captured Blue, Green, Red, Red Edge, and Near Infrared (NIR) bands at ~5 cm resolution. Validation accuracy scores are promising and justify continual pipeline development to enhance marine habitat identification at different locations and times."
      },
      {
        "title": "Semantic Spatial Search with Natural Language: Integrating NL2SQL with PostGIS & pgVector",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "We propose a semantic spatial search architecture that understands users’ natural language requests and precisely retrieves optimal locations from PostGIS and pgVector using NL2SQL.",
        "description": "A Semantic Spatial Search Architecture Based on Natural Language: Integrating NL2SQL with PostGIS and pgVector\n\nTraditional location searches have relied on explicit keywords or structured database queries, such as “Auckland Sky Tower.”\n\nHowever, with the widespread adoption of LLMs, users increasingly expect to explore places using ambiguous and abstract expressions, like “the tallest building nearby” or “a cafe with a great view.”\n\nTo overcome the limitations of these conventional search methods, we propose an architecture capable of directly understanding users’ natural language and performing semantic spatial search.\n\nThis architecture is designed around an AI agent that autonomously interprets and executes tasks.\n\nThe AI agent first utilizes natural language processing (NLP) techniques to analyze abstract user requirements—such as “cozy atmosphere” or “cost-effective”—and identifies their core intentions.\n\nBased on these intentions, it employs NL2SQL techniques to directly query spatial and vector data stored in PostGIS and pgVector, enabling precise retrieval of relevant objects.\n\nFurthermore, the AI agent leverages the MCP (Model Context Protocol) to safely and efficiently access a wide range of external data sources, such as map data, user reviews, and real-time information.\n\nThrough the organic integration of these components, the proposed architecture goes beyond simple information retrieval—performing multi-step reasoning on user intent and delivering high-level semantic spatial search that recommends the optimal location.\n\n---\n\nThis work is supported by the Korea Agency for Infrastructure Technology Advancement(KAIA) grant funded by the Ministry of Land, Infrastructure and Transport (Grant RS-2022-00143336, NTIS Grant: 2610000396)"
      },
      {
        "title": "“Develop Traffic Simulator for Urban Planning” — Integrated 2D/3D Platform Workflow for Urban Planning",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This project builds a scenario-based simulation platform for urban planners and policy makers, presenting a real-time integrated workflow that supports the entire process from scenario creation to result analysis and comparison.",
        "description": "Urban traffic systems are rapidly becoming more complex due to urban expansion, new town developments, and the diversification of transportation modes. Therefore, a user-centered system is needed to examine various scenarios during planning and to intuitively understand the results.\n\nPurpose:\n- To support urban planners and policy makers with an end-to-end traffic simulation platform that enables interactive scenario editing, execution, and comparative visualization based on real city data.\n\nThis system is built based on actual urban data, utilizing transportation network and station data from Seoul and Daejeon in GeoJSON and PostGIS formats as simulation input. Users can modify road networks and route structures through a scenario editor, configure various demand conditions, and execute simulations. Scenarios are edited and visualized in 2D and 3D using OpenLayers and CesiumJS, and the results can be explored in real-time through analysis tools such as scenario comparison, time filtering, and route change analysis.\n\nKey Features:\n- Scenario builder for editing roads, routes, and simulation parameters\n- Integrated 2D (OpenLayers) and 3D (CesiumJS) visualization per scenario\n- Time filter, route change analyzer, and side-by-side comparison UI\n- Workflow: Edit → Simulate → Visualize → Analyze\n\nThe entire structure is implemented based on a microservice architecture, allowing each function to be independently scalable and flexibly integrated with external traffic modeling engines.\n\nThis presentation shares the practical implementation experience of an open-source-based integrated workflow platform, applied to real urban data from Seoul and Daejeon.\n\nOpen Source Technologies Used:\n- **CesiumJS** for immersive 3D scenario playback\n- **OpenLayers** for 2D route editing and spatial analysis tools\n- **PostGIS** and **GeoJSON** for simulation data input\n\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (RS-2024-00459703, Development of next-generation AI integrated mobility simulation and prediction/application technologies for metropolitan cities)"
      },
      {
        "title": "GIS for football",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "It can be a hard problem for casual community football groups to line-mark the outline of a football pitch rapidly and dynamically, in varying temporary locations and sizes, on public grounds. Can GIS fix it?",
        "description": "This will be a demonstration of a prototype mobile tool for generating football pitch geometries dynamically on location, that allows casual community football groups to make temporary line markings easily and accurately when combined with a suitable consumer-grade mobile location device."
      },
      {
        "title": "Tracing the Chaos: Observability for Web Applications",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Learn how to bring real-time observability to your web applications using monitoring tools. We’ll cover how to track errors, performance bottlenecks, and user-impacting issues without drowning in logs.",
        "description": "Observability isn’t just for big tech companies, large IT infrastructure or non spatial projects it’s for all web applications too. In this lightning talk, I’ll show how you can get  immediate insights into the real-world behaviour of your applications by using monitoring tools such as Sentry.  \n\nThe talk will use real world examples using Sentry which is a source-available platform with a strong open core model. While not fully OSI open source, it offers transparency, self-hosting, and a generous license for individuals and small teams. In this talk Sentry is used as a practical example, the focus is on core principles of observability and monitoring that apply broadly.\n\nWe’ll start with a quick look at what observability means when working with modern web applications, and why traditional logging alone doesn’t cut it. Then we will show examples of how monitoring integrates into applications, capturing uncaught exceptions, tracking performance issues like slow API calls or render delays, and tying everything back to real user sessions.\n\nI’ll highlight how observability fits into your development workflow and surfacing issues in real time, grouping similar errors,. You’ll also see how performance monitoring works out of the box and might you might need to  tweak to  get useful information, for example retrieving what is displayed on a spatial map.\n\nWhether you’re debugging spatial maps not showing, flaky frontends or mysterious server errors, this talk will give you a fast, practical intro to making your apps more observable using open source tools you can start with today.\n\nCode Examples shown will be provided via GitHub after the session"
      },
      {
        "title": "Beyond Square Pixels: H3 Spatial Indexing for Global Raster Data",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Workflow demonstration converting 20GB+ raster datasets like GEDTM30 global DEM to H3 hexagonal grids at resolution 12. Complete pipeline uses GDAL, DuckDB, and GeoParquet for QGIS visualization. Benefits include eliminated projection boundaries, consistent global coverage, and scalable OGC REST API with O(1) retrieval performance for modern cloud-native workflows",
        "description": "This lightning talk demonstrates a practical workflow for converting raster data (like 20GB+ DEM datasets such as  GEDTM30 global 1-arc-second (~30m or ~900m²) Digital Terrain Model) to H3 Discrete Global Grid Systems (DGGS) with resolution 12 (around ~300m²),enabling global spatial analysis minimising  projection distortions. H3 provides much more consistent spatial partitioning compared to traditional raster projections, especially at global scales, with less proportional distortion (closer to the poles) compared to square pixels in Web Mercator or other commonly used projections.\nThe presentation showcases a complete pipeline: H3 hashes parquet file →  Raster  reading to H3 parquet → Parquet storage and processing (using GDAL + DuckDB) → Parquet to Geopackage conversion → QGIS visualization. Using the GEDTM30 global DEM as example data, pixel values are mapped to H3 cells at specified resolution and stored efficiently in columnar format, then converted to GeoParquet for seamless visualization workflows. When deployed as a OGC REST API, single H3 hash requests achieve near-constant time O(1) cell data retrieval, enabling the system to scale to massive datasets while maintaining consistent per-request performance.\nKey benefits include eliminated projection edge effects, consistent global hexagonal coverage, hierarchical multi-scale analysis, and compatibility with modern cloud-native geospatial tools. Attendees will see live demonstrations of H3 tools, practical code examples, performance comparisons and usage of a H3 REST API as data source on a local calculation workflow. This approach transforms how we handle large-scale geospatial raster data, making global analysis more accessible and accurate."
      },
      {
        "title": "Safe n Redi: Community-Led Resilience Mapping through Open Source Tools in the Pacific",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Safe n Redi is a regional platform supporting disaster preparedness through local mapping of churches, community halls, and shelters. This talk highlights how trained volunteers use KoboToolbox and QGIS to assess evacuation readiness and improve resilience from the ground up in Fiji, Tonga, Vanuatu, and Solomon Islands.",
        "description": "In the Pacific, when disaster strikes, people turn first to places they trust: churches, community halls, and schools. Many of these act as informal evacuation centers—spaces that communities rely on but are rarely documented in formal preparedness systems. The Safe n Redi (SnR) platform was created to change this.\n\nDeveloped through the CAN DO Network and implemented by agencies like ADRA Fiji, SnR is a platform used across Fiji, Tonga, Solomon Islands, and Vanuatu to assess how prepared local buildings are to serve as safe spaces. Whether a church or community hall, each compound is mapped and evaluated on structural integrity, water and sanitation, electricity, accessibility, and evacuation capacity.\n\nWhat makes this initiative unique:\n\nIt combines faith-based coordination (churches still make up the majority of compounds) with inclusive community infrastructure.\n\nIt is open to anyone—enumerators, project officers, and church or community leaders—who receives training on KoboToolbox, mobile-based data collection, and geospatial assessment tools.\n\nThe data is visualized using QGIS, integrated into Power BI, and hosted on a web-based dashboard that supports decision-making at national and regional levels.\n\nThis session will walk through:\n\nThe registration and vetting process for using Safe n Redi across countries.\n\nField implementation: how enumerators are trained in open-source tools to assess buildings on disability suitability, water access, ventilation, structural safety, and more.\n\nExamples from Fiji and beyond, where communities used the platform to advocate for safer evacuation infrastructure.\n\nHow church and community leaders use this platform not just as a technical tool, but as a conversation starter for resilience.\n\nSafe n Redi is an open, adaptable, and locally owned solution for countries that rely on community-driven action in the face of disaster. Built with trust, backed by data, and powered by open tech—it’s a model for resilience from the Pacific to the world."
      },
      {
        "title": "Maintaining a small research tool: Experiences from the R package seg",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This presentation introduces the R package seg developed nearly fifteen years ago. I would like to share the difficulties encountered while developing and maintaining a small academic package, especially as job changes and passions shift.",
        "description": "Approximately fifteen years ago, I developed the R package seg as part of my doctoral research. The package implemented a set of spatial segregation measures used in geography and sociology. Although it remained on CRAN for many years, it received few updates, as I started a job and my interests changed to other areas. With the transition in R spatial infrastructure from sp to sf, the package became incompatible with newer versions and was eventually archived. At the time, I regarded its removal as inconsequential, assuming its utility had lapsed. However, after its removal from CRAN, I started receiving a few inquiries from users requesting an update. Motivated by these requests, I undertook the task of updating seg. While technically straightforward, the process was prolonged by the challenges of revisiting old code and producing adequate documentation. This presentation is about that experience: the reasons behind the package's long period of neglect, the effort required to restore it, and the difficulties I experienced in maintaining small-scale academic software projects."
      },
      {
        "title": "Rebuilding TerriaMap UI: A Minimalist Approach Without Forking TerriaJS",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "TerriaMap is just a wrapper around TerriaJS core. This talk will demonstrate how to strip away the default UI without touching the core and add two custom controls: switching between 2D/3D view modes and toggling base maps.",
        "description": "TerriaMap wraps the powerful TerriaJS engine, but it comes with a pre-built UI that may not suit every project. In this talk, I’ll show how you can completely remove the standard UI in TerriaMap and build a simple custom interface, without touching TerriaJS core code or maintaining a fork.\n\nI’ll walk through how to hook into UserInterface.tsx, connect to the Terria instance, and wire up two minimalist controls: switching between 2D/3D view modes and toggling base maps. The talk will feature a few before-and-after screenshots, as well as a link to a public repository with the full implementation for anyone interested in diving deeper."
      },
      {
        "title": "Kart: Git for Geospatial - Version Control for Vector, Raster, and Point Cloud Data",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Kart brings Git-style version control to geospatial data, enabling teams to track changes, create branches, and collaborate on spatial datasets just like code.\n\nhttps://kartproject.org/",
        "description": "Why Kart?\n- Native support for Postgres, SQL Server, MySQL, and GeoPackage working copies\n- Git under the hood - Works with Git hosting platforms\n- Built-in import/export for common geospatial formats\n- Unified management of vector data, tiled rasters and point clouds\n\nWhat you'll learn:\n- See Kart in action with real-world examples across vector, raster, and point cloud datasets\n- Architectural insights: How we adapted Git's model for spatial data\n- Lessons learned: Technical challenges and solutions in geospatial versioning\n- Hosting options for your Kart data\n\nWho should attend: GIS professionals, data engineers, and developers working with evolving spatial datasets who want better collaboration and change tracking."
      },
      {
        "title": "Wait... QGIS Can Do What?",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "This talk will take you on a whirlwind tour of hidden and advanced features of the world's most popular open-source desktop GIS.",
        "description": "*Did you know you can use QGIS from the command-line?* ... or\n\n*Make a map using just expressions?* ... or\n\n*Perform SQL queries on vector layers?* ... or\n\n*Create 3D fly-throughs?* .. \n\nCome and discover new features and tools you didn't know existed in QGIS and - help you work faster and smarter."
      },
      {
        "title": "State of UN Open GIS Initiative",
        "type": "Talk",
        "track": "State of software",
        "abstract": "UN Open GIS Initiative has been developing an open source GIS bundle for UN operations since 2016. This presentation will introduce its activities such as hybrid GIS development and capacity building. This presentation will also introduce the future directions for the second decade of the initiative, for open discussion.",
        "description": "The UN Open GIS Initiative enters its second decade of transforming geospatial technology within the United Nations system. Since 2016, the initiative has developed open source GIS bundle for UN peace operations. This presentation introduces key activities including hybrid GIS development, capacity building, and drone mapping while exploring future directions for community collaboration.\n\nLaunched in 2016 under the Partnership for Technology in Peacekeeping initiative of the United Nations Department of Operational Support (DOS), the UN Open GIS Initiative emerged from the critical need to efficient geospatial technology across UN operations. Over the past nine years, the initiative has established itself through several key activity areas. Hybrid GIS development pioneered a pragmatic approach to GIS implementation, integrating open-source and proprietary solutions based on operational needs. Comprehensive capacity building programs were developed recognizing that technology alone cannot solve complex geospatial challenges. These programs range from basic GIS literacy for field staff to advanced technical training for geospatial specialists.\n\nThe UN Open GIS Initiative now operates through a federated structure that includes multiple Domain Working Groups (DWGs), each focusing on specific aspects of geospatial technology and application. Among these is DWG 7, the UN Smart Maps Group, which specializes in testing emerging technologies for future geospatial operations. The initiative maintains strategic partnerships with the OSGeo Foundation to ensure alignment with global open-source geospatial development.\n\nThe second decade brings new challenges that require innovative approaches. Scaling across diverse contexts involves adapting solutions that work in peacekeeping contexts for humanitarian, development, and specialized agency needs. Technology evolution requires keeping pace with rapid technological change while maintaining stability and reliability in operational environments. Capacity sustainability ensures that capacity-building efforts create lasting institutional change rather than temporary improvements. Data sovereignty and security considerations balance open data principles with legitimate security concerns and national sovereignty requirements.\n\nLooking ahead, the UN Open GIS Initiative is positioning itself to address emerging global challenges through several strategic directions. Enhanced integration involves deeper integration with UN reform initiatives and the UN's digital transformation agenda. Community-centric development increases focus on community-driven development and local ownership of geospatial solutions. Emerging technology adoption includes systematic exploration and adoption of emerging technologies including distributed web systems, Internet of Things integration, and advanced AI applications. Global partnerships expand partnerships with national governments, academic institutions, and civil society organizations to create a truly global geospatial commons.\n\nThe UN Open GIS Initiative's first decade demonstrates the transformative potential of open geospatial technology in international cooperation. As the initiative enters its second decade, it seeks to deepen engagement with the global FOSS4G community, leveraging collective expertise to address increasingly complex global challenges."
      },
      {
        "title": "How (not!) to shoot yourself in the foot with PyQGIS",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "PyQGIS is great! Except for when it isn't. The incredible flexibility of PyQGIS gives you great power, but with great power comes... great risk of shooting yourself in the foot. In this talk I'll explore PyQGIS, and give tip on writing stable, best-practice PyQGIS code and plugins.",
        "description": "While PyQGIS is great, it exposes many traps for developers. It's EASY to write bad PyQGIS code, and EASY to crash QGIS with this code! 🙃\n\nI'm a QGIS developer with over 15 years experience in writing PyQGIS, and I've experienced many of these issues first-hand. In this talk I'll be drawing on this experience and discussing:\n\n- Common pitfalls when using PyQGIS\n- How to avoid (and debug) crashes and instability\n- What's improved in PyQGIS, and what's coming\n\nI'll also include an expose on QGIS 4.0, Qt6, what these changes mean for your PyQGIS and plugins."
      },
      {
        "title": "vrpRouting: Vehicle-Routing Optimization inside PostgreSQL",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "We introduce vrpRouting, a PostgreSQL/PostGIS extension embedding meta-heuristic VRP solvers. It returns LINESTRING routes and stop attributes, solving multi-depot, pickup-and-delivery, and time-window problems at city scale while enabling fully in-database optimization of stored road networks and logistics data.",
        "description": "Complex logistics—such as parcel delivery, curbside recycling, and emergency response—depend on Vehicle Routing Problem (VRP) solvers that can handle multiple stops, depots, and tight time windows. Most open solutions run outside the spatial database, forcing data to shuttle through files and scripts. vrpRouting eliminates that bottleneck. Distributed as a PostgreSQL extension on top of PostGIS and pgRouting, it embeds optimization algorithms—including tabu-search and other meta-heuristics—directly in database-native functions. Users store streets, depots, and orders where they already live in the database and receive optimized LINESTRING routes and per-stop attributes with no ETL overhead. Under the hood, vrpRouting streams network geometry to its engines, respects turn restrictions and one-way rules from PostGIS, and solves city-scale instances under tight time frames. A single query can pivot from capacitated one-depot scenarios to multi-depot pickup-and-delivery or strict time-window problems by toggling function parameters. We showcase this capability using OSM data for Auckland, routing parcel stops, scores of vehicles, and depot-specific time windows across the entire urban road graph, then visualizing driver sequences live in QGIS through a direct PostGIS connection. The presentation will unpack the extension’s architecture and current function set and walk through the Auckland case study. By keeping optimization within PostgreSQL, vrpRouting enables practitioners to transition from raw road graphs to production-ready routes with minimal database calls, thereby preserving the open-source ethos at the heart of FOSS4G."
      },
      {
        "title": "GeoServer 3 Status Report",
        "type": "Talk",
        "track": "State of software",
        "abstract": "This presentation is an in-depth status update on GeoServer 3, an overhaul of the popular open-source server for spatial data and web services. Supported by a community-driven crowdfunding effort, GeoServer 3 modernizes the platform’s foundation to ensure it meets the growing demands of the geospatial community.",
        "description": "We’ll first analyze the GeoServer 2.x status quo, and the effect of cascading changes that a “simple” Spring upgrade caused, turning the activity into a cross project overhaul, and how the large effort required got socialized and eventually brought to implementation via in-kind volunteering and a crowdfunding campaign driven by Camp2Camp, GeoCat and GeoSolutions.\n\nWe will explore the planned milestones in the transition to GeoServer 3. These include critical refactorings, such as replacing aging libraries, adopting modern Java frameworks, and integrating support for the latest versions of GeoTools and GeoWebCache. Key technical advancements include the evolution and integration of ImageN for improved raster data processing, the migration from Wicket 7 to Wicket 10 for a modernized and more secure web user interface, and the adoption of Jakarta EE and Spring 6 to support enhanced security, scalability, and long-term compatibility with modern Java ecosystems.\n\nJoin us to investigate progress, reflect on the lessons learned, and get inspired by what’s possible with GeoServer 3—a project that continues to empower geospatial professionals and organizations worldwide."
      },
      {
        "title": "Open Source Intellectual Property",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "What even is Intellectual Property? I thought this was a Free Open Source Software conference!\n\nI'll talk about potential issues with software licensing and other boring but important topics to ensure everyone keeps having a good time (except the lawyers).",
        "description": "Intellectual Property laws and rights are a \"Boring but Important” part of the open-source software ecosystem. \n\nIn this talk I'll cover a few topics:\n* The nascent EU Cyber Resilience Act, a new governing framework outlining some additional rights/restrictions/responsibilities for software producers.\n* Why you should care about Intellectual Property Rights.\n* Defining Copyright and Licensing\n* How to choose a Software License\n* How to apply a Software License"
      },
      {
        "title": "Fighting Invasive Predators with Open Source | How QField Empowers ZIP’s Mission for a Predator-Free New Zealand",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "ZIP uses QGIS, QField, and QFieldCloud to coordinate rapid field responses against invasive predators, helping achieve their mission of a predator-free New Zealand with powerful, real-time open-source tools.",
        "description": "Zero Invasive Predators (ZIP), a New Zealand-based charity, is on a bold mission: to eliminate invasive predators and restore New Zealand’s native ecosystems. To meet this challenge, ZIP relies on a powerful open-source geospatial stack—QGIS, QField, and QFieldCloud—to bridge technical planning and field operations.\n\nThis talk will showcase how QField and QFieldCloud enable ZIP’s field teams to efficiently capture accurate data in remote locations and instantly sync updates. This near real-time feedback loop allows for rapid, data-driven decisions in the fight against invasive species.\n\nWe’ll explore ZIP’s field workflows, discuss the advantages of using open-source tools in conservation, and highlight how QFieldCloud’s collaboration capabilities are central to ZIP’s success in combining innovative thinking with boots-on-the-ground action.\n\nJoin us to learn how geospatial open source software is helping protect biodiversity—one synced observation at a time."
      },
      {
        "title": "Open source software in turbulent times",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "AI systems writing code, international tensions, trade wars and real wars. What challenges do we face, and what opportunities do we have as free and open source communities?",
        "description": "A talk about the challenges and opportunities Free and Open Source Software communities and related businesses face."
      },
      {
        "title": "Getting Sentinel Data within seconds",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Explore how to quickly access and analyze Sentinel satellite data using STAC APIs and the Microsoft Planetary Computer. In this talk, we’ll demonstrate how Python makes it easy to fetch imagery and calculate vegetation indices in just minutes.",
        "description": "Satellite imagery is more accessible than ever—but getting the data you need quickly and efficiently can still be a challenge. In this talk, we’ll explore how STAC (SpatioTemporal Asset Catalog) and the Microsoft Planetary Computer simplify the process of accessing and analyzing Sentinel imagery at scale.\n\nYou’ll learn how to use Python and modern geospatial libraries like `pystac-client`, `odc-stac`, and `xarray` to query, filter, and load Sentinel-2 and Sentinel-1 imagery—no downloading or unzipping required. We’ll demonstrate how to filter scenes by date, cloud cover, and region of interest, and then quickly calculate vegetation indices such as NDVI, EVI, and RVI for environmental or agricultural analysis.\n\nWe’ll cover:\n\nWhat STAC is and why it matters\n\nHow the Microsoft Planetary Computer provides cloud-hosted, analysis-ready data\n\nHow to build fast, scriptable analysis pipelines using Python\n\nBy the end of this session, attendees will understand how to go from a geographic query to insightful raster analysis in just a few lines of code—making remote sensing workflows faster, reproducible, and scalable.\n\nPerfect for data scientists, remote sensing professionals, and developers looking to cut through the complexity of traditional satellite data access."
      },
      {
        "title": "A5: Revolutionizing Spatial Indexing",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Meet [A5](https://a5geo.org), a revolutionary pentagonal geospatial indexing system with truly equal area cells offering millimeter precision. Discover why pentagons solve fundamental limitations of existing hexagonal systems like H3, and explore real-world applications from urban planning to environmental monitoring.",
        "description": "For millennia, humans have been fascinated by tiling patterns, but when it comes to partitioning our planet, we've been limited by the assumption that regular polygons are the only option. A5 challenges this assumption and introduces a paradigm shift in spatial indexing through pentagonal cells that offer truly equal areas and superior accuracy.\n\n### The Problem with Current Systems\n\nH3, developed by Uber and now the leading spatial indexing system used across industry, has revolutionized geospatial analysis with its hexagonal approach. However, even H3 faces fundamental mathematical limitations that create real-world challenges for spatial analysis.\n\nThe most significant issue is cell area variation: H3 cells vary by a factor of nearly 2 across the globe, with the largest hexagons being around 2 times larger than the smallest ones. This variation introduces systematic bias in spatial analysis - identical densities appear different depending on location, and statistical comparisons across regions become unreliable.\n\nAnother limitation is that H3's finest resolution offers cells around 1 square meter. As a result, the cells cannot be used to index positions with high accuracy.\n\n\n### The A5 Solution: Embracing Pentagons\n\nA5 takes a radically different approach by embracing pentagons from the start - marking the first time pentagons have been used as the primary building block for a Discrete Global Grid System.\n\nThe system partitions the world into pentagonal cells across 32 different resolution levels, from 12 cells covering the entire world down to cells smaller than 30mm². This extraordinary precision is encoded as a 64-bit integer, making A5 computationally efficient while maintaining millimeter-level accuracy - orders of magnitude finer than H3's smallest cells.\n\n\n### Key Advantages Over Alternative Systems\n\n- **Uniform Cell Sizes**: Unlike H3 and other DGGSs, A5 provides completely equal area cells within each resolution level - 0% error thanks to a novel polyhedral projection based on the Snyder projection. This eliminates bias in spatial analysis and ensures perfect statistical validity across all geographic regions.\n\n- **Minimal Distortion**: A5's geometric construction based on a dodecahedron - the platonic solid with the lowest vertex curvature - results in minimal distortion when projecting onto the sphere\n\n- **High Resolution**: With cells as small as 30mm² at the finest resolution level, A5 enables applications requiring extreme precision, from precision agriculture to infrastructure monitoring.\n\n- **No Special Cases**: Every A5 cell is a pentagon, eliminating the complex special case handling required by systems with mixed polygon types."
      },
      {
        "title": "Accelerating GeoTIFF readers with Rust",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Reading a Cloud-optimized GeoTIFF involves several steps, from fetching compressed bytes over a network/disk, decompressing those bytes, to finally parsing of TIFF tag metadata. Can we speed up the decoding using asynchronous methods, or even GPU-accelerated libraries? Let's see how we can program this in Rust!",
        "description": "How can we compose together a modern library to decode Cloud-optimized GeoTIFFs (COGs) efficiently? By using a programming language called Rust, with bindings to Python, WebAssembly and more, our goal is to enable applications that demand high-performance reads, such as web-based COG tilers or machine learning workflows leveraging Graphical Processing Units (GPUs). For CPU workflows, we delegate the network/disk transfer handling to the [`object_store`](https://crates.io/crates/object_store) crate, use various Rust-based algorithms for decompressing raw bytes, and let the [`async-tiff`](https://crates.io/crates/async-tiff) crate do the actual TIFF tag metadata and pixel data parsing. For GPU workflows, we swap the decompression library for [`nvCOMP`](https://developer.nvidia.com/nvcomp), and do the TIFF parsing using [`nvTIFF`](https://developer.nvidia.com/nvtiff), with the resulting pixel data decoded directly into CUDA device memory. Come and see how these asynchronous and GPU-accelerated GeoTIFF readers compare against GDAL's [`libertiff`](https://gdal.org/en/release-3.11/drivers/raster/libertiff.html) driver, and find out how we're making these performant low-level Rust-based readers more accessible by integrating with the [xarray](https://xarray.dev) ecosystem and beyond!"
      },
      {
        "title": "Bathymetry Data Wrangling",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "This talk discusses workflows implemented in Python for processing bathymetry data for hydrodynamic modelling applications.",
        "description": "While all of Earth’s land surfaces have been mapped at 30 m resolution or finer, the topography of the seabed is still largely a black box, with only 26.1% of the seabed mapped to “adequate resolution”  thus far. In practice, this means that compiling a gridded bathymetric dataset often requires “filling in” areas of missing data, “smoothing” conflicts between overlapping datasets, and “blending” information from multiple sources. This talk discusses several strategies for fusing and interpolating datasets such as coastal lidar, high resolution multibeam echosounder data, chart depths, and the globally available GEBCO grid in real world application. The “fuzzy” boundary between land and sea is emphasized, as terrestrial and marine datasets often disagree when their coverage overlaps. Interpolation and smoothing methods are explored as well. The techniques discussed are implemented in Python using geospatial libraries and thus open source, easily scaled, and reproducible."
      },
      {
        "title": "\"Chef's Kiss\" Webmaps with Svelte, MapLibre & PMTiles",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "An elegant, functional and well-built webmap is a true work of art. With our friends Svelte, MapLibre & PMTiles, you can make your dream map come true.",
        "description": "Since the dawn of interactive webmaps in the mid-1990s, map developers have cycled through many generations of technologies. From the early days of vanilla JS with native DOM and server-side tile rendering, to the modern days of Virtual DOM-based frameworks with WebGL and vector tiles, each generation of technology has utilized the cutting-edge to build the best possible map applications.\n\nJust like the evolution from candles to gas lamps to electric bulbs, each iteration of map technology has brought an overall improvement in functionality while it matures and stabilizes over time. And while the current paradigm of React + ${Map Library} works pretty well, what if we could do it better?\n\nIn pursuit of elegant, highly functional, “chef’s kiss” interactive web maps, this talk presents a pattern of building applications with vanilla MapLibre, Svelte, and PMTiles, and compares the approach to the ways of old. Using the interactive Auckland map integrated into the FOSS4G conference website as an example, it gives heaps of practical advice for developers new and old."
      },
      {
        "title": "eo-tides: Open-source tide modelling tools for large-scale satellite Earth observation analysis",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "`eo-tides` provides powerful open-source tools for combining satellite Earth observation data with tide modelling. These tools can be applied to petabytes of freely available satellite data, providing a suite of flexible tools for efficient monitoring and mapping of coastal and ocean environments – from regional, continental, to global scale.",
        "description": "Freely available Earth observation (EO) satellite data is a powerful resource for mapping and monitoring dynamic coastal environments over time and across large areas. However, the influence of ocean tides means satellite data is often acquired at vastly different tidal stages. This can make it difficult to distinguish true patterns of coastal change from short-term tidal variability, leading to inaccurate or misleading insights into coastal processes. To address this challenge, there is a pressing need for scalable open source tools that can account for tidal variability and make tides an explicit part of coastal EO analysis.\n\nThe new Geoscience Australia `eo-tides` package (https://github.com/GeoscienceAustralia/eo-tides) offers powerful open-source tools for integrating satellite EO data with ocean tide modelling. It provides a flexible Python toolkit for attributing modelled tide heights to satellite data time series, based on each satellite image's spatial extent and acquisition time. eo-tides builds on advanced tide prediction capability from the open-source `pyTMD` library, combining this with spatial analysis tools from the Open Data Cube (ODC)'s `odc-geo`. This enables efficient, parallelised modelling using over 50 supported tidal models, with outputs returned in standardised pandas and xarray formats for further analysis.\n\n`eo-tides` can be applied to petabytes of freely available satellite data accessed via the cloud using ODC’s `odc-stac` or `datacube` packages (e.g. using Digital Earth Australia or Microsoft’s Planetary Computer). Additional functionality supports validation with external tide gauge data and the assessment of potential satellite-tide biases - critical considerations for ensuring the reliability and accuracy of coastal EO workflows. These open-source tools support the efficient, scalable and robust analysis of coastal EO data for any time period or location globally."
      },
      {
        "title": "Exploring urban form in New Zealand",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "We interact with urban form everyday. It dictates where and how we live, move, build and interact with people and services in our cities. This talk will introduce the exciting world of urban morphometrics - measurements of urban form and their implications on urban evolution, function and performance.",
        "description": "This talk will showcase how open data from OpenStreetMap and other resources combined with open source tools from Python and R can help us build a deep understanding of our urban environment. The talk will be linked to a Github repo with reproducible code (and environment) for any interested enthusiast who wants to delve deeper into this topic."
      },
      {
        "title": "Vector Tile Deployment with the United Nations Vector Tile Toolkit (UNVT)",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "The United Nations Vector Tile Toolkit (UNVT) is a collection of Open Source Software (OSS) to produce, host, style and optimize vector tiles. This session introduces how the UN uses this technology in web mapping.",
        "description": "Vector tile technology is becoming more popular due to its various advantages. Vector tiles are smaller in size, allow for dynamic styling without regenerating the tiles, and improve performance for end users. These features make vector tiles useful for base maps in the United Nations.\n\nThe United Nations Vector Tile Toolkit (UNVT) was established in 2018 under the UN Open GIS Initiative, and has provided various tools to support the use of vector tiles. UNVT has grown with the help of many contributors, including the UN Smart Maps Group, which works under the initiative to develop a wide range of features. In the United Nations, UNVT plays a key role in three main processes: Produce, Style, and Host. The following is an overview of each process.\n\nProduce:\nVector tiles are generated by extracting geospatial data from a PostGIS database and converting it using Node.js scripts and Tippecanoe. By selectively removing unnecessary attributes and simplifying geometries, we reduce data size, leading to faster rendering and improved performance for web users. In addition, we update our priority areas daily and run the scripts as a scheduled task to minimize manual effort.\n\nStyle:\nunvt/charites is an easy-to-use, intuitive, and efficient command-line tool for managing vector map styles. With Charites, a long style JSON file can be split into multiple smaller YAML files—one per layer—making it much easier to edit styles. It also provides a convenient live style viewer in a local browser, where changes are reflected immediately.\n\nHost:\nWe have developed a simple Node.js-based vector tile hosting server that delivers PBF files derived from MBTiles in response to each request. The map is rendered using MapLibre GL JS. Additionally, we have created an interface for ArcGIS Online to enable the map to be displayed within the UN system.\n\nThis session will showcase practical use cases, technical implementation details, and lessons learned through the adoption of vector tile technology in the UN context. It will be relevant to GIS professionals, developers, and organizations looking to modernize their web mapping infrastructure using open source tools.\n\nOur toolkit is available on our GitHub account:\nhttps://github.com/unvt"
      },
      {
        "title": "State of GRASS",
        "type": "Talk",
        "track": "State of software",
        "abstract": "GRASS is an open source geoprocessing engine for efficient spatio-temporal data management, analysis, and modeling. The software comes with C / Python / R API, command line and graphical user interfaces.\n\nIn this talk we will give a comprehensive overview of the latest GRASS developments and upcoming new features.",
        "description": "Join us for a lively overview of the current state of the GRASS project, where community meets cutting-edge geospatial technology. Whether you're a longtime power user or a newcomer curious about GRASS, this talk will highlight the major strides the project has made in the past year – from revitalized governance and community growth to technical breakthroughs – and offer a glimpse into what's next.\n\nDuring the talk, we will address how GRASS has strengthened its governance and support structure by bringing in new members to bolster sustainable leadership and new fiscal sponsorship with NumFOCUS. We will also review GRASS community-building initiatives, such as the NSF-backed efforts that allowed GRASS to establish a mentoring program for new contributors, support our Student Grant program, and hold the GRASS Developer Summit 2025 in Raleigh, NC. We will highlight this past summer's Google Summer of Code project, which demonstrates how community mentoring feeds innovation.\n\nThe talk will also address GRASS's new logo and branding initiative over the past year, aiming to give the project a modern look while keeping its iconic elements. Notably, \"GRASS GIS\" is now officially just GRASS – a simpler name that the community has used colloquially for years. To celebrate, the team launched an online swag shop with GRASS-themed apparel, stickers, and more. We will also look at recent strides in community outreach and learning resources, such as a new tutorial website and the modernization of GRASS's documentation platform.\n\nOn the development side, we will show off what the GRASS development team has been hard at work delivering in terms of new features, improved performance, and better integration as part of GRASS 8.5. Under the hood, the team made significant code quality and security improvements, fixing issues flagged by automated linters and code scanners. These efforts pave the way for stricter continuous integration checks and a more robust codebase. The build system is also being modernized: GRASS is transitioning to CMake for easier compilation and maintenance, and an official Conda package is on the way, simplifying installation for Python/R data scientists and lowering entry barriers.\n\nAs we celebrate these achievements, we're also looking ahead. The GRASS roadmap outlines ambitious goals for the next few years. We plan to maintain annual releases (GRASS 8.6 is already on the horizon for 2026) and continue improving distribution and integration – think one-click installs via Conda, tighter bridges to QGIS and R, and refined Python and R APIs for smooth scripting. Sustainability remains a core focus: the project actively pursues new grants, sponsors, and community donations to ensure long-term development while spreading infrastructure knowledge and lowering maintenance overhead to avoid burnout.\n\nIn short, the state of GRASS is strong and dynamic. This talk will offer an informative yet exciting tour of the project's recent milestones across community and technology. We invite everyone – from newbies to veteran developers – to see how far GRASS has come and to get inspired about where it's heading. Learn about the latest capabilities, meet the people behind the project, and discover how you can be part of the next chapter of GRASS!"
      },
      {
        "title": "Developing a user-oriented data cube for biodiversity and carbon dynamics assessment in Estonia with remote sensing data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This presentation covers the development of a national data cube for Estonia, integrating remote sensing data and using open-source tools. It provides analysis-ready data for biodiversity and carbon research, overcoming technical hurdles. User-friendly tools and cloud computing enhance data access, empowering informed decision-making for sustainable development.",
        "description": "Introduction\nAddressing global environmental challenges like land use and climate change requires timely, accurate information. Earth Observation (EO) data, from satellites and UAVs, is essential for monitoring these dynamics. Thanks to open data policies and advancements in software and cloud computing, EO data enhances environmental management and policy assessment, contributing to sustainable development. However, there are technical challenges, including data storage and analysis, and the need for computational architectures that handle large datasets.\nTraditional data cubes often lack the readiness needed for advanced AI and machine learning techniques, which require structured, rich datasets. User-friendly platforms with intuitive access and customizable tools are crucial for researchers and policymakers.\nOur project aims to create a comprehensive data cube for Estonia, utilizing remote sensing and geospatial data and open-source tools to advance biodiversity and carbon dynamics research. The fusion of LiDAR, radar, and passive remote sensing offers untapped potential for modeling, and multi-temporal datasets can predict vegetation and environmental variables effectively.\n\nData and Methods\nWe incorporated data from Sentinel-1, Sentinel-2, Landsat, and high-resolution airborne LiDAR. We used Google Earth Engine and Python for data pre-processing. Additionally, digital elevation models and the Estonian soil map were used to prepare the data cube layers. The study area was divided into manageable tiles using a spatial grid, creating 10m resolution Cloud Optimized GeoTIFFs (COGs) to facilitate efficient processing and downloading.\n\nLiDAR data allowed us to calculate biodiversity-relevant indices, including ecosystem height, cover, and structural complexity. These were processed with tools like PDAL and laspy for precise classification and filtering.\n\nThe data cube runs on a high-performance cloud platform, using S3 storage for COGs and libraries such as rasterio to gather metadata. This metadata is integrated into a STAC-compatible web service, enabling seamless access through platforms like QGIS and Python for efficient querying and processing.\n\nData Cube Access\nOur data cube portal (https://geokuup.ee/estonia) , developed with the Phoenix framework, utilizes MapLibre for data visualization. This setup supports quick visualizations and queries, organizing datasets into collections for user convenience. Users can create custom collections, tailoring data sets to specific research needs, which enhances the system’s flexibility.\n\nBy adopting best practices in geospatial data management, we leveraged open-source tools like GeoServer and pygeoapi, along with the Pangeo ecosystem, to streamline processing. The Phoenix framework offers a robust and efficient solution for managing concurrent users, ensuring stability and performance.\n\nOutcomes and Future Work\nThe data cube provides high-resolution spatial data for academic and governmental purposes, with a strong focus on biodiversity and carbon research. It offers a scalable solution that can be extended to other research domains by incorporating additional data layers.\nFuture work will focus on processing data into multiple resolutions and expanding the range of datasets and workflows to enhance data retrieval and analysis. This will further support informed decision-making and sustainable development initiatives, empowering researchers and policymakers with timely environmental information."
      },
      {
        "title": "From Collaboration to Action: Unlocking Ocean Information Through Pacific Ocean Portal 2.0",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Pacific Ocean Portal 2.0 is a robust, open-source platform empowering Pacific nations with seamless ocean data access, advanced GIS management, visualization, and sector-specific tools. It supports collaboration, real-time monitoring, and expert resources, strengthening ocean services and climate resilience through unified, actionable information for regional stakeholders.",
        "description": "The Pacific Ocean Portal 2.0 is a transformative platform advancing ocean science in the Pacific region by serving as a centralized hub for the dissemination and visualization of oceanographic information. Designed to empower National Meteorological and Hydrological Services (NMHSs) and support key sectors such as tourism, fisheries, ocean monitoring, sea level, and coral reef management, the portal provides seamless access to a comprehensive suite of datasets, including forecasts, near real-time and historical data, and in situ observations.\n\nAt its core, the portal is powered by a robust GIS data management system, ensuring high-quality geospatial data handling and integration for visualization and analysis. Built with a modular, open-source architecture—including THREDDS, GeoServer, FastAPI, and a Next.js frontend—the portal enables developers to create country-specific products and tools that meet unique national needs. New functionalities include timeseries extraction from NetCDF files, on-the-fly map generation, near real-time in situ monitoring, a resource library, and a directory of regional ocean experts. Controlled access to restricted datasets, such as high-resolution wave and inundation forecasts, is also supported for designated countries.\n\nBy fostering interoperability with platforms like PACIOOS and reducing dashboard fragmentation, the portal transforms collaborative ocean data efforts into actionable insights and services. Through unified access to information, expert resources, and advanced GIS capabilities, Pacific Ocean Portal 2.0 empowers communities, strengthens regional scientific capacity, and supports sustainable development and climate resilience across the Pacific."
      },
      {
        "title": "“How to Draw, Urban Traffic Data?” — From Trip Lines to OD Matrices: Visualization Techniques for Traffic Simulation",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This project develops an open-source-based platform that visualizes urban traffic simulation data using various techniques and presents a way to intuitively understand complex movement patterns.",
        "description": "Urban traffic flow is becoming increasingly complex, and it is difficult to grasp its meaning with only numerical data. In particular, there is a need for technical approaches to visually interpret simulation data that includes temporal and spatial patterns.\n\nPurpose:\n- To provide an intuitive and visual way of understanding complex urban traffic simulation results by implementing various spatial visualization techniques using open-source tools.\n\nThis system utilizes spatial data based on GeoJSON and PostGIS to generate various visualization layers, such as vehicle movement trajectories (trip lines), OD matrices, heatmaps, and time-based density changes. Simulation results are visualized in 2D and 3D using OpenLayers and CesiumJS, respectively, and by expressing the same data differently in 2D/3D, interpretability and communication are enhanced.\n\nKey Features:\n- Trip line animations to trace vehicle movement\n- OD matrix visualization with directional flow arrows\n- Heatmap layers for density and congestion hotspots\n- Time-series based map layers with playback\n- Dual 2D/3D rendering and synchronized view interaction\n\nAs a result, users can explore simulation data over time, analyze congestion patterns in specific areas, and intuitively identify complex OD patterns. All components are designed based on a microservice architecture, allowing scalability and integration with various simulation engines.\n\nThis project focuses on implementing various visualization techniques using only open-source technologies and expressing complex traffic flows in a form that anyone can understand.\n\nOpen Source Technologies Used:\n- **CesiumJS** for 3D spatial visualization and animation\n- **OpenLayers** for 2D WebGL vector layer rendering\n- **PostGIS** for spatial data storage and analysis\n- **GeoJSON** as the primary transport and visualization data format\n\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (RS-2024-00459703, Development of next-generation AI integrated mobility simulation and prediction/application technologies for metropolitan cities)"
      },
      {
        "title": "Staying on Track: How we built turn-by-turn navigation for bus drivers with FOSS and Open Data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Learn how AnyTrip built a turn-by-turn navigation and real-time tracking app for bus drivers using GTFS, OSM, Valhalla, OSRM, and other open-source tools - tailored for the unique needs of public transport and deployed in the real world on low-cost mobile devices.",
        "description": "Public transport operators have long struggled to provide bus drivers with accurate navigation. While commercial navigation systems are optimised for general motorists, they fail to take into account bus-specific requirements like bus stops and bus specific routing. Many operators continue to provide drivers with primitive paper navigation guidance which is hard to interpret whilst driving.\n\nAt AnyTrip, we've built a turn-by-turn navigation and real-time bus tracking app tailored specifically for bus operators and bus drivers. Powered using open data, open source tools and free & open source software, the solution integrates:\n\n• GTFS data from operators describing services, stops and pathing\n• OpenStreetMap as the base map and routing graph\n• Valhalla and OSRM for routing and maneuver generation\n• MapLibre Native and Protomaps for custom vector basemaps\n• React Native for cross-platform mobile development\n\nThis talk will cover the architecture and design of the app, including:\n\n• How we transform GTFS static schedule data into navigable paths\n• Choosing between Valhalla and OSRM for different routing needs\n• Handling tricky edge cases like loops, u-turns and busways\n• Deployment on consumer-grade mobile devices for low-cost and ease of deployment\n• Dealing with connectivity constraints and supporting offline use\n\nWe'll also share some lessons learned from real-world deployments.\n\nThis is a practical, field-tested example of how open source geospatial tools can be used to solve real world transport problems. If you’re interested in GTFS, routing, or building geospatial apps for public good, this talk is for you."
      },
      {
        "title": "Saving lives with GIS: engineering our open-source mapping stack",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Abley’s open-source mapping stack powers high performance apps and APIs using PostGIS, GeoServer and MapLibre. The architecture underpins Abley’s SafeSystem data platform and road safety applications. This presentation, aimed at spatial developers, explores the stack’s architecture, discusses engineering challenges encountered and presents key learnings.",
        "description": "Abley’s open-source mapping stack powers its SafeSystem suite of road safety applications and data APIs, helping US transportation agencies optimise investments and reduce deaths and serious injuries on their road networks. The suite delivers consistently fast performance and smooth user interaction, even under heavy data loads.\n\nThe architecture comprises a PostGIS database, GeoServer (serving cached vector tile operational layers and managing security), MBTileserver for contextual data layers, and Nginx as a reverse proxy. Core road safety applications use MapLibre GL JS, plus a standalone API Explorer app enables seamless integration with desktop GIS tools via open standards like WMS and WFS. Docker Compose orchestrates the stack, ensuring consistent environments and enabling robust and scalable deployments, and simplifying testing and debugging.\n\nTopics discussed in this presentation include pros/cons of alternative architectures considered, techniques for securing GeoServer within Docker, challenges and solutions when integrating secure services with desktop GIS and contributing back to open-source communities. The presentation also outlines a pragmatic, agile engineering approach that balances requirements, stability and system security while avoiding speculative over-engineering . Spatial developers will gain insights into performant spatial data hosting and discover practical guidance for getting started with GeoServer, as well as vector tiles and integrating with desktop GIS."
      },
      {
        "title": "Raster processing on HPC without coding? Sure!",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In this presentation I demonstrate how to use the LUMASS visual modelling environment to develop high-performance parallel raster processing models without writing a single line of code. LUMASS models are scalable and run on laptops as well as distributed memory systems. https://manaakiwhenua.github.io/LUMASS",
        "description": "Wrangling large and/or many raster datasets on a laptop or small workstation is no fun! Unfortunately, parallelising models or workflows to run on distributed memory compute clusters requires more than entry level coding skills. In this presentation I introduce the parallel extension to LUMASS' high-level visual raster processing framework. It enables the development of complex models and workflows without writing a single line of code! Check out this short playlist for getting a better idea: https://youtube.com/playlist?list=PL_CsDVZ4IPO-D87TgO0awddJGl2gUYIaD&si=siH6lT_3pwdN3Q-W \nLUMASS provides a range of processing components, including map algebra, zonal summaries, terrain attributes, and more. It uses GDAL for 2D raster I/O and NetCDF for data cubes. It supports SQLite-based raster attribute tables alongside any supported raster format.  Furthermore, it enables the integration of any external command line program or script to be included into the pipeline to extend processing capabilities. Programmers find a Python interface for writing ‘moving window’ functions without having to worry about multi-threading or streaming, which comes out of the box! LUMASS is free and open-source software ( https://github.com/manaakiwhenua/LUMASS )."
      },
      {
        "title": "Democratising earth observation data: co-creating localised national-scale machine learning classification models through country-driven field surveys and Digital Earth Pacific.",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "This paper highlights the datasets, analytical tools, computational capacity and insights made possible through Digital Earth Pacific (DE Pacific). The paper focuses on the use case of participatory land use land cover model calibration and validation using QField and the Digital Earth Pacific Jupyter Analytics Hub.",
        "description": "Introduction\n\nEarth and Ocean observation technologies have advanced rapidly over the past decades, becoming not only more detailed in terms of spectral and temporal coverage but also increasingly accessible for a wide range of users. \nYet there have been ongoing barriers to the uptake and adoption of earth observation analytics needed to inform policy makers. Often these barriers have included complex and overly technocratic language and workflows whereby obstructing access and obscuring insights from satellite data. This has resulted in most analysis of satellite data being limited to academic and research oriented groups.\nOther options have recently emerged with the potential to support a wider range of users to gain access to insights from earth observations. However, many of these work workflows remain obscured due to technical and emerging cost barriers. Others may rely on specific packages and libraries that sometimes become deprecated overtime, reducing the overall long-term replicability of these workflows for wider users.\nRecent advancements in cloud computing infrastructure in the Pacific region have the potential to enable wider access for various users to access standardized and customised replicable workflows  in the long term without cost. \nThis paper highlights the datasets, analytical tools, computational capacity and insights made possible through Digital Earth Pacific (DE Pacific). This is a public technology infrastructure which has learned from the models of Digital Earth Africa, and Digital Earth Australia. \n\nLand cover monitoring:\nLand Use Land Cover (LULC) models shed light on the proportions and distributions of different natural and man-made environments across landscapes at given points in time. When multiple land cover model maps are generated for different points in time, the results can be analysed to detect changes in different land use and land cover classes over time. This analysis is commonly applied to the monitoring and management of a wide range of sectors including, but not limited to, forestry, agriculture, urban planning, infrastructure, water management and mining (Topuz and Deniz, 2023). Yet, there have been persisting challenges for machine learning approaches to meet thresholds of accuracy while generating LULC classification models at scale. Some of these challenges have included:\nGenerating LULC models that provide an accurate prediction of land use and land cover distribution at the local scale (meeting accuracy assessment thresholds of X) \nScaling of LULC models across diverse ecosystems and geographies while continuing to still meet accuracy assessment thresholds. \nReconciling between local needs inputs including groundcover observation points and globally standardised LULC classes and models (for reporting purposes).\nThere is no one ideal classification of land use and land cover, and it is unlikely that one could ever be developed. There are different perspectives in the classification process, and the process itself tends to be subjective, even when an objective numerical approach is used (USGS, 1976). \n\n\n\n\nAims\nThe aim of this paper is to provide an overview of the Digital Earth Pacific and some of its recent scientific benchmarking. \nThis paper focuses on the used case of nationally driven land service for land cover machine learning classification models. This participatory workflow may be of interest to other stakeholders in the Pacific and more widely who are interested in replicating this for other use cases and sectors. In doing so, the paper may shed some light on best practices within the Pacific region for land use land cover model calibration and validation. The paper will also seek to highlight the long-term replicability of these open-source workflows that are not subject to pay walls or commercial platforms. \nThe paper is also intended to raise greater awareness of the current datasets, regional products as well as the methods and workflows used in Digital Earth Pacific.\n\n2. Materials and methods\n2.1. Study area(s)\nStudy areas included PICTs that have participated in past DE Pacific Land Cover Assessment Skills Transfer (LCAST) workshops: Tonga, Fiji, The Republic of the Marshall Islands (RMI), Palau, Tuvalu and the Cook Islands. \n\n2.2. Data and processing\n2.2.1. Satellite imagery and the Digital Earth Pacific GeoMAD\nMen of the analysis ready data products are made possible through the DE Pacific GeoMAD (Leith, forthcoming). \n\n2.2.2. Participatory field data calibration and validation using QField\nQField is an open access and open source mobile application that is connected to quantum geographic information systems (QGIS) software. This mobile app allows for the collection of Geotagged data points, transects, polygons, and other field data features. In this use case, these data sets are crucial for the calibration and validation of machine learning land cover classification models.\nThe participatory elements of these workflows are supported through open-source approaches. The country driven surveys involved use of QField to GPS points that provide a range of different values for the different land cover classes. There are six standardized land cover classes as defined by the IPCC in the Chapter 2 of Chapter 2: Generic Methodologies Applicable to Multiple Land-Use Categories. \nThrough country-driven workshops and surveys, local participants are able to contribute to a spectral database that allows for the training of machine learning land cover models. There are options to collect more detailed classes, including other land uses and land cover types outside of the standardized six classes. By including Traditional Ecological Knowledge (TEK) there is also greater room for localization and customization of capabilities with greater room for local inputs into capturing more complexity in terms of Land Use and Land Cover (LULC) changes . A longer list of land cover classes can also be aggregated into a simpler list, including for compatibility with the IPCC classes.\n\n2.2.3. Participatory post-processing \nThis process involved collating, cleaning and validating all of the data sets collected through the field surveys. Participants were then able to ingest these datasets into Digital Earth Pacific through a Jupyter Notebooks environment to then run the random forest classification and other classifier models through SciKit-Learn libraries. \n\nThe results are shared in this paper including:\n\n1) LULC model results maps and tables\n2) Models trained at a national-scale \n3) Skills transfer for ongoing replication \n\nThere are also areas of Intrinsic value and ongoing capacity building in the region. The paper also shows feedback on the results of pre-and post survey and workshop capacity building shared by the workshop and survey participants."
      },
      {
        "title": "Unlocking the Treasure Trove of Government Data: How LINKS Veda is Advancing Data-Driven Governance",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "To overcome the long-standing underutilization of paper and unstructured data accumulated by MLIT, LINKS Veda has been launched. This talk introduces methods to enhance the quality and speed of policy-making through AI technology, no-code processing, and geospatial data utilization.",
        "description": "LINKS Veda is an initiative by MLIT to build a centralized and reusable data infrastructure by automatically structuring administrative information using AI technologies such as LLM, NLP, and image recognition.\n\n### **Key features include:**\n\n- **Automated Structuring of Unstructured Data**\n    \n    Tasks traditionally done manually, such as organizing PDFs and Excel files, are now automated using AI technologies, significantly improving operational efficiency and data usability.\n    \n- **No-Code Data Processing**\n    \n    Enables intuitive operations using data processing templates, even for non-specialists. It also promotes the reusability and continuous improvement of structured processes.\n    \n- **Implementation Through EBPM and Open Data**\n    \n    Facilitates quantitative analysis for policy evaluation and operational improvement, with use cases spanning areas such as regional tourism trends, logistics, and productivity.\n    \n- **Support for Geospatial Data**\n    \n    Outputs in GeoJSON/CSV formats enable high compatibility with GIS tools and web applications for effective data visualization.\n    \n\nThis talk will share real-world use cases to illustrate how previously untapped government data is being transformed into a vital resource for open innovation.\n\n### **Who Should Attend:**\n\n- Government and municipal officials interested in digital transformation and data infrastructure\n- Researchers and think tank professionals involved in EBPM\n- Data engineers, GIS developers, and no-code/low-code developers\n- Startups and corporate representative working on projects that utilize open or public data"
      },
      {
        "title": "Empowering Urban Planning With Open Geospatial Technologies: The I-Plan Experience In Malaysia",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This presentation highlights Malaysia’s I-Plan system, which empowers urban planning through open geospatial technologies. By integrating multi-source land use data, I-Plan enhances planning efficiency, transparency, and collaboration. Key lessons on data harmonization, stakeholder support, and capacity building will be shared to inspire similar initiatives.",
        "description": "Urban planning increasingly relies on accurate, accessible, and up-to-date geospatial information to support sustainable development and informed decision-making. However, traditional planning systems often suffer from fragmented data, inconsistent standards, and limited stakeholder engagement.\n\nThis presentation shares Malaysia’s experience in addressing these challenges through the development and implementation of I-Plan — an integrated planning land use information system built on open geospatial technologies. I-Plan consolidates multi-source planning data into a centralized, interactive web-GIS platform that empowers planners, agencies, and the public to access, analyze, and utilize spatial information more effectively.\n\nThe presentation highlights I-Plan’s architecture, key features, and the open-source tools that make it scalable and adaptable to various planning needs. It also discusses practical lessons learned in harmonizing diverse datasets, ensuring multi-level support, and building local capacity to manage and maintain the system.\n\nBy showcasing the I-Plan experience, this session demonstrates how open geospatial solutions can drive better urban governance, promote data-driven planning, and foster greater transparency and collaboration among stakeholders."
      },
      {
        "title": "PyForestScan: A Python Library for Large-Scale LiDAR Forest-Structure Metrics",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "We introduce PyForestScan, a Python library that streams LiDAR data and outputs GeoTIFF canopy height, cover, foliage-height diversity, plant-area-index and density layers. We demonstrate it by mapping forest metrics on Hawai‘i Island’s 1 M-ha, 1.8 B-point survey into 1–30 m grids.",
        "description": "Airborne LiDAR now blankets landscapes with up to sub-centimeter-scale detail. Yet, researchers and land managers have lacked a fully open, Python-native workflow to transform billions of points into actionable information on forest structure. Existing solutions are either proprietary or anchored in R, leaving the rapidly growing Python geospatial stack without a scalable counterpart. PyForestScan bridges this gap. Built on PDAL’s streaming I/O, the library reads traditional LAS/LAZ alongside hierarchical octree formats such as COPC and EPT, tiles point clouds automatically, and exports GeoTIFF layers of canopy height, canopy cover, foliage-height diversity (FHD), plant-area density (PAD), plant-area index (PAI), and digital-terrain models in a single command. We demonstrate these capabilities by generating island-wide canopy height, canopy cover, FHD, and PAI mosaics for Hawai‘i Island - an area with over 1 million hectares and over 1.8 billion points, in under 24 hours of wall time on commodity hardware, producing multi-resolution (1 m–30 m) grids ready for carbon accounting, biodiversity assessment, and geoAI training. The talk will outline PyForestScan's architecture, highlight performance benchmarks, and invite contributions via its open governance model, Docker images, and CI-tested notebooks. By placing robust LiDAR analytics squarely within the free and open Python ecosystem, PyForestScan equips the FOSS4G community to move seamlessly from raw point clouds to island- and continent-scale forest insights."
      },
      {
        "title": "Network Analysis at the continental scale, determining new measures for accessibility.",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "The Centre for Australian Research into Access has been utilising the fast-calculating methods of the Pandana library in combination with Geopandas methods to perform continental-scale network analysis. Outputs of our work attempt to highlight the importance of address-level spatial analysis when assessing accessibility to services.",
        "description": "The Centre for Australian Research into Access (CARA) has been utilising the fast-calculating methods of the Pandana Python library in combination with Geopandas methods to perform continental-scale network analysis. CARA's objective is to provide address-level accessibility calculations for health and education with a particular focus on rurality, to assist academic researchers and to inform policymakers. The work we are producing is a result of an Australian Research Council Linkage Infrastructure, Equipment and Facilities (LIEF) grant, which has recently finalised building a spatially detailed infrastructure for a more equitable nation, representing a digital twin for modelling the patterns and processes impacting the Australian population.\n\nCARA has adapted open-source Python libraries, file formats, and data to develop network analysis methods that operate on continental scales that have previously been too costly to process. The Pandana open-source Python library enables fast network calculations to find shortest paths using contraction hierarchies (Foti & Waddell 2012). We have used methods from Pandana to perform ‘number of nearest’ calculations (n-nearest), matrix calculations, and catchment area calculations for both time and distance impedances. Calculations have been carried out using origin data for approximately 10.6 million residential addresses across Australia to various health and education destination datasets. Spatial methods from the Geopandas library were also used to account for the times and distances between origin/destination points and the network, and also to aggregate outputs to spatial units such as those in the Australian Statistical Geography Standard (ASGS). \n\nOutputs from our n-nearest network calculations using the Pandana nearest_pois method (3.7 million nodes, 7.8 million edges, two impedances, and 7,073 destinations) have been completed in less than 3.5 minutes. Outputs from our matrix and catchment network calculations using the Pandana shortest_path_lengths method have fluctuating run times due to the varying number of input origin and destination points with each calculation. However, they are robust in handling a large number of inputs.\n\nThese methods have recently been adapted to use Overture Maps data that will allow them to be applied to a wide variety of countries throughout the world."
      },
      {
        "title": "A Framework of GeoAI for Object Detection and Classification using OGC Standards - From Data Collection to Visualization",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "In this presentation, we discuss a framework of GeoAI for object detection and classification based on OGC standards. This framework has been developed as OGC pilot project, tested for UN VMC, and composed of multiple open-source modules communicating via OGC standards.",
        "description": "Object detection and feature classification are one of the most common applications of Geo-AI. It consists of multiple modules such as training data collection of geospatial data such as geo-referenced images and video, training deep learning model, object classification and detection, and visualization of the results. As diverse environments may be included in the system, the interoperability between modules becomes a critical challenge to develop an integral GeoAI solution. OGC UDTIP (Urban Digital Twin Interoperability Pilot) project aims to provide a GeoAI framework based on OGC standards as part of its requirements. The system is composed of four building blocks – data collection module for geo-reference image or video training datasets, GeoAI analytics module for deep learning model, visualization for presenting the result of detection or classification on a map, and a geospatial platform for coordinating these modules. Many OGC standards are applied such as GeoPose, TrainDML for AI, and OGC-API. The solution has been tested with UN VMC (Verification Mission in Colombia) for collecting the training data. While the first target application was to classify the road surfaces, it is extensible to other types of GeoAI applications such as object detections by simply modifying the code list of TrainDML for AI and replacing proper deep learning model."
      },
      {
        "title": "From Thousands to Seamless: The Power of Virtual Tilesets in Thailand's 3D Landscape",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "Explore how extensive building data in Thailand is efficiently managed using innovative virtual tilesets. Learn about data partitioning with Google’s S2 grid system, transforming and merging thousands of 3D assets for scalable visualization, and the advantages this method offers for effective geospatial data handling.",
        "description": "This presentation explains how large-scale 3D geospatial data for Thailand is managed efficiently. It covers the dataset’s scale and the reasoning behind organizing data using Google’s S2 grid system to improve management and processing. The talk explores the differences between using a single tileset versus multiple tilesets, and how virtual tilesets enable the efficient combination of thousands of 3D assets. Attendees will gain insights into the data and the key benefits of virtual tilesets, including enhanced performance, flexibility, and improved 3D visualization quality. Whether your work involves urban planning, disaster management, or geospatial mapping, this session offers practical strategies for handling complex 3D geospatial datasets."
      },
      {
        "title": "From Data to Decisions: Harnessing Open Geospatial Data and GeoAI for Urban Predictive Analytics",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "This presentation examines the transformative impact of open geospatial data (CSDI Portal) on geospatial analysis and predictive modeling in Hong Kong. It showcases innovative projects that leverage AIoTs and GeoAI to offer actionable insights for urban planning/public health. Forecasted outputs disseminates through interactive dashboard for real-time visualization.",
        "description": "The emergence of open geospatial data from governmental sources regulated by Hong Kong’s open data policy (formalized in 2018) has transformed the methodology for geospatial analysis and predictive modeling. This presentation will highlight the importance of open geospatial data (CSDI Portal) and introduce how innovative projects could leverage the open data to develop near real-time predictive models and analyses using Artificial Intelligence of Things (AIoTs) and Geospatial Artificial Intelligence (GeoAI) technology. Utilizing a variety of open-source libraries and tools for machine learning algorithms, integrating both static and dynamic datasets through API functionalities, to enhance the understanding of urban dynamics in GeoAI applications.\n\nThe showcased innovative projects will illustrate how AIoTs and GeoAI techniques can provide actionable insights for urban planning and public health initiatives. Furthermore, the outputs of these projects will be shared publicly through a map-based dashboard, allowing users and the general public to interact with and visualize the output in real time. Participants will gain insights into the methodologies employed, the challenges faced, and the implications of the findings for smart city development. This presentation hopes to demonstrate the efficacy of open geospatial data and open-source technologies in fostering collaboration and innovation in geospatial research, ultimately contributing to more resilient and adaptive urban environments."
      },
      {
        "title": "Awesome Cloud-based Open-source GIS apps from RMIT students",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "This presentation highlights the feasibility and impact of teaching “Cloud-based Open-source GIS Solutions” through student-developed applications. Featuring five selected apps, it demonstrates how project-based learning fosters practical skills, innovation, engagement with open geospatial technologies, preparing students to contribute effectively to the evolving landscape of open GIS.",
        "description": "This presentation shares the experience of designing and delivering a university course centered on Scaffolded Project-Based Learning (SPL) in the context of cloud-based open-source GIS. Drawing from Cloud-based Open-source GIS Solutions at RMIT University, the session highlights how SPL can be used to equip students with both foundational geospatial knowledge and cutting-edge technical skills, aligning their learning with the demands of a rapidly evolving geospatial industry.\n\nCloud-based, open-source GIS represents an emerging paradigm in geospatial science. It goes beyond teaching students to operate individual tools by emphasizing GIS as a system for spatial thinking and decision-making. Through the SPL framework, students are guided from being tool users to becoming system designers and tool builders. The course structure integrates theory, new technologies, and iterative, scaffolded practice to help students design cloud-native geospatial solutions to real-world challenges.\n\nAt the core of the course is a semester-long, individual project in which students conceptualize, develop, and deploy a functional GIS application. These projects are not just technical exercises—they are opportunities to design systems that perform spatial analysis, support decision-making, and address practical challenges such as urban heat islands, flooding, environmental degradation, and spatial inequality. Students use open-source tools and platforms such as PostGIS, QGIS, GitHub, Cesium Ion, Earth Engine Apps, Mapbox, Leaflet, and GeoAI frameworks, while learning deployment strategies using cloud infrastructure and serverless technologies.\n\nThe SPL pedagogy ensures students build confidence and competence through structured phases:\n\nEarly-stage assignments introduce essential tools, spatial data formats, and cloud workflows;\n\nMid-semester milestones guide system architecture, backend/frontend integration, and data sourcing;\n\nPeer feedback and mentoring support project refinement and encourage collaborative learning.\n\nThis pedagogical strategy fosters not only technical fluency but also critical thinking about how to design geospatial systems that are scalable, interoperable, and user-focused. It reinforces the understanding that spatial technologies are most powerful when they are purposefully assembled to solve complex problems—something increasingly demanded by industry, government, and community sectors.\n\nThe presentation will feature lightning talks by five students whose applications reflect creativity, technical rigor, and engagement with real-world issues. While the applications are in progress, each talk will emphasize the problem addressed, the open-source stack used, and the design decisions made. This format aims to share process over product—highlighting learning outcomes, problem-solving strategies, and reflections on using open-source technologies in a cloud-native environment.\n\nAdditionally, this session will present a broader reflection on how scaffolded project-based learning can transform geospatial education. It will explore questions such as:\n\nHow can SPL help students move from passive users to active developers of spatial solutions?\n\nWhat competencies are most critical for graduates entering cloud-native geospatial workforces?\n\nHow can open-source and academic communities better support each other in this transition?\n\nPreliminary evaluations of the course indicate that this approach promotes student engagement, deepens understanding of geospatial systems architecture, and encourages students to see themselves as contributors to the open-source ecosystem. Several students have expressed interest in continuing their projects post-course or using them as part of a professional portfolio.\n\nReal-world case studies—including the development of virtual flooding digital twins and visualizations for climate resilience—illustrate the alignment between classroom learning and industry needs. These examples demonstrate how SPL, combined with cloud-native open-source GIS, offers a scalable and adaptable model for preparing students to work at the intersection of spatial science, digital technologies, and societal challenges.\n\nBy sharing the design, philosophy, and outcomes of this course, the presentation aims to contribute to broader discussions on how to teach the next generation of geospatial professionals in ways that are future-ready, open, and grounded in systems thinking. It will be of particular interest to educators, developers, curriculum designers, and anyone engaged in building capacity in the FOSS4G ecosystem."
      },
      {
        "title": "Ploughing the Digital field: Redefining farming with Customer-Centric Digital Design",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "In the rapidly evolving world of agriculture, the integration of digital technology has become imperative for advancing productivity, sustainability, and customer satisfaction. Join us to discover how placing farmers at the heart of digital innovation is revolutionizing agriculture, making it more responsive, efficient, and sustainable.",
        "description": "In the rapidly evolving world of agriculture, the integration of digital technology has become imperative for advancing productivity, sustainability, and customer satisfaction. Join us to discover how placing farmers at the heart of digital innovation is revolutionizing agriculture, making it more responsive, efficient, and sustainable.\n\nOur presentation will show how customer-centric design principles can be applied to create digital mapping solutions that directly address the challenges faced by farmers. By prioritising the end-user experience, we've ensured that the tools are not only effective, but also intuitive and accessible for end users with varying levels of digital literacy.\n\nWe will explain the journey we went through to identify and analyse the specific pain points and requirements of farmers, to both provide solutions to their needs, and drive towards a leading, simple customer experience.\n\nThe session will also provide insights into how we blended proprietary GIS capabilities with open-source technologies to deliver a seamless, customer-focused experience. We’ll discuss how we integrated ESRI Maps technology within a React web application, incorporated ArcGIS Enterprise, and implemented performance and error monitoring to support a stable, responsive user experience.\n\nWe found that by focusing on the real needs and challenges of farmers, we can create digital solutions that are not only technologically advanced but also practical, user-friendly, and capable of delivering tangible benefits in terms of efficiency, productivity, and sustainability."
      },
      {
        "title": "DigiAgriApp: development of a free and open source app for digital agriculture",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "**DigiAgriApp** is a multilingual, open-source platform for precision agriculture, supporting field monitoring down to individual plants. In 2025, there are several new innovations at the software level, but especially for the new public servers.",
        "description": "**DigiAgriApp** is a free and open-source client-server application built on PostgreSQL/PostGIS, Django (backend), and Flutter (frontend), with an integrated QGIS plugin. The development was started and carried out by the Digital Agriculture Unit of the Fondazione Edmund Mach with the support of a small number of companies. Over the past year, the platform has consolidated its architecture and expanded its features to better support precision agriculture.\n\nIts primary focus is the monitoring of agricultural fields and its sub-elements: subfields, rows, until individual plants. A major advancement was the integration of the Pest Patrol module, which leverages computer vision algorithms to automatically analyze images from chromotropic traps and identify insect vectors such as Scaphoideus titanus and Orientus ishidae. This sets the stage for a robust decision support system for pest and disease management.\n\nIn addition to ongoing development work, significant efforts were made toward multilingual support—DigiAgriApp is now fully available in four languages—and in project outreach, seeking collaborators and institutions interested in adopting or contributing to the platform.\n\nA major milestone in 2025 has been the launch of two public instances of the DigiAgriApp server, both released with the support of the Fondazione Edmund Mach. One instance, in Italian (https://digiagriapp.fmach.it), is tailored to viticulture and fruit production. The second, in English (https://digiagriapp-en.fmach.it), offers broader crop support and allows the addition of new species. These public instances are limited in functionality, as they currently do not provide access to remote sensing or weather station data."
      },
      {
        "title": "Building an Ocean Data Ingestion and Discovery system with Open Source Software",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "The Australian Ocean Data Network has built a system to ingest ocean data and make it discoverable to end users. This talk will demo the system and look at the open source technologies, emerging and traditional, that we've used to make it secure, scalable and adaptable.",
        "description": "The new Australian Ocean Data Network system which has been built over the last three years uses a host of open source technologies including:\n- Elastic Search and Geonetwork for metadata management and search\n- Zarr, Parquet and NetCDF for data storage and retrieval\n- Prefect and dask for data ingestion and optimisation pipelines\n- Terraform for infrastructure management\n- SpringBoot and FastAPI for backend apis following modern OGC standards\n- React for frontends\n- Jupyter Notebooks for data access tutorials for advanced users\n\nOur open data discovery portal launches in August 2025."
      },
      {
        "title": "Surveys for Infrastructure Resilience and Geospatial Exposure: Applying Open-Source Tools and Methods in the Pacific.",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This presentation shares how open-source mapping tools were used to collect data on buildings and infrastructure in Kiribati, Tonga, and Vanuatu. The data helps governments understand which areas are most at risk from disasters like cyclones and floods, and supports better planning for safer, stronger infrastructure.",
        "description": "This presentation will showcase the innovative application of open-source geospatial and participatory methods in the \"National surveys for infrastructure resilience geospatial databases to support exposure and hazard modelling for Kiribati, Vanuatu, and Tonga\" project. This project addresses urgent needs in the Pacific, in terms of the lack of comprehensive, up-to-date data on buildings, roads, and critical infrastructure, and the need for robust hazard and exposure models to inform disaster risk reduction and climate adaptation.\n\nThrough extensive field surveys, local teams from various government ministries were trained and supported by SPC, with the assistance of existing technical staff in the country, to collect detailed geotagged data on infrastructure assets, buildings, and information on roads, bridges, and utilities. These datasets are validated, cleaned, and uploaded to the country level or regional open-source geodatabases, ensuring accessibility and long-term sustainability.\n\nOpen-source tools, including QGIS, QField, and Kobo Toolbox, were utilized for data collection. The approach emphasizes capacity building, with a strong focus on gender equality and social inclusion. The data collected and the results of the geospatial databases and hazard models (for cyclone, floods, sea-level-rise) directly support the government in planning resilient infrastructure, developing early warning systems, and advocating for loss and damage funding.\nBy leveraging open-source methods, the project promotes sustainable, scalable, and locally adaptable approaches to resilience planning in the Pacific."
      },
      {
        "title": "Icechunk 2.0",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "[Icechunk 2.0](https://icechunk.io/) is a transactional, cloud-native storage engine for [Zarr](https://zarr.dev/). This talk introduces powerful new features for transactionally managing massive geospatial arrays, including efficient appends/inserts.",
        "description": "[Icechunk](https://icechunk.io/) is an open-source, cloud-native transactional storage engine for multi-dimensional arrays, designed to manage massive geospatial datasets. Building upon the foundational features of data versioning and schema evolution presented in its first iteration, Icechunk 2.0 introduces a more stable and performant on-disk format that unlocks powerful new capabilities for managing large-scale array data. This presentation will introduce the new features of Icechunk 2.0 and demonstrate their application to common challenges in geospatial data analysis.\n\nManaging petabyte-scale geospatial data, such as satellite imagery time-series, gridded weather forecasts, and climate model outputs, requires tools that can handle evolving data and complex operational pipelines efficiently. Icechunk 2.0 directly addresses these needs with several key innovations:\n\n- **Efficient Array Manipulation:** A new indexing capability allows for cheap appends, prepends, and inserts into an array without rewriting existing data chunks. This is transformative for managing growing time-series datasets.\n- **Flexible Data Organization:** Users can now rename or move arrays and groups within the [Zarr](https://zarr.dev/) hierarchy without costly data duplication, simplifying the curation and organization of large data repositories.\n- **Enhanced Data Governance:** The introduction of an amendable commit option simplifies the version history, while a comprehensive operation log and support for repository-level metadata provide crucial data provenance.\n- **Improved Performance and Stability:** The new format enables significantly faster and safer garbage collection and more efficient queries of a repository’s history.\n\nWe will demonstrate how Icechunk 2.0 integrates seamlessly into the Scientific Python ecosystem ([Xarray](https://xarray.dev/), [Dask](https://www.dask.org/)) via its Zarr store interface and how we have built Icechunk support into the managed [Earthmover platform](https://earthmover.io/platform). Through real-world examples, we will showcase how these new features can be used to build robust, high-performance data pipelines for cloud-based geospatial analytics and machine learning."
      },
      {
        "title": "From Data to Dynamic Maps: Integrating STAC Catalogs with Titiler for Scalable Map Rendering",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "This talk will guide you through the process of creating STAC catalogs and integrating them with Titiler for dynamic, real-time map rendering. I will share my personal experience with using these tools to streamline geospatial workflows and scalable map visualizations.",
        "description": "In this talk, I will take you through the process of creating and integrating STAC catalogs with Titiler to enable dynamic and real-time map rendering. I will begin by explaining how to create a STAC catalog to efficiently organize and store your geospatial data, making it easy to search and access. Next, I'll demonstrate how to host the catalog so it can be easily retrieved for map visualization. The main focus of the talk will be on integrating STAC with Titiler, a powerful tool that transforms your data into interactive, zoomable map tiles. Drawing from my personal experience, I will share how this integration has streamlined workflows and improved the performance of map rendering, making it faster and more scalable. By the end of the talk, you'll gain a clear understanding of how STAC and Titiler can work together to bring your geospatial data to life through scalable and dynamic maps."
      },
      {
        "title": "Centering Communities in Early Action Local Knowledge: Open Data, and OSM in Practice",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Behind every early warning system is a community that knows where it floods, who is at risk, and how to respond. This session shows how OMDTZ uses OSM and open tools to empower youth, elders, and leaders, placing them at the center of anticipatory action and transforming vulnerability into resilience.",
        "description": "Dar es Salaam is a region where urban flooding disproportionately affects informal settlements. The power to act early often lies not with technology, but with people.\nThis talk showcases how OpenMap Development Tanzania (OMDTZ) has worked to place communities, not just tools, at the center of anticipatory action. By training and mobilizing youth, local leaders, and elders based on data collection, OMDTZ enabled vulnerable communities to lead flood preparedness efforts from within.\nParticipants were trained to use the simple, open tool ODK to map risk prone areas such as blocked drains and report on water level on the river during the rain . But more than just mapping, they became part of the decision making process, identifying: Safe shelters and evacuation paths, Impassable roads during floods, and areas with inadequate drainage or past flood impact through participatory mapping.\n\n\nBeyond the data, this process built a sense of ownership, Local leaders and community participants during the group discussion and training are now ambassadors in their community on why they need to take early action, such as clearing their blocked drains and making sure all trash is collected before the rainy season, to reduce the impact of flooding in their communities \nWe’ll share:\nThe model for engaging different community groups in anticipatory action\n\n\nLessons from working in both urban and peri-urban settings\n\n\nThe transformation of mappers into advocates and preparedness leaders\n\n\nReal-world action: how local leader in informal  settlements inform their community on early warnings \n\n\nAttendees will walk away with a community first approach to preparedness, useful for any context where local knowledge is rich but formal warning systems are weak."
      },
      {
        "title": "Participatory mapping in South Kivu province: uses of QGIS and QpenstreetMap for local development",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "This presentation describes the use of QGIS and OpenStreetMap in a participatory mapping project in South Kivu (DRC), aimed at supporting local development, resource management and community engagement through FOSS4G tools adapted to low-resource contexts.",
        "description": "This presentation describes the use of QGIS and OpenStreetMap in a participatory mapping project in South Kivu (DRC), aimed at supporting local development, resource management and community engagement through FOSS4G tools adapted to low-resource contexts.\n\n\n\ncette présentation décrit l'utilisation de QGIS et OpenStreetMap dans un projet de cartographie participative au Sud-kivu (RDC), visant à soutenir le développement local , la gestion des ressources et l'engagement communautaire à travers des outils FOSS4G adaptés aux contextes à faibles ressources\n\nLa province du sud-kivu , dans L'est de la République démocratique du Congo , fait face des nombreux défis  liés au développement ,à l'environnement et à la gouvernance territoriale .Dans ce contexte, nous avons initié un projet de cartographie participative impliquant des communautés locales , des ONG et des acteurs institutionnels .L'objectifs :\nproduire des données géographiques fiables et accessibles pour appuyer les décisions locales.\nGrace à l'utilisation d'outils open source tels que QGIS , OpenSTreetMap et KoboToolbox,les participants ont pu collecter ,structurer et visualiser des données sur les infrastructures ,les ressources naturelles et les dynamiques foncières .Les cartes produites ont servi à la planification d'interventions , communautaires , au suivi environnemental, et à la gestion de conflits d'usage des terres.\ncette présentation partagera le déroulement du projet ,les défis rencontrés (accès à l'énergie, connectivité ,formation ),les résultats concrets obtenus ,et les leçons apprises sur l'intégration des outils FOSS4G dans un contexte à faibles ressources .Elle illustrera comment ces technologies peuvent favoriser l'autonomisation locale et la co-construction de solutions durables dans des contextes fragiles  \n Pour le parcours académique mon mémoire s'intitule : Cartographie Participative en Province du Sud-Kivu : Usages de QGIS et OpenStreetMap pour le Développement Local\nRésumé\nLa province du Sud-Kivu, en République Démocratique du Congo (RDC), connaît depuis plusieurs années de fortes pressions sur son territoire en raison de l’expansion démographique, des tensions foncières et des défis liés à l’aménagement du territoire. Dans ce contexte, la disponibilité de données géographiques précises et actualisées reste un problème majeur, limitant ainsi la capacité des autorités locales et des communautés à planifier et à gérer efficacement leur environnement.\n\nLa cartographie participative s’est récemment imposée comme une approche innovante et inclusive pour combler ce déficit d’informations. Elle repose sur la participation active des populations locales dans le processus de collecte, d’analyse et de mise à disposition des données spatiales. Ce mémoire examine les usages et les apports de deux outils libres et accessibles, QGIS et OpenStreetMap (OSM), dans des initiatives de cartographie participative en province du Sud-Kivu et leur contribution au développement local.\nIntroduction\nLa cartographie est un outil indispensable à la gouvernance locale et à la planification des services de base tels que l’éducation, la santé, la gestion foncière et la prévention des risques naturels. Toutefois, dans plusieurs provinces de la RDC, dont le Sud-Kivu, les cartes disponibles sont souvent obsolètes ou inexistantes dans certaines zones rurales. Les acteurs locaux et les organisations de la société civile manquent de ressources et de compétences techniques pour produire et mettre à jour ces données.\n\nDans ce contexte, la cartographie participative apparaît comme une alternative efficace, en impliquant les habitants eux-mêmes dans l’identification, la collecte et la valorisation des informations géographiques. L’émergence d’outils libres et gratuits comme QGIS et OpenStreetMap facilite cette démarche et ouvre de nouvelles perspectives pour le développement local.\nMéthodologie et Outils Utilisés\nLa cartographie participative au Sud-Kivu se déroule en plusieurs étapes : formation des acteurs locaux, collecte des données sur le terrain à l’aide de GPS ou de smartphones, saisie et traitement des informations via QGIS, puis diffusion et partage à travers OpenStreetMap. Des formations courtes sont dispensées aux agents des services communaux, étudiants et membres d’associations locales pour leur permettre de manipuler ces outils.\nQGIS est un système d’information géographique open-source qui permet d’importer, d’analyser et de produire des cartes thématiques adaptées aux besoins locaux. Quant à OpenStreetMap, il s’agit d’une plateforme collaborative mondiale où les contributeurs peuvent intégrer et partager en ligne des données géographiques librement accessibles.\nRésultats et Applications Pratiques\nPlusieurs projets menés au Sud-Kivu ont démontré l’efficacité de cette approche. À Bukavu, Kalehe et Idjwi par exemple, des initiatives de cartographie participative ont permis de localiser des infrastructures sociales comme les écoles, centres de santé, marchés, réseaux routiers et points d’eau potable. Les cartes produites ont facilité la prise de décisions et la planification communale, notamment pour l’identification des zones prioritaires d’intervention.\nDe plus, la cartographie participative s’est révélée utile dans la prévention et la gestion des conflits fonciers. En permettant aux communautés de participer à la délimitation de leurs terres et à l’identification des espaces collectifs, elle a contribué à réduire les tensions et à instaurer un dialogue entre villages et autorités\nL’utilisation d’OpenStreetMap a également favorisé l’accès rapide et libre aux données produites. Celles-ci sont immédiatement disponibles en ligne, ce qui permet leur consultation par divers acteurs (autorités, humanitaires, chercheurs, etc.) et leur réutilisation dans d’autres projets.\nLimites et Contraintes\nMalgré les résultats encourageants, la cartographie participative au Sud-Kivu se heurte encore à certaines contraintes. Le manque d’accès à une connexion internet stable dans les zones rurales complique la mise à jour régulière des données sur OpenStreetMap. De plus, l’absence de matériel informatique et de ressources financières suffisantes limite la généralisation de ces initiatives.\nPar ailleurs, la pérennité des compétences acquises par les participants constitue un défi. Les formations, souvent ponctuelles et liées à des projets à court terme, ne garantissent pas une continuité dans la maîtrise des outils et des méthodes.\nPerspectives et Recommandations\nPour renforcer l’impact de la cartographie participative et assurer la durabilité des acquis, il est essentiel de :\n\nMettre en place des clubs SIG communautaires et des cellules de cartographes citoyens dans les communes et institutions académiques.\n\nEncourager la création de partenariats entre autorités locales, ONG, universités et organisations internationales.\n\nIntégrer la cartographie participative et l’utilisation des outils libres dans les politiques publiques de développement territorial.\n\nAssurer des formations continues pour les acteurs locaux afin de maintenir et de développer leurs compétences techniques.\nConclusion\nLa cartographie participative, soutenue par des outils libres tels que QGIS et OpenStreetMap, constitue une réponse pertinente aux défis de gestion du territoire et de développement local au Sud-Kivu. Elle permet d’associer les communautés à la production et à l’exploitation de données géographiques fiables, favorisant ainsi la gouvernance participative et la planification efficace des services. Pour pérenniser ces initiatives, il est indispensable de structurer des réseaux locaux de cartographes citoyens et de soutenir la diffusion des compétences et des ressources à l’échelle provinciale."
      },
      {
        "title": "pygeometa project status",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pygeometa project status presentation.  Come and find out the latest news on the project as well as future plans, and how to get involved!",
        "description": "pygeometa provides a lightweight and Pythonic approach for users to easily create geospatial metadata in standards-based formats using simple configuration files (affectionately called metadata control files [MCF]). Leveraging the simple but powerful YAML format, pygeometa can generate metadata in numerous standards. Users can also create their own custom metadata formats which can be plugged into pygeometa for custom metadata format output.\n\nFor developers, pygeometa provides a Pythonic API that allows developers to tightly couple metadata generation within their systems and integrate nicely into metadata production pipelines.\n\nThe project supports various metadata formats out of the box including ISO 19115, the WMO Core Metadata Profile, and the WIGOS Metadata Standard.\n\npygeometa has minimal dependencies (install is less than 50 kB), and provides a flexible extension mechanism leveraging the Jinja2 templating system.\n\nThis presentation will provide an update on recent enhancements, use in high profile projects as well as future plans and roadmap."
      },
      {
        "title": "The Tanzanian Open Mapping Journey (Building Awareness, Capacity, and a Sustainable Ecosystem)",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Tanzania has become a model for using open mapping to support development, disaster preparedness, and climate resilience. This session explores OMDTZ led efforts in awareness, capacity building, and partnerships highlighting projects like Dar Ramani Huria and showcasing how communities and institutions use OSM for real world impact.",
        "description": "This presentation will take participants through Tanzania’s open mapping journey starting from early pilot projects like Dar Ramani Huria, to the growth of national and regional mapping communities, and the integration of open data into real world decision making. We will share how young people, universities, government institutions, and communities have been engaged and empowered to map their own environments using open tools and technologies.\nKey focus areas include:\n*Raising awareness of the value of open geospatial data.\n\n*Training and capacity building for students, professionals, and local governments.\n\n*Creating partnerships to support long-term sustainability and data use.\n\nLed by initiatives like OpenMap Development Tanzania (OMDTZ), the journey showcases how awareness, capacity, and sustainable systems have been built through collaboration with youth, universities (such as University of Dar es Salaam and Ardhi University), communities, and government institutions. \nParticipants will gain insights into key milestones, impactful partnerships, and practical lessons learned in data quality, institutional adoption, and long term ecosystem growth offering a roadmap for building open mapping ecosystems in similar contexts worldwide."
      },
      {
        "title": "Embracing Cloud-Native Geospatial with AWS",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Join me on this tour de force to learn how to optimize and cross-connect AWS services alongside common GIS software integrations.",
        "description": "Embracing cloud-native geospatial with AWS represents a paradigm shift in how organizations handle location-based data and spatial analytics. This modern approach combines the power of cloud computing with geospatial technologies to create scalable, efficient, and cost-effective solutions.\n\nAWS provides a comprehensive ecosystem of services fully capable of embracing geospatial applications.  Key components of cloud-native geospatial on AWS include:\n\n1. Storage Solutions: Amazon S3 for storing spatial data files, while Amazon Aurora with PostGIS extension handles spatial databases.\n\n2. Compute Services: AWS Lambda for serverless processing of spatial data, Amazon EC2 and AWS Batch for more intensive computational tasks, and Amazon EMR to embrace Spark and Apache Sedona.\n\n3. Analytics Tools: Amazon Athena for querying geospatial data, and Amazon SageMaker for spatial machine learning applications.\n\n4. Visualization: Amazon QuickSight for creating interactive maps and dashboards.\n\nThe benefits of this approach include:\n- Scalability to handle varying workloads\n- Pay-as-you-go pricing model\n- Reduced infrastructure management overhead\n- Global availability and low latency\n- Built-in security and compliance features\n\nThis cloud-native approach enables you to deliver value through geospatial applications rather than managing infrastructure."
      },
      {
        "title": "ZOO-Project: news about the Open Source Generic Processing Engine",
        "type": "Talk",
        "track": "State of software",
        "abstract": "The ZOO-Project is an open-source software platform that implements the OGC API Processes standard, providing a comprehensive solution for creating, deploying, and managing web processing services tailored to geospatial and Earth Observation (EO) applications.",
        "description": "The project began in 2008 with the implementation of the Web Processing Service (WPS), a standard defined by the Open Geospatial Consortium (OGC) that enables the exposure of geospatial processes through standardized web interfaces. This initial step established the ZOO-Project as a key tool for the geospatial community, facilitating the integration of geospatial workflows into web-based environments.\n\nAs a core component of the European Space Agency’s EO Exploitation Platform Common Architecture (EOEPCA), the ZOO-Project plays a pivotal role in enabling interoperable workflows that can operate seamlessly across diverse platforms. By adhering to the OGC API Processes standard, the ZOO-Project ensures that workflows are compatible with a wide range of systems, facilitating integration into various applications without reliance on specific infrastructures or interfaces.\n\nThe ZOO-Project’s support for the Common Workflow Language (CWL) further enhances its utility, enabling workflows to be described in a standardized format that ensures portability and scalability. This capability is particularly valuable in the context of EOEPCA, which promotes the sharing of data and services while advocating for the use of open standards. The ZOO-Project is actively sponsored by EOEPCA, transforming its vision into practical tools for researchers, developers, and platform operators.\n\nThis presentation will explore the current state of the ZOO-Project, highlighting its ongoing evolution, including new features that enable the execution of workflows in containerized environments such as high-performance computing (HPC) systems and Kubernetes, leveraging distributed platforms like Argo Workflows. These advancements ensure that the ZOO-Project remains at the forefront of addressing modern EO challenges, offering scalable and interoperable solutions for a broad spectrum of users."
      },
      {
        "title": "pgRouting feature frenzy",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Feature frenzy of 10 years of pgRouting evolution: On 2013 with the new version 2.0 there were only 11 graph functions, now after 12 years we have 89 functions and a grand total of  222 different signatures, when variations or overloads of the functions are considered.",
        "description": "We will cover:\n- What is pgRouting?\n- Where can pgRouting be used?\n- pgRouting function classification.\n- Frenzy of pgRouting features from A to Z"
      },
      {
        "title": "State of STAPI: A community tasking standard",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Explore STAPI, a specification for a Sensor Tasking API. We’ll highlight recent developments, showcase the open-source projects being developed in the ecosystem, and share the community's vision of increased interoperability driving the next generation of geospatial workflows.",
        "description": "Community standards, created through collaborative grassroots efforts before being widely adopted, play a crucial role in geospatial interoperability as exemplified by specifications like SpatioTemporal Asset Catalog (STAC) and Cloud-Optimized GeoTIFFs (COGs). Efforts like these not only enable seamless data interoperability but also form the backbone of robust, scalable systems that support critical geospatial operations.\n\nThe Sensor Tasking API (STAPI) is an emerging community standard aiming to standardize sensor tasking and spatiotemporal data ordering through a unified API and an ecosystem of tooling. Five community sprints have been held across the US and Europe, most recently this past April in Lisbon, where the spec reached the major milestone of a version 0.1.0 release. Featuring collaboration amongst government groups, commercial satellite operators, data integrators, and other community members, these iterative and collaborative sprints have worked and continue working toward developing a robust API specification and tooling. A sixth sprint is planned for the fall in Philadelphia.\n\nThis talk will showcase the concrete achievements of the STAPI community, demonstrating how a collaborative approach can lead to a tangible and impactful standard for accessing future geospatial data. We will delve into the specification, highlighting its key features and recent developments. We will also look at the open-source ecosystem growing out of this effort, including projects like stapi-fastapi, stapi-pydantic, and pystapi-client that are empowering the community to create their own STAPI-compliant services and tooling, and several practical implementations from commercial providers."
      },
      {
        "title": "How to use data from the AWS Open Data program in Amazon Bedrock",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "The AWS  Open Data team will discuss how to use datasets in the Registry of Open Data as Amazon Bedrock Knowledge Bases. With Amazon Bedrock Knowledge Bases, you can give foundation models and agents contextual information from private and public data sources to deliver more relevant, accurate, and customized responses.",
        "description": "Sharing data in the cloud lets data users spend more time on data analysis rather than data acquisition. Amazon Web Services (AWS) provides a catalog of publicly available datasets on AWS through the Registry of Open Data on AWS (https://registry.opendata.aws/). The registry has over 650 datasets open to the public, such as government data, scientific research, life sciences, climate, satellite imagery, geospatial, and genomic data.  \n\nFirst, we will cover the Registry of Open Data on AWS, how to search for datasets and how to search for data objects.  Second, we will cover how to use datasets in Open Data as Knowledge Bases in Amazon Bedrock with a vector embedding store. Then we will cover how to use certain datasets in Open Data as Knowledge Bases with a structured dataset, and the trade offs between each type. Last, we will have take-home information to try this at home."
      },
      {
        "title": "How to Complete Japan’s OSM Building Data in One Year using the citygml-osm Converter",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "Since 2020, Japan’s MLIT has led Project PLATEAU, releasing 3D city models. Despite efforts, only 62% of buildings are mapped. OSM Japan is using the \"citygml-osm\" converter. This presentation proposes a one-year roadmap to complete building mapping, optimizing data imports, and community collaboration.",
        "description": "— Strategies for Integrating PLATEAU Data into OpenStreetMap\n\nSince 2020, Japan’s Ministry of Land, Infrastructure, Transport and Tourism (MLIT) has led Project PLATEAU, developing and releasing 3D city models in CityGML format. By 2025, over 236 municipalities have published detailed building datasets compatible with the Open Database License (ODbL), making them suitable for integration into OpenStreetMap (OSM). These high-precision, semantically rich datasets are valuable for urban planning, research, and civic technology.\n\nBuilding upon previous FOSS4G and SotM presentations, this talk highlights the progress made since 2022 by OpenStreetMap Japan volunteers in importing PLATEAU’s Level of Detail 1 (LOD1) building data into OSM. As of May 2025, imports have been completed for 13 cities. In addition to these imports, numerous volunteer mappers continue to enhance OSM’s building data through aerial imagery tracing and other methods.\n\nDespite these efforts, only about 62% of Japan’s estimated 38 million building polygons are currently mapped in OSM, leaving approximately 14 million polygons (38%) yet to be integrated. Addressing this gap requires a clear numerical target and a strategic approach.\n\nThis presentation outlines an ambitious yet achievable goal: to complete the mapping of Japan’s building data using the CityGML-OSM converter within one year. We will provide a quantitative analysis of current progress and outline a roadmap to achieve this objective. Key strategies include optimizing the importation of PLATEAU data, ensuring data consistency between PLATEAU and OSM, and strengthening collaboration within the mapping community.\n\nBy sharing these insights and methodologies, we aim to accelerate the integration of PLATEAU data into OSM and serve as a model for other countries seeking to enhance their building mapping efforts."
      },
      {
        "title": "QGIS and Democracy - Defining New Zealand’s Electoral Boundaries",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "How QGIS was used to help review and define New Zealand's electoral boundaries.",
        "description": "This year the Representation Commission reviewed and updated New Zealand’s electoral boundaries. This talk will give a high-level overview on the process and describe how a QGIS plugin was the core decision making tool that enabled the Commission to quickly investigate different boundary scenarios and ensure electorates were within legislated population tolerances."
      },
      {
        "title": "QGIS to PostGIS: Building Automated Labeling Pipelines with Open Tools",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "A open-source workflow using QGIS and PostGIS to label satellite imagery for object detection and segmentation. It includes AOI management, versioned label tracking, and post-labeling automation such as bounding box generation and training data export—laying the foundation for scalable, ML-ready geospatial datasets.",
        "description": "Labeling satellite imagery remains one of the most time-consuming and critical steps in training geospatial machine learning models. In this talk, we present a practical, open-source workflow that leverages QGIS and PostGIS to streamline and semi-automate the labeling process for object detection and semantic segmentation tasks.\n\nOur workflow begins with data ingestion directly in QGIS using STAC/Copernicus plugins, allowing users to visualize and select scenes of interest. We then walk through an Area of Interest (AOI) management process where users can draw or select regions, and store the geometries in a PostGIS database with metadata such as label type, user, and timestamp. The labeling process is fully integrated into QGIS, enabling polygon or bounding box creation using native tools.\n\nEach labeling session is versioned and tracked, allowing teams to audit, export, and manage datasets systematically. We demonstrate how to generate training-ready datasets—including clipped image chips and masks or bounding box annotations—from within this pipeline, ensuring compatibility with downstream ML training frameworks."
      },
      {
        "title": "From Raw to Ready: Rapid Sentinel-2 Index Workflows for Time-Sensitive Use Cases",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "When critical Sentinel-2 index data went missing before a deadline, we rapidly rebuilt coverage using Sentinel-2 UTM Tiling Grid (ESA), filtered cloud and shadow pixels, and inserted clean values into the database. This talk shares practical steps, code, and lessons learned from urgent environmental monitoring under time constraints.",
        "description": "This presentation explores the urgent recovery of missing Sentinel-2 index data using the ESA UTM tiling grid. It details the preprocessing pipeline, including cloud and shadow masking, null-value extraction, and optimized insertion into geospatial databases. Emphasis is placed on a real-world agriculture use case—monitoring crop health during a narrow seasonal window—where fast, accurate index data is critical for timely decision-making. Attendees will gain insights into handling remote sensing data under pressure, including code examples and workflow strategies tailored for rendering the recovered index data as CovJSON, enabling efficient visualization and analysis within agricultural decision-support systems."
      },
      {
        "title": "Open Source and Open Standard Offline Mobile Data Collection with KoboToolbox",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "Review of the current state-of-art of open source, offline capable, mobile geospatial data collection using the KoboToolbox toolset, based on Open Data Kit standards.",
        "description": "KoboToolbox (Kobo) is an open source, non-profit project based in the USA from the Harvard Humanitarian Initiative, dedicated to making data collection accessible to everyone, everywhere. We are one of the leading global open source tools for collecting, managing, and visualizing data. Our primary sectors include supporting humanitarian aid, environmental protection, human rights, public health, and social development, by providing real data to drive positive change.\n\nKoboToolbox is used across the globe for mobile data collection in some of the world's most remote locations, handling over 20 million form submissions per month. KoboToolbox is entirely open source and is freely available to anyone who wishes to run or customize it. Kobo also provides a free hosting service to non-profit organisations on its public servers, as well as offering dedicated private servers to humanitarian institutions, such United Nations, Red Cross, Save the Children, which helps subsidize our free services.\n\nKoboToolbox provides both a native Android app - KoboCollect - and a browser-based client - Enketo. All data collection in KoboToolbox can be performed entirely offline, with form definitions and new submissions permanently stored on the device until the user is back in coverage. In addition to a rich set of basic form controls, such as text, selections, photos, QR codes, etc, KoboToolbox also supports acquiring geospatial data in the form of points, paths, and polygons, and includes geo-centric functions like area and distance calculations as well as on-device geofencing. The KoboCollect app supports an extensive range of mapping options, including the ability to preload and display offline MapBox map tiles, and making selections from a map. All form elements can be further embellished with skip logic and validation checking to support custom workflows. Longitudinal studies can be undertaken by referencing previously acquired datasets whilst in the field, again, in an entirely offline manner.\n\nKoboToolbox is deployed globally and permits extensive internationalization with over 60 languages supported. All data collected can be visualized through rich dashboards - including geospatially - and is fully accessible to other integration platforms via KoboToolbox’s extensive API and export interfaces."
      },
      {
        "title": "Forecasting Flash Floods in Seoul Using Tiny Radars without FOSS4G in the Operational Phase",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "Seoul experiences concentrated rainfall. Our team is trying to mitigate the flooding by using tiny radars. During the development phase, our software architecture relied heavily on FOSS4G. However, for the operational phase, we have removed our dependency on FOSS4G. We have found that this approach is more suitable.",
        "description": "."
      },
      {
        "title": "Keynote 3",
        "type": "Keynote",
        "track": "Keynote",
        "abstract": "The third keynote for FOSS4G 2025 Auckland",
        "description": "The third keynote for FOSS4G 2025 Auckland"
      },
      {
        "title": "Scaling Impact: Enabling Open Source Adoption in Business and Government",
        "type": "Keynote",
        "track": "Keynote",
        "abstract": "Open source communities have created transformative technologies, but many organisations still hesitate to adopt them fully. This presentation explores practical ways open source contributors can make their projects more approachable, sustainable, and strategically valuable to businesses and governments.",
        "description": "Open source has become foundational to modern technology, yet businesses and public institutions often treat it with suspicion or uncertainty. For open source communities, this represents both a challenge and an opportunity: how do we adapt our practices to meet the expectations of organizations that require predictability, accountability, and clear value?\nThis session is designed for developers, project maintainers, and advocates who want to see their work adopted more widely in enterprise and government contexts. We’ll cover:\n- Understanding orgnisational barriers\n- Improving project readiness for adoption\n- Aligning with compliance and legal requirements\n- Building trust and communicating value."
      },
      {
        "title": "Knowledge sharing and building a sustainable community culture",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This study examines knowledge-sharing practices among professionals in the free and open-source software community. It emphasizes the concept of a knowledge community and knowledge sharing across boundaries from a cross-cultural perspective. It offers practical implications to professionals facing linguistic challenges and nourish a sustainable community culture.",
        "description": "The authors conducted 30 semi-structured, in-depth interviews with professionals working within free and open-source software communities worldwide. The first round of interviews is conducted in face-to-face settings during the Euro Foss4G conference in Mostar, Bosnia and Herzegovina. The second round of interviews is conducted online. Each interview lasts from 60-80 mins. The interview transcripts are analyzed with the software of MAXQDA. Interview questions are asked about the communication and collective experiences of collaborating with working professionals working with free and open-source software for geospatial (Foss4G) technologies. Particularly, the individual motives and collaboration experiences from the cross-cultural perspectives are examined. Further, interviewees were also asked to describe how their personal experiences impacted their perception of community and a sustainable community culture.  Insights and findings from the interviews are discussed."
      },
      {
        "title": "ImageN for GeoSpatial",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Introducing the re-newed raster image procession engine for the Java GeoSpatial Community.",
        "description": "I am *so* excited to share this one, I have been planning it literally for over a decade - and it is finally happening! Attend this talk and buy me a beer."
      },
      {
        "title": "Feed your spreadsheet to the Pandas!",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "The Python Pandas library makes data analysis fast and intuitive. Learn how to load datasets, filter and transform data, perform basic calculations, and export results - all using Pandas. Stacy will present a beginner-friendly workflow using Python Notebooks, tailor made for rapid experimentation and iteration.",
        "description": "The Pandas library for Python streamlines data analysis by making number crunching both fast and intuitive. It excels at handling common “load-calculate-save” workflows with concise, readable code—ideal for everyday data tasks.\n\nIn this practical lightning talk, we’ll walk through a beginner-friendly workflow using Python notebooks, designed for rapid experimentation and iteration. Attendees will learn how to:\n•\tLoad datasets\n•\tFilter and transform data\n•\tPerform basic calculations\n•\tExport results\n—\tall using Pandas.\n\nTo help participants get started quickly, a GitHub repository will be shared containing a minimal working setup. This ready-to-clone configuration will enable attendees to dive into their own analyses with ease."
      },
      {
        "title": "A study on the method of producing HD-map using high-resolution satellite images",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "We would like to propose a method to assist HD-Map production at a lower cost, faster, and more efficiently.\nThis is a technology for producing or updating lane layers of high-precision road maps using high-resolution satellite images.",
        "description": "To improve the safety and efficiency of autonomous driving and autonomous cooperative driving, it is necessary to produce and update high-precision road maps at a low cost quickly.\n\nIn general, HD-Map production uses MMS (Mobile Mapping System). The MMS method requires a lot of money for special surveying equipment and GNSS control points. It also takes a lot of money and time for control point surveying and processing of survey results.\n\nTherefore, we would like to propose a method to assist HD-Map production at a lower cost, faster, and more efficiently.\nThis is a technology for producing or updating lane layers of high-precision road maps using high-resolution satellite images.\nSince this uses satellite projection, it can quickly photograph a wide area, and it has the advantage of being relatively much cheaper and faster to produce or update large-scale, high-precision road maps."
      },
      {
        "title": "High-resolution, large-scale inundation mapping with basic python libraries",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Designing a flood mapping workflow using a few standard python libraries to efficiently process high resolution data at-scale.",
        "description": "Bathtub modelling is a simple approach to flood mapping, whereby inundation depths are computed via differencing a water height layer with the terrain elevation. However, bathtub models often overestimate flood depth and extent, as they fail to resolve underlying processes including hydraulic connectivity, attenuation, fluid flow direction, and structural barriers. Following concepts outlined in Kasmalkar et al. (2024), a bathtub inundation model was developed which accounts for hydraulic connectivity and path-based attenuation to improve the accuracy of flood mapping. The model was applied for multiple inundation scenarios over a large extent (approx. 15,000 sq. km) at high spatial resolution (4 m) which presented numerous challenges in terms of compute capacity, runtime, and generation of useable output data and maps. To address these challenges, the workflow was implemented in Python and involved segmenting the study area into smaller regions, relying on only numpy/cupy (for GPU) in the processing script, and using gdal for reading/writing raster and vector inputs/outputs. This talk will briefly outline the goals of this project, the hurdles encountered along the way, and the solutions designed to overcome them."
      },
      {
        "title": "GeoTools Update",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Quick check in with the GeoTools project!",
        "description": "GeoTools is the backbone of the Java Geospatial community, implementing a lot of the GIS functionality behind GeoServer, GeoNetwork and more.\n\nCheck in with this project as we update to Java 17, a new image processing engine, and general map-happy-ness."
      },
      {
        "title": "Confessions of a Natural Intelligence Vibe Coder?",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "“Vibe coding emphasizes creative flow: the human developer accepts AI-suggestions liberally and focuses on iterative experimentation rather than code correctness” (Wikipedia). I am an Academic working with FOSS4G for 20+ years. Am I a Natural Intelligence vibe coder? This is that story.",
        "description": "An examination of FOSS4G in academica from a personal perspective. Vibe coding ain't new."
      },
      {
        "title": "A GIS-Based Study on the Development of a Blue Carbon Database and Digital Mapping System",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "This study presents the accomplishments in establishing a Blue Carbon database and developing a GIS-based digital information map system to support effective management and decision-making along the Korean coast.",
        "description": "Coastal ecosystems such as tidal flats, salt marshes, and seagrass meadows are referred to as “Blue Carbon” ecosystems. Although they occupy smaller areas than terrestrial forests, they can absorb and store carbon up to 50 times faster. Recognizing this value, this study established a Blue Carbon database and developed a GIS-based digital information map system for the Korean coast. The system serves as a GIS-based decision-support platform for the effective management of marine carbon sinks and for supporting international certification.\n\nBlue Carbon ecosystems such as salt marshes, seagrasses, tidal flats, and sediments have attracted global attention for their remarkable ability to absorb and store carbon at much faster rates than terrestrial forests, yet examples of their integration into systematic management frameworks have been limited. To address this, between 2022 and 2024, data from 289 survey points along the Korean coastline were collected and integrated. The collected data included carbon cycling processes in salt marshes, biomass distribution of seagrasses and seaweeds, organic carbon content of intertidal and sublittoral sediments, and carbon storage in shellfish habitats. These diverse datasets were unified into a Blue Carbon database, enabling reliable estimation of carbon storage, time-series monitoring, and GIS-based visualization.\n\nA distinctive methodological feature of this study was the combination of remote sensing and ground verification to enhance data reliability. For example, salt marsh habitats were initially detected through aerial RGB imagery, divided into 5 km grids, and the top 20% of grids with the largest habitat areas were selected for detailed field surveys. This approach reinforced remotely sensed results with species-level ground truthing, providing a robust data foundation applicable to both research and policy. Based on this dataset, the GIS-based digital information map provides decision-support functions beyond simple visualization. Users can query layers related to vegetation, sediments, and new carbon sinks, analyze data by administrative units or standardized grids, and perform spatial analyses within user-defined areas. In addition, a Site Suitability Index (SSI) was designed to evaluate optimal sites for establishing carbon-absorbing coasts in Korea by weighting environmental variables such as wave energy, slope, depth, and tidal range. Through these functions, policymakers, researchers, and stakeholders can effectively identify priority areas for conservation, restoration, and sustainable coastal development.\n\nThe platform also expands accessibility and international cooperation through multilingual support in Korean and English, and ensures transparency by including reliability metadata that indicates whether each dataset was derived from remote sensing or validated through field surveys. This improves scientific credibility and supports the future international certification of tidal flat Blue Carbon under IPCC frameworks. Ultimately, the Blue Carbon Digital Information Map provides an integrated and user-friendly environment that connects ecological science with geospatial technology. It contributes to achieving Korea’s 2050 carbon-negative strategy, conserving coastal ecosystems, and demonstrating the potential of open-source GIS as a global climate solution. By combining innovation, transparency, and accessibility, this study serves not only as a practical management tool but also as an international cooperation model for the sustainable use of Blue Carbon resources."
      },
      {
        "title": "Advancing Spatial Demographic Insights through GIS: Results of Fiji Population Grid for 2023 and Implications",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "The Fiji Bureau of Statistics, in collaboration with SPC’s Statistics for Development Division, has developed the 2023 Fiji Population Grid—a high-resolution spatial dataset modeled from the 2017 Census and projected to 2023",
        "description": "The Fiji Bureau of Statistics, in collaboration with SPC’s Statistics for Development Division, has developed the 2023 Fiji Population Grid—a high-resolution spatial dataset modeled from the 2017 Census and projected to 2023. Represented in 100m x 100m cells, the grid consists of 66 datasets disaggregated by sex, major ethnicity, and 5-year age groups. This experimental GIS-based approach marks a significant step in spatial demographic analysis, offering detailed insights into population distribution, demographic shifts, and inter-ethnic dynamics across Fiji. The 2023 update reveals evolving patterns in gender and age distributions, supporting more informed policy-making and planning at national and subnational levels."
      },
      {
        "title": "Mapping Storm",
        "type": "Lightning Talk",
        "track": "Lightning talk",
        "abstract": "Mapping Storm (spoiler - not storm mapping!)",
        "description": "The story of how a cat became famous after being GPS tracked"
      },
      {
        "title": "UGRID and you - an unstructured journey",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Showcasing regional ocean model outputs, this talk will focus on structuring analysis-ready data using the unstructured grid (UGRID) standard, our successes (and failures) in our CSIRO team using Xarray and QGIS for these tasks, and improving the uptake of these standards in the FOSS community.",
        "description": "Not for academic track."
      },
      {
        "title": "GeoArrow on Web; Can We Live Without GeoJSON?",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "GeoArrow is a binary data format for geospatial data. It is designed for efficient data exchange, so is suitable for such usages as WebGIS. This talk will give the overview and the current status, including the limitation, of FOSS4G ecosystem around it, including DuckDB, Deck.gl, and MapLibre.",
        "description": "We all love binary data. In the world of WebGIS, binary vector tile quickly became the de-facto standard. However, when it comes to non-tiled data, the king of text data format, GeoJSON, still rules. GeoJSON is nice and handy, but not very suitable for big data. To unleash the potential of FOSS4G ecosystem, we need an efficient binary data format. GeoArrow, a specification based on Apache Arrow data format, is the most promising candidate for this.\n\nThe Apache Arrow ecosystem is already very rich. Apache Arrow officially provides libraries and tools for various programming languages. Many FOSS tools support reading and writing in the format. Thanks to the infrastructure, GeoArrow works very smoothly in many use cases.\n\nHowever, web-related technologies provide relatively less support for GeoArrow. At least, we haven't reached to the point where we can replace GeoJSON with GeoArrow freely. This talk reviews the current status of FOSS4G software related to GeoArrow."
      },
      {
        "title": "From Forest Types to Trees: A Web-based Digital Twin of Individual Trees Using OGC 3D Tiles",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "A browser-based digital twin of individual trees is possible—even at scale. Using OGC 3D Tiles, we visualized real tree models with species and height data across vast forests in Korea.",
        "description": "### 🌲 From Forest Types to Trees: A Web-based Digital Twin of Individual Trees Using OGC 3D Tiles\n\nCan we implement a large-scale forest—down to individual trees—as a digital twin on the web? We asked this question, and then we did it. The answer: **yes, it works beautifully.**\n\nIn this project, we converted Korea Forest Service’s *forest type map* into **OGC 3D Tiles** for web-based service. However, rather than stopping at generalized forest polygons, we focused on **individual tree-level data**—which includes species, height, crown diameter, and other attributes per tree.\n\nThis level of detail is often considered impractical to visualize on the web due to data volume and complexity. So we challenged that assumption by rendering **3D models of real tree species**, each placed in their **actual location with correct height and size**, not just symbolic representations. And yes—it works smoothly, in-browser.\n\nAll of this is made possible through the use of the **OGC 3D Tiles** standard. This means that once the data is converted, it becomes interoperable across multiple platforms: web, mobile, desktop GIS (QGIS, ArcGIS), and game engines (Unreal, Unity, O3DE, Omniverse) with no additional conversion required. This is the power of international open standards—**true “write once, use anywhere.”**\n\nDespite Korea's rich geospatial data, usage is often hindered by spatial data security policies. However, this project demonstrates how even highly detailed and sensitive datasets like individual tree inventories can be made accessible and useful—securely and efficiently.\n\nWith this, **digital twin forest management at the individual tree level** is no longer a distant future—it’s already happening.\n\n---\n\n### 📝 Acknowledgements\n\nThis study was carried out with the support of 'R&D Program for Forest Science Technology '(Project No. \"RS-2025-25404070\")' provided by Korea Forest Service(Korea Forestry Promotion Institute)."
      },
      {
        "title": "Who Pays Your Bills? Sustainability, Community and Business: The Open Source Triangle",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Learn how OPENGIS.ch built a sustainable business on QGIS and QField—where open source, community, and entrepreneurship come together to create real, lasting impact.",
        "description": "\"Who pays your bills?\" was the first question my now-CTO asked me—back when we didn’t even know each other. Years later, we co-founded OPENGIS.ch, a company that thrives on open-source software. In this talk, I’ll share our journey building a sustainable business model around QGIS and QField, explain how the QGIS.org community works, and show why open source is not just a philosophy, but a real business opportunity. We’ll explore sustainability, community, and entrepreneurship—the triangle that allows open-source software to flourish and its contributors to make a living from it."
      },
      {
        "title": "STAC 101: What You Really Need to Know",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "STAC 101 covers the essentials of the SpatioTemporal Asset Catalog, with practical tips for users and data providers to publish, find, and use geospatial data effectively.",
        "description": "The SpatioTemporal Asset Catalog (STAC) is an open standard that’s reshaping how we find, share, and use geospatial and Earth observation data. This session gives new users and data providers the practical essentials for getting started with STAC and using it well.\n\nWe’ll break down the core concepts—Catalogs, Collections, Items, and Extensions—and explain how they fit together to organize spatiotemporal data in a modern, cloud-native way. For users, we’ll cover how to search, filter, and work with STAC data efficiently. For data providers, we’ll share best practices for publishing well-structured metadata, choosing extensions, and lessons learned from real-world implementations.\n\nAttendees will leave with clear, actionable tips to avoid common pitfalls and get the most out of the growing STAC ecosystem—whether you’re building your first Collection or just trying to find and use data more effectively."
      },
      {
        "title": "Manage Geospatial Content with Re:Earth CMS: An Open-Source Headless CMS",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Re:Earth CMS is an open-source, headless CMS designed for managing and publishing geospatial data via API. This talk introduces its architecture, key features, and how it supports flexible, structured content workflows for maps, without requiring coding or complex GIS tools.",
        "description": "This talk introduces **Re:Earth CMS**, an open-source headless content management system designed specifically for managing and distributing geospatial data. Built for integration with modern GIS tools and web applications, Re:Earth CMS offers a structured, flexible, and API-driven approach to spatial content management.\n\nWe will explore the differences between traditional and headless CMS architectures, highlight their respective advantages, and demonstrate how Re:Earth CMS enables seamless collaboration between editors, developers, and map users. Through real-world examples, the talk will showcase how Re:Earth CMS simplifies the publication and control of geospatial content across multiple platforms.\n\n### **Key Features Include:**\n\n- 🗺️ **GIS-native data model**: Manage points, lines, polygons, and attributes in a structured way\n- 🔄 **Headless architecture**: API-first design for integration with any mapping tool or frontend\n- 🧩 **Open-source & extensible**: Continuously evolving as part of the Re:Earth ecosystem\n- 👥 **Collaboration-ready**: Roles, permissions, workflows, and history tracking for teams\n- 📦 **Asset management**: Upload, organize, and preview geospatial files\n\n### **Who Should Attend:**\n\n- GIS professionals seeking more flexible content workflows\n- Local governments and public sector teams managing spatial data\n- Web developers building custom mapping applications\n- Educators and researchers using maps for communication without needing code"
      },
      {
        "title": "Automatic conflation of OpenStreetMap and government data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Using Toitū Te Whenua LINZ as an example, we show how datasets from government agencies can be automatically synced and conflated with OpenStreetMap data, using an open-source data conflation tool that was pioneered in Aotearoa.",
        "description": "OpenStreetMap is the free and open-source infrastructure that powers a huge portion of the world's maps.\nOpenStreetMap Contributors often perform one-off imports of government datasets, such as street addresses or road geometry. However, this data becomes out-of-date very quickly, which is a prevalent problem across Oceania. Once the two datasets have diverged, there's no system to sync or conflate the changes. \nSo, we present an open-source tool to solve this problem, which was pioneered in Aotearoa for use with datasets from Toitū Te Whenua LINZ. This advanced conflation system supports OpenStreetMap Contributors to semi-automatically sync external datasets and merge them with existing crowd-sourced data in OpenStreetMap."
      },
      {
        "title": "Panel Session 1",
        "type": "Panel",
        "track": "Panel",
        "abstract": "Placeholder for Panel Session 1",
        "description": "Placeholder for Panel Session 1"
      },
      {
        "title": "AGM - OSGeo",
        "type": "AGM",
        "track": "AGM",
        "abstract": "OSGeo Global Annual General Meeting",
        "description": "OSGeo Global Annual General Meeting"
      },
      {
        "title": "Lonboard: Fast, interactive geospatial vector data visualization in Python",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Interactive visualization is often a precursor to extracting meaningful insights from data. Lonboard provides 30-40x faster performance for visualizing geospatial vector data than other Python libraries, supporting millions of coordinates.",
        "description": "Visualization, especially interactive visualization, is often the initial step in extracting meaningful insights from data. But it's too hard to quickly and interactively visualize large geospatial vector data in Python.\n\n[Ipyleaflet](https://ipyleaflet.readthedocs.io/en/latest/) and [Folium](https://python-visualization.github.io/folium/latest/) are great for small datasets, but their performance quickly suffers as data sizes grow into the tens of thousands. [Pydeck](https://deckgl.readthedocs.io/en/latest/) supports slightly larger datasets, but it, too, struggles with data sizes above 100,000 coordinates.\n\nThis presentation introduces [Lonboard](https://developmentseed.org/lonboard/latest/), a cutting-edge open-source Python library designed to address this challenge by enabling fast, interactive geospatial vector data visualization within Jupyter notebooks.\n\nLonboard's performance stems from its innovative architecture, built on four key technologies: [deck.gl](https://deck.gl/) for GPU-accelerated rendering, [GeoArrow](https://geoarrow.org/) for efficient in-memory representation, [GeoParquet](https://geoparquet.org/) for optimized data transfer to the browser, and [anywidget](https://anywidget.dev/) for easy Jupyter integration.\n\nOn a dataset with 3 million points, Ipyleaflet crashed after 3.5 minutes, Pydeck crashed after 2.5 minutes, but Lonboard successfully rendered in 2.5 **seconds**.\n\nThis talk will give a brief overview of the internal innovations that make Lonboard so fast, then detail how to make the most of Lonboard's high-level APIs for visualizing large data."
      },
      {
        "title": "Building Spatial APIs in PostgreSQL with PostgREST",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Learn how to turn your PostgreSQL database into a powerful RESTful API using PostgREST. This talk focuses on exposing geospatial data and leveraging PostGIS functions to build scalable, efficient spatial APIs—all without writing a single line of backend code.",
        "description": "In this talk, we’ll explore how you can create powerful, production-ready spatial APIs directly from your PostgreSQL database using PostgREST. PostgREST is an open-source tool that automatically generates a RESTful API from your database schema, eliminating the need for custom backend code. By integrating PostGIS, the geospatial extension for PostgreSQL, we’ll see how spatial data like points, polygons, and raster layers can be exposed through clean, performant REST endpoints.\nWe'll walk through the setup process, including enabling PostGIS, configuring PostgREST, and writing SQL views or functions to expose spatial operations. You'll learn how to use PostGIS functions—like distance calculations, spatial joins, and bounding box filters—directly through the API. We’ll also discuss how to secure, paginate, and filter spatial data, making your API both powerful and user-friendly.\nWhether you're a GIS developer, a backend engineer, or just curious about spatial data, this session will give you practical tools to build and scale spatial APIs using only SQL. By the end, you'll have a clear roadmap to go from a PostgreSQL database to a fully-featured geospatial API ready for frontend integration or data sharing."
      },
      {
        "title": "[Re]discover QField[Cloud]",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "Since its initial release, QField’s collaborative cloud project management QFieldCloud has seen a steady capabilities growth. This talk aims at introducing some of the more advanced functionalities introduced in the last year or so to new and preexisting users.",
        "description": "The talk will go over QFieldCloud functionalities and refinement that have been released in the last year or so, including shared datasets to better manage storage space, on-demand attachments, near-real time positioning of field workers, and more!"
      },
      {
        "title": "In CesiumJS & Deck.gl, AI-based Digital Twin Service with Conversational Interface",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "We present an AI-based conversational Digital Twin service leveraging open-source technologies such as CesiumJS and Deck.gl. This service enables 3D map control and geospatial information exploration through voice or text, supporting complex queries and real-time 3D visualization to deliver an intuitive user experience.",
        "description": "In this presentation, we introduce an innovative AI-based conversational Digital Twin service that enables users to control a 3D map and easily explore geospatial information through voice or text commands.\n\nThis service combines AI technology with S-Map, a 3D platform that integrates all spatial information of Seoul. It leverages open-source technologies such as CesiumJS for high-precision geospatial 3D rendering and Deck.gl for advanced data visualization, providing a rich geospatial interface.\n\nIn the previous of S-Map, users had to go through up to 11 steps using a mouse and keyboard to retrieve specific spatial data—such as apartment rental statistics by district. Now, with a simple command like \"Show me the apartment rental transaction statistics by district\", the desired results appear instantly.\n\nThe service supports a wide range of functions, including real estate search, building design simulation, city comparison, location search, map manipulation, and layer management—all through a conversational chat interface. By combining AI technologies with open-source tools, the system supports complex queries, real-time 3D visualization, and multi-turn interactions—making geospatial data more accessible for users.\n\n\n\nThis work is supported by the Korea Agency for Infrastructure Technology Advancement(KAIA) grant funded by the Ministry of Land, Infrastructure and Transport (Grant RS-2022-00143336, NTIS Grant: 2610000396)."
      },
      {
        "title": "Openness and Collaboration as Strategic Choices – case National Land Survey of Finland",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "This talk is about why National Land Survey of Finland made a strategic decision to use open solutions and collaborate actively with other agencies and OS communities. The talk will outline our experiences on open solutions and collaboration, as well as plans to strengthen the collaborative efforts going forward.",
        "description": "The National Land Survey of Finland has been an active user, and increasingly a developer, of FOSS4G solutions related to our core tasks for nearly two decades now. We also made a strategic decision in 2020 to leverage open source solutions for our major business critical IT systems.\n\nFollowing the strategic decision, the National Land Survey of Finland started building a new topographic data production system on open source components in autumn 2020. Core components of the system include QGIS, PostgreSQL/PostGIS and QField for field work.\n\nDuring spring 2025, the system was taken into full production use. Now it is time to put even more focus on the collaborative aspects: how we collaborate with other authorities, the QGIS user community and with IT companies to ensure continuity of the solution.\n\nThis talk will describe\n-\tOn what grounds we made the strategic decision of using open source\n-\tWhat are our most recent experiences with the open solution and collaboration\n-\tHow we are organizing collaboration in Finland with other stakeholders\n-\tHow we are looking at engaging in further collaboration with open source communities to ensure continuity of the solution\n-\tMost importantly, what benefits every public sector organization can gain by using and investing in open solutions."
      },
      {
        "title": "Faster, simpler access to cloud-based geospatial data with Obstore",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Obstore is the simplest, highest-throughput Python interface to Amazon S3, Google Cloud Storage, Azure Storage. This talk will explain what Obstore is, how it differs from existing Python libraries for cloud data access, and how it's being used to speed up cloud-based geospatial workflows.",
        "description": "[Obstore](https://developmentseed.org/obstore/latest/) is a Python library that abstracts how to access data on commercial cloud storage providers, like Amazon S3, Google Cloud Storage, and Azure Storage. Instead of writing code for each provider and manually creating the abstractions, use Obstore’s singular API for data access.\n\nWhile at its face Obstore is similar to [fsspec](https://github.com/fsspec/filesystem_spec)—they both provide abstracted interfaces to cloud storage—Obstore presents some core improvements:\n\n- Minimal API with native synchronous and asynchronous support.  \n- Fast with no Python dependencies: obstore wraps the Rust `object_store` library, meaning that your Python environment stays small and you won’t face dependency conflicts.  \n- Streaming downloads, uploads, and listings without manual pagination.  \n- Full type hinting for easier use in Python IDE environments.  \n- Simple access to [NASA Earthdata](https://developmentseed.org/obstore/latest/api/auth/earthdata/) and [Microsoft Planetary Computer](https://developmentseed.org/obstore/latest/api/auth/planetary-computer/) data collections with **automatic credential refreshing** when short-lived tokens expire.\n\nWhile Obstore is a foundational technology that can be used across many domains, this talk will focus on its use in geospatial-related projects: \n\n- [Zarr](https://zarr.dev/)\\-Python introduced an [Obstore-based backend](https://zarr.readthedocs.io/en/latest/api/zarr/storage/index.html#zarr.storage.ObjectStore) that can be [3x faster than the default fsspec-based backend](https://github.com/maxrjones/zarr-obstore-performance) when reading Zarr datasets. \n- [VirtualiZarr](https://github.com/zarr-developers/VirtualiZarr), a library to present non-cloud-native file formats like netCDF as virtual Zarr datasets, is being rewritten to use Obstore by default. \n- [Async-tiff](https://github.com/developmentseed/async-tiff), a fast, asynchronous, Python TIFF, GeoTIFF, and Cloud-Optimized GeoTIFF reader, uses Obstore under the hood to power its data fetching. \n- A [new Python GeoParquet library](https://geoarrow.org/geoarrow-rs/python/latest/api/io/geoparquet/) uses Obstore as well.\n\nThis talk will explain what Obstore is, how it differs from existing Python libraries, and how you might use it in your own projects to speed up your own data access."
      },
      {
        "title": "Vector tiles and GeoServer: dynamic vector tiles server, XYZ services, and base maps",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Did you know that GeoServer can both produce and consume vector tiles? Join to find out how.",
        "description": "Mapbox vector tiles have emerged as a popular format for delivering geospatial data, offering dynamic rendering and interactivity for modern web maps. While not an official OGC standard, this open specification has been widely adopted, making it a staple of web cartography. This presentation delves into GeoServer's evolving capabilities to serve Mapbox vector tiles, emphasizing recent enhancements and best practices.\n\nWe will explore how GeoServer leverages SLD and CSS to define the contents of vector tiles, ensuring tailored and efficient data delivery. New configuration options, such as label point generation, attribute selection and geometry coalescing, will be highlighted as tools to control and optimize tile outputs. Practical advice will also be provided for streamlining vector tile generation, helping users create seamless and scalable workflows.\n\nThe session will conclude with a look at how vector tiles can serve as an input for generating high-quality base maps in various coordinate reference systems. Using OpenMapTiles styles and Planetiler, we will demonstrate how to produce visually appealing, multi-projection base maps, unlocking the full potential of vector tiles for diverse applications.\n\nWhether you're building interactive maps or generating custom base maps, this talk will equip you with the knowledge and tools to make the most of GeoServer's vector tile capabilities."
      },
      {
        "title": "Geospatial Cloud-Native at Scale - LINZ’s Path from Legacy Stacks to a National Data Lake",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Land Information New Zealand is transitioning from siloed, middleware-heavy systems to a cloud-native geospatial data lake, built on STAC, COG, COPC, PMTiles, and GeoParquet. We’ll share our insights on modernising our data publishing, web mapping, analytics, and open data publishing across a complex national enterprise.",
        "description": "Land Information New Zealand (LINZ) is transforming its systems by replatforming a complex web of point-to-point feeds and isolated data stores in favour of a single, cloud-native geospatial data lake hosted on AWS. This shift involves pushing all datasets, including vector, tabular, raster, and point cloud data, into object storage and utilising STAC metadata and cloud-optimised formats. This approach decouples data producers from consumers, making the data more accessible, reusable, and self-describing from day one.\n\nWe are utilising open formats like PMTiles, COG, GeoParquet, and FlatGeobuf, enabling web applications, analytics, and external APIs to access the files through standard cloud tools. This eliminates the need for repetitive or ad-hoc ETL processes and reduces reliance on fragile bespoke solutions. \n\nThis presentation will discuss the progression from concept to pilot to production. Key topics will include metadata, versioning, incorporating data quality and lineage into our catalogue, and for both open and internal data. We will also share the organisational benefits, such as faster product creation, stable access patterns for both teams and customers, and a significant reduction in integration workloads. Additionally, we’ll address the cultural shifts necessary to move away from outdated thinking."
      },
      {
        "title": "12 billion tiles later.....",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Reflections on the development and delivery of a free public national-scale basemap service, built using open standards, open data and FOSS tech.",
        "description": "Five years after launch, the Toitū Te Whenua - Land Information New Zealand Basemaps service continues to grow and find new ways to make NZ's vast collections of open geospatial data simple for users to explore and build upon. This talk will provide a quick demo of the product, then dive into some of the trickier problems the Basemaps team has solved to keep the service dirt cheap, beautiful, and lightning-fast. You should leave with a clear idea of the potential outcomes and benefits of building something similar for your geography."
      },
      {
        "title": "Fast and Free: High-performance WebGL geospatial visualisation using Lonboard and Jupyter Notebooks",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "Lonboard transforms geospatial analysis by eliminating the need for costly tiling and database preprocessing. This WebGL-based visualisation library delivers superior performance in Jupyter Notebooks, reducing costs while enabling real-time interaction with large vector datasets. Open source demonstration with reproducible examples from Urban Intelligence's climate modelling work.",
        "description": "Climate modelling generates massive raster datasets that analysts must explore to identify critical patterns like flood risk zones, temperature anomalies, and adaptation pathways. However, meaningful analysis requires overlaying large vector datasets, such as roads, infrastructure points, and building footprints, to understand the real-world impacts. Geospatial analysts traditionally face a costly bottleneck: these vector datasets must be tiled and stored in databases before meaningful visualisation can occur. This preprocessing step incurs significant computational and storage costs while creating friction in exploratory workflows.\n\nThis presentation demonstrates how Lonboard, a high-performance WebGL-based vector visualisation library, eliminates this barrier entirely, reducing workflow costs to zero while delivering superior rendering performance for the critical vector overlays. Through demonstrations using Urban Intelligence's climate analysis workflows, we'll showcase how analysts can visualise massive vector datasets directly in Jupyter Notebooks without preprocessing. Lonboard's WebGL rendering achieves speeds orders of magnitude faster than traditional packages like ipyleaflet by leveraging cutting-edge technologies, such as GeoArrow and GeoParquet, in conjunction with GPU-based map rendering.\n\nKey benefits include elimination of costly vector preprocessing steps, instant visualisation feedback for rapid exploration, seamless Jupyter integration, and superior performance with large vector datasets essential for large datasets. The presentation features fully reproducible notebooks showcasing production-ready implementation practices.\n\nThis approach represents a fundamental shift from expensive, preprocessing-heavy workflows to cost-effective, exploration-first methodologies that accelerate workflows. By leveraging open-source tools, organisations achieve better analytical outcomes while reducing infrastructure costs and complexity."
      },
      {
        "title": "A Digital Twin of 80km of streetscapes, with FOSS4G? Sure!",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "Melbourne’s tram network is considering Mobility Aid Lifts for wheelchair access from street level on selected routes. We scanned and processed 80km or routes with OS only in an ROS sensor framework with cloud-optimised data processing to detect stops and simulate MAL deployments.",
        "description": "A hero story, featuring, COTS sensors, ROS, COPC, machine learning and friends. It can be done!"
      },
      {
        "title": "Open Source GIS for Placemaking Education: A Gamified Framework for Community-Driven Urban Learning",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "This study presents a gamified educational framework for teaching urbanism using open-source tools. Combining MLIT PLATEAU (GIS data), Blender (3D modeling), and Godot (simulation), it enables youth to engage in participatory placemaking while learning spatial thinking and urban design through an accessible, interactive digital workflow.",
        "description": "This study introduces an innovative educational framework that integrates open-source tools, geospatial data, and gamification to enhance the teaching of urban planning and placemaking. It is specifically designed to provide young learners (particularly in schools, youth workshops, and community-based learning environments) with accessible, engaging, and interactive experiences that foster spatial awareness, critical thinking, and civic participation. At the core of this framework lies a reproducible pipeline built upon three key open-source technologies: MLIT PLATEAU (open GIS-based 3D city models), Blender (a 3D modeling and animation suite), and Godot (a lightweight, scriptable game engine). Together, these tools form a complete workflow for visualizing, simulating, and manipulating urban space through an open and participatory lens.\n\nThe methodology begins with the use of MLIT PLATEAU, Japan’s national-scale open 3D geospatial dataset, which offers highly detailed, standardized models of buildings, land use, infrastructure, and terrain for dozens of cities across the country. These datasets are downloaded in CityGML or OBJ formats and imported into Blender using specialized plugins or preprocessing tools. Within Blender, the imported GIS data is converted into structured, editable 3D models that students can navigate, analyze, and reconfigure. For example, students can model new public spaces, add greenery, modify building forms, or simulate small-scale tactical interventions. Blender's node-based material system and animation capabilities also allow learners to visualize temporal changes (e.g., seasonal effects, traffic flows, construction stages), adding dynamic and narrative dimensions to their urban design proposals.\n\nBlender also plays a central role as a translation layer between GIS and game engines. After modeling and scene setup, assets are exported (typically in glTF, FBX, or OBJ formats) and imported into Godot, where the interactive and gamified layer of the project is developed. In Godot, students can script interactions using GDScript (a Python-like language), design basic user interfaces (UI), and simulate real-world phenomena such as walkability, visibility, accessibility, or user behavior. Example applications include:\n- creating a first-person walkthrough of a redesigned neighborhood;\n- developing a resource-management game to balance green space and development;\n- building a collaborative multiplayer simulation for participatory urban decision-making.\n\nThe integration of real-time simulation through Godot allows learners to test design hypotheses, observe user reactions, or iterate urban scenarios based on feedback—all within an immersive, game-like environment. Importantly, this approach does not require high-end hardware or costly licenses, making it ideal for public schools, NGOs, or municipalities with limited digital infrastructure. The entire pipeline (GIS data (PLATEAU), 3D modeling (Blender), and simulation (Godot)) is fully open-source, ensuring transparency, reproducibility, and adaptability to local contexts worldwide.\n\nPedagogically, the framework introduces a novel mapping between Levels of Detail (LOD) (a concept borrowed from GIS and 3D modeling) and stages of learning progression. For instance, LOD1 models are used for early-stage orientation and spatial comprehension; LOD2/3 data facilitates site-specific design challenges; and custom-built LOD4 scenarios simulate detailed human-scale interventions. This layered approach supports differentiated instruction and curriculum design, enabling instructors to tailor activities to students’ cognitive levels and technical skills.\n\nThe framework is situated within a broader discourse of open-source urbanism, tactical urbanism, and participatory design. It challenges traditional, expert-driven planning paradigms by promoting tools that empower communities and students to engage directly with spatial planning through iterative, visual, and interactive means. It also contributes to the field of geospatial education, offering a replicable methodology for integrating spatial literacy, creative modeling, and digital civic engagement into urban curricula. Ultimately, this study argues that a gamified, open-source approach to urban learning not only makes complex planning tools accessible but also nurtures the next generation of urban thinkers capable of co-creating inclusive and resilient urban futures."
      },
      {
        "title": "Mapping for Climate Resilience : Community Driven OSM Contributions in Tanzania (Case study of Dar Ramani Huria Project)",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Tanzania faces increasing flood risks due to climate change. This session highlights how the Dar Ramani Huria project empowered communities to map flood-prone areas using OSM, improving disaster preparedness and urban planning. It showcases a practical, community-driven approach to building climate resilience through open data and local collaboration.",
        "description": "How can communities take the lead in preparing for the impacts of climate change? In Dar es Salaam, Tanzania’s largest and most flood prone city, local people have found an answer through mapping.\nThis session will share the journey of Dar Ramani Huria, a community mapping initiative that empowered local residents, university students, and city officials to collaboratively map flood risks using OpenStreetMap (OSM) and open source tools. Led by OpenMap Development Tanzania (OMDTZ), the project showed how digital tools and local knowledge can come together to build climate resilience from the ground up.\nUsing mobile apps like OpenDataKit (ODK), OpenMap Kit , Maps.me, and Field Papers, communities collected detailed information on roads, drainage systems, buildings, amenities and flood prone zones in their own neighborhoods. With that data, they used tools like QGIS and InaSAFE to analyze risks, simulate flood scenarios, and support real planning decisions.\nThis talk goes beyond the technical process it’s about the people behind the maps:\n*Students and residents who learned how to map for the first time.\n\n*Youth groups who used data to advocate for better services.\n\n*Ward leaders who used maps to redesign drainage and plan safe evacuation routes.\n\nThe audience will gain insights into how mobile tools were used for field data collection, how community generated open data was transformed into actionable flood risk information, how inclusive training and capacity building fostered local ownership, and how the resulting maps have supported decision making by city planners and disaster response teams.\n\nThe impact of this work has been real. Maps produced through Dar Ramani Huria have helped:\n*Guide the design of drainage infrastructure in vulnerable areas.\n\n*Inform disaster preparedness plans across multiple wards.\n\n*Identify evacuation routes and safe zones ahead of heavy rains.\n\n*Support urban planning that considers future climate risks.\n\nWe’ll also share some of the challenges we’ve faced: keeping data current, ensuring it’s used by decision makers, and sustaining momentum after the project ends. But we’ll also share strategies like partnering with local disaster committees and involving youth that help keep the work going.\nAt its core, this session is a story about what happens when you trust people with tools, data, and decision-making power. When local knowledge is paired with open technologies, communities don’t just prepare for disasters they shape a more resilient future.\nKey takeaway for the audience:\nCommunity-driven, open-source mapping is more than a method, it’s a movement. It gives people the means to protect themselves and plan for a changing climate. This session offers both inspiration and practical guidance for anyone looking to build resilience from the ground up."
      },
      {
        "title": "How HOT and communities are building together the free & open humanitarian mapping suite",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "From aerial imagery, to digitization, field mapping and data use, we're building an integrated tech suite for humanitarian open mapping",
        "description": "How do we move from having no maps to using them effectively? We're building a set of open mapping tools for humanitarian use, developed with the help of communities around the world, drawing on shared experiences in disaster response, humanitarian relief, and community development.\n\nImagine taking a simple drone and quickly generating your own aerial imagery, then publishing it on an open repository. You can use this imagery for mapping, even creating your own AI models if you like. Then, go to the field and enrich your data with local knowledge, leveraging super-accessible citizen mapping tools for assistance. Combine all your data on a single map.\n\nHOT tools are already being used in a wide range of situations, not only in disaster response, but also in local business development, environmental projects, and innovative applications as communities adapt and make these tools their own.\n\nLet's explore how we’re building this free and open mapping suite together, by people, for people."
      },
      {
        "title": "pygeoapi project status",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pygeoapi project status presentation.  Come and find out the latest news on the project as well as future plans, and how to get involved!",
        "description": "pygeoapi is an OGC API Reference Implementation. Implemented in Python, pygeoapi supports numerous OGC APIs via a core agnostic API, different web frameworks (Flask, Starlette, Django) and a fully integrated OpenAPI capability. Lightweight, easy to deploy and cloud-ready, pygeoapi's architecture facilitates publishing datasets and processes from multiple sources. The project also provides an extensible plugin framework, enabling developers to implement custom data adapters, filters and processes to meet their specific requirements and workflows. pygeoapi also supports the STAC specification in support of static data publishing.\n\npygeoapi has a significant install base around the world, with numerous projects in academia, government and industry deployments. The project is also an OGC API Reference Implementation, lowering the barrier to publishing geospatial data for all users.\n\nThis presentation will provide an update on the current status, latest developments in the project, including new core features and plugins. In addition, the presentation will highlight key projects using pygeoapi for geospatial data discovery, access and visualization."
      },
      {
        "title": "OGC APIs with GeoServer: implementation, availability, and next steps",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Discover OGC APIs and their implementation in GeoServer",
        "description": "The OGC APIs are a fresh take at doing geo-spatial APIs, based on WEB API concepts and modern formats, including:\n\n- Small core with basic functionality, extra functionality provided by extensions\n- OpenAPI/RESTful based\n- JSON first, while still allowing to provide data in other formats\n- No mandate to publish schemas for data\n- Improved support for data tiles (e.g., vector tiles)\n- Specialized APIs in addition to general ones (e.g., DAPA vs OGC API - Processes)\n- Full blown services, building blocks, and ease of extensibility\n\nThis presentation will provide an introduction to various OGC APIs and extensions, such as Features, Styles, Maps and Tiles, STAC and CQL2 filtering. Some of specs are finalized and complete enough that they have a GeoServer supported extensions, while others are provided as community modules. Join us to find out the current state of implementation, our future steps, and how you can participate in it."
      },
      {
        "title": "State of GeoNetwork",
        "type": "Talk",
        "track": "State of software",
        "abstract": "Join us for the latest from the GeoNetwork community",
        "description": "The GeoNetwork opensource project is a catalog application that makes it easy to discover resources across any local, regional, national or global Spatial Data Infrastructure (SDI). GeoNetwork is a mature technology, an OSGeo Project and a proud member of the FOSS4G community for over 20 years.\nThe GeoNetwork team invites you to hear what we’ve been up to in 2025!\nWe will showcase the many community-driven projects that have added new features over the past twelve months. Our thriving ecosystem of schema plugins keeps growing, with national teams contributing fixes, enhancements and entirely new capabilities, now including DCAT-AP output support and integrations.\nYou will also get an insider’s look at the GeoNetwork team’s plans: the current status of our 4.2.x and 4.4.x branches, our upcoming release schedule and an early preview of GeoNetwork 5. We’ll share a high-level roadmap for GeoNetwork 5 and updates on the crowdfunding effort to help bring it to life.\n\nJoin us for the latest from the GeoNetwork community and this ever-evolving platform!"
      },
      {
        "title": "A cloud based solution for Indigenous data sovereignty: protecting biodiversity management data in Aotearoa New Zealand",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "This presentation introduces a trustless, browser-based system for Indigenous geospatial data sovereignty. It combines geomasking, encryption, and blockchain to enable secure, third-party-free data sharing. Developed for Māori-managed Biodiversity Areas in Aotearoa New Zealand, the tool ensures privacy and control over sensitive spatial data in cloud environments.",
        "description": "How can Indigenous communities retain full control over their geospatial data in a world dominated by cloud platforms? This talk offers a guided walkthrough of a working prototype that protects Indigenous geospatial knowledge through privacy-first design. Attendees will see how technologies like geomasking, in-browser encryption, and blockchain notarization come together in a seamless tool for secure data sharing — all without relying on external intermediaries. The session will highlight real-world use in Māori-managed Biodiversity Areas in Aotearoa New Zealand and discuss broader ethical and technical considerations around Indigenous data governance. The talk is especially relevant for those interested in GIS, privacy tech, Indigenous rights, and open-source innovation.\n\nLink to article: https://onlinelibrary.wiley.com/doi/10.1111/tgis.13153"
      },
      {
        "title": "MapSafe QGIS plugin: a complete open-source geoprivacy tool for desktop GIS applications",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "MapSafe is a QGIS plugin that enables users to protect and share sensitive geospatial data with privacy controls like donut masking, encryption, and blockchain verification. It brings advanced geoprivacy functions into desktop GIS workflows, eliminating reliance on third parties and supporting compliance with data protection regulations.",
        "description": "This session introduces MapSafe, a desktop geoprivacy plugin for QGIS that empowers users to secure sensitive spatial data directly within their existing GIS workflows. Through a live demonstration, attendees will see how the plugin anonymizes data using techniques like donut masking and hexagonal binning, protects original datasets via public-key encryption, and verifies dataset authenticity using blockchain notarization. The talk will highlight how MapSafe avoids reliance on third parties, scales to large datasets, and supports regulatory compliance. Designed for usability, the plugin includes tooltips, documentation, and video guides, making geoprivacy accessible to users across skill levels. Whether you're a GIS professional, data steward, or researcher, this talk will offer practical insights into integrating geoprivacy directly into your desktop environment.\n\nLink to article: https://www.tandfonline.com/doi/full/10.1080/17489725.2025.2497752?src=exp-la"
      },
      {
        "title": "Approaching Security with Kindness and Compassion",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Wow it has been a busy time for security vulnerabilities. FOSS4G software is getting caught up in the general push to regulate IT and impose “security” on the technology that powers society.",
        "description": "This talk explores the tensions, expectations, terrors and triumphs on this hot button topic. We will look at a sensible response to calls for regulation and how GeoSever and GeoNetwork policies have been updated to address these concerns for developers, participating organizations and members of the public.\n\nThis talk unpacks what this can look like for foss4g projects using real world examples.\n\n* Built around the experience of the GeoServer project, and the resulting security policy and practices that can serve as a template for our foss4g community.\n* Public institutions can attend this talk to learn how their security policies interact with and affect foss4g technologies.\n* Vendors and service providers can learn how open source supply chains affect their products.\n* FOSS4G projects can attend to learn how to approach security reports with compassion, and a bit of boundary setting, to take care of your codebase and community.\n\nSecurity is difficult with consequences being felt at all levels. Help meet this challenge by supporting yourself and each other."
      },
      {
        "title": "The birth of the FOSS4G Educator and Teacher Community - Vision and Opportunities",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "At FOSS4G Europe, a working group of GIS educators was established to shape the future of open geospatial learning. This talk explores the community’s vision, shares our achievements, and highlights how you can get involved in advancing open-source geospatial education worldwide.",
        "description": "At FOSS4G Europe 2025 in Mostar, a passionate group of trainers and educators in open-source geospatial technology came together during a Birds of a Feather event to explore synergies and opportunities for collaboration. This gathering sparked the formation of a dedicated working group, united by the vision to define our global role and responsibility in fostering a more inclusive and impactful geospatial community.\n\n\n\nWith a clear mission and shared values, this talk will present **the journey of our emerging community**—highlighting our achievements so far and outlining the next steps we envision. Our goal is to inspire others to join us in shaping the future of open geospatial education.\n\n**Key challenges we aim to address:**\n- How can we engage high school students early and welcome them into the community?\n- How can we reach mapping enthusiasts who lack access to expensive educational programs and  provide them with meaningful learning experiences?\n- Can we collaboratively create and maintain a portfolio of essential open-source learning materials?\n- What makes documentation truly empowering, enabling anyone to become an active GIS user?\n\nWe warmly invite all interested individuals to get involved, contribute, and help us grow this vibrant community. Be inspired, get curious, and discover how you can make a difference in the world of open geospatial education!"
      },
      {
        "title": "Earth Search: Expanding Open Access to Sentinel-2 and Beyond",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Earth Search is Element 84’s open, cloud-native STAC API for public Earth observation data. This talk covers pipeline updates, metadata fixes, and open-source plans—making geospatial data easier to find and use.",
        "description": "Earth Search is Element 84’s free, cloud-native catalog that makes public Earth observation data—like Sentinel-1 and Sentinel-2—discoverable, analysis-ready, and open to all. This talk will share an in-depth update on the current state of Earth Search, the pipelines that power it, and our recent work to ensure long-term stability and transparency.\n\nWe’ll cover how the team has taken over the Sentinel ingestion pipeline on AWS, tackled metadata gaps and ingestion bugs, and improved the catalog’s coverage and accuracy. Attendees will learn how the pipeline generates Cloud-Optimized GeoTIFFs and rich STAC metadata that follows current best practices and extensions—making advanced cloud-native workflows possible.\n\nWe’ll also discuss our plans to open source the entire Earth Search processing stack, explain how the community can benefit from and contribute to it, and share progress on addressing known gaps in the Sentinel-2 archive. From debugging broken asset links to filling missing scenes and deprecating legacy collections, this talk will give a transparent look at the challenges of maintaining a massive, open geospatial archive—and what’s next as we continue to expand reliable, standards-based access for everyone."
      },
      {
        "title": "OGC CITE Runner - a new runner for checking compliance with OGC standards",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "ogc-cite-runner is a software to help automate OGC CITE compliance testing for geospatial web applications. It can be used either as a GitHub action or standalone. Learn more about it at https://osgeo.github.io/ogc-cite-runner/",
        "description": "The Open Geospatial Consortium API family of standards (OGC API) are being developed to make it easy for anyone to provide geospatial data to the web, and are the next generation of geospatial web API standards designed with resource-oriented architecture, RESTful principles and OpenAPI. In addition, the OGC has the CITE compliance program, which aims at providing test suites that can be used to verify if web applications implement the standards correctly.\n\nOfficial CITE testing is done using OGC infrastructure and is subject to a review process which can be time consuming for implementations seeking compliance certification. The official CITE test suites and test runner (OGC TeamEngine) are made available by the OGC so that implementations are able to test their compliance beforehand. This process is however not very straightforward to automate.\n\nThe ogc-cite-runner software aims at reducing the friction of running the mentioned official CITE test suites with the OGC TeamEngine test runner. It is a lightweight Python CLI application which can be used directly as a GitHub action or as a standalone tool, thus making it easy to include in Continuous Integration systems. This means that implementations become able to test their CITE compliance as part of their normal development workflow and gain near instant feedback on their compliance status.\n\nAt its core, ogc-cite-runner implements a thin layer of  automation over OGC TeamEngine, asking it to run CITE test suites. It then processes the output results into a number of output formats, including a Markdown report which is embedded directly in the GitHub actions workflow log.\n\nThis presentation will provide an overview of ogc-cite-runner, demonstrating how it can be run both as a GitHub action and in standalone in order to test OGC CITE compliance of geospatial web applications.\n\nhttps://osgeo.github.io/ogc-cite-runner/"
      },
      {
        "title": "Introduction to the Discrete Global Grid Abstraction Library (DGGAL)",
        "type": "Talk",
        "track": "AI, Data Science & Analytics",
        "abstract": "An introduction and overview of the capabilities provided by the Free and Open Source Software Discrete Global Grid Abstraction Library (DGGAL) https://dggal.org https://github.com/ecere/dggal",
        "description": "DGGAL provides a common interface to perform various operations on Discrete Global Grid Reference Systems (DGGRS), facilitating the implementation of Discrete Global Grid Systems (DGGS),\nincluding implementing Web APIs based on the [OGC API - DGGS Standard](https://docs.ogc.org/DRAFTS/21-038r1.html).\n\nDGGAL, including the `dgg` command-line utility as well as the Python bindings to the library, can be installed from [from PyPI](https://pypi.org/project/dggal/) with `pip install dggal`.\n\nDGGAL currently supports all three DGGRS described in [OGC API - DGGS Annex B](https://docs.ogc.org/DRAFTS/21-038r1.html#annex-dggrs-def), as well as additional DGGRSs:\n\n* [GNOSIS Global Grid](https://docs.ogc.org/DRAFTS/21-038r1.html#ggg-dggrs): An axis-aligned quad-tree defined in WGS84 latitude and longitude, with special handling of polar regions, corresponding to the [OGC 2D Tile Matrix Set of the same name](https://docs.ogc.org/is/17-083r4/17-083r4.html#toc58)\n* [ISEA3H](https://docs.ogc.org/DRAFTS/21-038r1.html#isea3h-dggrs): An equal area hexagonal grid with a refinement ratio of 3 defined in the Icosahedral Snyder Equal Area (ISEA) projection\n* [ISEA9R](https://docs.ogc.org/DRAFTS/21-038r1.html#isea9r-dggrs): An equal area rhombic grid with a refinement ratio of 9 defined in the ISEA projection transformed into a 5x6 Cartesian space resulting in axis-aligned square zones\n* **IVEA3H**: An equal area hexagonal grid with a refinement ratio of 3 defined in the Icosahedral Vertex-oriented Great Circle Equal Area (tentatively called IVEA) projection based on [Slice & Dice (2006)](https://www.tandfonline.com/doi/abs/10.1559/152304006779500687), using the same global indexing and sub-zone ordering as for ISEA3H\n* **IVEA9R**: An equal area rhombic grid with a refinement ratio of 9 defined in the IVEA projection transformed into a 5x6 Cartesian space resulting in axis-aligned square zones, using the same global indexing and sub-zone ordering as for ISEA9R\n* **RTEA3H**: An equal area hexagonal grid with a refinement ratio of 3 defined in the Rhombic Triacontahedron Equal Area (RTEA) projection, a configuration of the Slice & Dice great circle projection equivalent to applying Snyder's projection to the RT, using the same global indexing and sub-zone ordering as for ISEA3H\n* **RTEA9R**: An equal area rhombic grid with a refinement ratio of 9 defined in the RTEA projection transformed into a 5x6 Cartesian space resulting in axis-aligned square zones, using the same global indexing and sub-zone ordering as for ISEA9R\n* [rHEALPix](https://iopscience.iop.org/article/10.1088/1755-1315/34/1/012012): An equal area and axis-aligned grid with square zones topology and a refinement ratio of 9 defined in the rHEALPix projection (using same parameters as default [PROJ implementation](https://proj.org/en/stable/operations/projections/rhealpix.html)) with the original hierarchical indexing and scanline-based sub-zone ordering\n\nThe API documentation can be [found here](https://dggal.org/docs/html/dggal/Classes/DGGRS.html).\n\nThe `DGGRS` class provides most of the functionality of the library, allowing to resolve DGGRS zones by textual ID to a unique 64-bit zone integer identifier (`DGGRSZone`).\nThe geometry and sub-zones of a particular zone can also be queried.\nThe concept of [sub-zones](https://docs.ogc.org/DRAFTS/21-038r1.html#term-sub-zone) is key to encoding both vector and raster geospatial data quantized to a DGGRS.\nThe DGGAL library also allows to resolve a sub-zone index at a particular depth from a parent zone, allowing to read DGGS-optimized data such as [DGGS-JSON](http://dggs-json.org) and [DGGS-JSON-FG](https://docs.ogc.org/DRAFTS/21-038r1.html#rc_data-dggs-jsonfg).\n\nThe recommended method to obtain and build DGGAL and the `dgg` tool is to follow the instructions in [BUILDING.md](BUILDING.md), or running [fetchAndBuild.sh](fetchAndBuild.sh) / [fetchAndBuild.bat](fetchAndBuild.bat).\n\nWhile the library is written in the [eC programming language](https://ec-lang.org), object-oriented bindings for C, C++ and Python generated using the Ecere SDK's [`bgen` tool](https://github.com/ecere/bgen) are provided. Bindings for Rust are available as well. Support for additional languages may be added in the future.\n\nExample usage of the bindings to the different programming languages can be [found here](https://github.com/ecere/dggal/tree/eC-core/bindings_examples).\n\nThe `dgg` tool provides the ability to perform various operations from the command-line, including generating grids at different refinement levels, querying information about a particular zone identifier, identifying the zone a particular set of geospatial coordinates, listing the zones within a certain bounding box, resolving sub-zone indices and converting compact DGGS-JSON data files to GeoJSON files.\n\n_Acknowledgement_\nFinancial support provided by GeoConnections, a national collaborative initiative led by Natural Resources Canada. GeoConnections supports the modernization of the Canadian Geospatial Data Infrastructure (CGDI). The CGDI is the collection of geospatial data, standards, policies, applications, and governance that facilitate its access, use, integration, and preservation."
      },
      {
        "title": "pycsw project status",
        "type": "Talk",
        "track": "State of software",
        "abstract": "pycsw project status presentation.  Come and find out the latest news on the project as well as future plans, and how to get involved!",
        "description": "pycsw is an OGC API - Records and OGC CSW server implementation written in Python. Started in 2010 (more formally announced in 2011), pycsw allows for the publishing and discovery of geospatial metadata via numerous APIs (CSW 2/CSW 3, OpenSearch, OAI-PMH, SRU), providing a standards-based metadata and catalogue component of spatial data infrastructures. pycsw is Open Source, released under an MIT license, and runs on all major platforms (Windows, Linux, Mac OS X).The project is certified OGC Compliant, and is an OGC Reference Implementation.\n\nThe project currently powers numerous high profile catalogues such as EOEPCA, IOOS, NGDS, NOAA, US Department of State, US Department of Interior, geodata.gov.gr, Met Norway and WMO WOUDC. This session starts with a status report of the project, followed by an open question answer session to give a chance to users to interact with members of the pycsw project team. This session will cover how the project PSC operates, the current project roadmap, and recent enhancements focused on ESA's EOEPCA, Open Science Data Catalogue and OGC API - Records."
      },
      {
        "title": "Introducing ChatMap: Open mapping with chat apps",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Mapping can look easy for some people, but it could be really hard for others. What if communities could just simply and easily utilize the existing apps for mapping?",
        "description": "Nearly 3.5 billion people in the world already use an instant messaging app for communications on a routine basis, many of them in high vulnerability locations. We've created a solution that enables mapping with instant messaging apps. From the chat to the map!\n\n- No need to install new apps\n- Learn how to map in 1 min\n- No installations or integrations needed\n- Everyone can access and contribute to the map\n\nWe'll explore how it works, successful use cases and the future of this new way of mapping."
      },
      {
        "title": "Automatic map deconstruction using QGIS: Leveraging open-source algorithms such as map2loop and LoopStructural",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "We present an open-source workflow integrating QGIS, map2loop, and LoopStructural to automate geological map deconstruction into 3D models. This approach streamlines extracting contacts, faults, and stratigraphy from legacy maps, reducing manual effort and enhancing reproducibility. Results show rapid, scalable model generation, improving geological understanding in data-sparse regions.",
        "description": "The integration of geological knowledge from legacy maps into modern 3D geological models remains a critical challenge, particularly in regions where data is sparse or inconsistently digitized. This research presents an automated workflow for geological map deconstruction using QGIS, and 3D modelling, using open-source algorithms such as map2loop and LoopStructural. The aim is to streamline the transformation of 2D geological maps into structured data, reducing manual interpretation efforts and increasing reproducibility.\nThe proposed methodology leverages map2loop to extract and pre-process from georeferenced maps geological features such as contacts, faults, and stratigraphic boundaries. Some of these features are then structured into formats compatible with the input of LoopStructural, a Python-based library that builds rule-based 3D geological models. The QGIS environment acts as a central platform for visualizing and manipulating these spatial data layers, while our plugins facilitate automation and interoperability between components.\nThis approach underscores the value of open-source tools in geoscience workflows, fostering collaboration, transparency, and scalability. By automating the extraction and structuring of geological features, the workflow significantly reduces the subjectivity and time requirements traditionally associated with map deconstruction and 3D model builds. \nThe results demonstrate the feasibility of automated map deconstruction within an open-source GIS environment, paving the way for rapid geological model development in underexplored or poorly mapped regions. This work contributes to the broader geoscience community by offering a replicable and extensible framework for geological data translation, bridging the gap between static to support systems for integration with additional data sources such as drillholes and geophysical data, enhancing the fidelity of downstream 3D models.\nRepository:\nLoop3D\n•\thttps://github.com/Loop3D/map2loop\n•\thttps://github.com/Loop3D/LoopStructural\n•\thttps://github.com/Loop3D/qgis-loopplugin\nField Mapping Tool\n•\thttps://github.com/swaxi/GEOL-QMAPS\n•\thttps://zenodo.org/records/13374088\n\nGeophysical Processing Tool\n•\thttps://github.com/swaxi/SGTool \nTomofast-x Grav/Mag Inversion Tool\n•\thttps://github.com/TOMOFAST/Tomofast-x\n•\thttps://github.com/TOMOFAST/tomofast-x-q"
      },
      {
        "title": "Empowering Everyone with Satellite Data – Agile Development Powered by FOSS4G",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "ArkEdge Space Inc. has developed a Free and Open platform to democratize satellite-derived geospatial data. Leveraging FOSS4G technologies and rapid iterative development, we've quickly deployed forestry, agriculture, environmental, and water management apps globally, actively sharing source code to enable easy satellite-data access for everyone, including non-experts.",
        "description": "ArkEdge Space Inc. is a system integrator specializing in ultra-small satellites, having successfully launched and operated thirteen spacecraft to date. In 2024, based on the \"Free and Open\" philosophy, we initiated development of a platform aimed at making satellite-derived geospatial data easily accessible to everyone.\n\nWith the support of FOSS4G, within just one year, we developed multiple practical applications, including a forestry-management tool for local governments, an agricultural decision-support system for smallholder farmers and ministries in Global South countries, an environmental monitoring application, and applications for flood monitoring and water resource management. These tools enable non-experts to easily access, visualize, and analyze satellite data without requiring specialized knowledge.\n\nWe have collaborated with government agencies in Global South countries during the development of these applications. Currently, many of these applications have been deployed free of charge to government agencies and primary-sector workers in Global South countries, as well as to local municipalities and educational institutions within Japan.\n\nFurthermore, we have actively provided the source code of our applications to other Web IT application companies. By doing so, we aim to enable a broader range of players—including ordinary IT companies without specialized satellite data expertise—to develop applications leveraging satellite data.\n\nBy adopting a rapid iteration approach—frequently developing, testing, and refining mock-up applications based on user feedback—we quickly produced a wide range of solutions tailored to user needs. FOSS4G technologies played a crucial role in enabling this agile development process. Specifically, we actively utilized various FOSS4G technologies such as MapLibre GL JS, PMTiles, TiTiler, and GDAL. Additionally, we leveraged open-source image processing models such as Segment Anything Model 2. The combination of TiTiler and AWS Lambda proved particularly powerful, supporting not only fast raster data delivery, rapid time-series processing, and data download tools, but also more specialized use cases like rainfall analysis applications.\n\nIn this way, we adopt the perspective of end users working with satellite-derived geospatial data. This hands-on experience gives us insight into the challenges of leveraging geospatial information—especially satellite data, which is large in size, stored in specialized formats, and requires expert knowledge for processing and analysis. Drawing on these insights, we are committed to further improving the usability of Free and Open geospatial data—including satellite datasets—and to providing an accessible data-delivery experience for all.\n\nIn this presentation, we will introduce our journey, discuss the underlying technologies, and share our vision for creating a society where satellite data is effortlessly accessible to everyone."
      },
      {
        "title": "Can FOSS4G be utilized to support the post-mortem assessment process of large-scale chemical spills?",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "We present our research on a system prototype that combines a digital twin-based web service using Cesiumjs and 3D Tiles with atmospheric diffusion models to predict the diffusion of chemicals in the atmosphere to support post-mortem assessments of chemical spills.",
        "description": "When toxic substances that are lethal to humans are accidentally released, for example, in a large-scale chemical plant that leaks gaseous toxic substances into the atmosphere, or in a car accident that causes liquid toxic substances to leak from a vehicle carrying toxic substances and seep into the soil or flow into a river, it is very difficult to assess the damage caused by the accident.\n\nThis is because these large-scale incidents occur so infrequently that there is very little data available for engineering and mathematical analysis, it is difficult to re-create the incident for experiments, and it is difficult to identify the victims in the vicinity of the incident.\n\nThe idea behind the study was that, aside from soil and water contamination, which requires long-term monitoring of both people and the environment, it may be possible to assess short-term damage caused by toxic gases leaking into the air in an environment like South Korea, where the number of mobile phone subscribers is high relative to the total population.\n\nIf you have an atmospheric diffusion model backed by computing power, accurate information about the incident (type of leak, amount of leak, duration, location, etc.), weather information about the location at the time of the incident, and terrain and building model data that can be used as background data for the atmospheric diffusion model, you can obtain a three-dimensional grid with predictions of the spread of the leaked substance over time.\nAnd then you can add the prediction result data on the moving paths of the expected victims in the vicinity of the incident over time provided by the mobile service provider and the physical information of the expected victims (gender, age, weight, etc.) to create a model that can calculate the expected damage to each expected victim.\nFurthermore, if the results of the spread prediction and the estimated damage to the expected victims can be made available to decision makers involved in damage assessment on a digital twin basis, it can support the damage assessment process for chemical spills.\n\nIn this talk, we will present a prototype of a system that can support damage assessment tasks by combining the results of atmospheric diffusion model-based spread predictions, location information of expected victims, and estimated damages assessed based on these predictions with digital twin-based web services, which we are working on in our fourth year of research."
      },
      {
        "title": "Maps That Save Lives: Connecting PLHIV to Care in Delhi Through Community Mapping",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "This talk presents a volunteer-driven HIV mapping project in New Delhi using OpenStreetMap and VGI. In collaboration with NGOs and advocates, we map HIV support services to improve visibility, access, and dignity for PLHIV communities—showing how open data can drive health equity and community empowerment across India.",
        "description": "In India, people living with HIV (PLHIV) and members of the LGBTQ+ community continue to face structural and social barriers in accessing quality, stigma-free health services. Despite efforts by government and civil society to expand HIV prevention, testing, and treatment services, many individuals still struggle to find safe, welcoming spaces where they can receive support without discrimination. The problem is not just a lack of services—it is also the lack of visibility and access to accurate, up-to-date information about these services.\n\nThis session introduces an ongoing initiative to use OpenStreetMap (OSM) and Volunteered Geographic Information (VGI) to create an HIV Support Services Map in New Delhi, India, with the goal of scaling the model to other parts of the country. The project aims to empower PLHIV, LGBTQ+ individuals, health advocates, and local NGOs by mapping the landscape of HIV-related support—such as testing and treatment centers, counselling facilities, community clinics, and queer-friendly service providers. This data, when made openly available and community-maintained, has the potential to connect vulnerable populations with life-saving care."
      },
      {
        "title": "Building Communities: This is broken, and WE'RE going to fix it",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "Community: \"This is broken, and WE'RE going to fix it\"\nLone Wolf: \"This is broken, and I'M going to fix it (alone)\"",
        "description": "The Lone Wolf Trap\nMany geospatial open source projects start with a brilliant developer saying \"This is broken and I'm going to fix it.\" They build something amazing, gather users, and then... burn out. The project stagnates or dies with its creator. Sound familiar?\nThe Community Alternative\nWhat if instead we built projects where the response to problems is \"This is broken and WE'RE going to fix it\"? Where multiple people can spot issues, propose solutions, and drive improvements? Where the project outlives any single contributor?"
      },
      {
        "title": "Make the most of LINZ Open Data",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "LINZ publish so much open data that it's easy to get lost. Learn about which delivery channel is best suited for your next project.",
        "description": "LINZ publish a lot of open data on a range of platforms. So much, that it's easy to become confused. How do you know you're getting the best data from the best source? This talk will cover LINZ's primary data delivery channels (LINZ Data Service, Basemaps, REST Services, AWS Open Data registry), explaining the difference and value of each. By the end, you should be comfortable knowing where to grab data for your next project, whether it's cartography, building an app or national-scale analysis."
      },
      {
        "title": "Mapping Landcover in Arid Regions of Saudi Arabia",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "We mapped landcover in the Saudi Arabia arid region using Sentinel-2, Spot imagery, which we processed using QGIS, Python, and Earth Engine. We developed machine learning classifiers and segmentation workflows. The workflow generated LCCS-compatible classifications with enhanced vegetation detection accuracy for applications like land management and environmental analysis",
        "description": "In this project, we developed a scalable, reproducible workflow to map land cover and vegetation density across the arid and semi-arid regions of Saudi Arabia. The approach integrates open and commercial satellite imagery, machine learning classification, and segmentation techniques to generate Land Cover Classification System (LCCS)-compatible outputs suitable for national-scale monitoring and management.\nThe workflow produced two core outputs for each region:\nA detailed land cover map distinguishing major classes, including barren land, sand, settlements, and vegetation.\nA vegetation density map classifying areas into low, medium, and high vegetation cover.\nThese outputs are already supporting:\nBiomass estimation to quantify vegetative carbon stocks and inform climate-related reporting.\nGrazing capacity analysis to guide sustainable rangeland management.\nIdentification of priority zones for vegetation conservation.\nTogether, these products provide a baseline dataset to inform environmental management, policy development, and research across Saudi Arabia’s diverse landscapes.\nTo account for ecological diversity, Saudi Arabia was divided into 168 primary tiles, each approximately 1° × 1° in size, grouped into broad environmental zones, including forest areas, arid flats, sandy regions, and hilly terrain. Within each tile, smaller working grids (~12 km × 9 km) were defined to enable systematic processing and data management.\nImagery sources included Sentinel-2 (providing 10 m multispectral data) and Maxar high-resolution imagery. Sentinel-2 scenes were selected to maximize vegetation contrast and minimize cloud cover by choosing the most appropriate season. Data were exported from Google Earth Engine in 32-bit format and converted to 8-bit to facilitate efficient analysis.\nContrast Limited Adaptive Histogram Equalization (CLAHE) was applied to improve visual contrast and feature discrimination, particularly in areas with subtle spectral differences between vegetation and bare ground. This method was selected because each tile covers a vast area with significant variation in surface brightness and reflectance. Applying a global contrast adjustment, such as histogram stretching, can distort some regions by over-enhancing bright areas while compressing darker zones. CLAHE, on the other hand, employs a localized kernel-based approach to adjust contrast within smaller sections independently, ensuring consistent enhancement across different landforms and preserving detail without introducing artifacts in uniform sandy or rocky areas.\nFor each tile, multi-resolution segmentation was performed using eCognition software on the enhanced Sentinel-2 imagery. Multi-resolution segmentation grouped adjacent pixels into homogeneous objects based on their spectral and spatial characteristics.\nUsing very high-resolution imagery from SPOT, the normalized difference vegetation index (NDVI) was computed from the Red and near-infrared bands to detect active vegetation. Given the variability of NDVI values across regions, single thresholding was unsuitable for consistent detection. To address this, NDVI was squared and cubed to create enhanced layers that amplified vegetation signals while maintaining relative differences among land cover types.\nWhile NDVI effectively highlights vegetation, it can also produce artificially high values over bright sand dunes due to strong near-infrared reflectance. This artifact is common in arid landscapes where sparse vegetation mixes with reflective substrates. To mitigate these effects, the areas were identified manually after vectorization and excluded by removing false-positive polygons from the vegetation layer.\nAn unsupervised classification approach was selected for its ability to operate effectively across large, heterogeneous datasets without requiring uniform training samples. Input features included the four spectral bands (Red, Green, Blue, NIR) along with the enhanced NDVI layers. Classification was performed using K-Means clustering, with the optimal number of classes automatically determined based on the spectral characteristics of each working grid.\nAfter classification, approximately 100 sample points were generated within each class to calculate mean NDVI values. The class with the highest mean NDVI was identified as the vegetation class and vectorized for further processing.\nThis process produced two geospatial layers for each working grid:\nA vegetation density map depicting crown cover areas.\nAn LCCS-compatible land cover classification.\nWithin each segment, the proportion of vegetation cover was computed to re-code classes accurately and ensure consistent representation of vegetation density within the broader land cover context.\nTo build seamless national coverage, tile-level outputs were mosaicked. Grid boundaries were processed by extracting edge segments, reclassifying them, and merging adjacent classes to maintain consistent classification across neighboring tiles.\nHigh-density vegetation zones were analyzed to assess grazing patterns and temporal dynamics of vegetation. For each highly vegetation-dense segment, the NDVI was computed monthly using time-series Sentinel-2 imagery. By tracking the variability and seasonal trends in NDVI across multiple years, the workflow identified areas with gradual vegetation decline, which may indicate overgrazing or other ecological pressures such as drought or soil disturbance. Recognizing these vulnerable zones is critical in arid environments, where any loss of vegetation can have significant long-term impacts on soil stability, biodiversity, and ecosystem resilience.\nThe resulting datasets are being used to develop biomass estimation models to quantify carbon stocks across ecosystems. Grazing capacity assessments leverage temporal NDVI trends to prioritize areas for sustainable management and intervention. Land cover data also supports the identification of priority areas for conservation and vegetation protection.\nBy combining freely available satellite data, machine learning classifiers, and reproducible open workflows, this project demonstrates a practical approach to national-scale mapping in challenging arid environments. The methods and outputs can be adapted and reused by organizations working in similar landscapes."
      },
      {
        "title": "Benchmarking OGC Services: No More Surprises When 20 Students Try to Access Your WMS",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Most OGC services are deployed with only functional testing, leading to failures under real user loads. Our open-source Locust framework generates realistic geospatial requests with random bounding boxes and dynamic coordinates for proper load testing",
        "description": "Picture this, Sicily 2025 : You've carefully set up a WMS service, tested it works perfectly, and then invited 20 students to use it while lecturing about OGC web services. Suddenly, your service crawls to a halt. This scenario highlights a critical gap in how we deploy OGC services - most organizations perform only functional testing, checking if services work but not how they perform under real user loads.\nStandard load testing tools like ApacheBench fall short for geospatial services because they are not oriented to deal with  the geographic complexity of OGC protocols. Real users don't request the same map tile repeatedly - they pan across different bounding boxes, zoom through multiple levels, switch layer combinations, and explore various geographic extents. OGC services require specialized testing that can generate random bounding boxes for WMS GetMap requests, create dynamic tile coordinates for WMTS across zoom levels, handle proper CRS and format parameters, and simulate realistic navigation patterns.\nWe developed an open-source solution using the Locust framework enhanced with OGC-specific Python code modules. Our approach leverages owslib.wms for proper capabilities parsing and request generation, custom bbox generators that create random geographic extents within service bounds, tile coordinate algorithms for realistic requests, and layer randomization from GetCapabilities responses. The framework includes seed-based reproducible testing for consistent benchmark comparisons and simulates authentic user behavior patterns including concurrent users requesting different geographic areas and dynamic layer combinations during sessions.\nThis work addresses a fundamental challenge in the FOSS4G ecosystem where standard load testing misses the geographic complexity that makes OGC services unique. By providing specialized benchmarking tools that understand geospatial request patterns, organizations can systematically validate performance before deployment and avoid discovering critical issues when users start actually exploring their services geographically. The toolkit enables proactive capacity planning and helps prevent those embarrassing moments when carefully planned demonstrations fail under realistic load conditions."
      },
      {
        "title": "Building a simple geospatial web app",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "I'll show you how i built a simple web application that uses open source tools and open data to estimate NZ census counts within custom geographies.\nI'll share some tips and tricks i learned along the way and hopefully inspire you to make your own geo web app.",
        "description": "I'll show you how i built a relatively simple web application that uses open source tools (Python, Dash, Leaflet, GeoParquet) and open data (NZ Census 2023, NZ building outlines) to estimate NZ census counts within custom geographies.\nI'll share some tips and tricks i learned along the way and hopefully inspire you to make your own geo web app."
      },
      {
        "title": "Mapping the World, Empowering People: QField’s Vision in Practice",
        "type": "Talk",
        "track": "Desktop GIS & Data Collection",
        "abstract": "Explore how QField empowers people to map and understand the world—supporting daily tasks, global challenges, and the UN SDGs through open-source, intuitive, and collaborative mobile geospatial tools.",
        "description": "QField helps people map and understand the world—enabling them to solve everyday tasks and tackle global challenges. In this talk, we’ll dive into how QField is used in diverse contexts: from conservation and climate action to infrastructure planning and public service delivery.\n\nYou’ll hear how the app’s open-source foundation, intuitive interface, and powerful features are making high-quality field data collection accessible to all. We’ll also showcase how QField supports the UN Sustainable Development Goals and fosters a global community of practitioners working for impact. Whether you're new to QField or a long-time contributor, this session will show how the vision of \"mapping the world, empowering people\" is being realized every day."
      },
      {
        "title": "Use of Freeware to support Regional Capability Development",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "IIC have intentionally incorporated the benefits of free and open source software into recent projects for the benefit of the client and regional development. These have included training and SDI resolves for the Pacific, a QGIS charting plugin and a free Electronic Charting System.",
        "description": "IIC have recently undertaken a series of unique projects where we have intentionally leveraged the benefits and cost savings of freeware and open source software to achieve greater outcomes for our clients and the community at large. These projects have included the delivery of Capacity Development Training in the Pacific to nearly 100 students maximising the use of freeware and online tools; the development of a Spatial Data Infrastructure and knowledge base for Pacific Islands nations that leverages opensource software to maximise its outcomes; a QGIS plugin that is available to the community that allows the automated conversion of bathymetric data to a chart-like product for use in Electronic Chart Systems (ECS), and finally the development of an advanced Electronic Chart System App that utilises National Authority official S-57 data, that is freely available for use by the  public."
      },
      {
        "title": "Women in OSM Tech - What worked best for me",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "This session shares my journey from a non tech background to a skilled OSM and QGIS user, highlighting how supportive, inclusive environments like those fostered by OMDTZ can empower women in tech and demonstrate OSM’s potential for personal growth and professional development.",
        "description": "I aim to share, reflecting on the supportive environment that facilitated my growth while arguing for the importance of creating an inclusive space in OSM communities where women and other underrepresented groups can aspire to achieve anything. With less than 2 years in OSM Tech, I am already actively participating in organizational projects, including the solid waste management baseline survey in an area I am interested in as an environmental personnel.\nI will also share what has worked best for me and how I overcame challenges so that other women out there who want to take on the challenge can feel empowered and able to take the first steps.\n\nKey Takeaways for the Audience\n*Inspiration & Confidence: A firsthand account proving that anyone regardless of background can succeed in tech and mapping with curiosity, commitment, and community support.\n\n*Inclusive Impact: A deeper understanding of why inclusive, supportive spaces are essential in OSM and how they can empower underrepresented groups, especially women.\n\n*Resilience & Growth: Lessons on overcoming self doubt and external barriers, with actionable advice for others beginning their journey in OSM tech and  a renewed motivation to advocate for equity in tech and contribute to building more inclusive and diverse OSM communities."
      },
      {
        "title": "Look ma, no hands: Automating Open-Source Geospatial Infrastructure with AWS CDK (IaC)",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Learn how Abley’s mapping stack leverages Amazon Web Services (AWS) and the AWS Cloud Development Kit (CDK) to automate, standardize, and scale geospatial infrastructure. Discover how our reusable CDK library enables rapid, environment-driven deployment of high-performance mapping services that power the Abley  SafeSystem  and geospatial APIs.",
        "description": "Modern geospatial applications demand robust, scalable, and repeatable infrastructure. At Abley, we’ve developed a mapping stack—built on open-source components like PostGIS, GeoServer, and MapLibre—deployed using a fully automated Amazon Web Services (AWS) infrastructure-as-code (IaC) pipeline. Central to our approach is a reusable AWS Cloud Development Kit (CDK) library that encapsulates best practices for naming, tagging, and resource retention, ensuring consistency and compliance across all environments.\nThis talk will showcase how our infrastructure-as-code (Iac) strategy accelerates the deployment of Abley’s SafeSystem suite, a set of road safety applications and APIs. By centralizing infrastructure logic in a shared CDK library, we enable rapid provisioning, environment-specific configuration, and seamless integration with continuous integration and continuous deployment (CI/CD) pipelines. Our architecture supports high-availability and cost-effective geospatial services.\nAttendees will gain practical insights into:\n•\tStructuring a reusable CDK library for shared infrastructure code and multiple CDK applications.\n•\tUsing the AWS CDK to enforce organizational standards and automate resource management.\n•\tLeveraging environment-driven configuration for safe, flexible deployments.\n•\tIntegrating open-source geospatial tools with modern cloud infrastructure.\nWhether you’re building public mapping services or internal analytics, this session will provide actionable patterns for deploying and managing geospatial infrastructure at scale with AWS and the CDK."
      },
      {
        "title": "Securing STAC APIs: Auth Patterns and a Proxy-Based Approach",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "Learn how to secure STAC APIs using OIDC, CQL filtering, and existing STAC extensions. We present **stac-auth-proxy**, a backend-agnostic FastAPI proxy for enforcing flexible auth policies, including integration with Open Policy Agent.",
        "description": "As STAC APIs power more real-world applications, authentication (authN) and authorization (authZ) become essential. Yet, the STAC specification leaves these concerns to be addressed by implementers.\n\nIn this talk, we outline common auth needs seen across STAC deployments, including:\n\n- **Route-level access control** - marking some or all endpoints as private\n- **Record-level filtering** - limiting collections or items by request context such as user, group, or role\n- **Asset-level access** - transferring our authN policies to the asset files themselves\n\nWe’ll introduce [**stac-auth-proxy**](https://github.com/developmentseed/stac-auth-proxy), a backend-agnostic FastAPI-based proxy that integrates with any modern STAC API and OIDC authentication server (e.g., Keycloak, AWS, Cognito, Auth0). We will discuss how we utilize existing extensions such as the [Authentication Extension](https://github.com/stac-extensions/authentication), [Filter Extension](https://github.com/stac-api-extensions/filter), [Collection Search](https://github.com/stac-api-extensions/collection-search), and [Transaction Extension](https://github.com/stac-api-extensions/transaction) to build a secure and self-descriptive STAC API. Finally, we will discuss how stac-auth-proxy can be integrated with external policy engines, such as [Open Policy Agent](https://www.openpolicyagent.org/), to provide a comprehensive decoupled solution."
      },
      {
        "title": "The Pacific Data Hub, a federated data platform",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "A deep dive into the open-source based tech stack behind the Pacific Data Hub.",
        "description": "The **Pacific Data Hub** platform is the one-stop-shop for anyone who seeks data about the Pacific region across multiple domains including population statistics, fisheries science, climate change adaptation, disaster risk reduction, public health, food security and human rights. \nIt was designed as a set of highly-specialised, open-source based data management systems covering the different types of data (array-oriented scientific data, geospatial datasets, statistical data cubes, and documents) overarched by a catalog of metadata forming a **federated data platform**. The catalog also references data from national governements, regional organizations, and global partners.\n\nThe datasets are indexed in the CKAN-based catalog via harvesters or entry forms. The metadata is then re-disseminated by CKAN with a centralised schema and taxonomy via Data Catalog Vocabulary (DCAT). Whenever it is possible, the catalog provides links to the data either as direct download link or OGC standards APIs.\nThe federated data platforms includes:\n- [**Geonode**](https://geonode.org/) for Geospatial datasets (DEMs, bathymetry, administrative boundaries, risk assessment output models, ...)\n- THREDDS and [**TDS**](https://www.unidata.ucar.edu/software/tds/) for scientific data arrays (sea surface height, temperature, wave, current, tides, salinity, ...)\n- [**.Stat Suite**](https://sis-cc.gitlab.io/dotstatsuite-documentation/) for statistical indicators (SDGs, population, economy, trade, ...)\n\nThose platforms are completed by dashboards, tools and visualisation apps connected to the data via APIs to provide insights to inform better policy development and decision-making including:\n- the [Pacific Maritime Boundaries Dashboard](https://pacificdata.org/dashboard/maritime-boundaries) is the regional tool for sharing information about the Pacific island maritime boundaries and zones. Based on Drupal pages, it provides map views of EEZs and other maritime zones and links to their associated treaties.\n- the [Blue Pacific 2050 Dashboard](https://blue-pacific-2050.pacificdata.org/) is a NextJS dashboard that display charts of key development indicators pulling data from a SDMX APIs.\n- [PacificMap](https://map.pacificdata.org/) is a generic map viewer that can connect to the CKAN catalog to display the __Geonode__-hosted GIS datasets but also the .Stat statistical indicators as chloropeth maps.\n- the [Pacific Ocean Portal](https://oceanportal2.spc.int/) provides access to historical and forecast ocean conditions as well as in-situ observations provided by __TDS__.\n\nThis presentation is about giving details on the standards, technologies and tools deployed within the platform, describing the API-based interconnections and the data workflows involved. It will also provide an overview of the challenges encountered when maintaining a platform with reliable and timely data coming from a wide diversity of providers."
      },
      {
        "title": "EOPF Zarr Access",
        "type": "Talk",
        "track": "Cloud, APIs & Data Infrastructure",
        "abstract": "EOPF Toolkit - Python and R Zarr access",
        "description": "This talk will describe the EOPF Zarr format and present practical notebooks in Python and R for accessing ESA Sentinel data. It will touch on practical use cases for Sentinel 1, 2, and 3 Zarr data access and help users understand the EOPF ecosystem and the tooling best suited to access and exploit EOPF data."
      },
      {
        "title": "Development of an Information Extraction System for Mobile LiDAR Survey Data using Free and Open Source Technologies",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "A 3D WebGIS Information Extraction System was developed to retrieve and visualize Mobile LiDAR Survey data by integrating spatial objects with attribute information. The system aims to have a user-friendly interface and uses free and open-source technologies including Python, HTML5, JavaScript, PostGIS, GeoServer. Leaflet and Cesium.",
        "description": "1.\tIntroduction\n•\tRapid advancements in technology have improved 3D visualization techniques in Geographic Information Systems (GIS). Open-source solutions can provide the functionality to store and visualise large quantities of GIS data in multiple formats.\n•\tThis paper describes a WebGIS application for visualizing Mobile LiDAR GIS data across various Levels of Detail (LoDs).\n2. Mobile LiDAR Survey Technologies\n•\tMobile LiDAR is a powerful geospatial technology for surveying complex urban landscapes including roads and roadside infrastructure. It provides accurate 3D data by integrating laser scanning, Inertial Measurement Unit (IMU), and Global Navigation Satellite System (GNSS) technologies to produce detailed point clouds. \n•\tA schematic truck-mounted Mobile LiDAR system is shown in Figure 1.\n\n \nFigure 1: Mobile LiDAR system (adapted from [1]\n\n•\tMobile LiDAR Surveys (MLS) capture X, Y, Z coordinates of laser light reflecting objects along with attributes like intensity and RGB colour values. They also capture imagery that can be fused with the point cloud data to improve object classification.\n•\tTo identify and semantically classify objects such as roadside features requires the use of Artificial Intelligence (AI) software which incorporates complex algorithms which may be tailored to particular types of object. Thus detecting the road surface may require a different algorithm from pole-like objects such as street signs.\n•\tThis study is based on survey data captured by Cyvl  LiDAR and 360-degree optical imagery sensors mounted on vehicles which travel on targeted roads at the same speed as other traffic. The Cyvl system also contains a suite of Artificial Intelligence (AI) software which is mainly used for road inventory recording, defect detection and road safety management. Road defects include road pavement damage and potholes. Road safety issues include street trees encroaching on the road, damaged signage, lighting fixtures and lane markings and driving obstacles on the road. The road infrastructure data relevant to this project are indicated in the diagram below.\n \nFigure 2: Taxonomy of elements for road surface and object identification (adapted from [2]). Features relevant to this project are indicated by orange boxes.\n•\tAn additional type of infrastructure feature not shown above is powerlines, both roadside and cross country. The Cyvl system is capable of identifying powerlines as well as vegetation which may impinge on them, and the project may involve development of specialized algorithms to assess the risk and advise on vegetation removal action.\n•\tThe Cyvl system identifies and semantically classifies roadside features captured by its optical imagery sensors and matches these to the point cloud data collected by its LiDAR sensor. This is a more accurate method of identifying roadside features than methods which rely on LiDAR data alone, see eg [3].\n \n3.\tInformation Extraction System Overview\n•\tRoadside objects are segmented using the Cyvl2 AI system. Each object is stored in an ontological format, reducing data size for efficient querying. The integration process results in a semantically rich master geo-database for various roadside feature classes.\n•\tThe system will allow for various types of queries, including spatial location-based, attribute-based, and aggregate queries.\n•\tThis approach will be an ensemble of (i) data pre-processing (ii) data integration, schema development and 3D database development and (iii) data extraction and an integrated visualisation module.\n•\tA system architecture diagram (simplified) is depicted in Figure 3 below.\n  \nFigure 3: Systems Architecture\n•\tThe system architecture is entirely open-source, comprising multiple python packages including Geopandas, TensorFlow, Keras, PyTorch and NumPy, together with well-established database package PostgreSQL/PostGIS and server GeoServer. GeoServer is able to serve both geospatial data and HTML (www) files. The front end is under development, with software packages for 2D and 3D data including Leaflet and OL3-Cesium under evaluation.\n\n4.\tPreliminary results: a Leaflet-based map viewer\n•\tData provided by the Cyvl system was processed using 16 GeoTIFF tiles into a GeoServer Image Mosaic. These geotiff files were transformed from .LAS point cloud files collected by Civiltech.\n• \tConfigured Web Map Service (WMS) rendering using coordinate reference system EPSG:28355\n• \tThe map added grayscale SLD for better visibility\n•\tSee screenshot produced by Ryan Watson Consulting is shown below as Figure 4.\n \nFigure 4: Screenshot of 16 geotiff files as a mosaic\n5.\tConclusions\n•\tThe developed WebGIS application effectively extracts and visualizes Mobile LiDAR survey data. This will help road authorities make more effective use of the data being collected.\n•\tFuture research will focus on optimizing querying algorithms and expanding the application scope of the proposed framework.\n•\tFuture applications may include complex queries such as assigning Pavement Condition Index (PCI) and road safety ratings such as the International Road Assessment Program (iRAP) star rating.\n\n6.\tReferences\n1.\tNational Cooperative Highway Research Program (US) (2025). Chapter 9: Typical components of mobile LIDAR systems. In: Transportation Research Board of the National Academies (ed.) Guidelines for the Use of Mobile LIDAR in Transportation Applications.  Available from: https://learnmobilelidar.com/guidance-document/chapter-9-background/.\n2.\tLuo, Z., Gao, L., Xiang, H. and Li, J. (2023). Road object detection for HD map: Full-element survey, analysis and perspectives. ISPRS Journal of Photogrammetry and Remote Sensing 197 2023/03/01/ 122-144 DOI: https://doi.org/10.1016/j.isprsjprs.2023.01.009\n3.\tAbro, G.-E. M., Zahid, F., Rajput, S., Azhar Ali, S. S. and Aromoye, I. A. (2025). Challenges and Innovations in 3D Object Recognition: The Integration of LiDAR and Camera Sensors for Autonomous Applications. Transportation Research Procedia 84 2025/01/01/ 618-624 DOI: https://doi.org/10.1016/j.trpro.2025.03.116"
      },
      {
        "title": "Why Scalability Matters: Lessons from Scaling Digital Earth Australia to the Pacific",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Digital Earth Pacific (DEP) adapts proven Earth Observation frameworks to the unique needs of Pacific island states. Presentation explores DEP’s regional scaling journey, highlighting design choices, partnerships, data architecture, and strategies that address scale, capacity, and sustainability - advancing equitable cloud-native EO infrastructure for sustainable development in the Pacific.",
        "description": "Translating a successful public-good Earth Observation (EO) infrastructure across regions is not a straightforward process of replicating technology stacks and methodologies. Digital Earth Pacific (DEP) builds upon the established open-source, standards-based foundations of Digital Earth Australia and Digital Earth Africa, yet it adapts these frameworks to suit the distinctive environmental, political, and technical circumstances of the small island states in the Pacific region. This presentation provides an overview of the journey undertaken by DEP to scale Earth observation capabilities to a regional, multi-agency context within the Pacific. We will delve into the design choices, partnership models, data architecture transformations, and scaling-up strategies that are pivotal to DEP’s success. From the challenges posed by disparity in scale to the capacity-building requirements, this presentation highlights both the limitations and innovations inherent in constructing a cloud-native platform designed for equitable access, scalability, and long-term sustainability, to inform sustainable development efforts within the Pacific region."
      },
      {
        "title": "ROCS: Extending Romania’s National Infrastructure within the European Collaborative Ground Segment with FOSS4G Solutions",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "ROCS is building Romania’s Earth Observation infrastructure using FOSS4G tools like STAC, COG, Zarr, MinIO and Kubernetes. The platform enables scalable, cloud-native data access, processing and analytics. Real-world use cases include crop monitoring, forest compliance, EO education, all developed in an open and reusable manner.",
        "description": "The ROCS project represents a national effort to build a scalable, cloud-native infrastructure for EO data management and analysis in Romania, fully based on FOSS4G tools. The presentation will cover the project’s architectural design, technical stack (including STAC, COG, Zarr, OGC APIs and Kubernetes) and implementation of federated data centers that ensure efficient, distributed EO processing.\nA key focus is on real-world applications, including automated crop monitoring, forestry compliance (e.g., EUDR), and the integration of EO tools in education. ROCS is committed to open development, reproducibility and interoperability, aiming to contribute back to the FOSS4G community. The session will be valuable for developers, platform builders and public sector stakeholders interested in building sustainable, cloud-ready EO platforms with open-source technologies.\n\nThe talk will also reflect on practical challenges encountered, including the growing concern over license volatility in widely used open-source projects and how this affects long-term sustainability."
      },
      {
        "title": "From a geo portal for air pollution assessment system to digital twin: a case study in Catania, Italy",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "This work presents the Urban Intelligence Science Hub for City Network project, an innovative initiative aimed at advancing digital analysis of complex urban environments, where  an open-source digital infrastructure was designed for the acquisition, processing, and interactive visualization of environmental data, ensuring reusability and interoperability with scientific standards.",
        "description": "The UISH (Urban Intelligence Science Hub for City Network) project is an initiative that aims to develop an innovative concept of digital analysis of complex urban realities, using the city of Catania (Sicily) as a pilot testing ground.\nThrough the project, an innovative approach to the digital analysis of complex urban centers is being developed to facilitate decision support for spatial planning and management activities. Simulators based on artificial intelligence models are being built for each physical subsystem of the city, the interaction of which forms a digital twin of the city. UISH is funded by the Department for Cohesion Policies of the Prime Minister's Office and is part of a strategic CNR (Italian National Research Council) project called Urban Intelligence (UI). This aspires to develop and propose an urban intelligence model for all Italian urban areas, from metropolitan areas to small municipalities. CNR's UI model is characterized by a modular Urban Digital Twin (UDT) architecture, which aims to analyze the city in its various subsystems (such as mobility, the built environment, water, green spaces, etc.), matching each of them with a thematic Digital Twin characterized by scientific approaches, numerical models, services and functionalities, as well as specific data sources and software modules interfaced with each other through special Application Programming Interfaces (APIs), which allow dialogue and data exchange between models, and then integrated with systems based on AI techniques.\nThis UDT architecture makes it possible to apply the CNR's UI model to the different needs of Italian urban areas regardless of their level of “smartness readiness”, as it can be flexibly and adaptively integrated into the heterogeneous context of existing digital tools and expertise, be they sensor networks, databases, smart services, hardware and software infrastructures, etc., and the entities called upon to use and manage them.\nThe core part of the module, is based on the use of the Air-Portal platform (https://business.esa.int/projects/air-portal). an urban air quality dashboard developed by S[&]T (https://air-portal.nl/?lang=en). Air-Portal employs a customized local air quality model that integrates remote sensing data, land use information, and local monitoring to generate urban-scale air quality data with a spatial resolution of 100x100 meters and a temporal resolution of 1 hour.\nThe main input data sources used by the Air-Portal system are:\n-\tAtmospheric modeling data obtained from CAMS (Copernicus Atmosphere Monitoring Service)\n-\tLand use data provided by CLMS (Copernicus Land Monitoring Service)\n-\tIn situ air quality measurements obtained directly from local environmental protection agencies or through the European Environment Agency (EEA)\n-\tMeteorological data from various sources, such as the Copernicus Climate Data Store, ECMWF, or national and regional meteorological services\nThe Institute of Applied Physics \"Nello Carrara\" of the National Research Council (CNR-IFAC) and S[&]T are cooperating on the development of the AirPortal-Catania module, following operational procedures already tested and consolidated through the H2020 AURORA project (Advanced Ultraviolet Radiation and Ozone Retrieval for Applications). Within that project, the AirPortal4Florence application was implemented and tested for high-resolution forecasting and analysis of ozone concentrations in the city center of Florence.\nIFAC is integrating its capabilities within UISH project with regard to the collection, analysis, processing and display of satellite geographic data that are used to produce concentration maps of air pollutants in the vicinity of the city. This covers a 10 x 30 km analysis area, extending in a south-north direction from the airport serving the city of Catania to the summit of the volcano Etna.\nAs such, an open-source digital infrastructure for the acquisition, processing and interactive visualization of environmental data related to air pollution was designed and implemented. The work was developed with a view to reusability, openness of code and interoperability with scientific standards and GIS tools.\nThe goal is to provide a replicable and flexible tool for air quality monitoring and analysis by integrating heterogeneous data sources (NetCDF formats, time series, gridded models) into a coherent and accessible platform, supported by a map publishing system based on open technologies such as Postgres/PostGIS, QGIS-Server, Lizmap, and G3W-Suite.\nTo ensure scalability, security, and performance, the system was structured on two separate servers: a database server and a web/file server, to optimize computational resources, logically isolate critical components, and ensure easy extension to future scenarios (e.g., multi-site deployment, database clustering).\nAt the core of the system is an automatic environmental data import and conversion flow, capable of processing daily multi-variable NetCDF files (NO₂, SO2, PM₂.₅, PM10, O₃ concentrations) and transforming them into georeferenced time series.\nThe pipeline is developed in Python and involves reading and interpreting NetCDF content, generation of a regular grid of cells for the area of interest, extraction and spatial aggregation of variables on a daily basis, normalized storage in optimized PostGIS tables, logging and error handling.\nIn addition to point values, the system is able to derive isohypses by interpolating the data for better spatial visualization, which are also saved to databases as vector geometries.\nTo enable interactive access to the data, two mapping environments have been configured and customized: Lizmap Web Client (which allows dynamic layer browsing, temporal selection, filter application and thematic visualization of air quality indicators) and G3W-Suite, with advanced features such as thematic dashboards, editing modules and integrated workflows.\nBoth environments are designed to operate with publishable QGIS Desktop projects with ease thanks to a consistent repository and permission structure.\nIn addition, the project integrates the implementation of open-source software for easy uploading of QGIS projects to Lizmap. This tool allows users to quickly publish maps and layers with predefined configurations, reducing WebGIS content update times and minimizing errors in content synchronization.\nThe software will be made available under a free license, along with the entire set of provisioning scripts, data import routines and server configurations, making the entire infrastructure replicable to other spatial contexts.\nAll components developed will be publicly accessible through open-source channels, including the server installation and configuration scripts, the Python tools for importing and processing NetCDF files, the database templates and ready-to-use QGIS project examples and the user manual and detailed technical documentation.\nThis choice is in line with the principles of transparency, replicability, and interoperability that guide scientific research in the environmental field, and allows other research groups, local governments, or environmental protection agencies to adopt, customize, and extend what has been implemented.\nThe infrastructure is currently operational and will be kept up to date, with a view to stimulating international collaboration in the field of open environmental data and contributing to greater public awareness of pollution and health issues."
      },
      {
        "title": "Combining remote sensing data and geospatial datasets to improve a national wetlands inventory",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "Combining geospatial and remote sensing data to create accurate representations of national wetlands from diverse datasets raises technical and real-world challenges. \nThis talk covers how we minimize negative impacts of technical decisions while maximizing positive outcomes, and how we combine multiple datasets to support national wetlands mapping and monitoring.",
        "description": "_"
      },
      {
        "title": "Empowering Reservoir Water Managers: Tool for Assessment of Lakes using FOSS4G",
        "type": "Talk",
        "track": "Use cases & applications",
        "abstract": "To bridge the gap between research and field requirements, an explorer for water managers is actively being developed using Earth Observation (EO) data and FOSS4G techniques. The explorer will enable the water managers to remotely monitor key reservoir parameters, hence enabling informed decision making.",
        "description": "The double whammy of increasing human water consumption and the effects of warming world are poised to further shrinking of large lakes, especially in arid and semi-arid regions. To determine reservoir’s live storage dynamics, updated area-elevation-capacity curve and field water level observation are required. The lack of hydrological observation for several lakes greatly limits the actual estimation of their storage dynamics and related assessments on water-related disasters (like flood or drought). Recent studies show that satellite altimetry offers a powerful new way to gauge lake water levels and lake storage. Yet the storage accuracy of these estimates is still affected by inherent limitations due to data capturing procedure. The motivation of the present study is based on how using geospatial data can help overcome limitations, reduce uncertainties in estimating lake storage and provide a platform to reservoir managers, where more reliable and granular lake storage capacity dynamics can be explored.\nThe research work incorporates various steps and strategies to minimize the uncertainty in computing incremental volume between two consecutive contour levels. These strategies to lessen the propagation of various sources of uncertainty collectively lead to more accurate estimates. Further to this, a novel methodology was developed to estimate a reservoir's live storage capacity based on geospatial inputs without needing ground-based observations. \nThis work aims to provide a platform which would empower the stakeholders, who wish to remotely monitor their respective reservoir storage dynamics. The explorer aims at providing a ready-to-use, click-to-view platform, where water managers can see remotely the water boundary extent and time-series without the need to understand the backend code."
      },
      {
        "title": "Introduction to libCartoSym, libCQL2 and libDE9IM",
        "type": "Talk",
        "track": "Tools, Libraries & Visualisation",
        "abstract": "An introduction and overview of the capabilities provided by the Free and Open Source Software libCartoSym and related dependency libraries (libCQL2, libDE9IM...) implementing the candidate [OGC Cartographic Symbology 2.0 Standard](https://github.com/opengeospatial/cartographic-symbology). http://cartosym.org/ https://github.com/ecere/libCartoSym",
        "description": "libCartoSym is a Free and Open-Source Software library implementing [OGC Cartographic Symbology 2.0](https://github.com/opengeospatial/cartographic-symbology)\n\n_libCartoSym_ aims to be an implementation of the [CartoSym-CSS](https://docs.ogc.org/DRAFTS/18-067r4.html#rc-cscss) and\n[CartoSym-JSON](https://docs.ogc.org/DRAFTS/18-067r4.html#rc-json) encodings defined in the candidate\n[OGC Cartographic Symbology - Part 1: Core Model and Encodings Standard version 2.0](https://docs.ogc.org/DRAFTS/18-067r4.html) Standard.\n\nThe library allows to read and write these CartoSym encodings, as well as import from and export to additional encodings of portrayal rules such as OGC [SLD](https://portal.ogc.org/files/?artifact_id=22364)/[SE](https://portal.ogc.org/files/?artifact_id=16700) and [Mapbox GL Styles](https://docs.mapbox.com/mapbox-gl-js/guides/styles/).\n\nSince the CartoSym encodings extend the [OGC Common Query Language (CQL2)](https://www.opengis.net/doc/IS/cql2/1.0), the library also relies on a related open-source libCQL2 library providing support for\nparsing and writing CQL2-Text and CQL2-JSON expressions, as well as run-time evaluation of CQL2 expressions. Support for performing spatial relation queries based on the\n[Dimensionally Extended-9 Intersection Model](https://en.wikipedia.org/wiki/DE-9IM) is also\nintegrated within a jointly developed _libDE9IM_ open-source library, and support for OGC Simple Features as well as parsing and writing geometries defined in\n[Well-Known Text (WKT)](http://portal.opengeospatial.org/files/?artifact_id=25355) and [GeoJSON](https://tools.ietf.org/rfc/rfc7946.txt) is also provided by related open-source libraries.\n\nWhile these libraries are written in the [eC programming language](https://ec-lang.org), object-oriented bindings for _libCartoSym_ automatically generated using Ecere's [binding generating tool (bgen)](https://github.com/ecere/bgen) will also be made available for the C, C++ and Python programming languages, with additional support planned for Java and Rust.\n\n_Acknowledgement_\nFinancial support provided by GeoConnections, a national collaborative initiative led by Natural Resources Canada. GeoConnections supports the modernization of the Canadian Geospatial Data Infrastructure (CGDI). The CGDI is the collection of geospatial data, standards, policies, applications, and governance that facilitate its access, use, integration, and preservation."
      },
      {
        "title": "From Edits to Impact - TomTom’s Journey with Open Communities",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "TomTom partners with communities worldwide to build richer maps through collective action. In this FOSS4G session, we share how we support university mapathons, YouthMappers, and humanitarian mapping in 189 countries, reflecting on learnings and challenges in scaling community engagement while advancing OSM’s mission through meaningful, local partnerships.",
        "description": "TomTom and the Open Map Community share a common vision: building a map that reflects and serves the world, powered by local knowledge and collective action. In this session, We will be taking you behind the scenes of TomTom’s partnerships with OSM communities worldwide, demonstrating how collaborative efforts are strengthening the quality, freshness, and inclusiveness of OpenStreetMap data.\nFrom hosting mapathons with universities and YouthMappers chapters to supporting humanitarian mapping during crises, TomTom’s initiatives have connected 3,397+ OSM contributors across 189 countries. We will share real examples of how these collaborations are transforming local mapping ecosystems while providing technical resources, training, and data that support contributors in making impactful edits.\nThe session will also cover learnings on We will also discuss scaling corporate-community collaborations responsibly, challenges faced in diverse regions, and opportunities for co-creating projects that benefit both OSM and local communities. Participants will leave with ideas and inspiration on how to replicate or adapt these approaches to engage more mappers, foster skill-building, and deepen OSM’s reach.\nJoin us to discover how mapping together can empower communities and create a richer, more inclusive map of the world."
      },
      {
        "title": "Moving from Science to Product: Making Open GIS Software Work for Us",
        "type": "Talk",
        "track": "Community, Collaboration & Impact",
        "abstract": "How CTrees wrote open source code to complete zonal stats in order to move from science results (rasters) to interactive mapping apps.",
        "description": "At CTrees, we create machine learning models that integrate multiple data sources to produce high-resolution, time-series datasets on forest carbon and activity. Our outputs— estimates of carbon stocks, emissions, removals, and forest change—support projects ranging from jurisdictional analysis to deforestation monitoring. However, scaling from research-driven models to a production-ready platform required solving two major challenges: (1) data management and (2) enabling fast, on-demand analysis that serves both internal researchers and external users.\nOn the data side, we faced the all-too-familiar chaos of manual and ad-hoc dataset versioning (v2, v2_final, v2_final_final), inconsistent folder structures where important data dates could be recorded in the filename or prefix, and directories packed with thousands of tiny GeoTIFFs meant to be mosaicked together. Instead of chasing S3 paths and manually stitching ad-hoc datacube files, we adopted Icechunk (open source) by Earthmover, which leverages Zarr and Icechunk to enable structured, versioned cloud-native datacube access. As a bonus, Arraylake (built on icechunk) makes it easy to view the data via a WMS service, streamlining the visualization process for both internal users and external stakeholders.\nThe second challenge was transitioning from scientific scripts—often written in R or relying on heavy GDAL operations—to a streamlined, scalable approach. To enable on-demand analysis, we refactored these scripts into ctreeskit (https://github.com/ctrees-products/ctreeskit) , our open-source Python package that consolidates spatial processing and zonal statistics using Xarray. Currently in its pre-release (beta) version, ctreeskit is designed for flexibility, it supports researchers across CTrees in papers, reports, and internal workflows. More broadly, ctreeskit is available to anyone performing similar calculations. By standardizing these processes and ensuring transparency, we enhance reproducibility, efficiency, and accessibility, allowing both internal teams and external users to understand and verify our methodologies.\nWith ctreeskit and Arraylake, we now have three key things: (1) a reliable way to save and access data, (2) a simple way to slice and dice high-dimensional data in the time and spatial domains, and (3) lightweight tools for running reproducible calculations. Moving from raw GeoTIFFs to an array-based Zarr model gives us fast, structured access to data without the complexity of STAC catalogs. By shifting from scattered S3 paths and heavyweight processing to a structured, cloud-native approach, we’ve made geospatial data easier to access, faster to analyze, and more transparent—helping both internal teams and external users work more efficiently."
      },
      {
        "title": "QGIS Community Day Event",
        "type": "Community Day Event",
        "track": "Associated Event",
        "abstract": "QGIS Events organized by QGIS Australia",
        "description": "QGIS Events organized by QGIS Australia"
      },
      {
        "title": "QGIS Community Day Event",
        "type": "Community Day Event",
        "track": "Associated Event",
        "abstract": "QGIS Events organized by QGIS Australia",
        "description": "QGIS Events organized by QGIS Australia"
      },
      {
        "title": "QGIS Community Day Event",
        "type": "Community Day Event",
        "track": "Associated Event",
        "abstract": "QGIS Events organized by QGIS Australia",
        "description": "QGIS Events organized by QGIS Australia"
      },
      {
        "title": "QGIS Community Day Event",
        "type": "Community Day Event",
        "track": "Associated Event",
        "abstract": "QGIS Events organized by QGIS Australia",
        "description": "QGIS Events organized by QGIS Australia"
      }
    ]
  }
]