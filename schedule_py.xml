<?xml version='1.0' encoding='utf-8'?>
<schedule><generator name="pretalx" version="2024.1.0" />
    <version>1.6</version>
    <conference>
        <title>FOSS4G 2025</title>
        <acronym>foss4g-2025</acronym>
        <start>2025-11-17</start>
        <end>2025-11-23</end>
        <days>7</days>
        <timeslot_duration>00:05</timeslot_duration>
        <base_url>https://talks.osgeo.org</base_url>
        <logo>https://talks.osgeo.org/media/foss4g-2025/img/Full_colour_KQPM5iW.png</logo>
        <time_zone_name>Pacific/Auckland</time_zone_name>
        
        
        <track name="Registration" slug="360-registration" color="#848484" />
        
        <track name="B2B" slug="359-b2b" color="#900c3f" />
        
        <track name="AGM" slug="358-agm" color="#289ad9" />
        
        <track name="Talk" slug="237-talk" color="#e67e22" />
        
        <track name="Use Cases and Applications" slug="357-use-cases-and-applications" color="#78dae3" />
        
        <track name="Desktop GIS &amp; Data Collection" slug="356-desktop-gis-data-collection" color="#d88484" />
        
        <track name="Cloud, APIs &amp; Data Infrastructure" slug="353-cloud-apis-data-infrastructure" color="#b2df8a" />
        
        <track name="Tools, Libraries &amp; Visualisation" slug="355-tools-libraries-visualisation" color="#a68ce0" />
        
        <track name="Community, Collaboration &amp; Impact" slug="354-community-collaboration-impact" color="#d1b598" />
        
        <track name="State of software" slug="351-state-of-software" color="#2ecc71" />
        
        <track name="AI, Data Science &amp; Analytics" slug="352-ai-data-science-analytics" color="#cb824f" />
        
        <track name="Academic" slug="246-academic" color="#84a0ae" />
        
        <track name="Lightning talk" slug="235-lightning-talk" color="#ffc300" />
        
        <track name="Workshop - Beginner Friendly" slug="238-workshop-beginner-friendly" color="#46e796" />
        
        <track name="Workshop - Intermediate Level" slug="369-workshop-intermediate-level" color="#eab844" />
        
        <track name="Workshop - Advanced Level" slug="370-workshop-advanced-level" color="#eb6437" />
        
        <track name="Ceremony" slug="236-ceremony" color="#4ca57b" />
        
        <track name="Keynote" slug="240-keynote" color="#aa1456" />
        
        <track name="Panel" slug="239-panel" color="#6ac2ea" />
        
        <track name="Mapathon" slug="242-mapathon" color="#928787" />
        
        <track name="Associated Event" slug="241-associated-event" color="#9a5bd7" />
        
        <track name="Break" slug="244-break" color="#000000" />
        
        <track name="Community Day Event" slug="243-community-day-event" color="#ed4ce3" />
        
        <track name="Social Event" slug="245-social-event" color="#fe69d0" />
        
    </conference>
    <day index="1" date="2025-11-17" start="2025-11-17T04:00:00+13:00" end="2025-11-18T03:59:00+13:00"><room name="WF613" guid="4b8eb6af-8bec-52d0-92fe-d5ec4df0b8d3"><event guid="4a268bc7-0a04-5298-9d66-a6eeee3a82df" id="4386">
                <room>WF613</room>
                <title>Cloud-Native Geospatial for Earth Observation Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>03:00</duration>
                <abstract>Explore cloud-native geospatial tools through this hands-on workshop utilizing Python Notebooks. This workshop introduces key concepts like STAC and COGs, and walks participants through real-world Earth observation analyses, empowering EO professionals to apply modern tools in their own work.</abstract>
                <slug>foss4g-2025-4386-cloud-native-geospatial-for-earth-observation-workshop</slug>
                <track>Workshop - Intermediate Level</track>
                
                    
                <language>en</language>
                <description>The advent of cloud computing has revolutionised the capabilities of researchers and professionals globally, helping them to access and analyse Earth observation (EO) data more easily than ever. Despite the well-understood tools and technologies, such as cloud-optimised GeoTIFFs and the Spatio-Temporal Asset Catalog (STAC) and STAC API specifications, many EO professionals have not yet had the opportunity to practically apply these innovations. This workshop aims to bridge that gap by showcasing how cloud-native geospatial technologies simplify the process of working with EO data, using Python as the primary programming language.
In part one of the workshop, we’ll give an introduction to cloud native geospatial, and then participants will get hands-on coding an Earth observation data science notebook from scratch, loading and visualising Landsat data.
In part two, we’ll talk about Al Gore’s vision for a digital earth, and how we’re on the path to realising that vision, and then we’ll delve into a real-world case study focused on documenting land productivity metrics, a crucial component for monitoring the UN Sustainable Development Goal (SDG) indicators for 15.3.1, also using Landsat data. Then we’ll shift over to another example of a long time series of sea-surface temperature data, accessed from Source Coop, before concluding with a discussion session.
Throughout the workshop, participants will gain hands-on experience and insights into how cloud-native geospatial technologies have significantly enhanced the ability to access and analyze large volumes of EO data. By the end of the session, attendees will have acquired practical examples and knowledge to further develop their skills in this innovative field.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/ZWVCEH/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/ZWVCEH/feedback/</feedback_url>
            </event></room><room name="WF610" guid="996d6a98-96cd-5725-a014-5768c74bccc8"><event guid="4b136b7e-ed79-5509-9ce3-631aed1134aa" id="3719">
                <room>WF610</room>
                <title>Diving into pygeoapi Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>03:00</duration>
                <abstract>pygeoapi is an OGC Reference Implementation supporting numerous OGC API specifications. This workshop will cover publishing geospatial data to the Web using pygeoapi in support of the suite of OGC API standards.</abstract>
                <slug>foss4g-2025-3719-diving-into-pygeoapi-workshop</slug>
                <track>Workshop - Intermediate Level</track>
                <logo>/media/foss4g-2025/submissions/83XWUZ/pygeoapi-logo-notrans_1db1zTO.png</logo>
                    
                <language>en</language>
                <description>pygeoapi is an OGC Reference Implementation supporting numerous OGC API specifications. Lightweight, easy to deploy and cloud-ready, pygeoapi's architecture facilitates publishing datasets and processes from multiple data sources to the Web. This tutorial will cover publishing geospatial data to the Web, and using the API from QGIS, OWSLib and a web browser. The workshop will cover the following OGC API standards:

- OGC API - Features
- OGC API - Coverages (OACov)
- OGC API - Maps (OAMaps)
- OGC API - Tiles (OATiles)
- OGC API - Processes (OAProc)
- OGC API - Records (OARec)
- OGC API - Environmental Data Retrieval (EDR)
- SpatioTemporal Asset Catalog (STAC)

**Requirements for the Attendees**

Please consult the workshop documentation at https://dive.pygeoapi.io, and ensure you are setup accordingly (https://dive.pygeoapi.io/setup) prior to attending the workshop.

A Gitter channel exists at https://gitter.im/geopython/diving-into-pygeoapi for discussion and live support from the developers of the workshop.

As the installation of all dependencies on all platforms (Windows, Mac, Linux) can be quite involved and complex, this workshop provides all components within a Docker Image.

The core requirement is to have Docker and Docker Compose installed on the system. Once you have Docker and Docker Compose installed you will be able to install the workshop without any other dependencies.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/83XWUZ/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/83XWUZ/feedback/</feedback_url>
            </event><event guid="866afbcb-64ca-5c05-a776-64d4efa64a2c" id="3720">
                <room>WF610</room>
                <title>Doing Geospatial with Python</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>03:00</duration>
                <abstract>This workshop will provide an introduction to performing common GIS/geospatial tasks using Python geospatial tools such as OWSLib, Shapely, Fiona/Rasterio, and common geospatial libraries like GDAL, PROJ, pycsw, as well as other tools from the geopython toolchain.</abstract>
                <slug>foss4g-2025-3720-doing-geospatial-with-python</slug>
                <track>Workshop - Beginner Friendly</track>
                
                    
                <language>en</language>
                <description>With a low barrier to entry and large ecosystem of tools and libraries, Python is the lingua franca for geospatial development. Whether you are doing data acquisition, processing, publishing, integration or analysis, there is no shortage of solid Python tools to assist in your daily workflows.

This workshop will provide an introduction to performing common GIS/geospatial tasks using Python geospatial tools such as OWSLib, Shapely, Fiona/Rasterio, and common geospatial libraries like GDAL, PROJ, pycsw, as well as other tools from the geopython toolchain. Manipulate vector/raster data using Shapely, Fiona and Rasterio. Publish data and metadata to OGC web services using pygeoapi, pygeometa, pycsw, and more. Visualize your data on a map using Jupyter and Folium. Plus a few extras in between!

The workshop is provided using the Jupyter Notebook environment with Python 3.

**Requirements for the Attendees**

Please see https://geopython.github.io/geopython-workshop for details on how to setup the workshop before you attend.

A Gitter channel exists at https://gitter.im/geopython/geopython-workshop for discussion and live support from the developers of the workshop.

The workshop uses Jupyter Notebooks. Jupyter is an interactive development environment suitable for documenting and reproducing workflows using live code.

As the installation of all dependencies on all platforms (Windows, Mac, Linux) can be quite involved and complex, this workshop provides all components within a Docker Image.

In addition, geospatial web services like pygeoapi and pycsw in this workshop are provided by Docker images.

The core requirement is to have Docker and Docker Compose installed on the system. Once you have Docker and Docker Compose installed you will be able to install the workshop without any other dependencies.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/XD8Z8K/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/XD8Z8K/feedback/</feedback_url>
            </event></room><room name="WF502" guid="43704638-97ac-5315-895e-0e25b52aa56e"><event guid="12ac6f0d-da67-5cdd-ad40-8ab2c67ec19d" id="4395">
                <room>WF502</room>
                <title>Microwave Image Processing: Exploring realms of Earth  through spaceborne Radars using Python Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>03:00</duration>
                <abstract>Synthetic Aperture Radar(SAR) is an Imaging Radar that acquires images of a particular area in the microwave region  and possesses  all-weather,all-time imaging capabilities.This workshop deals with the processing of SAR Images and how these images can be beneficial in a variety of geographical applications.</abstract>
                <slug>foss4g-2025-4395-microwave-image-processing-exploring-realms-of-earth-through-spaceborne-radars-using-python-workshop</slug>
                <track>Workshop - Intermediate Level</track>
                
                    
                <language>en</language>
                <description>Intended Audience : The workshop will be aimed at the audience belonging to any level of education. It will introduce them to the wonderful class of SAR images and how are these images useful from the perspective of various applications.

Expected Outcomes (after the workshop, the audience will be) :

i) Able to understand the acquisition of SAR imagery.

ii) Able to understand the types of datasets utilized in remote sensing

iii) Able to use the GDAL library to perform operations on images

iv) Able to efficiently process SAR imagery using Python

 v) Able to draw a roadmap in order to utilize SAR imagery for various geographic applications

Outline

The workshop will be divided into the following sub-sessions :

Sub-Session-1: Introduction to Microwave Remote Sensing (30 minutes) - This part will discuss the foundations of Microwave Remote Sensing. Theoretical aspects regarding the acquisition of images, the formation of images encompassing the generation of complex images and ground range detected images will be discussed.

Sub-Session-2: Pythonic Way to SAR Image Processing (150 minutes): This part will focus on achieving the following Key points:

1) Basic utilization of GDAL, Numpy and Matplotlib Libraries for opening and visualizing Images(30 minutes)

2) Codes will be developed separately for calibration for each SAR sensor(esp. Sentinel-1, Radarsat-2) from scratch.(45 minutes)

3) Utilization of the codes developed in (2) for various applications such as Oceanography, Forestry, etc.(75 minutes)

Datasets: Free Imagery data sets of Sentinel-1 SAR will be utilized. In addition, Sample Data sets of Radarsat-2, RISAT- 1 which are freely downloadable will be utilized.The sample datasets will be provided. For better understanding of the datasets, the participants may download and utilize the Sentinel-1 SAR free Image dataset initially .Sentinel-1 Free SAR Imagery(https://browser.dataspace.copernicus.eu/)

Conduct of the workshop : The workshop will be conducted through the means of Jupyter Notebooks. Along with the sessions, the audience will be provided with the exercises to clear their concepts of SAR Imagery.

Total Duration

The duration of the workshop will be  3 hours.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/EQTRSA/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/EQTRSA/feedback/</feedback_url>
            </event><event guid="68a7f8fa-64e2-5570-90c8-a8a77e92378c" id="4279">
                <room>WF502</room>
                <title>Scalable Remote Sensing Workflows with Xarray Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>03:00</duration>
                <abstract>XArray is a powerful Python package for working with climate and earth observation datasets. This workshop will be give you a structured introduction to XArray with use-cases focused on remote sensing applications.</abstract>
                <slug>foss4g-2025-4279-scalable-remote-sensing-workflows-with-xarray-workshop</slug>
                <track>Workshop - Intermediate Level</track>
                <logo>/media/foss4g-2025/submissions/RBEXYV/xarray_sn4lzPN.png</logo>
                    
                <language>en</language>
                <description>Xarray is an evolution of rasterio and is inspired by libraries like pandas to work with raster datasets. It is particularly suited for working with multi-dimensional time-series raster datasets. With the growing ecosystem of spatial extensions like rioxarray and xarray-spatial and built-in support for parallel computing with Dask, it has become the de-facto standard for working with large spatio-temporal raster datasets. This workshop will show you how you can use it to effectively process large volumes of earth observation data. We will cover the following topics:

1. Computing Remote Sensing Indicies
2. Cloud Masking
3. Extracting Time-Series
4. Calculating Zonal Statistics
5. Analyzing Landcover Data


Pre-requisites:
- Familiarity with Python programming and Remote Sensing datasets.
- We will be using [Google Colab](https://colab.google/) as the computing environment for the workshop. Participants will need a Google Account to access the platform.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/RBEXYV/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/RBEXYV/feedback/</feedback_url>
            </event></room><room name="WF503" guid="77ce8f26-86c8-5fa9-a5d4-82b6ee2f3e5a"><event guid="2648be56-4e24-5ccd-8c5f-b16ac15ad65d" id="4337">
                <room>WF503</room>
                <title>Hands-on DGGS and OGC DGGS-API with DGGRID and pydggsapi Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>03:00</duration>
                <abstract>This hands-on workshop walks through a full DGGS data pipeline. You'll use the FOSS tool DGGRID to index geospatial data and then publish it using pydggsapi, a new open-source server for the OGC API DGGS standard. Leave with a running web service on your laptop.</abstract>
                <slug>foss4g-2025-4337-hands-on-dggs-and-ogc-dggs-api-with-dggrid-and-pydggsapi-workshop</slug>
                <track>Workshop - Intermediate Level</track>
                <logo>/media/foss4g-2025/submissions/AGFBHA/day-04-hexa_7hrjJyo.png</logo>
                    
                <language>en</language>
                <description>Discrete Global Grid Systems (DGGS) are getting more attention, and with the new OGC API - DGGS standard released, it's a good time for the open-source community to get practical, hands-on experience. This workshop bridges the gap between the theory of DGGS and a working implementation.

We'll show you why DGGS are so useful for integrating and indexing different data sources and spatial data analysis without the usual headache of map projections. Then, we work through a complete, real-world data pipeline using FOSS tools.

In this workshop, you will:

- take a standard geospatial file (like a GeoTIFF or GeoPackage).
- use the command-line tool DGGRID (https://github.com/sahrk/DGGRID | https://dggrid.readthedocs.io/latest/ ), generate grids and index this data onto a hexagonal grid
- we will introduce hierarchical indexing for ISEA3H and ISEA7H with the new Z3 and Z7 indexing systems in DGGRID
- in the decond part, we set up and configure pydggsapi (https://github.com/LandscapeGeoinformatics/pydggsapi/ | https://pydggsapi.readthedocs.io/en/latest/), an open-source Python server that implements the newly (to be) released OGC API - DGGS standard.

- we then point pydggsapi at the data you just created and launch the service
- we explore several ways how to interact with your new web service using a browser, or curl/Python notebook, and maybe even QGIS, to make queries and retrieve data.

Who should attend?

This workshop is for developers, data managers, and generally DGGS and geospatial enthusiasts who want to learn how to publish their data using this new paradigm.

Prerequisites:

Participants should be comfortable with the command line and have a basic understanding of what geospatial raster and vector data are. We can use Docker or plain Miniconda/Micromamba/Pixi Python environments to ensure the dependencies are easy to set up for everyone. Bring your laptop (Win, Mac, Linux, *BSD might all work).

By the end, you will understand the concepts of DGGS-indexed data and will have built a functioning DGGS-based web service yourself.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/AGFBHA/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/AGFBHA/feedback/</feedback_url>
            </event></room><room name="WF510" guid="76e27120-f58d-590f-9851-cf78ba0a217a"><event guid="3254feb1-db66-53e7-a461-a51eb89e5518" id="3744">
                <room>WF510</room>
                <title>QGIS PLUGIN DEVELOPMENT Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>03:00</duration>
                <abstract>This hands-on workshop teaches the basics of QGIS plugin development with Python. Participants will learn to set up their environment, build functional tools, and extend QGIS capabilities. Ideal for GIS users and developers, the session includes a practical project where attendees create and test a simple plugin.</abstract>
                <slug>foss4g-2025-3744-qgis-plugin-development-workshop</slug>
                <track>Workshop - Advanced Level</track>
                
                    
                <language>en</language>
                <description>👥 Target Audience:

GIS analysts, researchers, environmental scientists, developers and GIS enthusiasts

Anyone with basic Python knowledge and interest in geospatial tools

🎯 Learning Objectives:

Understand the QGIS plugin architecture and how QGIS interacts with Python
Learn to set up a development environment for plugin creation
Build and package a basic functional plugin
Understand GUI design using Qt Designer
Use PyQGIS API to access layers, features, and perform spatial tasks
Learn tips for plugin debugging, deployment, and sharing via QGIS Plugin Repository</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/ZFPBLE/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/ZFPBLE/feedback/</feedback_url>
            </event></room><room name="WF511" guid="134d8686-2e43-5f63-a6ea-0cec9bd63759"><event guid="b86db7cb-9f50-587a-9e70-5e0b3d199e46" id="4462">
                <room>WF511</room>
                <title>Achieving Reproducibility in Geospatial Data Processing with Apache Airflow Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>03:00</duration>
                <abstract>In this workshop, participants will learn to automate geospatial data processing using Apache Airflow, focusing on designing workflows, integrating geospatial data, and leveraging tools/libraries like GeoPandas, GDAL, MinIO, and python debugger. The session will emphasize efficiency and reproducibility, enabling participants to create reliable and repeatable geospatial data pipelines.</abstract>
                <slug>foss4g-2025-4462-achieving-reproducibility-in-geospatial-data-processing-with-apache-airflow-workshop</slug>
                <track>Workshop - Beginner Friendly</track>
                
                    
                <language>en</language>
                <description>Tools &amp; Technologies
- Apache Airflow (Workflow management)
- GeoPandas / GDAL (Geospatial data processing libraries)
- MinIO (S3 compatible object storage)
- debugpy (Debugger for Python)
- GeoNetwork (Geospatial data catalog)


Key Takeaways
- Deploy and manage services MinIO and Apache Airflow with docker container
- Automate geospatial data processing workflows using Apache Airflow
- Ensure reproducibility in geospatial data pipelines for consistent results
- Store and manage geospatial data efficiently with MinIO
- Debug and troubleshoot Python code in geospatial workflows using debugpy
- Organize and catalog geospatial datasets with GeoNetwork (if time permits)</description>
                <recording>
                    <license />
                    <optout>true</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/VTVQQL/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/VTVQQL/feedback/</feedback_url>
            </event></room><room name="WF710" guid="deb11013-07b1-51bb-afb1-4b6fdde878e3"><event guid="509a4de4-966d-5c58-b375-4bbd87474d7d" id="4188">
                <room>WF710</room>
                <title>Exploring Cloud-Native Geospatial Formats: Hands-on with Raster Data Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>03:00</duration>
                <abstract>Dig into three cloud-native raster formats—COGs, Zarr, and Kerchunk—and learn how data access works under the hood with hands-on Python exercises, no image libraries required!</abstract>
                <slug>foss4g-2025-4188-exploring-cloud-native-geospatial-formats-hands-on-with-raster-data-workshop</slug>
                <track>Workshop - Advanced Level</track>
                
                    
                <language>en</language>
                <description>Ever wonder what GDAL is doing under the hood when you read a GeoTIFF file? Doubly so when the file is a Cloud-optimized GeoTIFF (COG) on a remote server somewhere? Have you been wondering what this new Zarr thing is all about and how it actually works? Then there's the whole Kerchunk/VirtualiZarr indexing to get cloud-native access for non-cloud-native data formats, what's that about?

Cloud-native geospatial is all the rage these days, and for good reason. As file sizes grow, layer counts increase, and analytical methods become more complex, the traditional download-to-the-desktop approach is quickly becoming untenable for many applications. It's no surprise then that users are turning to cloud-based tools such as Dask to scale out their analyses, or that traditional tooling is adopting new ways of finding and accessing data from cloud-based sources. But as we transition away from opening whole files to now grabbing ranges of bytes off remote servers it seems all the more important to understand exactly how cloud native data formats actually store data and what tools are doing to access it.

This workshop aims to dig into how cloud-native geospatial data formats are enabling new operational paradigms, with a particular focus on raster data formats. We'll start on the surface by surveying the current cloud-native geospatial landscape to gain an understanding of why cloud native is important and how it is being used, including:

* the core tenets of cloud-native geospatial data formats
* cloud-native data formats for both raster and non-raster geospatial data
* the intersection with SpatioTemporal Asset Catalogs (STAC) and how higher-level STAC-based tooling can leverage cloud-native formats for efficient raster data access processing of cloud-native data

Then we'll get hands-on and go deep to build up an in-depth understanding of how cloud native raster formats work. We'll examine the COG format and read a COG from a cloud source by hand using just Python, progressively grabbing data from the image until we can extract a target tile, all without using any image libraries. We'll repeat the same exercise for geospatial data in Zarr format to see how that compares to our experience with COGs. Lastly we'll turn our attention to Kerchunk/VirtualiZarr to see how these technologies might allow us to better optimize data access with non-cloud-native formats.

#### Prerequisites

This workshop expects some familiarity with geospatial programming in Python. Most of the notebook code is already provided, so any gaps in understanding don't necessarily prohibit completing the exercises. That said, a basic knowledge of STAC and Cloud-Native Geospatial Python tooling and working with rasters as single and multidimensional arrays is quite helpful.

A good primer workshop is Alex Leith of Auspatious's [Cloud-Native Geospatial for Earth Observation Workshop](https://github.com/auspatious/cloud-native-geospatial-eo-workshop). It is recommended to work through those activities or have an equivalent knowledge prior to working through the notebooks in this workshop.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/KZGHTZ/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/KZGHTZ/feedback/</feedback_url>
            </event></room><room name="WF711" guid="5041bc96-c06d-5ed0-acf1-c71cfefe5219"><event guid="841ba313-898c-56c2-b167-3756df3f529d" id="3769">
                <room>WF711</room>
                <title>Getting Sentinel Data within Seconds with STAC Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-17T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>03:00</duration>
                <abstract>Discover how to access and analyze Sentinel satellite imagery in seconds using STAC APIs and Microsoft Planetary Computer. In this hands-on workshop, you'll use Python to fetch data, calculate vegetation indices, and build efficient Earth observation workflows.</abstract>
                <slug>foss4g-2025-3769-getting-sentinel-data-within-seconds-with-stac-workshop</slug>
                <track>Workshop - Beginner Friendly</track>
                
                    
                <language>en</language>
                <description>This hands-on workshop introduces a modern and efficient way to access and analyze Sentinel satellite data using STAC (SpatioTemporal Asset Catalog) APIs and the Microsoft Planetary Computer. You'll learn how to build scalable Earth observation workflows in Python—without downloading massive datasets manually.
We’ll begin with an overview of STAC and the role of the Microsoft Planetary Computer as a cloud-native source for open geospatial data. Participants will learn how to search and access Sentinel-2 and Sentinel-1 imagery based on time, location, and cloud coverage—directly within Python using libraries like pystac-client, odc-stac, and xarray.
During the workshop, you'll:
- Search and preview Sentinel datasets using STAC
- Fetch cloud-hosted imagery from Microsoft Planetary Computer
- Visualize bands and calculate vegetation indices like NDVI, EVI, and RVI
- Perform pixel-level analysis with minimal compute time
- Build efficient, reproducible workflows using Jupyter notebooks

By the end, you’ll have a working pipeline to go from area of interest to actionable insights in minutes—ideal for environmental monitoring, agriculture, and forest analysis. This workshop is perfect for developers, remote sensing analysts, or GIS professionals looking to simplify and accelerate their satellite data workflows.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/CVT8GC/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/CVT8GC/feedback/</feedback_url>
            </event></room></day><day index="2" date="2025-11-18" start="2025-11-18T04:00:00+13:00" end="2025-11-19T03:59:00+13:00"><room name="WF502" guid="43704638-97ac-5315-895e-0e25b52aa56e"><event guid="bac5de11-3fca-5019-be74-ae608a6f3d8b" id="4325">
                <room>WF502</room>
                <title>Participatory mapping field survey and computer lab: QField integration into machine learning landcover classification within Digital Earth Pacific. Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-18T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>03:00</duration>
                <abstract>The workshop will include a) field survey component where participants will be able to walk around Auckland to collect data points in QField for QGIS and b) a computer lab component where participants will use Digital Earth Pacific Python Notebooks to generate a land cover map.</abstract>
                <slug>foss4g-2025-4325-participatory-mapping-field-survey-and-computer-lab-qfield-integration-into-machine-learning-landcover-classification-within-digital-earth-pacific-workshop</slug>
                <track>Workshop - Beginner Friendly</track>
                <logo>/media/foss4g-2025/submissions/S9ENL7/DEP_8tmhIvr.png</logo>
                    
                <language>en</language>
                <description>FOSS4G abstracts:
Workshop abstract:
Participatory mapping field survey and computer lab: QField integration into machine learning landcover classification within Digital Earth Pacific.

3-hour workshop
The objective of the workshop is to share a workflow to allow for field data, calibration, and validation of a land cover classification model output. 

Land cover classification:
Land cover classification forms serves numerous functions including land cover accounting, monitoring land use changes, biodiversity and conservation monitoring, measurements of urban and agricultural expansion as well as forest inventories and national greenhouse gas inventories.  

This workshop will include two main components:
1.	a field survey component where participants will be able to walk around Auckland to collect data points in QField for QGIS. 
2.	a computer lab component where participants will use Digital Earth Pacific Python Notebooks to generate a land cover map based on the data collected in their respective surveys.
These participatory mapping workflows enable users from a range of disciplinary backgrounds to contribute to land cover mapping outputs. These outputs may be used for a range of applications including land cover classification. 
The learning outcomes of this workshop will include the following: 
1.	Participants will learn how to collect field data using QField 
2.	Participants will also be able to ingest this data into Digital Earth Pacific and through a Jupyter Notebook Environment
3.	Participants will be able to build on introductory levels of Python programming knowledge. 
Within QField and Python, participants will be making use of the following tools and libraries:
 

QField workflows to be covered:
Point data collection	Collection of points for different land cover classes
Transects 	Transects 
Polygons	Collecting polygon areas of interest
Accuracy assessments	Ensuring data collected is within set thresholds of horizontal accuracy

Python libraries to be covered:
Pandas / Geopandas	Vector data analysis and plotting
odc-geo	Web map plotting
Rasterio	Raster data analysis and plotting
odc.stac	Loading satellite data through Digital Earth Pacific Spatiotemporal Asset Catalogues.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/S9ENL7/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/S9ENL7/feedback/</feedback_url>
            </event></room><room name="WF503" guid="77ce8f26-86c8-5fa9-a5d4-82b6ee2f3e5a"><event guid="2c39bd34-0872-51fb-af37-ab2793355f16" id="4537">
                <room>WF503</room>
                <title>Oxidize to Decarbonize. Building more sustainable geospatial processes with Rust Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-18T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>03:00</duration>
                <abstract>Geospatial workflows can be surprisingly energy-intensive. This workshop introduces Rust as a fast, efficient alternative to interpreted languages like Python and R. Learn how Rust’s performance and safety make it ideal for sustainable geospatial analysis, and explore the eorst toolkit for spatial data, raster processing, and modelling.</abstract>
                <slug>foss4g-2025-4537-oxidize-to-decarbonize-building-more-sustainable-geospatial-processes-with-rust-workshop</slug>
                <track>Workshop - Beginner Friendly</track>
                
                    
                <language>en</language>
                <description>Even though servers, data centers, and high-performance computers use a considerable amount of energy—and thus contribute significantly to carbon emissions, the environmental impact of computation is often overlooked. This is especially ironic in geospatial science, where much of the work aims to understand and protect natural resources. Remote sensing workflows can be resource intensive, and as data volumes and analysis complexity grows, so too do the computational demands. Efficiency becomes not just a practical issue, but an environmental concern. The tools we use may matter more than we think.
 
This workshop explores how programming language choice can meaningfully reduce the environmental cost of geospatial analysis. While interpreted languages like Python and R are the most popular choices due to their accessibility and rich ecosystems, they often fall short at scale. Their reliance on runtime interpretation and garbage collection often translates into longer compute times and higher energy usage, and thus generate more emissions. 
 
Compiled languages like Rust and C++ can offer an alternative path. These languages are significantly more efficient, often completing tasks faster and with less energy consumption. Rust in particular is emerging as a powerful option for scientific computing. It provides memory safety without a garbage collector, using a system of ownership and borrowing to manage memory at compile time. The result is software that is fast and reliable, with minimal runtime overhead. Unlike C or C++, Rust helps prevent common errors such as buffer overflows, null pointer dereferencing, and data races on parallel processes, without sacrifice in performance.
 
Rust’s growing adoption by major tech players, like Microsoft, AWS, Google, and even the Linux kernel project reflects its maturity and reliability. This support, highlight that Rust is more than an academic or niche choice, but a practical, long-term solution.
 
This workshop will introduce core Rust concepts relevant to geospatial analysis and explore the growing Rust geospatial ecosystem—including libraries for spatial data handling, raster processing, modelling, and machine learning.
 
In this hands-on session, we will explore Rust fundamentals and use eorst to compute cloud frequency over a two-year period from satellite Sentinel-2 imagery.
 
The eorst crate is an open-source Rust library designed to simplify and accelerate geospatial raster processing. Inspired by tools like Rasterio, RIOS, Dask, and Open Data Cube, eorst wraps complex geospatial operations in high-level abstractions, while preserving the performance benefits of systems programming. It minimizes the overhead of abstraction, letting developers focus on the science rather than the plumbing.
 
Originally developed to support the Spatial BioCondition framework for modeling ecosystem condition, eorst now underpins large-scale operational workflows for Queensland’s Department of the Environment, Tourism, Science and Innovation. The library's main features are:
 
Efficient geospatial raster I/O
On-the-fly projection and alignment.
Tiling and parallel out-of-core processing.
Raster point sampling.
Zonal statistics.
Mosaicking.
Band math and time series operations.
STAC integration.
XGBoost and LightGBM inference.
Optional OpenCV image processing.
 
 
Rust also interoperates well with Python and R, making it a pragmatic choice for hybrid workflows and for teams gradually transitioning toward more efficient computation. As part of the session, we will also demonstrate how to create a simple Python wrapper using the pyo3 library, allowing Rust functionality to be accessed from Python.
 
If you are curious about producing more sustainable geospatial analysis, this session will be a practical starting point.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/RDZMQP/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/RDZMQP/feedback/</feedback_url>
            </event></room><room name="WF510" guid="76e27120-f58d-590f-9851-cf78ba0a217a"><event guid="b781d102-41e3-5470-8452-bb72cbb4a010" id="4311">
                <room>WF510</room>
                <title>Modelling Climate Risks Using NASA Earthdata Cloud &amp; Python APIs Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-18T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>03:00</duration>
                <abstract>This workshop is built around guided computational analyses of two particular climate risk scenarios—wildfires &amp; floods. The goal is to support independent geospatial explorations using publicly available data products from [NASA Earthdata Cloud](https://www.earthdata.nasa.gov/) and FOSS with Pythonic APIs on a cloud computing environment.</abstract>
                <slug>foss4g-2025-4311-modelling-climate-risks-using-nasa-earthdata-cloud-python-apis-workshop</slug>
                <track>Workshop - Beginner Friendly</track>
                
                    
                <language>en</language>
                <description>Predicting and managing environmental risks of various climate-related disasters—e.g., wildfires, drought, and floods—is challenging and critical worldwide. Part of the difficulty is that historical norms (e.g., from last century) for the frequency of such extreme climate events are no longer sufficient to infer the frequency of future disasters. These natural risks are intrinsically linked to the dynamic distributions—varying both temporally and spatially—of surface water, precipitation, vegetation, and land use. These distributions can be modelled for forecasting and analysis (enabling quantification of these environmental risks) using hundreds of petabytes of relevant Earth science data available through [NASA's Earthdata Cloud](https://www.earthdata.nasa.gov/). With the dramatic growth in the availability of such data, today's earth scientists benefit from a strong understanding of open science practices and of cloud-based data intensive computing that enable reproducibly analyzing and assessing changing risk profiles.

This workshop provides hands-on examples of using cloud-based infrastructure and data products from [NASA Earthdata Cloud](https://www.earthdata.nasa.gov/) for the analysis of environmental risk scenarios. This involves constructing quantitative estimates of changes in hydrological water mass balance over various defined geographical regions of interest and time windows. The goal is to build enough familiarity with generic cloud-based Jupyer/Python workflows and with remote-sensing data to enable adapting and remixing examples for other region-specific contexts. The workshop's design reinforces best practices of data-proximate computing and of reproducibility (as supported by NASA's [Open Science](https://science.nasa.gov/open-science-overview) and [Transform to Open Science (TOPS)](https://nasa.github.io/Transform-to-Open-Science/) initiatives).

Participants are expected be familiar with raster data and common geospatial data conventions. Ideally, they are comfortable using a shell or a command-line interface to interact with data &amp; programs. They should also be comfortable using common scientific Python libraries (e.g., NumPy, Pandas) and related Python data structures (e.g., tuples, dicts, lists, NumPy arrays, Pandas dataframes). There is a brief overview of Xarray, Hvplot, &amp; Geoviews; prior exposure to those Python libraries is useful but not mandatory. Prior experience using Jupyter notebooks and writing short snippets of Python code is helpful.

**Approximate schedule**:

+ *minute 0-19*: Introduction &amp; Setup (logging in, configuring NASA Earthdata credentials)
+ *minute 20-29*: Reminders about GIS prerequisites: coordinate systems, data formats (if required)
+ *minute 30-49*: Overview of PyData tools for geographic data: [Rasterio](https://rasterio.readthedocs.io/en/stable/index.html) &amp; [Xarray](https://docs.xarray.dev/en/stable/index.html) (if required)
+ *minute 50-59*: Break
+ *minute 60-79*: Overview of PyData visualisation tools: [Hvplot](https://hvplot.holoviz.org/) &amp; [Geoviews](https://geoviews.org/) (if required)
+ *minute 80-99*: Using NASA Earthdata Products (DIST, DWSx)
+ *minute 100-109*: Using PyStac for retrieving data
+ *minute 110-119*: Break
+ *minute 120-144*: Case study: wildfires
+ *minute 145-169*: Case study: flooding
+ *minute 170-179*: Wrap-up

The workshop starts by getting participants logged into the cloud infrastructure and verifying their NASA Earthdata Cloud credentials. This is followed by a quick, non-comprehensive overview of GIS prerequisites and Python approaches to manipulating and visualizing geospatial data. The schedule above will be adapted to suit the audience needs (i.e., by increasing or decreading time allocated in each section as appropriate).

The hands-on case studies rely on the [OPERA (Observational Products for End-Users from Remote Sensing Analysis)](https://podaac.jpl.nasa.gov/OPERA) suite of data products; in particular, they use two particular categories of data products: [DSWx (Dynamic Surface Water Extent)](https://d2pn8kiwq2w21t.cloudfront.net/documents/ProductSpec_DSWX_URS309746.pdf) and [DIST (Land Surface Disturbance)](https://lpdaac.usgs.gov/documents/1766/OPERA_DIST_HLS_Product_Specification_V1.pdf). The workflows presented extend notebook examples drawn from the extensive [OPERA Applications repository](https://github.com/OPERA-Cal-Val/OPERA_Applications).

This workshop—co-developed by [MetaDocencia](https://www.metadocencia.org) &amp; [2i2c](https://2i2c.org)—is part of NASA's [Open Science](https://science.nasa.gov/open-science-overview) and [Transform to Open Science (TOPS)](https://nasa.github.io/Transform-to-Open-Science/) initiatives. An important goal is to reinforce principles of reproducibility and open science-based workflows (as exemplified in TOPS OpenCore, the introductory suite of open science curricula including Open Science 101).</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/QTAWL7/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/QTAWL7/feedback/</feedback_url>
            </event></room><room name="WF511" guid="134d8686-2e43-5f63-a6ea-0cec9bd63759"><event guid="21da1da1-7a83-5fd2-9edd-10d82042f5e0" id="4338">
                <room>WF511</room>
                <title>Exploring Cloud-Native Geospatial Formats: Hands-on with Vector Data Workshop</title>
                <subtitle />
                <type>Workshop</type>
                <date>2025-11-18T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>03:00</duration>
                <abstract>Dig into geospatial vector formats—including GeoJSON, WKT/WKB, and cloud-native GeoParquet—using Python to see in detail how vector features are stored in each format and to understand what cloud-native means for vector data.</abstract>
                <slug>foss4g-2025-4338-exploring-cloud-native-geospatial-formats-hands-on-with-vector-data-workshop</slug>
                <track>Workshop - Advanced Level</track>
                
                    
                <language>en</language>
                <description>Cloud-native geospatial is all the rage these days, and for good reason. As file sizes grow, layer counts increase, and analytical methods become more complex, the traditional download-to-the-desktop approach is quickly becoming untenable for many applications. It's no surprise then that users are turning to cloud-based tools to scale out their analyses, or that traditional tooling is adopting new ways of finding and accessing data from cloud-based sources. But as we transition away from opening whole files to now grabbing ranges of bytes off remote servers it seems all the more important to understand exactly how cloud-native data formats actually store data and what tools are doing to access it.

This workshop aims to dig into how cloud-native geospatial data formats are enabling new operational paradigms, with a particular focus on vector data formats. Unlike its raster workshop counterpart, this workshop will be a bit more experimental. Vector data formats tend towards greater complexity than raster formats, so exactly how deep we get into which topics will be dependent on the audience’s interests and the time available. Broad themes to explore might include:

* GeoJSON: what is it, what does it represent, and how it is not cloud-native
* Well-Known Text/Binary (WKT/WKB): how these vector formats work and why they are important in GeoParquet
* GeoParquet: how does parquet store data, how geo maps into that paradigm, and what it takes to read some subset of data from a parquet file
* FlatGeoBuff: what is is, how it works, why it might be “more” cloud-native than GeoParquet
* Practical considerations when using these formats

The content of this workshop aims to not only be theoretical: a strong goal is to be as hands-on with these formats as possible by working with them in Python without any specific geospatial format libraries. We’ll look at interacting with object storage directly, to pull down files and fragments and inspect them, to build up working understanding of what common higher-level tooling does under the hood and abstracts away from users.

#### Prerequisites

This workshop expects some familiarity with geospatial programming in Python and a basic understanding of the vector data model and its utility. Most of the notebook code is already provided, so any gaps in understanding don't necessarily prohibit completing the exercises. That said, some knowledge of the geospatial vector formats and tooling is quite helpful.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/MHHJE7/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/MHHJE7/feedback/</feedback_url>
            </event></room></day><day index="3" date="2025-11-19" start="2025-11-19T04:00:00+13:00" end="2025-11-20T03:59:00+13:00"><room name="WG403" guid="a2eb48cd-245e-5328-a7d6-7eaa52403bf2"><event guid="77ed0ff7-3792-5b53-b659-b3f72eea30eb" id="4211">
                <room>WG403</room>
                <title>IGEO7 and DGGRID - Like H3, but an Equal-Area Hexagonal DGGS for Fairer Global Analysis</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-19T16:00:00+13:00</date>
                <start>16:00</start>
                <duration>00:05</duration>
                <abstract>Uber H3 has revolutionized spatial indexing, but its cell sizes vary a lot. I will introduce IGEO7, an aperture 7 hexagonal equal-area DGGS with its hierarchical indexing system Z7. It is now implemented in the open-source software  DGGRID and has a handy Python wrapper, dggrid4py.</abstract>
                <slug>foss4g-2025-4211-igeo7-and-dggrid-like-h3-but-an-equal-area-hexagonal-dggs-for-fairer-global-analysis</slug>
                <track>Lightning talk</track>
                <logo>/media/foss4g-2025/submissions/PQJVKK/plot3_pentagon_noquad_AgPqNPc.png</logo>
                    
                <language>en</language>
                <description>Uber's H3 has revolutionized spatial indexing, but its cells aren't equal-area, skewing global analyses and visualizations. What if you could have H3's elegant hierarchical indexing and true equal-area cells for statistically sound results?

IGEO7 is a pure aperture 7 hexagonal DGGS with a hierarchical indexing system named Z7. It is implemented in the long-standing open-source DGGRID software and has a handy Python wrapper, dggrid4py.

- https://dggrid.readthedocs.io/
- https://github.com/sahrk/DGGRID
- https://dggrid4py.readthedocs.io/
- https://github.com/allixender/dggrid4py

In this talk, I'll shortly introduce IGEO7's capabilities and show how to use it with DGGRID and dggrid4py. Come see how you no longer have to choose between handy hexagon indexing and statistically sound analysis with open-source world.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/PQJVKK/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/PQJVKK/feedback/</feedback_url>
            </event><event guid="05e8167e-b0e2-55de-b266-85d060541f4a" id="4394">
                <room>WG403</room>
                <title>Portable CQL2: A Rust Core for Queries Everywhere</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-19T16:05:00+13:00</date>
                <start>16:05</start>
                <duration>00:05</duration>
                <abstract>Explore how [**cql2-rs**](https://developmentseed.org/cql2-rs/), a compact Rust library for working with OGC CQL2 expressions, powers validation, conversion, and evaluation across environments — from Rust to CLI, Python, and WebAssembly — and how writing small, reusable Rust utilities can extend impact across languages and platforms.</abstract>
                <slug>foss4g-2025-4394-portable-cql2-a-rust-core-for-queries-everywhere</slug>
                <track>Lightning talk</track>
                
                    
                <language>en</language>
                <description>[CQL2](https://www.ogc.org/standards/cql2/) is a powerful and flexible query language designed by the OGC to support advanced filtering and subsetting of geospatial data, particularly for features and records. It enables expression of complex spatial, temporal, and logical conditions in both text and JSON forms.

[**cql2-rs**](https://developmentseed.org/cql2-rs/) is a Rust library built to work with CQL2 expressions. It provides tools to validate expressions, convert between text and JSON formats, combine multiple expressions, simplify logical trees, and evaluate expressions against JSON input. It is intended as a lightweight and reusable core for working with CQL2 in any setting.

Because it’s written in Rust, `cql2-rs` can be exposed across many platforms and runtimes. Along with publishing a Rust crate for usage within the Rust ecosystem, we've published a [Python module](http://developmentseed.org/cql2-rs/latest/python/) via PyO3, a [command-line interface](https://developmentseed.org/cql2-rs/latest/cli/), and a [browser-based playground](https://developmentseed.org/cql2-rs/latest/playground/) using WebAssembly; all built from the same codebase.

This talk will highlight both the functionality of the library and the philosophy behind it: building small, focused Rust utilities that embrace composability and portability. Whether you're scripting, building APIs, or creating interactive tools, `cql2-rs` demonstrates how a single Rust core can power diverse workflows and tools.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/MM3UUH/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/MM3UUH/feedback/</feedback_url>
            </event></room><room name="WA220" guid="132a9daa-6bcb-593a-b15d-b8ddf7d1d99c"><event guid="a8e0e5fd-664f-5bc1-a2a6-47838199bb41" id="4327">
                <room>WA220</room>
                <title>OSGeo tool stack in a container for a team of geospatial analysts/engineers in 2025</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-19T14:00:00+13:00</date>
                <start>14:00</start>
                <duration>00:25</duration>
                <abstract>We put together the most popular opensource GIS dataset in Australia. We've been working on a standardised OSGeo-toolstack for a team of geospatial analysts and engineers to deploy fast. We're here to share some use cases as well as this journey and some of its challenges.</abstract>
                <slug>foss4g-2025-4327-osgeo-tool-stack-in-a-container-for-a-team-of-geospatial-analysts-engineers-in-2025</slug>
                <track>Use Cases and Applications</track>
                
                    
                <language>en</language>
                <description>We intensively use a broad range of OSGeo tooling to put together the original source of truth location data for Australia that, among other things, is the most downloaded dataset provided by the Australian government from `data.gov.au`.

Our team takes things like SRIDs very seriously and we work hard to ensure our data is very accurate. Additionally we use these great GIS tools to construct many other data use cases such as Australian roads, buildings, trees, indigenous lands, disaster response and climate reporting information. We have encountered some fascinating problems that these tools help us solve.

We're constantly working to be more efficient and to better scale, specifically we have been working on "Standard Operating Environments" that are expected to be fast to deploy, work off the shelf and stay up to date. These are used by a large team of geospatial analysts and engineers. For our relatively mature use cases our development stack uses Python + PostGIS, GDAL, PROJ, Shapely, Fiona, GeoPandas, jupyter and much more. Making a development environment is a well trodden path, this should have been easy right. Right?

This talk will briefly touch on some cool uses we have for these tools; outline how we're working to make our processes better and will discuss some of the challenges we encountered in doing this work, in the hopes that others may avoid them.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/9KSQVT/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/9KSQVT/feedback/</feedback_url>
            </event><event guid="2d9f6650-816f-5430-814f-0c7fe5a22985" id="4347">
                <room>WA220</room>
                <title>1% AEP Current and Future Climate Flood Maps for Aotearoa New Zealand</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-19T16:30:00+13:00</date>
                <start>16:30</start>
                <duration>00:25</duration>
                <abstract>Flooding is one of the costliest hazards facing Aotearoa. We present a methodology for creating nationally consistent flood-maps for a range of current and future climate scenarios developed by the Endeavour Mā te haumaru ō te wai: flood resilience Aotearoa. The maps are shared in an open-data repository.</abstract>
                <slug>foss4g-2025-4347-1-aep-current-and-future-climate-flood-maps-for-aotearoa-new-zealand</slug>
                <track>Use Cases and Applications</track>
                <logo>/media/foss4g-2025/submissions/LHCTKU/Abstract_Figure_1B3pjiC.png</logo>
                    
                <language>en</language>
                <description>Introduction
Flood inundation modelling and the resultant flood maps are essential for understanding, planning for, and responding to flood events. Flooding is one of the most costly and impactful hazards facing Aotearoa, and the frequency and severity of flooding is projected to rise under a warmer future climate. 

Historically in Aotearoa, flood hazard products have been produced at the local and regional levels using locally defined methodologies leaving Aotearoa without nationally consistent flood map products available for nation-wide hazard, risk and future climate analyses. This is the aim of the Endeavour project Mā te haumaru ō te wai: flood resilience Aotearoa. 

Mā te haumaru ō te wai aims to adhere to the principles of open science and access. Our methodology uses open-source software where possible, while also contributing directly to the development of two open-source software projects: BG_Flood and GeoFabrics. Additionally, where possible we use open data sources as inputs into the modelling methodology. Finally, the current and future climate scenario flood maps are published in an open-access data repository. 

We present the automated cascaded modelling methodology used to produce nationally consistent fluvial and pluvial flood inundation maps by integrating climate science, rainfall statistics, hydrology, hydrodynamics and geospatial data for 256 flood domains across Aotearoa New Zealand. We apply this methodology to produce four current and future climate scenarios at a 1% Annual Exceedance Probability (AEP) for current, 1°, 2° and 3°C warmer than current conditions and present a summary of these results. 

Method
We created a cascaded modelling methodology to produce nationally consistent flood maps (Figure 1) consisting of five major stages: flood domain definition, topography and roughness generation, storm generation, hydrology modelling, and hydrodynamic modelling. The workflow was fully automated using Cylc [1], an open-source workflow engine, to control the progress between different stages. This allowed the workflow to be run across all flood domains for the current and future climate scenarios in an automated fashion.  

Our modelling methodology begins with the definition of floodplains and associated catchments (Figure A.1.) where we perform our coupled hydrology (Figure A.4.) and hydrodynamic (Figure A.5.) modelling. For each catchment, a design rainfall event is created in the storm generation stage (Figure A.3) which forms another key input to both the hydrology and hydrodynamic modelling stages. The topography and roughness stage (Figure A.2) produces hydrologically conditioned Digital Elevation Models (DEMs) and hydraulic roughness layers across Aotearoa New Zealand which form third key input to the hydrodynamic modelling stage. A flood inundation map showing the maximal flood depths across the current climate scenario is shown for an example domain, the West Coast Fox River (Figure B). 

The catchments and associated floodplains are defined from a set of basic manual outlines, a NZ wide river network, and nationwide population and building information. The manual outlines roughly indicate each floodplain in the country act to ensure appropriate groupings of nearby river courses. The floodplains are defined using a process to propagate up from the river mouth(s) (or downstream reach for inland catchments) to define relatively flat populated areas with available LiDAR where higher detail hydrodynamic modelling is undertaken. During this stage, river injection points are defined at the intersection between the flood plains and the river network; they are used to couple the hydrology model used in the upper catchment with the hydrodynamic model used over the floodplain in the lower catchment. 

We use the Aotearoa-specific High Intensity Rainfall Design System (HIRDS) [2] to generate our rainfall events. HIRDS is an open tool (https://hirds.niwa.co.nz/) for generating rainfall estimates for a specified AEP and duration at any location across New Zealand where the rainfall estimates are derived from historic rainfall observations as well as other climatic and topographical information.  

All hydrology modelling in our workflow is performed using the Aotearoa New Zealand TopNet model [3]. We hydrologically model rainfall events with durations between 6 and 72hrs to experimentally determine a realistic worst case storm duration for each catchment. In each catchment, the selected worst-case duration 1% AEP rainfall event was used to force the coupled hydrology and hydrodynamic model.  

Hydrodynamic modelling was performed using BG_Flood [4] an open-source software (OSS) GPU-enabled adaptive resolution shallow-water solver that supports rain-on-grid. BG_Flood was actively developed as part of this project. The TopNet river flows from the upper catchment are injected into the hydrodynamic model around the edge of the floodplain. The rainfall event over the floodplain is also included directly to the hydrodynamic model as rain-on-grid. The NZ tide model, with open access through an online tool (https://tides.niwa.co.nz/), was used to provide tidal forcings (mean high water spring tide) around the coast.  

The hydrodynamic modelling also requires a hydrologically conditioned DEM and hydraulic roughness. These were produced using the OSS Python package GeoFabrics [5], which was also developed for this project. GeoFabrics is a Dask enabled Python tool for creating hydro DEMs and hydraulic roughness from LiDAR point clouds, other elevation, natural feature and infrastructure information. This was included within the workflow so that the topography and roughness information could be updated as LiDAR coverage increased across New Zealand from 20% at the project inception to 80% today. 

Results
We developed our workflow with a focus on iterative improvement. As such, we performed our first nationwide run concluding June 2024. These results were limited to 1% AEP at current climate and an 8m resolution. These were reviewed to identify key areas of improvement. Specifically, we identified: more realistic storm durations, inclusion of inland floodplains, inclusion of lakes and 150 missing culverts, the opening of more than 100 river mouths, and modelling to a resolution of 4m. 

We have completed our second nationwide run to a resolution of 4m concluding June 2025 across four scenarios: 1% AEP at current, 1°, 2°, and 3°C warmer future climate. In our presentation we will cover several catchments in detail and share summary results comparing the current and future climate scenarios.  We will also share the open-data repository where the flood inundation maps across each catchment and scenario can be accessed. Finally, we will highlight how these products can be used to access impact through risk modelling in future studies.  



References
[1] Oliver et al., (2018). Cylc: A Workflow Engine for Cycling Systems. Journal of Open Source Software, 3(27), 737, https://doi.org/10.21105/joss.00737  

[2] Carey-Smith, T., et al., (2018) High Intensity Rainfall Design System Version 4, NIWA Client Report 2018022CH prepared for Envirolink, retrieved from https://niwa.co.nz/climate-and-weather/hirdsv4-usage 

[3] Bandaragoda, C., et al., (2004). Application of TOPNET in the distributed model intercomparison project. Journal of Hydrology. https://doi.org/10.1016/j.jhydrol.2004.03.038  

[4] Bosserelle, C., et al., (2022), BG-Flood: A GPU adaptive, open-source, general inundation hazard model; Australiasian Coasts and Ports 2021. https://github.com/CyprienBosserelle/BG_Flood. 

[5] Pearson, R et al., 2023, Geofabrics 1.0.0: An Open-Source Python Package for Automatic Hydrological Conditioning of Digital Elevation Models for Flood Modelling. Environmental Modelling and Software. http://dx.doi.org/10.2139/ssrn.4463610</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/LHCTKU/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/LHCTKU/feedback/</feedback_url>
            </event></room><room name="WG404" guid="8fad97e8-15a2-5185-b1c8-68e7d8beb155"><event guid="1d8ed0d9-9bd4-5e07-be26-5b0a108fab5f" id="4328">
                <room>WG404</room>
                <title>Development of open-source digital twins for automated analysis of flood risk</title>
                <subtitle />
                <type>Academic Paper</type>
                <date>2025-11-19T12:00:00+13:00</date>
                <start>12:00</start>
                <duration>00:25</duration>
                <abstract>Digital Twins address the fundamental challenges created by large volumes of geospatial data by enabling automated, near-real time processing and analysis, reducing the gap between data and the insights needed for decision-making. Here we present our open-source digital twin software framework, demonstrated through automated flood risk assessment.</abstract>
                <slug>foss4g-2025-4328-development-of-open-source-digital-twins-for-automated-analysis-of-flood-risk</slug>
                <track>Academic</track>
                <logo>/media/foss4g-2025/submissions/XKFCXZ/fredt_mkMKPrN.png</logo>
                    
                <language>en</language>
                <description>Digital Twins are dynamic virtual representations of physical systems, with automated data exchange and analytics being key attributes; they are enabling the development of smart cities and may also represent the natural environment. For example, over the next several years the EU's “Destination Earth” system is being developed as a Digital Twin for climate services, to facilitate access to weather and climate models which can be used for impact studies. Our research is developing scalable open-source environmental Digital Twin technology, applicable to diverse geospatial applications. We have applied this to address the complex geospatial analysis required for compound flood risk assessment (fluvial, pluvial and coastal). 
Our system enables automated scenarios for mitigation and adaptation, particularly those for natural flood management, while accounting for climate change; once complete, our system will be able to ingest weather and climate model data from the Destination Earth system, further enabling scalability. Through our close engagement with our indigenous partners, we are ensuring that mātauranga Māori (indigenous knowledge) is embedded within the system and scenarios developed. This will help to ensure that flood mitigation measures developed recognise Māori values and practices, as needed under the United Nations’ Declaration on the Rights of Indigenous Peoples. 
Here, we present our fully open-source Digital Twin software framework including the core Environmental Digital Data Intelligence Engine (EDDIE), a data management system which is generically applicable to multiple use cases, and a module which interfaces with this for flood risk assessment, the Flood Resilience Digital Twin (FReDT). This framework provides the generic base for data ingestion, model configuration, and data visualisation that can be applied to many distinct use cases through extensions. 
In the implementation presented here, FReDT interacts with EDDIE to automatically produce, run, ingest, analyse and visualise outputs from flood model simulations. Users can interact with FReDT by using a 3D geospatial web application based on the open-source geospatial library TerriaJS for control and visualisation. Alternatively, users can interact using an API via Open Geospatial Consortium standards such as WPS for starting models and WFS or WMS for retrieving data to use with existing tooling such as GIS software.
To begin processing a flood model scenario, users of FReDT must specify an area of interest and provide configuration parameters for the flood model, such as choosing the projected year for a climate scenario. Once the backend receives a processing request, it begins a Python Celery worker task to model the flood extents and depths and assess impact. Open data sources such as LiDAR terrain datasets, sea level rise predictions, river network shapes and statistics, and more are downloaded, processed and saved to database. If the same data is later required for another scenario run, then can be retrieved from the database, or it can be flagged for update and new data will be retrieved when required. Dynamic model forcing data (rainfall, river flows, tide levels) are currently produced statistically from open-data sources, and the system will be extended to include live observations and forecasts, predicted streamflow from a hydrological model, as well as user-provided scenarios.
Once the data is retrieved and processed it is passed through to the flood inundation numerical modelling software BG-Flood. Outputs from BG-Flood are passed through to the user and are also passed through to post-modelling analysis stages such as cross-referencing with building footprint polygon datasets to predict which buildings may be inundated for the scenario assessed. These data are presented to the users of the web application as geospatial layers on a 3D map, with a time slider to explore the depth of flooding over the course of the event, a comparison slider for viewing multiple scenarios side-by-side, and the ability to query the features such as individual buildings and the depth of a location to see a plot of depth over time.
The software system uses multiple containers to provide backend, frontend, and database services. The processing chains required for running flood models in new areas can take hours or more, especially for large domains. Currently work is under development to allow running the digital twin software on AWS cloud Elastic Container Services, which will allow for processing nodes to expand resources during periods of high demand and reduce outside of those times.
Physics-based Digital Twins such as FReDT will revolutionise access to and use of numerical model predictions, through a “digital twin web” powered by rapidly growing data and distributed cloud computing. Yet individual components need to be built and tested to ensure they are fit-for-purpose, democratic and adaptive to society’s needs. Our research is enabling this, initially in Aotearoa New Zealand but with global applicability. Building on the existing FReDT codebase, our current research is adding a hydrological model to allow upper river catchment changes to be accounted for in downstream flood risk and enable land management scenarios such as reforestation. Our aim is to facilitate rapid, low-cost risk assessments with on-demand scenario analytics, and effective ways to visualise and communicate this risk and its associated uncertainties, with communities placed at the centre by enabling them to participate in the design of solutions for flood mitigation and adaptation.
GitHub repository: https://github.com/GeospatialResearch/Digital-Twins</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/XKFCXZ/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/XKFCXZ/feedback/</feedback_url>
            </event><event guid="b8fa3de7-c0ce-558e-8bda-216d3a50fbc8" id="4407">
                <room>WG404</room>
                <title>Did you get that thing I sent you? Simplifying spatial data in Python</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-19T15:30:00+13:00</date>
                <start>15:30</start>
                <duration>00:25</duration>
                <abstract>Distributing spatial data is a major challenge. This talk walks-through different strategies you can take to simplify your spatial data for distribution in Python, and under what circumstances you might make each choice, including: reducing precision; filling holes; Ramer-Douglas-Peucker and friends; and raster and vector conversion.</abstract>
                <slug>foss4g-2025-4407-did-you-get-that-thing-i-sent-you-simplifying-spatial-data-in-python</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                
                    
                <language>en</language>
                <description>To quote Stewart Brand: On the one hand information wants to be expensive, because it’s so valuable. The right information in the right place just changes your life. On the other hand, information wants to be free, because the cost of getting it out is getting lower and lower all the time.

Regardless of which hand you hold, making sure you can deliver data efficiently and quickly is important. Spatial data is not special - it's just more data - but it poses major challenges in distribution for a number of reasons: competing file formats; multiple data types (vector and raster is just the beginning); and the most common issue: the quantity of data. This talk walks through different strategies for simplifying your data, why you might want to (and why you might not), and what choice you might make to perform the simplification using Python.

Topics covered include:

- reducing precision and delivering data suitable to scale;
- filling holes and removing noise;
- the Ramer-Douglas-Peucker algorithm and other common simplification algorithms;
- and, when to use raster vs vector data for distributing information.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/DZELVK/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/DZELVK/feedback/</feedback_url>
            </event><event guid="9d3a14dd-f0cf-52b4-b45d-1d253d533751" id="4346">
                <room>WG404</room>
                <title>Weavingspace: a new way to make multivariate maps</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-19T16:30:00+13:00</date>
                <start>16:30</start>
                <duration>00:25</duration>
                <abstract>The weavingspace python module enables creation of thematic maps using periodic tilings to produce attractive maps that weave together many data attributes in a single map display.</abstract>
                <slug>foss4g-2025-4346-weavingspace-a-new-way-to-make-multivariate-maps</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                <logo>/media/foss4g-2025/submissions/JGH8PK/mapweaver-app_0EdaLdF.png</logo>
                    
                <language>en</language>
                <description>The weavingspace python module enables creation of multivariate thematic maps by generating and overlaying a periodic tiling layer with polygon data. Geometries in the tiling symbolise data by choropleth colouring. Well over one hundred tiled patterns are supported, including a highly configurable tilings that give the appearance of biaxial or triaxial woven materials. Complex tilings with as many as twenty distinct elements yield maps with textures that convey combinations of attribute values as textures, while maps with up to around eight elements make it possible to read maps with that number of variables simultaneously. In this presentation I will discuss the motivation for this work, challenges in implementation, and also present a web app that allows code-free creation of such maps.</description>
                <recording>
                    <license />
                    <optout>true</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/JGH8PK/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/JGH8PK/feedback/</feedback_url>
            </event></room><room name="WG607" guid="4ae40321-2591-5002-82a8-0b12958015bc"><event guid="48953df9-cfc6-5467-b5b4-3d74e4caf660" id="4212">
                <room>WG607</room>
                <title>Towards universal building blocks for cloud-native digital-twins</title>
                <subtitle />
                <type>Academic Paper</type>
                <date>2025-11-19T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>00:25</duration>
                <abstract>We present a scalable, interoperable, and extensible FOSS architecture for modern geospatial data ecosystem, based on DGGS. Exemplary, we introduce pydggsapi, a Python server implementing the new OGC DGGS API, that can serve large geospatial datasets from cloud-native Zarr and Parquet data stores indexed by a DGGS.</abstract>
                <slug>foss4g-2025-4212-towards-universal-building-blocks-for-cloud-native-digital-twins</slug>
                <track>Academic</track>
                <logo>/media/foss4g-2025/submissions/FAF9BC/arch1_WMYxrWi.png</logo>
                    
                <language>en</language>
                <description>The exponential growth of Earth Observation (EO) data challenges our ability to efficiently access, process, and analyze it. Conventional web service standards like WCS and WMS have long provided standardized access, but there are challenges and work-arounds associated with the scale and complexity of global datasets, including coordinate system handling. In parallel, data cubes have become valued abstractions to analyse large-scale geospatial data over space and time. However, data cubes currently can only be implemented meaningfully in projected coordinate reference systems, which limits their extent before introducing too large areal distortions.

A new paradigm is emerging, built on modern data structures, formats, and APIs. This paper introduces an architecture that integrates these innovations into a universal building block for geospatial data management. The foundation of this approach is the Discrete Global Grid System (DGGS). A DGGS offers a unified spatial reference framework, partitioning the Earth's surface into a hierarchy of cells. Equal-area DGGS like ISEA or HEALPix are particularly valuable for applications in fields like catchment hydrology and land use analysis, as they ensure statistical validity by maintaining consistent cell areas across the globe. The Open Geospatial Consortium (OGC) has formalized this paradigm through its DGGS Abstract Specification and the recently finalized the OGC API - DGGS standard, which specifies a lightweight web service for accessing DGGS-organized data.

In parallel, the scientific Python ecosystem has revolutionized data handling with tools like Xarray for labeled multi-dimensional arrays and its XDGGS extension for native DGGS operations. This analytical power is maximized when paired with cloud-native storage formats like Zarr. Inspired by pioneering FOSS projects like pygeoapi, TiTiler, or XPublish, our work seeks to bridge the gap between these powerful analytical backends and standardized, web-friendly access patterns.

We present pydggsapi, an open-source Python server built with FastAPI that implements the OGC DGGS API standard. It exposes cloud-optimized Analysis-Ready Data (ARD), such as Zarr archives or Parquet files, where data is indexed by high-resolution DGGS cells. This architecture creates a seamless continuum between two distinct operational scales. On one end, data scientists can perform large-scale modeling by directly accessing the DGGS-indexed Zarr archives in object storage using Xarray. On the other, lightweight web and mobile clients can consume the same underlying data through the standardized, RESTful pydggsapi interface, which is documented via a built-in OpenAPI/Swagger UI.

We present a conceptual framework that links data access middleware via OGC API DGGS and direct data access via cloud storage. On initialization, it connects to cloud-storage Zarr archives and extracts DGGS parameters - such as the grid type, indexing scheme, available refinement levels and variables. To optimize performance for clients, we also leverage a concept similar to classic image pyramids or overviews in Cloud-Optimized GeoTIFFs - pre-aggregating data to several DGGS refinement levels. These pyramids are efficiently can be represented using Zarr groups and the Xarray DataTree model, or in Parquet using partitioning. For light-weight interactive visualization, we implement a tiles endpoint serving Mapbox Vector Tiles (MVT) on-the-fly. This enables highly efficient in-browser rendering with WebGL libraries like MapLibre GL JS. Alternatively, DGGS-aware clients can access the DGGS API endpoints directly and request data formats like DGGS-JSON. The architecture is also extensible, featuring a connector for the ClickHouse database to serve fast, on-demand analytical queries on DGGS-indexed tables, as explored in the OGC Testbed-16 report.

This overall architecture enables advanced analysis in geomorphology, land cover change, or hydrology, by accessing the full data via extensible compute frameworks like Xarray or even Apache Spark directly from cloud-based object storage. For instance, a researcher studying catchment-scale erosion can query pydggsapi for all land cover and slope data DGGS cells within their basin. The API backend accesses a massive, continental-scale Zarr dataset, and only extracts the required data. Alternatively, the user can execute an Xarray batch job computes the necessary statistics, and returns a comprehensive analysis result. In addition, user can easily coalesce data from other DGGS enabled data sources via simple cell-id based joins. This allows for a much simpler data federation.

While initial benchmarks are promising, the implementation has limitations that guide future work. Xarray’s indexing of dimensions can consume significant memory with very large DGGS archives, when trying to load the full array of all DGGS cell ids. Here we compare the access with Parquet, which always gets scanned on-demand, and explore more direct Zarr-native access patterns and DGGS cell index representations. Also, the strict requirements of the OGC DGGS API, such as subzone ordering, also present implementation challenges for certain DGGS types, highlighting ongoing needs in the FOSS DGGS software landscape.

The integration of a FOSS OGC DGGS API implementation with cloud-native Zarr storage represents a significant step toward universal building blocks for Earth Observation. This approach offers a powerful, dual-access pattern that serves both high-performance computing and lightweight clients from a single, consistent data foundation. In addition, joining various data variables from different data providers will be trivial based on the DGGS cell ids. We believe this model also charts a path toward a new generation of value-added, GeoAI-ready data market APIs.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/FAF9BC/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/FAF9BC/feedback/</feedback_url>
            </event><event guid="6dfa04ea-8789-5ee0-977c-c9aa34d617cd" id="4186">
                <room>WG607</room>
                <title>(Re)Making Cirrus: Five Years Building a Data Orchestration Framework</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-19T16:00:00+13:00</date>
                <start>16:00</start>
                <duration>00:25</duration>
                <abstract>A retrospective on building cirrus, a cloud-native framework for building STAC-based data orchestration pipelines. We'll look at the design and architecture evolution over five years of development and some lessons learned adapting to ecosystem and requirement changes.</abstract>
                <slug>foss4g-2025-4186--re-making-cirrus-five-years-building-a-data-orchestration-framework</slug>
                <track>Cloud, APIs &amp; Data Infrastructure</track>
                
                    
                <language>en</language>
                <description>Cirrus is an open source, cloud-native framework for orchestrating geospatial data pipelines built using the concept of STAC (SpatioTemporal Asset Catalog) workflows. It provides a flexible and modular approach to deploying and managing serverless pipelines in AWS via python components and a Terraform-based deployment mechanism. Cirrus enables scalable, repeatable data processing workflows in the cloud, and is designed to help teams transform, validate, and catalog geospatial data in STAC-compliant formats at scales both large and small.

Over the past five years, cirrus has evolved from a directory of loosely-organized bits of configuration and components built on top of the Serverless Framework to a robust, open source cloud-native data pipeline management system. In this talk, I’ll share my journey maintaining and evolving cirrus from what I inherited to its current state, and lessons I’ve learned along the way.

Together we’ll explore cirrus’ origins and the original architecture and its challenges. We’ll examine the decision to shift away from duplicating deployment code via the configuration merge system of the first cirrus CLI, and what benefits and pitfalls that brought along with it. We’ll trace some of the tooling and ideas that spun out along the way, like stac-task and swoop. Finally, we’ll look at the version 1.0 release’s move away from Serverless Framework and the decoupling of the deployment logic from the codebase, the new cirrus Terraform module, and how this 1.0 release has prompted the reconsideration of what actually constitutes cirrus now.

Whether you're maintaining your own internal tooling, building cloud-native data processing pipelines, or just trying to keep an open source project healthy through shifting technical landscapes, this talk will offer practical insights drawn from real-world experience. We'll cover the technical decisions, tradeoffs, and lessons learned—especially those relevant to anyone maintaining cloud-native tooling in a fast-moving landscape.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/KBYFFX/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/KBYFFX/feedback/</feedback_url>
            </event></room></day><day index="4" date="2025-11-20" start="2025-11-20T04:00:00+13:00" end="2025-11-21T03:59:00+13:00"><room name="WG403" guid="a2eb48cd-245e-5328-a7d6-7eaa52403bf2"><event guid="28fed200-c7b6-5daf-849e-4bac47406381" id="4375">
                <room>WG403</room>
                <title>Evaluating LLMs as Intermediaries for FOSS4G CLI-based Geospatial Analysis</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-20T09:05:00+13:00</date>
                <start>09:05</start>
                <duration>00:05</duration>
                <abstract>This presentation investigates whether Large Language Models (LLMs) possess adequate knowledge to function as effective intermediaries between non-expert users and FOSS4G CLI tools. We assess the capability of current LLMs to correctly respond to queries requiring geospatial domain-specific knowledge and generate appropriate solutions for spatial analysis tasks.</abstract>
                <slug>foss4g-2025-4375-evaluating-llms-as-intermediaries-for-foss4g-cli-based-geospatial-analysis</slug>
                <track>Lightning talk</track>
                
                    
                <language>en</language>
                <description>The complexity of CLI-based geospatial analysis tools presents a significant barrier to widespread adoption of FOSS4G technologies. While tools like GDAL, PDAL, and Python geospatial libraries offer powerful capabilities, their command-line interfaces require substantial technical expertise. This limits effective utilization to technical specialists, despite FOSS4G's promise of democratizing geospatial analysis.
Recent advances in Large Language Models (LLMs) suggest potential for bridging this technical gap. LLMs could theoretically interpret natural language requests and generate appropriate CLI commands, making these tools accessible to domain experts who possess valuable geospatial knowledge but lack programming backgrounds.
However, effective geospatial analysis requires understanding of domain-specific concepts such as coordinate reference systems, data formats, and regional standards. Our preliminary investigations reveal that current LLMs often fail to correctly handle country-specific geospatial information. 
This presentation evaluates whether existing LLMs possess sufficient geospatial domain knowledge to serve as reliable intermediaries. We examine their performance and evaluate specific knowledge gaps that prevent LLMs from effectively facilitating FOSS4G CLI tool usage for non-technical users.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/G79YEK/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/G79YEK/feedback/</feedback_url>
            </event><event guid="e8df4bd7-29c9-5ea9-b6c6-43273750bfaa" id="4343">
                <room>WG403</room>
                <title>Tree shadow modelling in QGIS + GRASS</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-20T09:25:00+13:00</date>
                <start>09:25</start>
                <duration>00:05</duration>
                <abstract>Using QGIS, GRASS and Python along with LINZ Dataservice to calculate shadow impact of trees 50 years in the future.</abstract>
                <slug>foss4g-2025-4343-tree-shadow-modelling-in-qgis-grass</slug>
                <track>Lightning talk</track>
                <logo>/media/foss4g-2025/submissions/CYL8GX/ShadowAnalysis_mkHCt5w.png</logo>
                    
                <language>en</language>
                <description>Using QGIS, GRASS and Python along with LINZ Dataservice to calculate shadow impact of trees 50 years in the future, Tim from New Zealand Carbon Farming briefly outlines their approach to maintain compliance with NZ National Environmental Standards for commercial forestry: What we tried; what worked and what didn't.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/CYL8GX/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/CYL8GX/feedback/</feedback_url>
            </event><event guid="d248a162-c9cf-5a5b-aaf5-3d0149a8e640" id="4242">
                <room>WG403</room>
                <title>The Open Data Cube is Dead, Long Live the Open Data Cube</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-20T09:30:00+13:00</date>
                <start>09:30</start>
                <duration>00:05</duration>
                <abstract>The Open Data Cube is a big, complicated tool that does a lot of things, and it's a little old. It's been replaced with a small complicated tool, which does less things more simply, which is great. This is a talk about that.</abstract>
                <slug>foss4g-2025-4242-the-open-data-cube-is-dead-long-live-the-open-data-cube</slug>
                <track>Lightning talk</track>
                <logo>/media/foss4g-2025/submissions/CRKRSF/Screenshot_2025-03-31_at_2.08.44pm_mQcJkXo.png</logo>
                    
                <language>en</language>
                <description>For years, the Open Data Cube has been the go-to framework for managing massive collections of satellite imagery. But with changing cloud-native practices, new open standards, and better APIs, there’s a simpler way forward.

In this talk, Alex Leith, Executive Director at Auspatious, explains how the heavy Postgres-driven ODC tool is being replaced by flexible, open STAC APIs and elegant Python tools like pystac-client and odc-stac.

The result? A leaner, easier way to discover and analyze EO data, with just a few lines of code. Long live the Open Data Cube.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/CRKRSF/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/CRKRSF/feedback/</feedback_url>
            </event><event guid="36b4b06f-c38f-58d6-aad9-6ad562f6c394" id="4371">
                <room>WG403</room>
                <title>Finding the Farthest Point: Implementing Longest Path Analysis in QGIS with NetworkX and Python</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-20T09:45:00+13:00</date>
                <start>09:45</start>
                <duration>00:05</duration>
                <abstract>I developed a Python-based solution that extends QGIS routing capabilities by calculating longest paths in road networks using NetworkX and modified Dijkstra's algorithm. This approach transforms traditional shortest-path analysis into maximum distance exploration, could be applied in tourism route design and hydrological analysis.</abstract>
                <slug>foss4g-2025-4371-finding-the-farthest-point-implementing-longest-path-analysis-in-qgis-with-networkx-and-python</slug>
                <track>Lightning talk</track>
                
                    
                <language>en</language>
                <description>This presentation demonstrates advanced network analysis methods using Python and NetworkX within QGIS to calculate the farthest reachable point from any starting location. Users can transform road network data into graph networks and visualize complex routing paths across geographic regions. This approach makes advanced network analysis accessible to GIS practitioners without requiring specialized graph theory knowledge.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/BYCVNH/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/BYCVNH/feedback/</feedback_url>
            </event><event guid="5e79d313-759e-5a07-996c-08b7951cd9e4" id="4417">
                <room>WG403</room>
                <title>Filling the Gaps: Mapping Marine Habitats with Divers, Aerial Imagery, and Algorithms</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-20T15:45:00+13:00</date>
                <start>15:45</start>
                <duration>00:05</duration>
                <abstract>ORA Reefs aims to restore degraded rocky reef ecosystems across Tīkapa Moana by removing large-scale kina barrens. To support marine habitat identification and monitoring, we are developing a classification pipeline in Python and QGIS. Using the Depth-Invariant Index to isolate seabed reflectance from water column reflectance.</abstract>
                <slug>foss4g-2025-4417-filling-the-gaps-mapping-marine-habitats-with-divers-aerial-imagery-and-algorithms</slug>
                <track>Lightning talk</track>
                
                    
                <language>en</language>
                <description>ORA Reefs is a new initiative developed by Ocean Regeneration Aotearoa (ORA) Trust to actively restore degraded rocky reef and benthic ecosystems across Tīkapa Moana/the Hauraki Gulf, New Zealand. Sea urchin barrens are a dominant stressor on rocky reef ecosystems worldwide due to the overfishing of key predators. Evidence shows that releasing barren rocky reefs from kina (Evechinus chloroticus) grazing pressure in the Hauraki Gulf through large-scale removal enables biodiversity regeneration within 2 years. ORA Reefs’ initial focus is to pilot large-scale kina barren removal alongside the development of artificial reefs on degraded benthic ecosystems. If successful, these interventions, alongside the development of Blue Nature Credits, may offer a way to unlock capital for ecosystems that have historically lacked funding. 

While dive surveys can provide high-resolution data including species composition and physical characteristics of the subtidal environment, they are expensive, require specialised scientific divers, and only cover small areas. Geospatial classification techniques could better support broad-scale habitat identification and monitoring over a significantly larger area at relatively little cost. However, at present it is difficult to accurately depict biodiversity and physical traits of subtidal marine habitats. Here, we aim to combine aerial imagery with diver surveys using machine learning and deep learning algorithms to classify near-shore broad-scale marine habitat.

We have begun developing a pipeline using Python and QGIS to classify these marine habitats, mitigating the issue of water column reflectance contribution to seabed reflectance by applying the Depth-Invariant Index (DII). The initial use-case trialled feeding DII layers through a Random Forest model with drone imagery that captured Blue, Green, Red, Red Edge, and Near Infrared (NIR) bands at ~5 cm resolution. Validation accuracy scores are promising and justify continual pipeline development to enhance marine habitat identification at different locations and times.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/3FH8AK/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/3FH8AK/feedback/</feedback_url>
            </event></room><room name="WG308 TE IRINGA" guid="0c4358b2-4659-574d-85fb-50c3b323caf3"><event guid="51cbabdf-31d1-55b6-ae7f-321066245d75" id="3770">
                <room>WG308 TE IRINGA</room>
                <title>Getting Sentinel Data within seconds</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T15:30:00+13:00</date>
                <start>15:30</start>
                <duration>00:25</duration>
                <abstract>Explore how to quickly access and analyze Sentinel satellite data using STAC APIs and the Microsoft Planetary Computer. In this talk, we’ll demonstrate how Python makes it easy to fetch imagery and calculate vegetation indices in just minutes.</abstract>
                <slug>foss4g-2025-3770-getting-sentinel-data-within-seconds</slug>
                <track>Cloud, APIs &amp; Data Infrastructure</track>
                
                    
                <language>en</language>
                <description>Satellite imagery is more accessible than ever—but getting the data you need quickly and efficiently can still be a challenge. In this talk, we’ll explore how STAC (SpatioTemporal Asset Catalog) and the Microsoft Planetary Computer simplify the process of accessing and analyzing Sentinel imagery at scale.

You’ll learn how to use Python and modern geospatial libraries like `pystac-client`, `odc-stac`, and `xarray` to query, filter, and load Sentinel-2 and Sentinel-1 imagery—no downloading or unzipping required. We’ll demonstrate how to filter scenes by date, cloud cover, and region of interest, and then quickly calculate vegetation indices such as NDVI, EVI, and RVI for environmental or agricultural analysis.

We’ll cover:

What STAC is and why it matters

How the Microsoft Planetary Computer provides cloud-hosted, analysis-ready data

How to build fast, scriptable analysis pipelines using Python

By the end of this session, attendees will understand how to go from a geographic query to insightful raster analysis in just a few lines of code—making remote sensing workflows faster, reproducible, and scalable.

Perfect for data scientists, remote sensing professionals, and developers looking to cut through the complexity of traditional satellite data access.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/CH9FK9/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/CH9FK9/feedback/</feedback_url>
            </event><event guid="846905b9-4f8f-5513-a077-9b9c553b0736" id="4234">
                <room>WG308 TE IRINGA</room>
                <title>Accelerating GeoTIFF readers with Rust</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T16:30:00+13:00</date>
                <start>16:30</start>
                <duration>00:25</duration>
                <abstract>Reading a Cloud-optimized GeoTIFF involves several steps, from fetching compressed bytes over a network/disk, decompressing those bytes, to finally parsing of TIFF tag metadata. Can we speed up the decoding using asynchronous methods, or even GPU-accelerated libraries? Let's see how we can program this in Rust!</abstract>
                <slug>foss4g-2025-4234-accelerating-geotiff-readers-with-rust</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                
                    
                <language>en</language>
                <description>How can we compose together a modern library to decode Cloud-optimized GeoTIFFs (COGs) efficiently? By using a programming language called Rust, with bindings to Python, WebAssembly and more, our goal is to enable applications that demand high-performance reads, such as web-based COG tilers or machine learning workflows leveraging Graphical Processing Units (GPUs). For CPU workflows, we delegate the network/disk transfer handling to the [`object_store`](https://crates.io/crates/object_store) crate, use various Rust-based algorithms for decompressing raw bytes, and let the [`async-tiff`](https://crates.io/crates/async-tiff) crate do the actual TIFF tag metadata and pixel data parsing. For GPU workflows, we swap the decompression library for [`nvCOMP`](https://developer.nvidia.com/nvcomp), and do the TIFF parsing using [`nvTIFF`](https://developer.nvidia.com/nvtiff), with the resulting pixel data decoded directly into CUDA device memory. Come and see how these asynchronous and GPU-accelerated GeoTIFF readers compare against GDAL's [`libertiff`](https://gdal.org/en/release-3.11/drivers/raster/libertiff.html) driver, and find out how we're making these performant low-level Rust-based readers more accessible by integrating with the [xarray](https://xarray.dev) ecosystem and beyond!</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/MRPVGL/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/MRPVGL/feedback/</feedback_url>
            </event></room><room name="WG126" guid="c289d89e-6932-5361-96fd-a6c53160dcd4"><event guid="799be19a-f443-5ed3-9361-7bcb12da6991" id="3801">
                <room>WG126</room>
                <title>Bathymetry Data Wrangling</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>00:25</duration>
                <abstract>This talk discusses workflows implemented in Python for processing bathymetry data for hydrodynamic modelling applications.</abstract>
                <slug>foss4g-2025-3801-bathymetry-data-wrangling</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                
                    
                <language>en</language>
                <description>While all of Earth’s land surfaces have been mapped at 30 m resolution or finer, the topography of the seabed is still largely a black box, with only 26.1% of the seabed mapped to “adequate resolution”  thus far. In practice, this means that compiling a gridded bathymetric dataset often requires “filling in” areas of missing data, “smoothing” conflicts between overlapping datasets, and “blending” information from multiple sources. This talk discusses several strategies for fusing and interpolating datasets such as coastal lidar, high resolution multibeam echosounder data, chart depths, and the globally available GEBCO grid in real world application. The “fuzzy” boundary between land and sea is emphasized, as terrestrial and marine datasets often disagree when their coverage overlaps. Interpolation and smoothing methods are explored as well. The techniques discussed are implemented in Python using geospatial libraries and thus open source, easily scaled, and reproducible.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/MKP7QY/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/MKP7QY/feedback/</feedback_url>
            </event><event guid="46df71af-9bd7-5a67-9920-2acfd255f86e" id="4359">
                <room>WG126</room>
                <title>eo-tides: Open-source tide modelling tools for large-scale satellite Earth observation analysis</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T10:00:00+13:00</date>
                <start>10:00</start>
                <duration>00:25</duration>
                <abstract>`eo-tides` provides powerful open-source tools for combining satellite Earth observation data with tide modelling. These tools can be applied to petabytes of freely available satellite data, providing a suite of flexible tools for efficient monitoring and mapping of coastal and ocean environments – from regional, continental, to global scale.</abstract>
                <slug>foss4g-2025-4359-eo-tides-open-source-tide-modelling-tools-for-large-scale-satellite-earth-observation-analysis</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                <logo>/media/foss4g-2025/submissions/KUPAWC/joss_abstract_h176hdW.png</logo>
                    
                <language>en</language>
                <description>Freely available Earth observation (EO) satellite data is a powerful resource for mapping and monitoring dynamic coastal environments over time and across large areas. However, the influence of ocean tides means satellite data is often acquired at vastly different tidal stages. This can make it difficult to distinguish true patterns of coastal change from short-term tidal variability, leading to inaccurate or misleading insights into coastal processes. To address this challenge, there is a pressing need for scalable open source tools that can account for tidal variability and make tides an explicit part of coastal EO analysis.

The new Geoscience Australia `eo-tides` package (https://github.com/GeoscienceAustralia/eo-tides) offers powerful open-source tools for integrating satellite EO data with ocean tide modelling. It provides a flexible Python toolkit for attributing modelled tide heights to satellite data time series, based on each satellite image's spatial extent and acquisition time. eo-tides builds on advanced tide prediction capability from the open-source `pyTMD` library, combining this with spatial analysis tools from the Open Data Cube (ODC)'s `odc-geo`. This enables efficient, parallelised modelling using over 50 supported tidal models, with outputs returned in standardised pandas and xarray formats for further analysis.

`eo-tides` can be applied to petabytes of freely available satellite data accessed via the cloud using ODC’s `odc-stac` or `datacube` packages (e.g. using Digital Earth Australia or Microsoft’s Planetary Computer). Additional functionality supports validation with external tide gauge data and the assessment of potential satellite-tide biases - critical considerations for ensuring the reliability and accuracy of coastal EO workflows. These open-source tools support the efficient, scalable and robust analysis of coastal EO data for any time period or location globally.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/KUPAWC/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/KUPAWC/feedback/</feedback_url>
            </event><event guid="cdf3c753-6e49-51fd-af38-e59d963153f3" id="4220">
                <room>WG126</room>
                <title>Exploring urban form in New Zealand</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T11:00:00+13:00</date>
                <start>11:00</start>
                <duration>00:25</duration>
                <abstract>We interact with urban form everyday. It dictates where and how we live, move, build and interact with people and services in our cities. This talk will introduce the exciting world of urban morphometrics - measurements of urban form and their implications on urban evolution, function and performance.</abstract>
                <slug>foss4g-2025-4220-exploring-urban-form-in-new-zealand</slug>
                <track>Use Cases and Applications</track>
                
                    
                <language>en</language>
                <description>This talk will showcase how open data from OpenStreetMap and other resources combined with open source tools from Python and R can help us build a deep understanding of our urban environment. The talk will be linked to a Github repo with reproducible code (and environment) for any interested enthusiast who wants to delve deeper into this topic.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/RE8VMR/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/RE8VMR/feedback/</feedback_url>
            </event><event guid="37ba67fe-18cf-5f3f-ad3e-caae6f7ebeb8" id="4505">
                <room>WG126</room>
                <title>State of GRASS</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T12:00:00+13:00</date>
                <start>12:00</start>
                <duration>00:25</duration>
                <abstract>GRASS is an open source geoprocessing engine for efficient spatio-temporal data management, analysis, and modeling. The software comes with C / Python / R API, command line and graphical user interfaces.

In this talk we will give a comprehensive overview of the latest GRASS developments and upcoming new features.</abstract>
                <slug>foss4g-2025-4505-state-of-grass</slug>
                <track>State of software</track>
                
                    
                <language>en</language>
                <description>Join us for a lively overview of the current state of the GRASS project, where community meets cutting-edge geospatial technology. Whether you're a longtime power user or a newcomer curious about GRASS, this talk will highlight the major strides the project has made in the past year – from revitalized governance and community growth to technical breakthroughs – and offer a glimpse into what's next.

During the talk, we will address how GRASS has strengthened its governance and support structure by bringing in new members to bolster sustainable leadership and new fiscal sponsorship with NumFOCUS. We will also review GRASS community-building initiatives, such as the NSF-backed efforts that allowed GRASS to establish a mentoring program for new contributors, support our Student Grant program, and hold the GRASS Developer Summit 2025 in Raleigh, NC. We will highlight this past summer's Google Summer of Code project, which demonstrates how community mentoring feeds innovation.

The talk will also address GRASS's new logo and branding initiative over the past year, aiming to give the project a modern look while keeping its iconic elements. Notably, "GRASS GIS" is now officially just GRASS – a simpler name that the community has used colloquially for years. To celebrate, the team launched an online swag shop with GRASS-themed apparel, stickers, and more. We will also look at recent strides in community outreach and learning resources, such as a new tutorial website and the modernization of GRASS's documentation platform.

On the development side, we will show off what the GRASS development team has been hard at work delivering in terms of new features, improved performance, and better integration as part of GRASS 8.5. Under the hood, the team made significant code quality and security improvements, fixing issues flagged by automated linters and code scanners. These efforts pave the way for stricter continuous integration checks and a more robust codebase. The build system is also being modernized: GRASS is transitioning to CMake for easier compilation and maintenance, and an official Conda package is on the way, simplifying installation for Python/R data scientists and lowering entry barriers.

As we celebrate these achievements, we're also looking ahead. The GRASS roadmap outlines ambitious goals for the next few years. We plan to maintain annual releases (GRASS 8.6 is already on the horizon for 2026) and continue improving distribution and integration – think one-click installs via Conda, tighter bridges to QGIS and R, and refined Python and R APIs for smooth scripting. Sustainability remains a core focus: the project actively pursues new grants, sponsors, and community donations to ensure long-term development while spreading infrastructure knowledge and lowering maintenance overhead to avoid burnout.

In short, the state of GRASS is strong and dynamic. This talk will offer an informative yet exciting tour of the project's recent milestones across community and technology. We invite everyone – from newbies to veteran developers – to see how far GRASS has come and to get inspired about where it's heading. Learn about the latest capabilities, meet the people behind the project, and discover how you can be part of the next chapter of GRASS!</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/MEHRQF/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/MEHRQF/feedback/</feedback_url>
            </event><event guid="885811a3-1b8f-5d58-a6a9-e070849f9bf6" id="4276">
                <room>WG126</room>
                <title>Developing a user-oriented data cube for biodiversity and carbon dynamics assessment in Estonia with remote sensing data</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>00:25</duration>
                <abstract>This presentation covers the development of a national data cube for Estonia, integrating remote sensing data and using open-source tools. It provides analysis-ready data for biodiversity and carbon research, overcoming technical hurdles. User-friendly tools and cloud computing enhance data access, empowering informed decision-making for sustainable development.</abstract>
                <slug>foss4g-2025-4276-developing-a-user-oriented-data-cube-for-biodiversity-and-carbon-dynamics-assessment-in-estonia-with-remote-sensing-data</slug>
                <track>Use Cases and Applications</track>
                
                    
                <language>en</language>
                <description>Introduction
Addressing global environmental challenges like land use and climate change requires timely, accurate information. Earth Observation (EO) data, from satellites and UAVs, is essential for monitoring these dynamics. Thanks to open data policies and advancements in software and cloud computing, EO data enhances environmental management and policy assessment, contributing to sustainable development. However, there are technical challenges, including data storage and analysis, and the need for computational architectures that handle large datasets.
Traditional data cubes often lack the readiness needed for advanced AI and machine learning techniques, which require structured, rich datasets. User-friendly platforms with intuitive access and customizable tools are crucial for researchers and policymakers.
Our project aims to create a comprehensive data cube for Estonia, utilizing remote sensing and geospatial data and open-source tools to advance biodiversity and carbon dynamics research. The fusion of LiDAR, radar, and passive remote sensing offers untapped potential for modeling, and multi-temporal datasets can predict vegetation and environmental variables effectively.

Data and Methods
We incorporated data from Sentinel-1, Sentinel-2, Landsat, and high-resolution airborne LiDAR. We used Google Earth Engine and Python for data pre-processing. Additionally, digital elevation models and the Estonian soil map were used to prepare the data cube layers. The study area was divided into manageable tiles using a spatial grid, creating 10m resolution Cloud Optimized GeoTIFFs (COGs) to facilitate efficient processing and downloading.

LiDAR data allowed us to calculate biodiversity-relevant indices, including ecosystem height, cover, and structural complexity. These were processed with tools like PDAL and laspy for precise classification and filtering.

The data cube runs on a high-performance cloud platform, using S3 storage for COGs and libraries such as rasterio to gather metadata. This metadata is integrated into a STAC-compatible web service, enabling seamless access through platforms like QGIS and Python for efficient querying and processing.

Data Cube Access
Our data cube portal (https://geokuup.ee/estonia) , developed with the Phoenix framework, utilizes MapLibre for data visualization. This setup supports quick visualizations and queries, organizing datasets into collections for user convenience. Users can create custom collections, tailoring data sets to specific research needs, which enhances the system’s flexibility.

By adopting best practices in geospatial data management, we leveraged open-source tools like GeoServer and pygeoapi, along with the Pangeo ecosystem, to streamline processing. The Phoenix framework offers a robust and efficient solution for managing concurrent users, ensuring stability and performance.

Outcomes and Future Work
The data cube provides high-resolution spatial data for academic and governmental purposes, with a strong focus on biodiversity and carbon research. It offers a scalable solution that can be extended to other research domains by incorporating additional data layers.
Future work will focus on processing data into multiple resolutions and expanding the range of datasets and workflows to enhance data retrieval and analysis. This will further support informed decision-making and sustainable development initiatives, empowering researchers and policymakers with timely environmental information.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/DUSEKB/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/DUSEKB/feedback/</feedback_url>
            </event><event guid="7ae220bf-42d8-580d-b4d2-598c447b5087" id="4433">
                <room>WG126</room>
                <title>Raster processing on HPC without coding? Sure!</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T16:30:00+13:00</date>
                <start>16:30</start>
                <duration>00:25</duration>
                <abstract>In this presentation I demonstrate how to use the LUMASS visual modelling environment to develop high-performance parallel raster processing models without writing a single line of code. LUMASS models are scalable and run on laptops as well as distributed memory systems. https://manaakiwhenua.github.io/LUMASS</abstract>
                <slug>foss4g-2025-4433-raster-processing-on-hpc-without-coding-sure-</slug>
                <track>Use Cases and Applications</track>
                <logo>/media/foss4g-2025/submissions/YND7DA/Session_image_W0EEs8r.png</logo>
                    
                <language>en</language>
                <description>Wrangling large and/or many raster datasets on a laptop or small workstation is no fun! Unfortunately, parallelising models or workflows to run on distributed memory compute clusters requires more than entry level coding skills. In this presentation I introduce the parallel extension to LUMASS' high-level visual raster processing framework. It enables the development of complex models and workflows without writing a single line of code! Check out this short playlist for getting a better idea: https://youtube.com/playlist?list=PL_CsDVZ4IPO-D87TgO0awddJGl2gUYIaD&amp;si=siH6lT_3pwdN3Q-W 
LUMASS provides a range of processing components, including map algebra, zonal summaries, terrain attributes, and more. It uses GDAL for 2D raster I/O and NetCDF for data cubes. It supports SQLite-based raster attribute tables alongside any supported raster format.  Furthermore, it enables the integration of any external command line program or script to be included into the pipeline to extend processing capabilities. Programmers find a Python interface for writing ‘moving window’ functions without having to worry about multi-threading or streaming, which comes out of the box! LUMASS is free and open-source software ( https://github.com/manaakiwhenua/LUMASS ).</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/YND7DA/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/YND7DA/feedback/</feedback_url>
            </event></room><room name="WA220" guid="132a9daa-6bcb-593a-b15d-b8ddf7d1d99c"><event guid="ee85e8fe-fc63-56ee-a88f-3b9e3862512e" id="4393">
                <room>WA220</room>
                <title>PyForestScan: A Python Library for Large-Scale LiDAR Forest-Structure Metrics</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T11:00:00+13:00</date>
                <start>11:00</start>
                <duration>00:25</duration>
                <abstract>We introduce PyForestScan, a Python library that streams LiDAR data and outputs GeoTIFF canopy height, cover, foliage-height diversity, plant-area-index and density layers. We demonstrate it by mapping forest metrics on Hawai‘i Island’s 1 M-ha, 1.8 B-point survey into 1–30 m grids.</abstract>
                <slug>foss4g-2025-4393-pyforestscan-a-python-library-for-large-scale-lidar-forest-structure-metrics</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                
                    
                <language>en</language>
                <description>Airborne LiDAR now blankets landscapes with up to sub-centimeter-scale detail. Yet, researchers and land managers have lacked a fully open, Python-native workflow to transform billions of points into actionable information on forest structure. Existing solutions are either proprietary or anchored in R, leaving the rapidly growing Python geospatial stack without a scalable counterpart. PyForestScan bridges this gap. Built on PDAL’s streaming I/O, the library reads traditional LAS/LAZ alongside hierarchical octree formats such as COPC and EPT, tiles point clouds automatically, and exports GeoTIFF layers of canopy height, canopy cover, foliage-height diversity (FHD), plant-area density (PAD), plant-area index (PAI), and digital-terrain models in a single command. We demonstrate these capabilities by generating island-wide canopy height, canopy cover, FHD, and PAI mosaics for Hawai‘i Island - an area with over 1 million hectares and over 1.8 billion points, in under 24 hours of wall time on commodity hardware, producing multi-resolution (1 m–30 m) grids ready for carbon accounting, biodiversity assessment, and geoAI training. The talk will outline PyForestScan's architecture, highlight performance benchmarks, and invite contributions via its open governance model, Docker images, and CI-tested notebooks. By placing robust LiDAR analytics squarely within the free and open Python ecosystem, PyForestScan equips the FOSS4G community to move seamlessly from raw point clouds to island- and continent-scale forest insights.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/ACSJKW/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/ACSJKW/feedback/</feedback_url>
            </event><event guid="8ac2b838-8043-5b5b-a6bf-aa635dec8b64" id="4429">
                <room>WA220</room>
                <title>Identifying Forest Invasive Species in Fiji and Tonga Using Machine Learning</title>
                <subtitle />
                <type>Academic Paper</type>
                <date>2025-11-20T12:00:00+13:00</date>
                <start>12:00</start>
                <duration>00:25</duration>
                <abstract>Deforestation and forest degradation in the Pacific is an ongoing threat to biodiversity, ecological connectivity and livelihoods. These processes have catalysed rapid expansion of invasive flora constituting severe land degradation. Using Digital Earth Pacific, we are better able to monitor the sprawling expansion of these invasive species.</abstract>
                <slug>foss4g-2025-4429-identifying-forest-invasive-species-in-fiji-and-tonga-using-machine-learning</slug>
                <track>Academic</track>
                
                    
                <language>en</language>
                <description>Introduction 

In the Pacific, the spread of invasive species has been a result of direct anthropogenic impacts of land use modifications as well as indirect anthropogenic impacts of natural occurrences like tropical cyclones. Such disturbances have accelerated the spread of invasive species, particularly over degraded and exposed landscapes, by clearing the land and changing it. 

Data were collected via GPS surveys in the field to identify confirmed invasive species sites, which were then processed with time-series satellite data over Digital Earth Pacific. Phenological characteristics and seasonal trends in plant vegetation were used to train the models to detect and track invasive species over an extent of time. The results demonstrate the feasibility of region-wide, large-scale monitoring of invasive plant species. The methods are a valuable tool to interpret spatial invasion patterns since 2017 until the present moment, contributing to more accurate ecosystem management and informing policy responses to land degradation and biodiversity loss across PICTs. 

Two of the key objectives of the project are to assess two high-priority invasive species: Spathodea campanulata (also referred to as African tulip tree) and Cordia alliodora (also referred to as Cordia, Salmwood, or Spanish Elm). They are targeted since they are extensively found and ecologically affecting native forests in both countries. Other invasive plant species, such as Leucaena leucocephala, Merremia peltata (Cook's Glory), Hevea brasiliensis (Para Rubber tree), and Acacia mangium (Black Wattle), have been identified as secondary problems in several Pacific Island Countries and Territories (PICTs). Their invasion is a daunting task due to the intensity and speed of invasion. This study proves the use of Earth observation technology and machine learning for mapping and monitoring the invasive tree species distribution across the Pacific. 

Methodology 

Field surveys were conducted using QField, an open-source mobile GIS that is integrated with QGIS, to collect georeferenced data on invasive species in Fiji and Tonga. 

Collecting the data was done through filling out custom forms to input date, species, landcover category, and location with line-of-sight mapping or locked GPS points, using high-accuracy devices (e.g. TDC 650), with accuracy ranging from 3 m to sub-metre.  

To complement model training, data on non-native species and other vegetations were also obtained to be utilized as machine learning (ML) classification training data. Upon deployment in the field, the data were compiled into a geodatabase, cleaned, validated, and overlaid on Sentinel-2 satellite Geo-Median composites (10 m resolution, 2024). These vector training points were loaded into the Digital Earth Pacific platform and used to sample Sentinel-2 image spectral and vegetation index values including NDVI, EVI, SWIR, chlorophyll index, and other band ratios. The data were then used to train a Random Forest machine learning model that was initially tested against small parcels and continuously improved. 

Phenological data (e.g., blooming period) from local witnesses informed seasonal satellite image filtering to optimize classification accuracy. STAC architecture enabled multi-sensor data integration and simplified access to spectral features. Validation for accuracy was conducted in collaboration with indigenous forestry and botany experts. Their input improved model reliability and contextual validity. Further ground-truthing was used to validate results and collect more data, further calibrating model projections. Once acceptable accuracy had been achieved, the workflows were reconciled into Python notebooks and disseminated to Forestry Ministries for further use and replication. 

 

Preliminary Results  

The attempts at classifying invasive species in Tonga and Fiji used machine learning (ML) models that were trained from ground-collected data and Sentinel-2 satellite images. Preliminary classifications in the Toloa Forest of Tonga overestimated the amount of Cordia salmwood due to unbalanced training data and the lack of seasonal filtering. Improvements in the third iteration enhanced the detection of Cordia, particularly at forest edges. Atele Forest model using the Toloa-trained classification identified African Tulip spread without ground points, verifying the prediction capability of the model. 

In Fiji, there were some forests that had widespread invasive species. Wainibuka Forest had a 43% African Tulip cover, aided by cyclone seed dispersal. Nadarivatu and Korotari Forests had widespread Cordia Alliodora invasions from previous plantations, with Korotari also having 40% African Tulip. Lololo and Bua Forests, both owned by Fiji Pine Limited, had widespread Acacia mangium invasions in Bua with 77% Acacia cover, hindering pine regeneration Capacity building was a main component in place to offer sustainability.  

There were two workshops held in Tonga with the Ministry of Agriculture, Food, and Forests. The initial one trained officers in QGIS and QField for data collection. The second workshop demonstrated ML principles through ground-truth points and Sentinel-2 images, and hands-on Python sessions. Participants collected additional Cordia and African Tulip data to improve model accuracy.  

In Fiji, a compressed one-week workshop in Nadi engaged 15 individuals from forestry, agriculture, and land management. Training was conducted on QGIS, QField, and ML processes using Sentinel-2 data. Booklets were provided to participants with guided activities so that they could replicate invasive mapping in their local area.  

Overall, these efforts strengthened national capacity to detect, monitor, and manage invasive plant species using advanced geospatial technologies and participatory learning approaches.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/KDTE9Z/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/KDTE9Z/feedback/</feedback_url>
            </event><event guid="178be3f8-2f21-5790-b66f-92ec22cd0366" id="4290">
                <room>WA220</room>
                <title>Network Analysis at the continental scale, determining new measures for accessibility.</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T13:30:00+13:00</date>
                <start>13:30</start>
                <duration>00:25</duration>
                <abstract>The Centre for Australian Research into Access has been utilising the fast-calculating methods of the Pandana library in combination with Geopandas methods to perform continental-scale network analysis. Outputs of our work attempt to highlight the importance of address-level spatial analysis when assessing accessibility to services.</abstract>
                <slug>foss4g-2025-4290-network-analysis-at-the-continental-scale-determining-new-measures-for-accessibility-</slug>
                <track>AI, Data Science &amp; Analytics</track>
                
                    
                <language>en</language>
                <description>The Centre for Australian Research into Access (CARA) has been utilising the fast-calculating methods of the Pandana Python library in combination with Geopandas methods to perform continental-scale network analysis. CARA's objective is to provide address-level accessibility calculations for health and education with a particular focus on rurality, to assist academic researchers and to inform policymakers. The work we are producing is a result of an Australian Research Council Linkage Infrastructure, Equipment and Facilities (LIEF) grant, which has recently finalised building a spatially detailed infrastructure for a more equitable nation, representing a digital twin for modelling the patterns and processes impacting the Australian population.

CARA has adapted open-source Python libraries, file formats, and data to develop network analysis methods that operate on continental scales that have previously been too costly to process. The Pandana open-source Python library enables fast network calculations to find shortest paths using contraction hierarchies (Foti &amp; Waddell 2012). We have used methods from Pandana to perform ‘number of nearest’ calculations (n-nearest), matrix calculations, and catchment area calculations for both time and distance impedances. Calculations have been carried out using origin data for approximately 10.6 million residential addresses across Australia to various health and education destination datasets. Spatial methods from the Geopandas library were also used to account for the times and distances between origin/destination points and the network, and also to aggregate outputs to spatial units such as those in the Australian Statistical Geography Standard (ASGS). 

Outputs from our n-nearest network calculations using the Pandana nearest_pois method (3.7 million nodes, 7.8 million edges, two impedances, and 7,073 destinations) have been completed in less than 3.5 minutes. Outputs from our matrix and catchment network calculations using the Pandana shortest_path_lengths method have fluctuating run times due to the varying number of input origin and destination points with each calculation. However, they are robust in handling a large number of inputs.

These methods have recently been adapted to use Overture Maps data that will allow them to be applied to a wide variety of countries throughout the world.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/GLLESU/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/GLLESU/feedback/</feedback_url>
            </event></room><room name="WG404" guid="8fad97e8-15a2-5185-b1c8-68e7d8beb155"><event guid="537a7206-c4fe-5377-92b2-5f6025cb3f1c" id="4682">
                <room>WG404</room>
                <title>Icechunk 2.0</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T11:30:00+13:00</date>
                <start>11:30</start>
                <duration>00:25</duration>
                <abstract>[Icechunk 2.0](https://icechunk.io/) is a transactional, cloud-native storage engine for [Zarr](https://zarr.dev/). This talk introduces powerful new features for transactionally managing massive geospatial arrays, including efficient appends/inserts.</abstract>
                <slug>foss4g-2025-4682-icechunk-2-0</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                
                    
                <language>en</language>
                <description>[Icechunk](https://icechunk.io/) is an open-source, cloud-native transactional storage engine for multi-dimensional arrays, designed to manage massive geospatial datasets. Building upon the foundational features of data versioning and schema evolution presented in its first iteration, Icechunk 2.0 introduces a more stable and performant on-disk format that unlocks powerful new capabilities for managing large-scale array data. This presentation will introduce the new features of Icechunk 2.0 and demonstrate their application to common challenges in geospatial data analysis.

Managing petabyte-scale geospatial data, such as satellite imagery time-series, gridded weather forecasts, and climate model outputs, requires tools that can handle evolving data and complex operational pipelines efficiently. Icechunk 2.0 directly addresses these needs with several key innovations:

- **Efficient Array Manipulation:** A new indexing capability allows for cheap appends, prepends, and inserts into an array without rewriting existing data chunks. This is transformative for managing growing time-series datasets.
- **Flexible Data Organization:** Users can now rename or move arrays and groups within the [Zarr](https://zarr.dev/) hierarchy without costly data duplication, simplifying the curation and organization of large data repositories.
- **Enhanced Data Governance:** The introduction of an amendable commit option simplifies the version history, while a comprehensive operation log and support for repository-level metadata provide crucial data provenance.
- **Improved Performance and Stability:** The new format enables significantly faster and safer garbage collection and more efficient queries of a repository’s history.

We will demonstrate how Icechunk 2.0 integrates seamlessly into the Scientific Python ecosystem ([Xarray](https://xarray.dev/), [Dask](https://www.dask.org/)) via its Zarr store interface and how we have built Icechunk support into the managed [Earthmover platform](https://earthmover.io/platform). Through real-world examples, we will showcase how these new features can be used to build robust, high-performance data pipelines for cloud-based geospatial analytics and machine learning.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/KZPVUC/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/KZPVUC/feedback/</feedback_url>
            </event><event guid="76c7596a-15d9-58da-9b12-776c87e9d3b5" id="3718">
                <room>WG404</room>
                <title>pygeometa project status</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-20T15:30:00+13:00</date>
                <start>15:30</start>
                <duration>00:25</duration>
                <abstract>pygeometa project status presentation.  Come and find out the latest news on the project as well as future plans, and how to get involved!</abstract>
                <slug>foss4g-2025-3718-pygeometa-project-status</slug>
                <track>State of software</track>
                
                    
                <language>en</language>
                <description>pygeometa provides a lightweight and Pythonic approach for users to easily create geospatial metadata in standards-based formats using simple configuration files (affectionately called metadata control files [MCF]). Leveraging the simple but powerful YAML format, pygeometa can generate metadata in numerous standards. Users can also create their own custom metadata formats which can be plugged into pygeometa for custom metadata format output.

For developers, pygeometa provides a Pythonic API that allows developers to tightly couple metadata generation within their systems and integrate nicely into metadata production pipelines.

The project supports various metadata formats out of the box including ISO 19115, the WMO Core Metadata Profile, and the WIGOS Metadata Standard.

pygeometa has minimal dependencies (install is less than 50 kB), and provides a flexible extension mechanism leveraging the Jinja2 templating system.

This presentation will provide an update on recent enhancements, use in high profile projects as well as future plans and roadmap.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/ZMECUX/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/ZMECUX/feedback/</feedback_url>
            </event><event guid="524e53cb-8900-5ea1-b180-9039b176b6c9" id="4378">
                <room>WG404</room>
                <title>A Scalable Open-Source System for Impervious Land Mapping Using GRASS and the Python Ecosystem</title>
                <subtitle />
                <type>Academic Paper</type>
                <date>2025-11-20T16:00:00+13:00</date>
                <start>16:00</start>
                <duration>00:25</duration>
                <abstract>This paper introduces a scalable open-source system using GRASS, Python libraries, and HDF5 to map impervious surfaces from orthophotos and Sentinel-2 imagery. Outputs are vectorized, compared with agricultural and forested lands, and analyzed for environmental impacts like soil sealing to support sustainable land management and restoration.</abstract>
                <slug>foss4g-2025-4378-a-scalable-open-source-system-for-impervious-land-mapping-using-grass-and-the-python-ecosystem</slug>
                <track>Academic</track>
                
                    
                <language>en</language>
                <description>## 1 Introduction
Impervious surface expansion significantly contributes to environmental degradation. Accurate and efficient mapping of such changes is essential in environmental monitoring and spatial analysis. While many tools exist for land cover classification, few provide a fully open-source, scalable solution capable of ingesting both orthophotos and multispectral data, performing temporal analysis, and supporting integration with spatial databases.
This paper presents a scalable, tile-based system for detecting and monitoring impervious land using orthophotos and Sentinel-2 data. The workflow integrates GRASS for spatial processing, HDF5 for high-performance raster storage, and Python-based machine learning libraries for classification and change detection. Supervised models are trained using labeled vector inputs and applied in a modular pipeline that supports multi-year analysis and large-scale processing. Predicted outputs are postprocessed, vectorized, and stored in spatial databases. Results are further compared with ancillary datasets, including agricultural and forested lands, to inform end users of the environmental implications. The system is designed to support rapid prototyping, reproducibility, and high-resolution monitoring across varying spatial and temporal domains. Performance is evaluated using standard classification metrics, and results demonstrate the applicability of this workflow in operational land change detection contexts.

## 2 Materials and Methods
### 2.1 Data Sources
The system supports multiple raster inputs. VHR orthophotos serve as the primary high-resolution data source, complemented by Sentinel-2 multispectral bands. Reference impervious layers from earlier periods generate labeled training data and establish baselines for change detection. Ancillary datasets, such as agricultural and forest land boundaries, are integrated for comprehensive environmental assessments.
### 2.2 Tiling and Data Management:
Raster and vector layers are preprocessed in GRASS, spatially tiled, and exported to HDF5 format, containing image features and corresponding labels. This enables efficient parallel processing and patch-based classification.
### 2.3 Classification Pipeline
Machine learning models (Random Forest, GPU-accelerated classifiers, U-Net) are trained using Python libraries such as scikit-learn, cuML, and PyTorch. Training and inference are executed tile-wise using joblib for multiprocessing. Evaluation metrics (F1-score, accuracy) guide model selection and validation.
### 2.4 Temporal Analysis
Change detection is performed by comparing predicted impervious layers across years. The system computes spatial differences at the tile level to identify newly developed impervious areas. Temporal stacking facilitates scalable monitoring of land surface dynamics.
### 2.5 Postprocessing and Vectorization
Postclassification filtering reduces noise. Cleaned rasters are vectorized and simplified within GRASS. Final geometries are stored in PostGIS, enabling spatial queries, external integration, and comparison with land-use data for assessing impacts on soil and potential for restoration.
## 3 Results and Discussion
The system was applied to multi-year orthophoto and Sentinel-2 datasets. Classification accuracy was validated with independent tiles. The HDF5-based tiling significantly reduced I/O bottlenecks. Change maps accurately identified new impervious areas. Vectorized outputs supported further spatial analysis, visualization, and comparative evaluation against agricultural and forested land-use data. These comparisons underscore the importance of monitoring soil sealing and highlight opportunities for targeted land restoration initiatives.
## 4 Conclusion
This study demonstrates a reproducible, open-source pipeline for high-resolution impervious land mapping. Integration of GRASS, HDF5, and Python libraries enables efficient classification, temporal analysis, spatial output management, and environmental impact assessment. The architecture is adaptable to diverse data sources and modeling approaches, thus suitable for research, operational applications, and informed land management and restoration practices.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/BSQLEH/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/BSQLEH/feedback/</feedback_url>
            </event></room><room name="WG607" guid="4ae40321-2591-5002-82a8-0b12958015bc"><event guid="ac3d8b1c-b52f-5ee7-962e-a0f43292ea28" id="4369">
                <room>WG607</room>
                <title>Geomatics and open-source for road infrastructure management: standards, technologies, and future directions</title>
                <subtitle />
                <type>Academic Paper</type>
                <date>2025-11-20T16:00:00+13:00</date>
                <start>16:00</start>
                <duration>00:25</duration>
                <abstract>This contribution reviews how open standards and open-source technologies enable integrated, transparent, and sustainable workflows for managing bridges, tunnels, and roads. It highlights how open ecosystems can overcome data silos, support digital twins, and enhance long-term monitoring, regulatory compliance, and collaboration in infrastructure asset management.</abstract>
                <slug>foss4g-2025-4369-geomatics-and-open-source-for-road-infrastructure-management-standards-technologies-and-future-directions</slug>
                <track>Academic</track>
                
                    
                <language>en</language>
                <description>Following the introduction of guidelines for documenting road assets in Europe, infrastructure asset management is undergoing a new digital transformation. The reuse and transparency principles are fostering the adoption of open standards and open-source technologies [1]. This paradigm shift toward digitising is evident in the monitoring and maintenance of critical infrastructure, such as bridges and road networks. New guidelines led to the proliferation of proprietary solutions, which, however, contributed to a fragmentation of data environments. Custom and licence-dependent formats limited the interoperability, scalability and transparency of infrastructure management workflows [2]. In contrast, the open-source approach fosters collaboration, enhances accessibility to information, and supports more resilient, cost-effective and democratic decision-making processes. To strategically develop operational solutions answering the guidelines needs, it is then needed to address the state-of-the-art of open standards, technologies and their potentials and limitations in documented applications.

Standards overcoming proprietary limitations

A core enabler of this transformation is the implementation of open standards for data interoperability. In the building and infrastructure sector, BIM standards such as Industry Foundation Classes (IFC, ISO 16739) offer a solid basis for representing built assets in a vendor-neutral format [3]. IFC enables the exchange and reuse of information throughout the design, construction and operational phases, incorporating essential elements such as geometric components, spatial relationships and functional systems. Recent IFC extensions specifically support bridges, and ongoing efforts are targeting tunnels and other infrastructure typologies [4]. In parallel, developments in the GIS domain have produced open standards such as CityGML and LandInfra, which enable the structured representation of infrastructure within its environmental context [5]. These standards facilitate the integration of semantic, spatial, and topological information across administrative and technical domains, especially when aligned with OGC specifications that adhere to the FAIR principles — Findable, Accessible, Interoperable, and Reusable [6].

However, the true digital integration of infrastructure assets requires more than the adoption of standalone standards. The convergence of BIM and GIS necessitates strategies that harmonise their respective models and semantics, especially for multi-scale, cross-domain applications. Projects are increasingly relying on Linked Data and Semantic Web technologies to achieve this integration. Using ontologies facilitates mapping sensor properties to models, embedding IoT data in the infrastructure’s digital representation [1]. Despite these advances, challenges persist. Semantic and schematic discrepancies between BIM and GIS standards continue to pose significant challenges. For example, efforts to map IFC to CityGML often involve trade-offs between semantic richness and geometric accuracy. Rather than creating new standards, the prevailing strategy is to extend and align existing ones to meet the evolving requirements of digital twin systems, particularly those focused on bridge management and road asset monitoring [7].

Technologies in operational systems

Complementing this standards foundation is an expanding ecosystem of open-source technologies that support the practical implementation of digital infrastructure management systems. PostgreSQL, enhanced with the PostGIS extension, remains essential for storing spatio-temporal data [8][9]. Graph databases like Neo4j offer an intuitive and scalable solution for modelling relationships between infrastructure components, inspections, and maintenance workflows [10]. Open-source GIS platforms such as QGIS facilitate desktop spatial analysis and visualisation [11], integrating seamlessly with mobile tools such as MerginMaps, QField, and ODK Collect to enable efficient in-situ data collection. These solutions have proven effective for managing transportation assets at the municipal level, such as signage and road condition inventories [12].

The web-based visualisation and analysis of road networks and assets has also advanced significantly through libraries such as Cesium, Deck.gl and Xeokit. Cesium supports interactive, 3D globe-based visualisation and streams 3D tiles to render large-scale infrastructure environments [7]. Xeokit specialises in BIM visualisation, enabling examination of IFC models within web platforms. These libraries enable infrastructure stakeholders to interact with 3D models independently of proprietary software, thereby expanding access to asset information across organisations [10]. Dashboards built with frameworks such as ECharts and enhanced with WebSocket protocols support the integration of real-time data from sensor networks, ensuring that digital models are dynamically linked to their physical counterparts.

In the field of road network analysis, open-source Python packages such as OSMnx, MovingPandas and Scikit-Mobility allow for a detailed examination of mobility patterns and accessibility [13]. Simulation tools such as SUMO and AequilibraE offer the ability to model multimodal traffic flows, test policy scenarios and evaluate infrastructure interventions [14]. When integrated with geodata from OpenStreetMap and coupled with routing engines like OSRM, these tools provide a comprehensive solution for network planning and optimisation [6].

Current potentials and limitations

By eliminating licensing fees, open-source approaches reduce long-term software costs and provide full transparency into the computational models and assumptions behind analyses—an essential aspect for public infrastructure projects where trust, accountability, and reproducibility are critical [15]. Open tools also give public agencies and asset owners control over their data and workflows, enabling them to inspect, adapt, and extend codebases to meet evolving needs without vendor lock-in.
The collaborative nature of open-source development further facilitates knowledge transfer and capacity building. Active communities, thorough documentation and modular architectures reduce the barriers to adoption, thereby fostering innovation [6]. Local governments and academic institutions can contribute to, and benefit from shared development to generate context-specific, sustainable solutions. Furthermore, open tools promote citizen engagement by making infrastructure data more accessible and easier to understand — a vital aspect of participatory planning and the development of inclusive smart cities [12, 15].

However, there are still challenges to overcome. For example, integrating different types of data, such as point clouds, GIS layers, BIM models and real-time sensor feeds, requires robust data pipelines. Ensuring data quality, particularly for legacy assets, remains challenging. Furthermore, performance and scalability issues arise with large-scale or high-resolution models, and efforts to optimise such workflows are ongoing.

Equally critical are the human and institutional factors. Although many open-source tools now have user-friendly interfaces, a certain level of technical expertise is still needed for setup, customisation and maintenance. Capacity building and continuous training are necessary to ensure that practitioners can leverage these tools effectively. At the policy level, questions relating to data ownership, privacy and digital sovereignty must be addressed by robust legal and governance frameworks. The adoption of digital twins for public infrastructure also raises important ethical and epistemological questions, particularly with regard to the authority of data-driven models in decision-making contexts.
Looking forward, research focus on enhancing semantic alignment between BIM and GIS models, improving the representation of defects and maintenance history within IFC schemas, and enabling greater automation in cross-platform integration. Further studies are needed to assess the comparative benefits and limitations of open-source versus proprietary solutions in different infrastructure contexts. Evaluating the long-term sustainability, replicability, and social impact of open-source digital twins will be key to their broader adoption.

This contribution aims to provide an overview of the open-source and open-standard scenario in infrastructure management with case studies from literature. It offers a technical synthesis and a forward-looking perspective, with a particular focus on documented case studies that adopt OSGeo tools, OGC standards and open data. Leveraging the strengths of openness  enables stakeholders to build more resilient infrastructure systems.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/UXPNJZ/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/UXPNJZ/feedback/</feedback_url>
            </event></room></day><day index="5" date="2025-11-21" start="2025-11-21T04:00:00+13:00" end="2025-11-22T03:59:00+13:00"><room name="WG403" guid="a2eb48cd-245e-5328-a7d6-7eaa52403bf2"><event guid="731fa5ba-d844-56c2-a926-11a43949f1e0" id="4404">
                <room>WG403</room>
                <title>Feed your spreadsheet to the Pandas!</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-21T09:20:00+13:00</date>
                <start>09:20</start>
                <duration>00:05</duration>
                <abstract>The Python Pandas library makes data analysis fast and intuitive. Learn how to load datasets, filter and transform data, perform basic calculations, and export results - all using Pandas. Stacy will present a beginner-friendly workflow using Python Notebooks, tailor made for rapid experimentation and iteration.</abstract>
                <slug>foss4g-2025-4404-feed-your-spreadsheet-to-the-pandas-</slug>
                <track>Lightning talk</track>
                
                    
                <language>en</language>
                <description>The Pandas library for Python streamlines data analysis by making number crunching both fast and intuitive. It excels at handling common “load-calculate-save” workflows with concise, readable code—ideal for everyday data tasks.

In this practical lightning talk, we’ll walk through a beginner-friendly workflow using Python notebooks, designed for rapid experimentation and iteration. Attendees will learn how to:
•	Load datasets
•	Filter and transform data
•	Perform basic calculations
•	Export results
—	all using Pandas.

To help participants get started quickly, a GitHub repository will be shared containing a minimal working setup. This ready-to-clone configuration will enable attendees to dive into their own analyses with ease.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/FRJJ7D/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/FRJJ7D/feedback/</feedback_url>
            </event><event guid="8cdf2601-aa0e-5fb7-9b6c-68ba3845f049" id="3800">
                <room>WG403</room>
                <title>High-resolution, large-scale inundation mapping with basic python libraries</title>
                <subtitle />
                <type>Lightning Talk</type>
                <date>2025-11-21T09:30:00+13:00</date>
                <start>09:30</start>
                <duration>00:05</duration>
                <abstract>Designing a flood mapping workflow using a few standard python libraries to efficiently process high resolution data at-scale.</abstract>
                <slug>foss4g-2025-3800-high-resolution-large-scale-inundation-mapping-with-basic-python-libraries</slug>
                <track>Lightning talk</track>
                
                    
                <language>en</language>
                <description>Bathtub modelling is a simple approach to flood mapping, whereby inundation depths are computed via differencing a water height layer with the terrain elevation. However, bathtub models often overestimate flood depth and extent, as they fail to resolve underlying processes including hydraulic connectivity, attenuation, fluid flow direction, and structural barriers. Following concepts outlined in Kasmalkar et al. (2024), a bathtub inundation model was developed which accounts for hydraulic connectivity and path-based attenuation to improve the accuracy of flood mapping. The model was applied for multiple inundation scenarios over a large extent (approx. 15,000 sq. km) at high spatial resolution (4 m) which presented numerous challenges in terms of compute capacity, runtime, and generation of useable output data and maps. To address these challenges, the workflow was implemented in Python and involved segmenting the study area into smaller regions, relying on only numpy/cupy (for GPU) in the processing script, and using gdal for reading/writing raster and vector inputs/outputs. This talk will briefly outline the goals of this project, the hurdles encountered along the way, and the solutions designed to overcome them.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/KTYGVZ/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/KTYGVZ/feedback/</feedback_url>
            </event></room><room name="WG308 TE IRINGA" guid="0c4358b2-4659-574d-85fb-50c3b323caf3"><event guid="935e8dbb-24ff-5348-aaf0-09c82a1ad88a" id="4314">
                <room>WG308 TE IRINGA</room>
                <title>Lonboard: Fast, interactive geospatial vector data visualization in Python</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>00:25</duration>
                <abstract>Interactive visualization is often a precursor to extracting meaningful insights from data. Lonboard provides 30-40x faster performance for visualizing geospatial vector data than other Python libraries, supporting millions of coordinates.</abstract>
                <slug>foss4g-2025-4314-lonboard-fast-interactive-geospatial-vector-data-visualization-in-python</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                
                    
                <language>en</language>
                <description>Visualization, especially interactive visualization, is often the initial step in extracting meaningful insights from data. But it's too hard to quickly and interactively visualize large geospatial vector data in Python.

[Ipyleaflet](https://ipyleaflet.readthedocs.io/en/latest/) and [Folium](https://python-visualization.github.io/folium/latest/) are great for small datasets, but their performance quickly suffers as data sizes grow into the tens of thousands. [Pydeck](https://deckgl.readthedocs.io/en/latest/) supports slightly larger datasets, but it, too, struggles with data sizes above 100,000 coordinates.

This presentation introduces [Lonboard](https://developmentseed.org/lonboard/latest/), a cutting-edge open-source Python library designed to address this challenge by enabling fast, interactive geospatial vector data visualization within Jupyter notebooks.

Lonboard's performance stems from its innovative architecture, built on four key technologies: [deck.gl](https://deck.gl/) for GPU-accelerated rendering, [GeoArrow](https://geoarrow.org/) for efficient in-memory representation, [GeoParquet](https://geoparquet.org/) for optimized data transfer to the browser, and [anywidget](https://anywidget.dev/) for easy Jupyter integration.

On a dataset with 3 million points, Ipyleaflet crashed after 3.5 minutes, Pydeck crashed after 2.5 minutes, but Lonboard successfully rendered in 2.5 **seconds**.

This talk will give a brief overview of the internal innovations that make Lonboard so fast, then detail how to make the most of Lonboard's high-level APIs for visualizing large data.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/UY3SBT/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/UY3SBT/feedback/</feedback_url>
            </event><event guid="6404c4bf-9780-5abd-aac4-a8ce2a6be570" id="4339">
                <room>WG308 TE IRINGA</room>
                <title>Faster, simpler access to cloud-based geospatial data with Obstore</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T12:00:00+13:00</date>
                <start>12:00</start>
                <duration>00:25</duration>
                <abstract>Obstore is the simplest, highest-throughput Python interface to Amazon S3, Google Cloud Storage, Azure Storage. This talk will explain what Obstore is, how it differs from existing Python libraries for cloud data access, and how it's being used to speed up cloud-based geospatial workflows.</abstract>
                <slug>foss4g-2025-4339-faster-simpler-access-to-cloud-based-geospatial-data-with-obstore</slug>
                <track>Cloud, APIs &amp; Data Infrastructure</track>
                
                    
                <language>en</language>
                <description>[Obstore](https://developmentseed.org/obstore/latest/) is a Python library that abstracts how to access data on commercial cloud storage providers, like Amazon S3, Google Cloud Storage, and Azure Storage. Instead of writing code for each provider and manually creating the abstractions, use Obstore’s singular API for data access.

While at its face Obstore is similar to [fsspec](https://github.com/fsspec/filesystem_spec)—they both provide abstracted interfaces to cloud storage—Obstore presents some core improvements:

- Minimal API with native synchronous and asynchronous support.  
- Fast with no Python dependencies: obstore wraps the Rust `object_store` library, meaning that your Python environment stays small and you won’t face dependency conflicts.  
- Streaming downloads, uploads, and listings without manual pagination.  
- Full type hinting for easier use in Python IDE environments.  
- Simple access to [NASA Earthdata](https://developmentseed.org/obstore/latest/api/auth/earthdata/) and [Microsoft Planetary Computer](https://developmentseed.org/obstore/latest/api/auth/planetary-computer/) data collections with **automatic credential refreshing** when short-lived tokens expire.

While Obstore is a foundational technology that can be used across many domains, this talk will focus on its use in geospatial-related projects: 

- [Zarr](https://zarr.dev/)\-Python introduced an [Obstore-based backend](https://zarr.readthedocs.io/en/latest/api/zarr/storage/index.html#zarr.storage.ObjectStore) that can be [3x faster than the default fsspec-based backend](https://github.com/maxrjones/zarr-obstore-performance) when reading Zarr datasets. 
- [VirtualiZarr](https://github.com/zarr-developers/VirtualiZarr), a library to present non-cloud-native file formats like netCDF as virtual Zarr datasets, is being rewritten to use Obstore by default. 
- [Async-tiff](https://github.com/developmentseed/async-tiff), a fast, asynchronous, Python TIFF, GeoTIFF, and Cloud-Optimized GeoTIFF reader, uses Obstore under the hood to power its data fetching. 
- A [new Python GeoParquet library](https://geoarrow.org/geoarrow-rs/python/latest/api/io/geoparquet/) uses Obstore as well.

This talk will explain what Obstore is, how it differs from existing Python libraries, and how you might use it in your own projects to speed up your own data access.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/BUHEKS/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/BUHEKS/feedback/</feedback_url>
            </event></room><room name="WG126" guid="c289d89e-6932-5361-96fd-a6c53160dcd4"><event guid="3f04f72a-f9c4-526d-9a22-909f5d572e6d" id="4362">
                <room>WG126</room>
                <title>Open Source GIS for Placemaking Education: A Gamified Framework for Community-Driven Urban Learning</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>00:25</duration>
                <abstract>This study presents a gamified educational framework for teaching urbanism using open-source tools. Combining MLIT PLATEAU (GIS data), Blender (3D modeling), and Godot (simulation), it enables youth to engage in participatory placemaking while learning spatial thinking and urban design through an accessible, interactive digital workflow.</abstract>
                <slug>foss4g-2025-4362-open-source-gis-for-placemaking-education-a-gamified-framework-for-community-driven-urban-learning</slug>
                <track>Community, Collaboration &amp; Impact</track>
                
                    
                <language>en</language>
                <description>This study introduces an innovative educational framework that integrates open-source tools, geospatial data, and gamification to enhance the teaching of urban planning and placemaking. It is specifically designed to provide young learners (particularly in schools, youth workshops, and community-based learning environments) with accessible, engaging, and interactive experiences that foster spatial awareness, critical thinking, and civic participation. At the core of this framework lies a reproducible pipeline built upon three key open-source technologies: MLIT PLATEAU (open GIS-based 3D city models), Blender (a 3D modeling and animation suite), and Godot (a lightweight, scriptable game engine). Together, these tools form a complete workflow for visualizing, simulating, and manipulating urban space through an open and participatory lens.

The methodology begins with the use of MLIT PLATEAU, Japan’s national-scale open 3D geospatial dataset, which offers highly detailed, standardized models of buildings, land use, infrastructure, and terrain for dozens of cities across the country. These datasets are downloaded in CityGML or OBJ formats and imported into Blender using specialized plugins or preprocessing tools. Within Blender, the imported GIS data is converted into structured, editable 3D models that students can navigate, analyze, and reconfigure. For example, students can model new public spaces, add greenery, modify building forms, or simulate small-scale tactical interventions. Blender's node-based material system and animation capabilities also allow learners to visualize temporal changes (e.g., seasonal effects, traffic flows, construction stages), adding dynamic and narrative dimensions to their urban design proposals.

Blender also plays a central role as a translation layer between GIS and game engines. After modeling and scene setup, assets are exported (typically in glTF, FBX, or OBJ formats) and imported into Godot, where the interactive and gamified layer of the project is developed. In Godot, students can script interactions using GDScript (a Python-like language), design basic user interfaces (UI), and simulate real-world phenomena such as walkability, visibility, accessibility, or user behavior. Example applications include:
- creating a first-person walkthrough of a redesigned neighborhood;
- developing a resource-management game to balance green space and development;
- building a collaborative multiplayer simulation for participatory urban decision-making.

The integration of real-time simulation through Godot allows learners to test design hypotheses, observe user reactions, or iterate urban scenarios based on feedback—all within an immersive, game-like environment. Importantly, this approach does not require high-end hardware or costly licenses, making it ideal for public schools, NGOs, or municipalities with limited digital infrastructure. The entire pipeline (GIS data (PLATEAU), 3D modeling (Blender), and simulation (Godot)) is fully open-source, ensuring transparency, reproducibility, and adaptability to local contexts worldwide.

Pedagogically, the framework introduces a novel mapping between Levels of Detail (LOD) (a concept borrowed from GIS and 3D modeling) and stages of learning progression. For instance, LOD1 models are used for early-stage orientation and spatial comprehension; LOD2/3 data facilitates site-specific design challenges; and custom-built LOD4 scenarios simulate detailed human-scale interventions. This layered approach supports differentiated instruction and curriculum design, enabling instructors to tailor activities to students’ cognitive levels and technical skills.

The framework is situated within a broader discourse of open-source urbanism, tactical urbanism, and participatory design. It challenges traditional, expert-driven planning paradigms by promoting tools that empower communities and students to engage directly with spatial planning through iterative, visual, and interactive means. It also contributes to the field of geospatial education, offering a replicable methodology for integrating spatial literacy, creative modeling, and digital civic engagement into urban curricula. Ultimately, this study argues that a gamified, open-source approach to urban learning not only makes complex planning tools accessible but also nurtures the next generation of urban thinkers capable of co-creating inclusive and resilient urban futures.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/G8NLJY/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/G8NLJY/feedback/</feedback_url>
            </event><event guid="c9003a19-79db-5ae6-aedf-e56bf206ceba" id="3716">
                <room>WG126</room>
                <title>pygeoapi project status</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T11:00:00+13:00</date>
                <start>11:00</start>
                <duration>00:25</duration>
                <abstract>pygeoapi project status presentation.  Come and find out the latest news on the project as well as future plans, and how to get involved!</abstract>
                <slug>foss4g-2025-3716-pygeoapi-project-status</slug>
                <track>State of software</track>
                <logo>/media/foss4g-2025/submissions/RZN87X/pygeoapi-logo-notrans_gmkOWmp.png</logo>
                    
                <language>en</language>
                <description>pygeoapi is an OGC API Reference Implementation. Implemented in Python, pygeoapi supports numerous OGC APIs via a core agnostic API, different web frameworks (Flask, Starlette, Django) and a fully integrated OpenAPI capability. Lightweight, easy to deploy and cloud-ready, pygeoapi's architecture facilitates publishing datasets and processes from multiple sources. The project also provides an extensible plugin framework, enabling developers to implement custom data adapters, filters and processes to meet their specific requirements and workflows. pygeoapi also supports the STAC specification in support of static data publishing.

pygeoapi has a significant install base around the world, with numerous projects in academia, government and industry deployments. The project is also an OGC API Reference Implementation, lowering the barrier to publishing geospatial data for all users.

This presentation will provide an update on the current status, latest developments in the project, including new core features and plugins. In addition, the presentation will highlight key projects using pygeoapi for geospatial data discovery, access and visualization.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/RZN87X/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/RZN87X/feedback/</feedback_url>
            </event></room><room name="WA220" guid="132a9daa-6bcb-593a-b15d-b8ddf7d1d99c"><event guid="d6882388-30c9-5d28-a1c7-9ab5af4c03f5" id="4382">
                <room>WA220</room>
                <title>OGC CITE Runner - a new runner for checking compliance with OGC standards</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>00:25</duration>
                <abstract>ogc-cite-runner is a software to help automate OGC CITE compliance testing for geospatial web applications. It can be used either as a GitHub action or standalone. Learn more about it at https://osgeo.github.io/ogc-cite-runner/</abstract>
                <slug>foss4g-2025-4382-ogc-cite-runner-a-new-runner-for-checking-compliance-with-ogc-standards</slug>
                <track>AI, Data Science &amp; Analytics</track>
                
                    
                <language>en</language>
                <description>The Open Geospatial Consortium API family of standards (OGC API) are being developed to make it easy for anyone to provide geospatial data to the web, and are the next generation of geospatial web API standards designed with resource-oriented architecture, RESTful principles and OpenAPI. In addition, the OGC has the CITE compliance program, which aims at providing test suites that can be used to verify if web applications implement the standards correctly.

Official CITE testing is done using OGC infrastructure and is subject to a review process which can be time consuming for implementations seeking compliance certification. The official CITE test suites and test runner (OGC TeamEngine) are made available by the OGC so that implementations are able to test their compliance beforehand. This process is however not very straightforward to automate.

The ogc-cite-runner software aims at reducing the friction of running the mentioned official CITE test suites with the OGC TeamEngine test runner. It is a lightweight Python CLI application which can be used directly as a GitHub action or as a standalone tool, thus making it easy to include in Continuous Integration systems. This means that implementations become able to test their CITE compliance as part of their normal development workflow and gain near instant feedback on their compliance status.

At its core, ogc-cite-runner implements a thin layer of  automation over OGC TeamEngine, asking it to run CITE test suites. It then processes the output results into a number of output formats, including a Markdown report which is embedded directly in the GitHub actions workflow log.

This presentation will provide an overview of ogc-cite-runner, demonstrating how it can be run both as a GitHub action and in standalone in order to test OGC CITE compliance of geospatial web applications.

https://osgeo.github.io/ogc-cite-runner/</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/Z3ASEE/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/Z3ASEE/feedback/</feedback_url>
            </event><event guid="21bc8406-1934-55ef-a0d9-4817c7e7ddcc" id="4403">
                <room>WA220</room>
                <title>Introduction to the Discrete Global Grid Abstraction Library (DGGAL)</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T09:30:00+13:00</date>
                <start>09:30</start>
                <duration>00:25</duration>
                <abstract>An introduction and overview of the capabilities provided by the Free and Open Source Software Discrete Global Grid Abstraction Library (DGGAL) https://dggal.org https://github.com/ecere/dggal</abstract>
                <slug>foss4g-2025-4403-introduction-to-the-discrete-global-grid-abstraction-library-dggal-</slug>
                <track>AI, Data Science &amp; Analytics</track>
                <logo>/media/foss4g-2025/submissions/EP3CXX/dggal_Q2qbCRc.png</logo>
                    
                <language>en</language>
                <description>DGGAL provides a common interface to perform various operations on Discrete Global Grid Reference Systems (DGGRS), facilitating the implementation of Discrete Global Grid Systems (DGGS),
including implementing Web APIs based on the [OGC API - DGGS Standard](https://docs.ogc.org/DRAFTS/21-038r1.html).

DGGAL, including the `dgg` command-line utility as well as the Python bindings to the library, can be installed from [from PyPI](https://pypi.org/project/dggal/) with `pip install dggal`.

DGGAL currently supports all three DGGRS described in [OGC API - DGGS Annex B](https://docs.ogc.org/DRAFTS/21-038r1.html#annex-dggrs-def), as well as additional DGGRSs:

* [GNOSIS Global Grid](https://docs.ogc.org/DRAFTS/21-038r1.html#ggg-dggrs): An axis-aligned quad-tree defined in WGS84 latitude and longitude, with special handling of polar regions, corresponding to the [OGC 2D Tile Matrix Set of the same name](https://docs.ogc.org/is/17-083r4/17-083r4.html#toc58)
* [ISEA3H](https://docs.ogc.org/DRAFTS/21-038r1.html#isea3h-dggrs): An equal area hexagonal grid with a refinement ratio of 3 defined in the Icosahedral Snyder Equal Area (ISEA) projection
* [ISEA9R](https://docs.ogc.org/DRAFTS/21-038r1.html#isea9r-dggrs): An equal area rhombic grid with a refinement ratio of 9 defined in the ISEA projection transformed into a 5x6 Cartesian space resulting in axis-aligned square zones
* **IVEA3H**: An equal area hexagonal grid with a refinement ratio of 3 defined in the Icosahedral Vertex-oriented Great Circle Equal Area (tentatively called IVEA) projection based on [Slice &amp; Dice (2006)](https://www.tandfonline.com/doi/abs/10.1559/152304006779500687), using the same global indexing and sub-zone ordering as for ISEA3H
* **IVEA9R**: An equal area rhombic grid with a refinement ratio of 9 defined in the IVEA projection transformed into a 5x6 Cartesian space resulting in axis-aligned square zones, using the same global indexing and sub-zone ordering as for ISEA9R
* **RTEA3H**: An equal area hexagonal grid with a refinement ratio of 3 defined in the Rhombic Triacontahedron Equal Area (RTEA) projection, a configuration of the Slice &amp; Dice great circle projection equivalent to applying Snyder's projection to the RT, using the same global indexing and sub-zone ordering as for ISEA3H
* **RTEA9R**: An equal area rhombic grid with a refinement ratio of 9 defined in the RTEA projection transformed into a 5x6 Cartesian space resulting in axis-aligned square zones, using the same global indexing and sub-zone ordering as for ISEA9R
* [rHEALPix](https://iopscience.iop.org/article/10.1088/1755-1315/34/1/012012): An equal area and axis-aligned grid with square zones topology and a refinement ratio of 9 defined in the rHEALPix projection (using same parameters as default [PROJ implementation](https://proj.org/en/stable/operations/projections/rhealpix.html)) with the original hierarchical indexing and scanline-based sub-zone ordering

The API documentation can be [found here](https://dggal.org/docs/html/dggal/Classes/DGGRS.html).

The `DGGRS` class provides most of the functionality of the library, allowing to resolve DGGRS zones by textual ID to a unique 64-bit zone integer identifier (`DGGRSZone`).
The geometry and sub-zones of a particular zone can also be queried.
The concept of [sub-zones](https://docs.ogc.org/DRAFTS/21-038r1.html#term-sub-zone) is key to encoding both vector and raster geospatial data quantized to a DGGRS.
The DGGAL library also allows to resolve a sub-zone index at a particular depth from a parent zone, allowing to read DGGS-optimized data such as [DGGS-JSON](http://dggs-json.org) and [DGGS-JSON-FG](https://docs.ogc.org/DRAFTS/21-038r1.html#rc_data-dggs-jsonfg).

The recommended method to obtain and build DGGAL and the `dgg` tool is to follow the instructions in [BUILDING.md](BUILDING.md), or running [fetchAndBuild.sh](fetchAndBuild.sh) / [fetchAndBuild.bat](fetchAndBuild.bat).

While the library is written in the [eC programming language](https://ec-lang.org), object-oriented bindings for C, C++ and Python generated using the Ecere SDK's [`bgen` tool](https://github.com/ecere/bgen) are provided. Bindings for Rust are available as well. Support for additional languages may be added in the future.

Example usage of the bindings to the different programming languages can be [found here](https://github.com/ecere/dggal/tree/eC-core/bindings_examples).

The `dgg` tool provides the ability to perform various operations from the command-line, including generating grids at different refinement levels, querying information about a particular zone identifier, identifying the zone a particular set of geospatial coordinates, listing the zones within a certain bounding box, resolving sub-zone indices and converting compact DGGS-JSON data files to GeoJSON files.

_Acknowledgement_
Financial support provided by GeoConnections, a national collaborative initiative led by Natural Resources Canada. GeoConnections supports the modernization of the Canadian Geospatial Data Infrastructure (CGDI). The CGDI is the collection of geospatial data, standards, policies, applications, and governance that facilitate its access, use, integration, and preservation.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/EP3CXX/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/EP3CXX/feedback/</feedback_url>
            </event><event guid="96003707-a25f-5a36-b5ab-a8274c8966a8" id="3717">
                <room>WA220</room>
                <title>pycsw project status</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T10:00:00+13:00</date>
                <start>10:00</start>
                <duration>00:25</duration>
                <abstract>pycsw project status presentation.  Come and find out the latest news on the project as well as future plans, and how to get involved!</abstract>
                <slug>foss4g-2025-3717-pycsw-project-status</slug>
                <track>State of software</track>
                <logo>/media/foss4g-2025/submissions/SMBE7D/logo-vertical-darkbg_LuodIkZ.png</logo>
                    
                <language>en</language>
                <description>pycsw is an OGC API - Records and OGC CSW server implementation written in Python. Started in 2010 (more formally announced in 2011), pycsw allows for the publishing and discovery of geospatial metadata via numerous APIs (CSW 2/CSW 3, OpenSearch, OAI-PMH, SRU), providing a standards-based metadata and catalogue component of spatial data infrastructures. pycsw is Open Source, released under an MIT license, and runs on all major platforms (Windows, Linux, Mac OS X).The project is certified OGC Compliant, and is an OGC Reference Implementation.

The project currently powers numerous high profile catalogues such as EOEPCA, IOOS, NGDS, NOAA, US Department of State, US Department of Interior, geodata.gov.gr, Met Norway and WMO WOUDC. This session starts with a status report of the project, followed by an open question answer session to give a chance to users to interact with members of the pycsw project team. This session will cover how the project PSC operates, the current project roadmap, and recent enhancements focused on ESA's EOEPCA, Open Science Data Catalogue and OGC API - Records.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/SMBE7D/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/SMBE7D/feedback/</feedback_url>
            </event><event guid="a3186dce-bed6-5d65-a577-ca7aba87bd4e" id="4265">
                <room>WA220</room>
                <title>Automatic map deconstruction using QGIS: Leveraging open-source algorithms such as map2loop and LoopStructural</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T11:30:00+13:00</date>
                <start>11:30</start>
                <duration>00:25</duration>
                <abstract>We present an open-source workflow integrating QGIS, map2loop, and LoopStructural to automate geological map deconstruction into 3D models. This approach streamlines extracting contacts, faults, and stratigraphy from legacy maps, reducing manual effort and enhancing reproducibility. Results show rapid, scalable model generation, improving geological understanding in data-sparse regions.</abstract>
                <slug>foss4g-2025-4265-automatic-map-deconstruction-using-qgis-leveraging-open-source-algorithms-such-as-map2loop-and-loopstructural</slug>
                <track>Use Cases and Applications</track>
                
                    
                <language>en</language>
                <description>The integration of geological knowledge from legacy maps into modern 3D geological models remains a critical challenge, particularly in regions where data is sparse or inconsistently digitized. This research presents an automated workflow for geological map deconstruction using QGIS, and 3D modelling, using open-source algorithms such as map2loop and LoopStructural. The aim is to streamline the transformation of 2D geological maps into structured data, reducing manual interpretation efforts and increasing reproducibility.
The proposed methodology leverages map2loop to extract and pre-process from georeferenced maps geological features such as contacts, faults, and stratigraphic boundaries. Some of these features are then structured into formats compatible with the input of LoopStructural, a Python-based library that builds rule-based 3D geological models. The QGIS environment acts as a central platform for visualizing and manipulating these spatial data layers, while our plugins facilitate automation and interoperability between components.
This approach underscores the value of open-source tools in geoscience workflows, fostering collaboration, transparency, and scalability. By automating the extraction and structuring of geological features, the workflow significantly reduces the subjectivity and time requirements traditionally associated with map deconstruction and 3D model builds. 
The results demonstrate the feasibility of automated map deconstruction within an open-source GIS environment, paving the way for rapid geological model development in underexplored or poorly mapped regions. This work contributes to the broader geoscience community by offering a replicable and extensible framework for geological data translation, bridging the gap between static to support systems for integration with additional data sources such as drillholes and geophysical data, enhancing the fidelity of downstream 3D models.
Repository:
Loop3D
•	https://github.com/Loop3D/map2loop
•	https://github.com/Loop3D/LoopStructural
•	https://github.com/Loop3D/qgis-loopplugin
Field Mapping Tool
•	https://github.com/swaxi/GEOL-QMAPS
•	https://zenodo.org/records/13374088

Geophysical Processing Tool
•	https://github.com/swaxi/SGTool 
Tomofast-x Grav/Mag Inversion Tool
•	https://github.com/TOMOFAST/Tomofast-x
•	https://github.com/TOMOFAST/tomofast-x-q</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/7LQM3T/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/7LQM3T/feedback/</feedback_url>
            </event><event guid="73659461-fdbe-597b-9db1-becb4658ed68" id="4277">
                <room>WA220</room>
                <title>Mapping Landcover in Arid Regions of Saudi Arabia</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T16:00:00+13:00</date>
                <start>16:00</start>
                <duration>00:25</duration>
                <abstract>We mapped landcover in the Saudi Arabia arid region using Sentinel-2, Spot imagery, which we processed using QGIS, Python, and Earth Engine. We developed machine learning classifiers and segmentation workflows. The workflow generated LCCS-compatible classifications with enhanced vegetation detection accuracy for applications like land management and environmental analysis</abstract>
                <slug>foss4g-2025-4277-mapping-landcover-in-arid-regions-of-saudi-arabia</slug>
                <track>Use Cases and Applications</track>
                
                    
                <language>en</language>
                <description>In this project, we developed a scalable, reproducible workflow to map land cover and vegetation density across the arid and semi-arid regions of Saudi Arabia. The approach integrates open and commercial satellite imagery, machine learning classification, and segmentation techniques to generate Land Cover Classification System (LCCS)-compatible outputs suitable for national-scale monitoring and management.
The workflow produced two core outputs for each region:
A detailed land cover map distinguishing major classes, including barren land, sand, settlements, and vegetation.
A vegetation density map classifying areas into low, medium, and high vegetation cover.
These outputs are already supporting:
Biomass estimation to quantify vegetative carbon stocks and inform climate-related reporting.
Grazing capacity analysis to guide sustainable rangeland management.
Identification of priority zones for vegetation conservation.
Together, these products provide a baseline dataset to inform environmental management, policy development, and research across Saudi Arabia’s diverse landscapes.
To account for ecological diversity, Saudi Arabia was divided into 168 primary tiles, each approximately 1° × 1° in size, grouped into broad environmental zones, including forest areas, arid flats, sandy regions, and hilly terrain. Within each tile, smaller working grids (~12 km × 9 km) were defined to enable systematic processing and data management.
Imagery sources included Sentinel-2 (providing 10 m multispectral data) and Maxar high-resolution imagery. Sentinel-2 scenes were selected to maximize vegetation contrast and minimize cloud cover by choosing the most appropriate season. Data were exported from Google Earth Engine in 32-bit format and converted to 8-bit to facilitate efficient analysis.
Contrast Limited Adaptive Histogram Equalization (CLAHE) was applied to improve visual contrast and feature discrimination, particularly in areas with subtle spectral differences between vegetation and bare ground. This method was selected because each tile covers a vast area with significant variation in surface brightness and reflectance. Applying a global contrast adjustment, such as histogram stretching, can distort some regions by over-enhancing bright areas while compressing darker zones. CLAHE, on the other hand, employs a localized kernel-based approach to adjust contrast within smaller sections independently, ensuring consistent enhancement across different landforms and preserving detail without introducing artifacts in uniform sandy or rocky areas.
For each tile, multi-resolution segmentation was performed using eCognition software on the enhanced Sentinel-2 imagery. Multi-resolution segmentation grouped adjacent pixels into homogeneous objects based on their spectral and spatial characteristics.
Using very high-resolution imagery from SPOT, the normalized difference vegetation index (NDVI) was computed from the Red and near-infrared bands to detect active vegetation. Given the variability of NDVI values across regions, single thresholding was unsuitable for consistent detection. To address this, NDVI was squared and cubed to create enhanced layers that amplified vegetation signals while maintaining relative differences among land cover types.
While NDVI effectively highlights vegetation, it can also produce artificially high values over bright sand dunes due to strong near-infrared reflectance. This artifact is common in arid landscapes where sparse vegetation mixes with reflective substrates. To mitigate these effects, the areas were identified manually after vectorization and excluded by removing false-positive polygons from the vegetation layer.
An unsupervised classification approach was selected for its ability to operate effectively across large, heterogeneous datasets without requiring uniform training samples. Input features included the four spectral bands (Red, Green, Blue, NIR) along with the enhanced NDVI layers. Classification was performed using K-Means clustering, with the optimal number of classes automatically determined based on the spectral characteristics of each working grid.
After classification, approximately 100 sample points were generated within each class to calculate mean NDVI values. The class with the highest mean NDVI was identified as the vegetation class and vectorized for further processing.
This process produced two geospatial layers for each working grid:
A vegetation density map depicting crown cover areas.
An LCCS-compatible land cover classification.
Within each segment, the proportion of vegetation cover was computed to re-code classes accurately and ensure consistent representation of vegetation density within the broader land cover context.
To build seamless national coverage, tile-level outputs were mosaicked. Grid boundaries were processed by extracting edge segments, reclassifying them, and merging adjacent classes to maintain consistent classification across neighboring tiles.
High-density vegetation zones were analyzed to assess grazing patterns and temporal dynamics of vegetation. For each highly vegetation-dense segment, the NDVI was computed monthly using time-series Sentinel-2 imagery. By tracking the variability and seasonal trends in NDVI across multiple years, the workflow identified areas with gradual vegetation decline, which may indicate overgrazing or other ecological pressures such as drought or soil disturbance. Recognizing these vulnerable zones is critical in arid environments, where any loss of vegetation can have significant long-term impacts on soil stability, biodiversity, and ecosystem resilience.
The resulting datasets are being used to develop biomass estimation models to quantify carbon stocks across ecosystems. Grazing capacity assessments leverage temporal NDVI trends to prioritize areas for sustainable management and intervention. Land cover data also supports the identification of priority areas for conservation and vegetation protection.
By combining freely available satellite data, machine learning classifiers, and reproducible open workflows, this project demonstrates a practical approach to national-scale mapping in challenging arid environments. The methods and outputs can be adapted and reused by organizations working in similar landscapes.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/7KPPRN/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/7KPPRN/feedback/</feedback_url>
            </event></room><room name="WG404" guid="8fad97e8-15a2-5185-b1c8-68e7d8beb155"><event guid="63d404ab-9a5f-5a76-b056-a0372726ca18" id="4372">
                <room>WG404</room>
                <title>i.hyper: processing hyperspectral imagery in GRASS</title>
                <subtitle />
                <type>Academic Paper</type>
                <date>2025-11-21T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>00:25</duration>
                <abstract>We present i.hyper, a multimodular toolset for processing hyperspectral satellite imagery in GRASS. It supports the import of EnMAP, PRISMA, and Hyperion products through a dedicated import module and provides preprocessing, spectral visualization, and export. The toolset facilitates open, reproducible workflows for hyperspectral analysis in geospatial research.</abstract>
                <slug>foss4g-2025-4372-i-hyper-processing-hyperspectral-imagery-in-grass</slug>
                <track>Academic</track>
                
                    
                <language>en</language>
                <description>Introduction

Hyperspectral remote sensing provides rich spectral information that enables advanced analysis across a wide range of environmental domains. Soil monitoring, biogeochemistry, vegetation dynamics, environmental conservation, and even marine and coastal remote sensing all benefit from the high spectral resolution of modern sensors.

Although missions like EnMAP (DLR) and PRISMA (ASI) are already operational, and Hyperion (NASA EO-1) has left behind a valuable legacy archive, new missions such as ESA’s FLEX and CHIME, along with superspectral systems like Landsat Next, are expected to dramatically expand the volume and variety of data. However, integrating this data into open-source spatial workflows remains limited.
To bridge this gap, we introduce i.hyper, a modular suite for GRASS that enables complete and reproducible hyperspectral workflows - from import and preprocessing to spectral analysis and export.

Overview of i.hyper

i.hyper is implemented in Python and tightly integrated into the GRASS environment, combining modern spectral processing with robust spatial analysis tools. The suite consists of four core modules:
    1. i.hyper.import – Import of hyperspectral satellite products
    2. i.hyper.preproc – Spectral preprocessing and transformation
    3. i.hyper.explore – Interactive spectral analysis, plot and export
    4. i.hyper.export – Conversion to external formats
These modules form a pipeline that allows users to ingest, process, and explore hyperspectral cubes using GRASS-native structures.

Module 1: i.hyper.import

Supports:
    • EnMAP L2A
    • PRISMA L2B–L2D
    • Hyperion
It imports imagery and metadata into GRASS 3D rasters (raster_3d), including per-band attributes (wavelength, FWHM, units, validity). Invalid bands are flagged. Optional composite layers (e.g., CIR, SWIR) can be created automatically for visualization. Modular backends make extension to new missions straightforward.

Module 2: i.hyper.preproc

Provides in-place spectral processing with support for:
    • Savitzky-Golay filtering (configurable order, derivative, window size, NaN handling)
    • Principal Component Analysis (PCA)
    • Kernel PCA (RBF kernel)
    • Continuum removal using local convex hull normalization
These transformations are applied spectrally per pixel, using garray.array3d, with preserved metadata throughout.

Module 3: i.hyper.explore

Enables:
    • Interactive selection of pixels/regions
    • Visualization of spectral signatures
    • Export of spectra to CSV
Valid bands are respected during exploration. This module is particularly useful for validating preprocessing steps and preparing datasets for modeling.

Module 4: i.hyper.export

Supports export to:
    • GeoTIFF
    • HDF5 (for machine learning or scientific workflows)
Users can select bands, apply scaling, and retain metadata during export.

Reproducibility and Integration

i.hyper is fully open-source (GNU GPL 2.0), integrated via the GRASS Python API, and leverages libraries like NumPy, scikit-learn and SciPy. It ensures reproducibility through:
    • Strict input structure and metadata validation
    • Logging of transformation parameters
    • Use of GRASS-native tools
All modules include documentation, CLI help, argument checks, and consistent metadata propagation.

Conclusion

i.hyper is novel in its integration of hyperspectral processing within GRASS, unlocking a powerful combination of spectral and spatial tools. It facilitates comprehensive workflows - from raw data to analysis-ready layers - and supports both legacy and current missions with a roadmap for future expansion.
As more hyperspectral and superspectral missions emerge - including FLEX, CHIME, and Landsat Next - the need for transparent, extensible, and scriptable pipelines becomes critical. i.hyper addresses this need by offering a scalable, modular, and reproducible solution that enables open-source geospatial research across domains such as soil science, geochemistry, vegetation monitoring, conservation planning, and marine ecosystems.
The GRASS + Python + i.hyper stack provides a robust foundation for both research and operational remote sensing.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/LCVGKG/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/LCVGKG/feedback/</feedback_url>
            </event><event guid="1d555f55-018f-57c9-aaec-cdb331e6ab60" id="4361">
                <room>WG404</room>
                <title>Benchmarking OGC Services: No More Surprises When 20 Students Try to Access Your WMS</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T09:30:00+13:00</date>
                <start>09:30</start>
                <duration>00:25</duration>
                <abstract>Most OGC services are deployed with only functional testing, leading to failures under real user loads. Our open-source Locust framework generates realistic geospatial requests with random bounding boxes and dynamic coordinates for proper load testing</abstract>
                <slug>foss4g-2025-4361-benchmarking-ogc-services-no-more-surprises-when-20-students-try-to-access-your-wms</slug>
                <track>Cloud, APIs &amp; Data Infrastructure</track>
                
                    
                <language>en</language>
                <description>Picture this, Sicily 2025 : You've carefully set up a WMS service, tested it works perfectly, and then invited 20 students to use it while lecturing about OGC web services. Suddenly, your service crawls to a halt. This scenario highlights a critical gap in how we deploy OGC services - most organizations perform only functional testing, checking if services work but not how they perform under real user loads.
Standard load testing tools like ApacheBench fall short for geospatial services because they are not oriented to deal with  the geographic complexity of OGC protocols. Real users don't request the same map tile repeatedly - they pan across different bounding boxes, zoom through multiple levels, switch layer combinations, and explore various geographic extents. OGC services require specialized testing that can generate random bounding boxes for WMS GetMap requests, create dynamic tile coordinates for WMTS across zoom levels, handle proper CRS and format parameters, and simulate realistic navigation patterns.
We developed an open-source solution using the Locust framework enhanced with OGC-specific Python code modules. Our approach leverages owslib.wms for proper capabilities parsing and request generation, custom bbox generators that create random geographic extents within service bounds, tile coordinate algorithms for realistic requests, and layer randomization from GetCapabilities responses. The framework includes seed-based reproducible testing for consistent benchmark comparisons and simulates authentic user behavior patterns including concurrent users requesting different geographic areas and dynamic layer combinations during sessions.
This work addresses a fundamental challenge in the FOSS4G ecosystem where standard load testing misses the geographic complexity that makes OGC services unique. By providing specialized benchmarking tools that understand geospatial request patterns, organizations can systematically validate performance before deployment and avoid discovering critical issues when users start actually exploring their services geographically. The toolkit enables proactive capacity planning and helps prevent those embarrassing moments when carefully planned demonstrations fail under realistic load conditions.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/89FTBG/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/89FTBG/feedback/</feedback_url>
            </event><event guid="ff493f81-7e10-5ee8-8511-815839b855a2" id="3789">
                <room>WG404</room>
                <title>Building a simple geospatial web app</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T10:00:00+13:00</date>
                <start>10:00</start>
                <duration>00:25</duration>
                <abstract>I'll show you how i built a simple web application that uses open source tools and open data to estimate NZ census counts within custom geographies.
I'll share some tips and tricks i learned along the way and hopefully inspire you to make your own geo web app.</abstract>
                <slug>foss4g-2025-3789-building-a-simple-geospatial-web-app</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                
                    
                <language>en</language>
                <description>I'll show you how i built a relatively simple web application that uses open source tools (Python, Dash, Leaflet, GeoParquet) and open data (NZ Census 2023, NZ building outlines) to estimate NZ census counts within custom geographies.
I'll share some tips and tricks i learned along the way and hopefully inspire you to make your own geo web app.</description>
                <recording>
                    <license />
                    <optout>true</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/8MQLNE/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/8MQLNE/feedback/</feedback_url>
            </event><event guid="9ae5e700-81e9-5527-b067-db007ec8ec89" id="4530">
                <room>WG404</room>
                <title>EOPF Zarr Access</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T15:30:00+13:00</date>
                <start>15:30</start>
                <duration>00:25</duration>
                <abstract>EOPF Toolkit - Python and R Zarr access</abstract>
                <slug>foss4g-2025-4530-eopf-zarr-access</slug>
                <track>Cloud, APIs &amp; Data Infrastructure</track>
                <logo>/media/foss4g-2025/submissions/K77CMD/EOPF_FINAL_LOGO_I7GD3R7.png</logo>
                    
                <language>en</language>
                <description>This talk will describe the EOPF Zarr format and present practical notebooks in Python and R for accessing ESA Sentinel data. It will touch on practical use cases for Sentinel 1, 2, and 3 Zarr data access and help users understand the EOPF ecosystem and the tooling best suited to access and exploit EOPF data.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/K77CMD/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/K77CMD/feedback/</feedback_url>
            </event></room><room name="WG607" guid="4ae40321-2591-5002-82a8-0b12958015bc"><event guid="121424d3-58da-5ea7-977f-f486e6557c90" id="4209">
                <room>WG607</room>
                <title>Development of an Information Extraction System for Mobile LiDAR Survey Data using Free and Open Source Technologies</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T09:00:00+13:00</date>
                <start>09:00</start>
                <duration>00:25</duration>
                <abstract>A 3D WebGIS Information Extraction System was developed to retrieve and visualize Mobile LiDAR Survey data by integrating spatial objects with attribute information. The system aims to have a user-friendly interface and uses free and open-source technologies including Python, HTML5, JavaScript, PostGIS, GeoServer. Leaflet and Cesium.</abstract>
                <slug>foss4g-2025-4209-development-of-an-information-extraction-system-for-mobile-lidar-survey-data-using-free-and-open-source-technologies</slug>
                <track>Use Cases and Applications</track>
                
                    
                <language>en</language>
                <description>1.	Introduction
•	Rapid advancements in technology have improved 3D visualization techniques in Geographic Information Systems (GIS). Open-source solutions can provide the functionality to store and visualise large quantities of GIS data in multiple formats.
•	This paper describes a WebGIS application for visualizing Mobile LiDAR GIS data across various Levels of Detail (LoDs).
2. Mobile LiDAR Survey Technologies
•	Mobile LiDAR is a powerful geospatial technology for surveying complex urban landscapes including roads and roadside infrastructure. It provides accurate 3D data by integrating laser scanning, Inertial Measurement Unit (IMU), and Global Navigation Satellite System (GNSS) technologies to produce detailed point clouds. 
•	A schematic truck-mounted Mobile LiDAR system is shown in Figure 1.

 
Figure 1: Mobile LiDAR system (adapted from [1]

•	Mobile LiDAR Surveys (MLS) capture X, Y, Z coordinates of laser light reflecting objects along with attributes like intensity and RGB colour values. They also capture imagery that can be fused with the point cloud data to improve object classification.
•	To identify and semantically classify objects such as roadside features requires the use of Artificial Intelligence (AI) software which incorporates complex algorithms which may be tailored to particular types of object. Thus detecting the road surface may require a different algorithm from pole-like objects such as street signs.
•	This study is based on survey data captured by Cyvl  LiDAR and 360-degree optical imagery sensors mounted on vehicles which travel on targeted roads at the same speed as other traffic. The Cyvl system also contains a suite of Artificial Intelligence (AI) software which is mainly used for road inventory recording, defect detection and road safety management. Road defects include road pavement damage and potholes. Road safety issues include street trees encroaching on the road, damaged signage, lighting fixtures and lane markings and driving obstacles on the road. The road infrastructure data relevant to this project are indicated in the diagram below.
 
Figure 2: Taxonomy of elements for road surface and object identification (adapted from [2]). Features relevant to this project are indicated by orange boxes.
•	An additional type of infrastructure feature not shown above is powerlines, both roadside and cross country. The Cyvl system is capable of identifying powerlines as well as vegetation which may impinge on them, and the project may involve development of specialized algorithms to assess the risk and advise on vegetation removal action.
•	The Cyvl system identifies and semantically classifies roadside features captured by its optical imagery sensors and matches these to the point cloud data collected by its LiDAR sensor. This is a more accurate method of identifying roadside features than methods which rely on LiDAR data alone, see eg [3].
 
3.	Information Extraction System Overview
•	Roadside objects are segmented using the Cyvl2 AI system. Each object is stored in an ontological format, reducing data size for efficient querying. The integration process results in a semantically rich master geo-database for various roadside feature classes.
•	The system will allow for various types of queries, including spatial location-based, attribute-based, and aggregate queries.
•	This approach will be an ensemble of (i) data pre-processing (ii) data integration, schema development and 3D database development and (iii) data extraction and an integrated visualisation module.
•	A system architecture diagram (simplified) is depicted in Figure 3 below.
  
Figure 3: Systems Architecture
•	The system architecture is entirely open-source, comprising multiple python packages including Geopandas, TensorFlow, Keras, PyTorch and NumPy, together with well-established database package PostgreSQL/PostGIS and server GeoServer. GeoServer is able to serve both geospatial data and HTML (www) files. The front end is under development, with software packages for 2D and 3D data including Leaflet and OL3-Cesium under evaluation.

4.	Preliminary results: a Leaflet-based map viewer
•	Data provided by the Cyvl system was processed using 16 GeoTIFF tiles into a GeoServer Image Mosaic. These geotiff files were transformed from .LAS point cloud files collected by Civiltech.
• 	Configured Web Map Service (WMS) rendering using coordinate reference system EPSG:28355
• 	The map added grayscale SLD for better visibility
•	See screenshot produced by Ryan Watson Consulting is shown below as Figure 4.
 
Figure 4: Screenshot of 16 geotiff files as a mosaic
5.	Conclusions
•	The developed WebGIS application effectively extracts and visualizes Mobile LiDAR survey data. This will help road authorities make more effective use of the data being collected.
•	Future research will focus on optimizing querying algorithms and expanding the application scope of the proposed framework.
•	Future applications may include complex queries such as assigning Pavement Condition Index (PCI) and road safety ratings such as the International Road Assessment Program (iRAP) star rating.

6.	References
1.	National Cooperative Highway Research Program (US) (2025). Chapter 9: Typical components of mobile LIDAR systems. In: Transportation Research Board of the National Academies (ed.) Guidelines for the Use of Mobile LIDAR in Transportation Applications.  Available from: https://learnmobilelidar.com/guidance-document/chapter-9-background/.
2.	Luo, Z., Gao, L., Xiang, H. and Li, J. (2023). Road object detection for HD map: Full-element survey, analysis and perspectives. ISPRS Journal of Photogrammetry and Remote Sensing 197 2023/03/01/ 122-144 DOI: https://doi.org/10.1016/j.isprsjprs.2023.01.009
3.	Abro, G.-E. M., Zahid, F., Rajput, S., Azhar Ali, S. S. and Aromoye, I. A. (2025). Challenges and Innovations in 3D Object Recognition: The Integration of LiDAR and Camera Sensors for Autonomous Applications. Transportation Research Procedia 84 2025/01/01/ 618-624 DOI: https://doi.org/10.1016/j.trpro.2025.03.116</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/3NRE7Q/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/3NRE7Q/feedback/</feedback_url>
            </event><event guid="186130dc-26da-548e-bdd8-58baf2e08174" id="4235">
                <room>WG607</room>
                <title>From a geo portal for air pollution assessment system to digital twin: a case study in Catania, Italy</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T11:00:00+13:00</date>
                <start>11:00</start>
                <duration>00:25</duration>
                <abstract>This work presents the Urban Intelligence Science Hub for City Network project, an innovative initiative aimed at advancing digital analysis of complex urban environments, where  an open-source digital infrastructure was designed for the acquisition, processing, and interactive visualization of environmental data, ensuring reusability and interoperability with scientific standards.</abstract>
                <slug>foss4g-2025-4235-from-a-geo-portal-for-air-pollution-assessment-system-to-digital-twin-a-case-study-in-catania-italy</slug>
                <track>Use Cases and Applications</track>
                
                    
                <language>en</language>
                <description>The UISH (Urban Intelligence Science Hub for City Network) project is an initiative that aims to develop an innovative concept of digital analysis of complex urban realities, using the city of Catania (Sicily) as a pilot testing ground.
Through the project, an innovative approach to the digital analysis of complex urban centers is being developed to facilitate decision support for spatial planning and management activities. Simulators based on artificial intelligence models are being built for each physical subsystem of the city, the interaction of which forms a digital twin of the city. UISH is funded by the Department for Cohesion Policies of the Prime Minister's Office and is part of a strategic CNR (Italian National Research Council) project called Urban Intelligence (UI). This aspires to develop and propose an urban intelligence model for all Italian urban areas, from metropolitan areas to small municipalities. CNR's UI model is characterized by a modular Urban Digital Twin (UDT) architecture, which aims to analyze the city in its various subsystems (such as mobility, the built environment, water, green spaces, etc.), matching each of them with a thematic Digital Twin characterized by scientific approaches, numerical models, services and functionalities, as well as specific data sources and software modules interfaced with each other through special Application Programming Interfaces (APIs), which allow dialogue and data exchange between models, and then integrated with systems based on AI techniques.
This UDT architecture makes it possible to apply the CNR's UI model to the different needs of Italian urban areas regardless of their level of “smartness readiness”, as it can be flexibly and adaptively integrated into the heterogeneous context of existing digital tools and expertise, be they sensor networks, databases, smart services, hardware and software infrastructures, etc., and the entities called upon to use and manage them.
The core part of the module, is based on the use of the Air-Portal platform (https://business.esa.int/projects/air-portal). an urban air quality dashboard developed by S[&amp;]T (https://air-portal.nl/?lang=en). Air-Portal employs a customized local air quality model that integrates remote sensing data, land use information, and local monitoring to generate urban-scale air quality data with a spatial resolution of 100x100 meters and a temporal resolution of 1 hour.
The main input data sources used by the Air-Portal system are:
-	Atmospheric modeling data obtained from CAMS (Copernicus Atmosphere Monitoring Service)
-	Land use data provided by CLMS (Copernicus Land Monitoring Service)
-	In situ air quality measurements obtained directly from local environmental protection agencies or through the European Environment Agency (EEA)
-	Meteorological data from various sources, such as the Copernicus Climate Data Store, ECMWF, or national and regional meteorological services
The Institute of Applied Physics "Nello Carrara" of the National Research Council (CNR-IFAC) and S[&amp;]T are cooperating on the development of the AirPortal-Catania module, following operational procedures already tested and consolidated through the H2020 AURORA project (Advanced Ultraviolet Radiation and Ozone Retrieval for Applications). Within that project, the AirPortal4Florence application was implemented and tested for high-resolution forecasting and analysis of ozone concentrations in the city center of Florence.
IFAC is integrating its capabilities within UISH project with regard to the collection, analysis, processing and display of satellite geographic data that are used to produce concentration maps of air pollutants in the vicinity of the city. This covers a 10 x 30 km analysis area, extending in a south-north direction from the airport serving the city of Catania to the summit of the volcano Etna.
As such, an open-source digital infrastructure for the acquisition, processing and interactive visualization of environmental data related to air pollution was designed and implemented. The work was developed with a view to reusability, openness of code and interoperability with scientific standards and GIS tools.
The goal is to provide a replicable and flexible tool for air quality monitoring and analysis by integrating heterogeneous data sources (NetCDF formats, time series, gridded models) into a coherent and accessible platform, supported by a map publishing system based on open technologies such as Postgres/PostGIS, QGIS-Server, Lizmap, and G3W-Suite.
To ensure scalability, security, and performance, the system was structured on two separate servers: a database server and a web/file server, to optimize computational resources, logically isolate critical components, and ensure easy extension to future scenarios (e.g., multi-site deployment, database clustering).
At the core of the system is an automatic environmental data import and conversion flow, capable of processing daily multi-variable NetCDF files (NO₂, SO2, PM₂.₅, PM10, O₃ concentrations) and transforming them into georeferenced time series.
The pipeline is developed in Python and involves reading and interpreting NetCDF content, generation of a regular grid of cells for the area of interest, extraction and spatial aggregation of variables on a daily basis, normalized storage in optimized PostGIS tables, logging and error handling.
In addition to point values, the system is able to derive isohypses by interpolating the data for better spatial visualization, which are also saved to databases as vector geometries.
To enable interactive access to the data, two mapping environments have been configured and customized: Lizmap Web Client (which allows dynamic layer browsing, temporal selection, filter application and thematic visualization of air quality indicators) and G3W-Suite, with advanced features such as thematic dashboards, editing modules and integrated workflows.
Both environments are designed to operate with publishable QGIS Desktop projects with ease thanks to a consistent repository and permission structure.
In addition, the project integrates the implementation of open-source software for easy uploading of QGIS projects to Lizmap. This tool allows users to quickly publish maps and layers with predefined configurations, reducing WebGIS content update times and minimizing errors in content synchronization.
The software will be made available under a free license, along with the entire set of provisioning scripts, data import routines and server configurations, making the entire infrastructure replicable to other spatial contexts.
All components developed will be publicly accessible through open-source channels, including the server installation and configuration scripts, the Python tools for importing and processing NetCDF files, the database templates and ready-to-use QGIS project examples and the user manual and detailed technical documentation.
This choice is in line with the principles of transparency, replicability, and interoperability that guide scientific research in the environmental field, and allows other research groups, local governments, or environmental protection agencies to adopt, customize, and extend what has been implemented.
The infrastructure is currently operational and will be kept up to date, with a view to stimulating international collaboration in the field of open environmental data and contributing to greater public awareness of pollution and health issues.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/EGK7QM/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/EGK7QM/feedback/</feedback_url>
            </event><event guid="b4068ee5-8dba-5912-80d4-3ee8291c3a84" id="4406">
                <room>WG607</room>
                <title>Introduction to libCartoSym, libCQL2 and libDE9IM</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T14:00:00+13:00</date>
                <start>14:00</start>
                <duration>00:25</duration>
                <abstract>An introduction and overview of the capabilities provided by the Free and Open Source Software libCartoSym and related dependency libraries (libCQL2, libDE9IM...) implementing the candidate [OGC Cartographic Symbology 2.0 Standard](https://github.com/opengeospatial/cartographic-symbology). http://cartosym.org/ https://github.com/ecere/libCartoSym</abstract>
                <slug>foss4g-2025-4406-introduction-to-libcartosym-libcql2-and-libde9im</slug>
                <track>Tools, Libraries &amp; Visualisation</track>
                <logo>/media/foss4g-2025/submissions/GMWCAD/CartoSymNE2_JLS1GFg.png</logo>
                    
                <language>en</language>
                <description>libCartoSym is a Free and Open-Source Software library implementing [OGC Cartographic Symbology 2.0](https://github.com/opengeospatial/cartographic-symbology)

_libCartoSym_ aims to be an implementation of the [CartoSym-CSS](https://docs.ogc.org/DRAFTS/18-067r4.html#rc-cscss) and
[CartoSym-JSON](https://docs.ogc.org/DRAFTS/18-067r4.html#rc-json) encodings defined in the candidate
[OGC Cartographic Symbology - Part 1: Core Model and Encodings Standard version 2.0](https://docs.ogc.org/DRAFTS/18-067r4.html) Standard.

The library allows to read and write these CartoSym encodings, as well as import from and export to additional encodings of portrayal rules such as OGC [SLD](https://portal.ogc.org/files/?artifact_id=22364)/[SE](https://portal.ogc.org/files/?artifact_id=16700) and [Mapbox GL Styles](https://docs.mapbox.com/mapbox-gl-js/guides/styles/).

Since the CartoSym encodings extend the [OGC Common Query Language (CQL2)](https://www.opengis.net/doc/IS/cql2/1.0), the library also relies on a related open-source libCQL2 library providing support for
parsing and writing CQL2-Text and CQL2-JSON expressions, as well as run-time evaluation of CQL2 expressions. Support for performing spatial relation queries based on the
[Dimensionally Extended-9 Intersection Model](https://en.wikipedia.org/wiki/DE-9IM) is also
integrated within a jointly developed _libDE9IM_ open-source library, and support for OGC Simple Features as well as parsing and writing geometries defined in
[Well-Known Text (WKT)](http://portal.opengeospatial.org/files/?artifact_id=25355) and [GeoJSON](https://tools.ietf.org/rfc/rfc7946.txt) is also provided by related open-source libraries.

While these libraries are written in the [eC programming language](https://ec-lang.org), object-oriented bindings for _libCartoSym_ automatically generated using Ecere's [binding generating tool (bgen)](https://github.com/ecere/bgen) will also be made available for the C, C++ and Python programming languages, with additional support planned for Java and Rust.

_Acknowledgement_
Financial support provided by GeoConnections, a national collaborative initiative led by Natural Resources Canada. GeoConnections supports the modernization of the Canadian Geospatial Data Infrastructure (CGDI). The CGDI is the collection of geospatial data, standards, policies, applications, and governance that facilitate its access, use, integration, and preservation.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/GMWCAD/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/GMWCAD/feedback/</feedback_url>
            </event><event guid="181afb80-a7f2-5567-af7b-e417a4fda67c" id="4341">
                <room>WG607</room>
                <title>Virtually flooded: representing flood model predictions in virtual reality for improved public engagement and understanding of risk</title>
                <subtitle />
                <type>Academic Paper</type>
                <date>2025-11-21T14:30:00+13:00</date>
                <start>14:30</start>
                <duration>00:25</duration>
                <abstract>Improved communication of flood risk via advanced visualisation could lead to increased and wider understanding of flooding, and therefore improved planning decisions and reduced impacts. We demonstrate the representation of modelled flood scenarios within virtual reality, and an open-source Python package for ingesting flood model outputs into Unreal Engine.</abstract>
                <slug>foss4g-2025-4341-virtually-flooded-representing-flood-model-predictions-in-virtual-reality-for-improved-public-engagement-and-understanding-of-risk</slug>
                <track>Academic</track>
                <logo>/media/foss4g-2025/submissions/DXRKGK/VR_Flood_Street_j8cbSZu.png</logo>
                    
                <language>en</language>
                <description>Flood inundation is a frequent, widespread, and impactful hazard, with flood risk expected to increase in future because of climate change through increased storminess coupled with rapid growth of urban areas. To manage flood risk efficiently and effectively, communication of risk assessments for multiple scenarios needs to be targeted with the right method for the right audience. However, there is limited information on what works best, why, and for whom. Flood risks may be poorly understood by the public, with flood events taking communities by surprise even if risk assessments have been completed. Further, traditional two-dimensional risk maps are limited by interpretability challenges, especially given the inherent complexity of the data shown (e.g., depth of flooding at a given annual exceedance probability), and differences cartographic decisions taken (e.g., spatial scale and what additional features to show). Three-dimensional visualisations and immersive Virtual Reality (VR) technology have been found to be more intuitive due to their increased realism, with several studies finding that is a useful tool for improved community preparedness.

Our research is developing advanced automated visualizations of flood risk scenarios using VR, with the aim to improve awareness of flood risk. Our work is building on our open-source flood risk assessment system, Flood Resilience Digital Twin (FReDT), which brings together computational models of flood inundation with other data for hazard assessment, management, and mitigation. A key objective of FReDT is to enable the automation of flood risk assessments, so that multiple scenarios can be assessed rapidly. Here, our focus is on the development of open-source software components which enable dynamic flood model predictions of water levels during a flood event to be ingested into virtual representations of modelled areas in Unreal Engine software from Epic Games. Thus, predictions for different scenarios (from any hydraulic model) are visualised in an immersive way in VR or with 360-degree video representations. 

We have created a processing pipeline that takes a modelled flood scenario from FReDT (currently, produced using the BG-Flood hydraulic engine) and adds a representation of that flooding into an Unreal Engine virtual environment (level) of the same area. The level landscape must be produced from the same LiDAR terrain dataset as used to generate the flood model, so that the Unreal Engine representation aligns vertically with the flood model. Further development will include workflows to produce an Unreal Engine level purely from environmental data but currently we require this to be created independently.

Our pipeline takes the flood scenario depth data, consisting of a time-series of geospatial raster layers in NetCDF format, and creates water “sources” within Unreal Engine that match the depth over time of the raster. These water sources use Fluid Flux 3.0 for realistic fluid simulations at runtime and is required to represent the dynamics of the flood event, including the interaction of flow with objects in the VR environment. Fluid Flux is a proprietary Unreal Engine plugin from Imaginary Blend that solves the full shallow water equations in real-time at a high spatial resolution.   Water source placement currently decided by developers choosing points within the flood scenario extent and saving these to a geospatial vector file, although in a future version this process will also be automated. 
Our processing pipeline begins by using Python with open-source libraries such as XArray and GeoPandas to extract the depths over time for each of the given points and collate these data into a CSV file containing each point. This CSV file allows the second processing stage to occur within Unreal Editor, without requiring libraries to be installed into the Unreal Editor Python environment.

In Unreal Editor, a second Python script is called, which reads the CSV and creates water source actors at each given location. Unreal Float Curves are used to represent the depth of the water source over time. These water sources use Fluid Flux 3.0 simulation modifiers  with a custom Blueprint plugin implementation to provide time-series depth modification. These water sources are set to add water to the domain while the immediate surrounding area does not match their water level and remove water where the surrounding area exceeds it. The more sources of water added the more closely the Unreal Engine simulation aligns with the outputs of the flood model scenario. Using Fluid Flux 3.0, simulation states can be pre-run to allow switching between stages of flooding that could be hours apart in real time. This allows us to demonstrate multiple different stages of flooding, from a normal day through to peak inundation.

Depending on how the simulations are setup, the topography of the VR domain can be substantially more detailed than the source flood model, and domain is likely to be a smaller spatial subset. The higher resolution allows the representation of flows around and between features such as buildings, although it should be noted that currently only the mass of water is included from source flood model simulations, not momentum, meaning that any highly dynamic flow (e.g., hydraulic jumps and super critical flow) is generated internally within Unreal Engine by Fluid Flux 3.0. Usual practice is to run flood model scenarios at around 5-10 m spatial resolution for river reaches of several kilometres, while the VR domain can be around 1 m spatial resolution for domains of approx. 2 x 2 km, depending on available video memory. Our current hardware comprises a NVidia RTX 4090 GPU with 24 Gb memory, of which around 8 Gb is used during simulations.

A key outcome of our research is the methods and software which build on FReDT to create advanced flood visualizations, using virtual reality (VR) technology. We are currently engaging with communities regarding these visualisations to assess their effectiveness in communicating flood risk, including multiple scenarios of different levels of flood likelihood. The software developed enables users to switch between scenarios from within the VR system, with appropriate flood levels pulled into the system in real time. Ultimately, our aim is to enable users to interact with the environment, for example to make changes which are aimed at flood mitigation (e.g., natural flood solutions such as wetland restoration). These mitigation scenarios will be assessed using FReDT, enabling dynamic updating of the visualisations through automated ingestion of model outputs. More broadly, our work demonstrates how the outputs of advanced numerical models can be used directly within VR to create intuitive visualisations, with widespread potential applications such as in the communication of the potential impacts of climate change.

GitHub repository: 
https://github.com/GeospatialResearch/UnrealFloodingScripts</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/DXRKGK/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/DXRKGK/feedback/</feedback_url>
            </event><event guid="82d5ad0f-67bf-5db7-af83-c61020a828c5" id="4414">
                <room>WG607</room>
                <title>Moving from Science to Product: Making Open GIS Software Work for Us</title>
                <subtitle />
                <type>Talk</type>
                <date>2025-11-21T16:00:00+13:00</date>
                <start>16:00</start>
                <duration>00:25</duration>
                <abstract>How CTrees wrote open source code to complete zonal stats in order to move from science results (rasters) to interactive mapping apps.</abstract>
                <slug>foss4g-2025-4414-moving-from-science-to-product-making-open-gis-software-work-for-us</slug>
                <track>Community, Collaboration &amp; Impact</track>
                
                    
                <language>en</language>
                <description>At CTrees, we create machine learning models that integrate multiple data sources to produce high-resolution, time-series datasets on forest carbon and activity. Our outputs— estimates of carbon stocks, emissions, removals, and forest change—support projects ranging from jurisdictional analysis to deforestation monitoring. However, scaling from research-driven models to a production-ready platform required solving two major challenges: (1) data management and (2) enabling fast, on-demand analysis that serves both internal researchers and external users.
On the data side, we faced the all-too-familiar chaos of manual and ad-hoc dataset versioning (v2, v2_final, v2_final_final), inconsistent folder structures where important data dates could be recorded in the filename or prefix, and directories packed with thousands of tiny GeoTIFFs meant to be mosaicked together. Instead of chasing S3 paths and manually stitching ad-hoc datacube files, we adopted Icechunk (open source) by Earthmover, which leverages Zarr and Icechunk to enable structured, versioned cloud-native datacube access. As a bonus, Arraylake (built on icechunk) makes it easy to view the data via a WMS service, streamlining the visualization process for both internal users and external stakeholders.
The second challenge was transitioning from scientific scripts—often written in R or relying on heavy GDAL operations—to a streamlined, scalable approach. To enable on-demand analysis, we refactored these scripts into ctreeskit (https://github.com/ctrees-products/ctreeskit) , our open-source Python package that consolidates spatial processing and zonal statistics using Xarray. Currently in its pre-release (beta) version, ctreeskit is designed for flexibility, it supports researchers across CTrees in papers, reports, and internal workflows. More broadly, ctreeskit is available to anyone performing similar calculations. By standardizing these processes and ensuring transparency, we enhance reproducibility, efficiency, and accessibility, allowing both internal teams and external users to understand and verify our methodologies.
With ctreeskit and Arraylake, we now have three key things: (1) a reliable way to save and access data, (2) a simple way to slice and dice high-dimensional data in the time and spatial domains, and (3) lightweight tools for running reproducible calculations. Moving from raw GeoTIFFs to an array-based Zarr model gives us fast, structured access to data without the complexity of STAC catalogs. By shifting from scattered S3 paths and heavyweight processing to a structured, cloud-native approach, we’ve made geospatial data easier to access, faster to analyze, and more transparent—helping both internal teams and external users work more efficiently.</description>
                <recording>
                    <license />
                    <optout>false</optout>
                </recording>
                <links />
                <attachments />

                <url>https://talks.osgeo.org/foss4g-2025/talk/VDJC93/</url>
                <feedback_url>https://talks.osgeo.org/foss4g-2025/talk/VDJC93/feedback/</feedback_url>
            </event></room></day></schedule>